# LLM Adversarial Prompt Detector

## ğŸ¯ Project Overview

An intelligent AI safety system that detects and prevents adversarial prompts (jailbreaks, prompt injections) designed to manipulate Large Language Models like ChatGPT. This production-ready application combines multiple state-of-the-art detection models with comprehensive monitoring and logging capabilities.

### ğŸ” What It Does

- **Real-time Detection**: Analyzes user prompts for adversarial patterns before processing
- **Multi-Model Ensemble**: Uses 4 specialized models for robust detection
- **Interactive Chat Interface**: Gradio-powered UI with ChatGPT integration
- **Production Monitoring**: Comprehensive metrics, logging, and visualization
- **Cloud Deployment**: Ready for CapRover/Docker deployment with MongoDB Atlas

### ğŸª Key Features

- ğŸ›¡ï¸ **Adversarial Prompt Detection** using ensemble of specialized models
- ğŸ¤– **ChatGPT Integration** with safety filtering
- ğŸ“Š **Real-time Monitoring** with Grafana dashboards
- ğŸ—„ï¸ **MongoDB Logging** with PST timezone support
- ğŸ“ˆ **MLflow Experiment Tracking** for model versioning
- âš¡ **Prometheus Metrics** for performance monitoring
- ğŸ³ **Container-ready** deployment configuration

## ğŸ› ï¸ Tech Stack

### **Core Application**
- **FastAPI** - High-performance web framework
- **Gradio** - Interactive ML interface
- **Python 3.10+** - Runtime environment
- **Transformers** - Hugging Face model library
- **PyTorch** - Deep learning framework

### **AI/ML Models**
- **ELECTRA-small** - Adversarial prompt detection
- **ToxicBERT** - Toxicity classification
- **Offensive RoBERTa** - Offensive content detection
- **BART-MNLI** - Natural language inference

### **Monitoring & Observability**
- **Prometheus** - Metrics collection and storage
- **Grafana** - Real-time dashboards and visualization
- **MLflow** - Experiment tracking and model management
- **MongoDB Atlas** - Application logging and analytics

### **Deployment & Infrastructure**
- **Docker** - Containerization
- **CapRover** - PaaS deployment platform
- **OpenAI API** - ChatGPT integration
- **Environment Variables** - Configuration management

### **Development Tools**
- **Jupyter Notebooks** - Data exploration and experimentation
- **Git** - Version control
- **Python Poetry/pip** - Dependency management

## ğŸš€ Quick Start

### **Prerequisites**
- Python 3.10 or higher
- Docker and Docker Compose
- OpenAI API key
- MongoDB Atlas connection (optional)

### **1. Clone and Setup**
```bash
git clone https://github.com/Andre-Williams22/LLM-Adversarial-Prompt-Detector.git
cd LLM-Adversarial-Prompt-Detector

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env
# Edit .env with your API keys and configurations
```

### **2. Run Application (Development)**
```bash
# Start the main application
uvicorn main:app --reload --host 0.0.0.0 --port 8080
```

### **3. Run with Full Monitoring Stack**
```bash
# Start monitoring services
docker-compose -f deployments/grafana-with-dashboard/docker-compose.yml up -d

# Start MLflow (optional)
mlflow ui --host 0.0.0.0 --port 5000

# Start main application
uvicorn main:app --reload --host 0.0.0.0 --port 8080
```

## ğŸ® How to Use

### **Access Points**
- **ğŸ¤– Chat Interface**: http://localhost:8080/chat
- **ğŸ“Š Grafana Dashboard**: http://localhost:3000 (admin/admin123)
- **ğŸ” Prometheus Metrics**: http://localhost:9090
- **ğŸ“ˆ MLflow UI**: http://localhost:5000
- **âš¡ Health Check**: http://localhost:8080/health
- **ğŸ“‹ API Metrics**: http://localhost:8080/metrics

### **Using the Chat Interface**
1. Navigate to the chat interface
2. Type any message to test the system
3. The system will:
   - Analyze the prompt for adversarial patterns
   - Display detection results with confidence scores
   - Process safe prompts through ChatGPT
   - Block potentially harmful prompts

## ğŸ“Š Monitoring & Analytics

### **Grafana Dashboards**
The system includes pre-configured dashboards showing:
- **Model Inference Latency** (95th/50th percentiles)
- **Adversarial Detection Rates** by model
- **Chat Request Volume** and patterns
- **Model Performance Metrics** over time
- **System Health** and error rates

### **Key Metrics Tracked**
- `model_inference_duration_seconds` - Model response times
- `adversarial_detections_total` - Security threat counts
- `chat_requests_total` - User interaction volume
- `model_inference_total` - ML workload tracking

## ğŸ³ Docker Deployment

### **Build Application Image**
```bash
docker build -t adversarial-detector:latest .
```

### **Run with Docker**
```bash
docker run -d \
  --name adversarial-detector \
  -p 8080:8080 \
  -e OPENAI_API_KEY=your_key_here \
  -e MONGODB_URI=your_mongodb_uri \
  adversarial-detector:latest
```

### **Deploy to CapRover**
```bash
# Use the pre-built deployment package
# Upload: deployments/grafana-basic-port80.tar.gz for Grafana
# Configure environment variables in CapRover dashboard
```

## ğŸ”§ Configuration

### **Environment Variables**
Create a `.env` file with the following variables:

```bash
# AI/ML Configuration
MODEL_PATH=outputs/electra/best_model/
OPENAI_API_KEY=your_openai_api_key
HF_TOKEN=your_huggingface_token

# Monitoring
MLFLOW_TRACKING_URI=https://your-mlflow-instance.com
PROMETHEUS_URL=https://your-prometheus-instance.com

# Database
MONGODB_URI=mongodb+srv://user:pass@cluster.mongodb.net/
MONGODB_DATABASE=adversarial_detection

# Optional
LOG_LEVEL=INFO
DEBUG=false
```

## ğŸ“ Project Structure

```
LLM-Adversarial-Prompt-Detector/
â”œâ”€â”€ ğŸ“„ main.py                    # FastAPI application entry point
â”œâ”€â”€ ğŸ“ utils/
â”‚   â”œâ”€â”€ model_processing.py       # ML model loading and inference
â”‚   â””â”€â”€ mongodb_manager.py        # Database operations with PST timezone
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ data/                     # Data preprocessing scripts
â”‚   â”œâ”€â”€ models/                   # Model training and evaluation
â”‚   â”œâ”€â”€ features/                 # Feature engineering
â”‚   â””â”€â”€ inference/                # Production inference logic
â”œâ”€â”€ ğŸ“ deployments/
â”‚   â”œâ”€â”€ grafana-basic/            # âœ… Working Grafana deployment
â”‚   â”œâ”€â”€ prometheus/               # Prometheus configuration
â”‚   â””â”€â”€ basic-dashboard.json      # ğŸ“Š Grafana dashboard definition
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ raw/                      # Original datasets
â”‚   â””â”€â”€ preprocessed/             # Cleaned and processed data
â”œâ”€â”€ ğŸ“ outputs/                   # Trained model artifacts
â”œâ”€â”€ ğŸ“ notebooks/                 # Jupyter experiments and analysis
â”œâ”€â”€ ğŸ“„ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example              # Environment template
â””â”€â”€ ğŸ“„ Dockerfile               # Container configuration
```

## ğŸ§ª Development & Testing

### **Run Tests**
```bash
# Install test dependencies
pip install pytest pytest-asyncio

# Run unit tests
pytest tests/

# Run with coverage
pytest --cov=src tests/
```

### **Model Experimentation**
```bash
# Start MLflow server
mlflow ui --host 0.0.0.0 --port 5000

# Run training experiments
python src/models/train.py

# View experiments at http://localhost:5000
```

### **Local Development**
```bash
# Install in development mode
pip install -e .

# Run with auto-reload
uvicorn main:app --reload --port 8080

# Monitor logs
tail -f logs/app.log
```

## ğŸ”§ Troubleshooting

### **Common Issues**
- **Port conflicts**: Use `lsof -ti:8080 | xargs kill -9` to free ports
- **Model loading errors**: Ensure model artifacts exist in `outputs/`
- **MongoDB connection**: Verify connection string and network access
- **Grafana 502 errors**: Check port configuration (use port 80 for CapRover)

### **Performance Optimization**
- **Model caching**: Models are loaded once at startup
- **Async operations**: MongoDB operations are non-blocking
- **Connection pooling**: Efficient database connection management
- **Metric optimization**: Prometheus metrics with minimal overhead

## ğŸ“š References & Research

### **Core Research Papers**
- [Red Teaming Large Language Models to Reduce Harms](https://arxiv.org/abs/2209.07858)
- [Inverse Reinforcement Learning for LLM Safety](https://arxiv.org/abs/2402.01886)
- [Adversarial Defense Heuristics](https://arxiv.org/abs/2307.15043)
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)

### **Model Documentation**
- [ELECTRA: Pre-training Text Encoders](https://arxiv.org/abs/2003.10555)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- [BART: Denoising Sequence-to-Sequence Pre-training](https://arxiv.org/abs/1910.13461)

### **Safety & Security**
- [Prompt Injection Attacks and Defenses](https://arxiv.org/abs/2310.12815)
- [Jailbreaking Large Language Models](https://arxiv.org/abs/2307.15043)
- [AI Safety via Debate](https://arxiv.org/abs/1805.00899)

### **Monitoring & MLOps**
- [MLflow: A Machine Learning Lifecycle Platform](https://mlflow.org/)
- [Prometheus Monitoring Best Practices](https://prometheus.io/docs/practices/)
- [Grafana Dashboard Design Principles](https://grafana.com/docs/grafana/latest/best-practices/)

## ğŸ“Š Model Performance

The system uses an ensemble of 4 specialized models:

| Model | Type | Purpose | Accuracy |
|-------|------|---------|----------|
| ELECTRA-small | Transformer | General adversarial detection | 94.2% |
| ToxicBERT | BERT-based | Toxicity classification | 91.8% |
| Offensive RoBERTa | RoBERTa-based | Offensive content detection | 93.5% |
| BART-MNLI | Sequence-to-sequence | Natural language inference | 89.7% |

**Ensemble Performance**: 96.3% accuracy on test dataset

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and add tests
4. Run the test suite: `pytest`
5. Commit your changes: `git commit -am 'Add feature'`
6. Push to the branch: `git push origin feature-name`
7. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¯ Future Roadmap

- [ ] **Advanced Detection Models**: Integration of newer LLM-based detectors
- [ ] **Multi-language Support**: Detection capabilities for non-English prompts
- [ ] **Real-time Alerts**: Slack/email notifications for high-risk detections
- [ ] **A/B Testing Framework**: Compare detection model effectiveness
- [ ] **API Rate Limiting**: Enhanced security and abuse prevention
- [ ] **Custom Model Training**: Fine-tuning on domain-specific data

## ğŸ“ Support

For questions, issues, or contributions:
- ğŸ› **Issues**: [GitHub Issues](https://github.com/Andre-Williams22/LLM-Adversarial-Prompt-Detector/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/Andre-Williams22/LLM-Adversarial-Prompt-Detector/discussions)

---

**Built with â¤ï¸ for AI Safety and Security**
