{
  "best_global_step": 65325,
  "best_metric": 0.9973593570608495,
  "best_model_checkpoint": "./roberta_output/checkpoint-65325",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 65325,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007654037504783774,
      "grad_norm": 3.2395949363708496,
      "learning_rate": 1.994907513713484e-05,
      "loss": 0.3221,
      "step": 500
    },
    {
      "epoch": 0.015308075009567547,
      "grad_norm": 0.014469440095126629,
      "learning_rate": 1.9898048220436282e-05,
      "loss": 0.0263,
      "step": 1000
    },
    {
      "epoch": 0.022962112514351322,
      "grad_norm": 0.006248740945011377,
      "learning_rate": 1.9847021303737723e-05,
      "loss": 0.0356,
      "step": 1500
    },
    {
      "epoch": 0.030616150019135095,
      "grad_norm": 0.015773477032780647,
      "learning_rate": 1.9795994387039164e-05,
      "loss": 0.0383,
      "step": 2000
    },
    {
      "epoch": 0.038270187523918864,
      "grad_norm": 0.017072847113013268,
      "learning_rate": 1.9744967470340605e-05,
      "loss": 0.0152,
      "step": 2500
    },
    {
      "epoch": 0.045924225028702644,
      "grad_norm": 0.03983291611075401,
      "learning_rate": 1.9693940553642046e-05,
      "loss": 0.0368,
      "step": 3000
    },
    {
      "epoch": 0.053578262533486416,
      "grad_norm": 0.015236363746225834,
      "learning_rate": 1.964291363694349e-05,
      "loss": 0.025,
      "step": 3500
    },
    {
      "epoch": 0.06123230003827019,
      "grad_norm": 0.010577350854873657,
      "learning_rate": 1.9591886720244932e-05,
      "loss": 0.0347,
      "step": 4000
    },
    {
      "epoch": 0.06888633754305395,
      "grad_norm": 0.031425055116415024,
      "learning_rate": 1.9540859803546373e-05,
      "loss": 0.0268,
      "step": 4500
    },
    {
      "epoch": 0.07654037504783773,
      "grad_norm": 0.042128462344408035,
      "learning_rate": 1.9489832886847814e-05,
      "loss": 0.0349,
      "step": 5000
    },
    {
      "epoch": 0.08419441255262151,
      "grad_norm": 0.04068923741579056,
      "learning_rate": 1.9438805970149255e-05,
      "loss": 0.0335,
      "step": 5500
    },
    {
      "epoch": 0.09184845005740529,
      "grad_norm": 0.01758774183690548,
      "learning_rate": 1.9387779053450696e-05,
      "loss": 0.0242,
      "step": 6000
    },
    {
      "epoch": 0.09950248756218906,
      "grad_norm": 0.001603434095159173,
      "learning_rate": 1.933675213675214e-05,
      "loss": 0.0239,
      "step": 6500
    },
    {
      "epoch": 0.10715652506697283,
      "grad_norm": 0.0024540319573134184,
      "learning_rate": 1.928572522005358e-05,
      "loss": 0.0268,
      "step": 7000
    },
    {
      "epoch": 0.1148105625717566,
      "grad_norm": 0.003460218431428075,
      "learning_rate": 1.9234698303355023e-05,
      "loss": 0.0154,
      "step": 7500
    },
    {
      "epoch": 0.12246460007654038,
      "grad_norm": 7.229053974151611,
      "learning_rate": 1.9183671386656464e-05,
      "loss": 0.0246,
      "step": 8000
    },
    {
      "epoch": 0.13011863758132414,
      "grad_norm": 0.008742266334593296,
      "learning_rate": 1.9132644469957905e-05,
      "loss": 0.0297,
      "step": 8500
    },
    {
      "epoch": 0.1377726750861079,
      "grad_norm": 0.016277043148875237,
      "learning_rate": 1.9081617553259346e-05,
      "loss": 0.0208,
      "step": 9000
    },
    {
      "epoch": 0.14542671259089168,
      "grad_norm": 0.013757850974798203,
      "learning_rate": 1.9030590636560787e-05,
      "loss": 0.0197,
      "step": 9500
    },
    {
      "epoch": 0.15308075009567546,
      "grad_norm": 0.048026442527770996,
      "learning_rate": 1.897956371986223e-05,
      "loss": 0.0348,
      "step": 10000
    },
    {
      "epoch": 0.16073478760045926,
      "grad_norm": 0.019974982365965843,
      "learning_rate": 1.892853680316367e-05,
      "loss": 0.0255,
      "step": 10500
    },
    {
      "epoch": 0.16838882510524303,
      "grad_norm": 0.003897551679983735,
      "learning_rate": 1.8877509886465114e-05,
      "loss": 0.0216,
      "step": 11000
    },
    {
      "epoch": 0.1760428626100268,
      "grad_norm": 0.0060216160491108894,
      "learning_rate": 1.8826482969766552e-05,
      "loss": 0.0227,
      "step": 11500
    },
    {
      "epoch": 0.18369690011481057,
      "grad_norm": 0.006329769734293222,
      "learning_rate": 1.8775456053067996e-05,
      "loss": 0.0262,
      "step": 12000
    },
    {
      "epoch": 0.19135093761959435,
      "grad_norm": 0.0236835815012455,
      "learning_rate": 1.8724429136369437e-05,
      "loss": 0.0243,
      "step": 12500
    },
    {
      "epoch": 0.19900497512437812,
      "grad_norm": 0.003404367947950959,
      "learning_rate": 1.867340221967088e-05,
      "loss": 0.0239,
      "step": 13000
    },
    {
      "epoch": 0.2066590126291619,
      "grad_norm": 0.019915513694286346,
      "learning_rate": 1.862237530297232e-05,
      "loss": 0.0219,
      "step": 13500
    },
    {
      "epoch": 0.21431305013394567,
      "grad_norm": 0.02339695207774639,
      "learning_rate": 1.857134838627376e-05,
      "loss": 0.0335,
      "step": 14000
    },
    {
      "epoch": 0.22196708763872944,
      "grad_norm": 0.01294382382184267,
      "learning_rate": 1.85203214695752e-05,
      "loss": 0.0179,
      "step": 14500
    },
    {
      "epoch": 0.2296211251435132,
      "grad_norm": 0.045531731098890305,
      "learning_rate": 1.8469294552876643e-05,
      "loss": 0.0255,
      "step": 15000
    },
    {
      "epoch": 0.23727516264829698,
      "grad_norm": 0.004110763780772686,
      "learning_rate": 1.8418267636178087e-05,
      "loss": 0.0109,
      "step": 15500
    },
    {
      "epoch": 0.24492920015308076,
      "grad_norm": 0.017721066251397133,
      "learning_rate": 1.8367240719479525e-05,
      "loss": 0.0184,
      "step": 16000
    },
    {
      "epoch": 0.2525832376578645,
      "grad_norm": 0.0064967782236635685,
      "learning_rate": 1.831621380278097e-05,
      "loss": 0.0233,
      "step": 16500
    },
    {
      "epoch": 0.2602372751626483,
      "grad_norm": 0.011525420472025871,
      "learning_rate": 1.826518688608241e-05,
      "loss": 0.0177,
      "step": 17000
    },
    {
      "epoch": 0.26789131266743205,
      "grad_norm": 0.0071679530665278435,
      "learning_rate": 1.821415996938385e-05,
      "loss": 0.0199,
      "step": 17500
    },
    {
      "epoch": 0.2755453501722158,
      "grad_norm": 0.014114764519035816,
      "learning_rate": 1.8163133052685293e-05,
      "loss": 0.021,
      "step": 18000
    },
    {
      "epoch": 0.2831993876769996,
      "grad_norm": 0.03036193922162056,
      "learning_rate": 1.8112106135986734e-05,
      "loss": 0.0253,
      "step": 18500
    },
    {
      "epoch": 0.29085342518178336,
      "grad_norm": 0.01967034488916397,
      "learning_rate": 1.8061079219288175e-05,
      "loss": 0.0153,
      "step": 19000
    },
    {
      "epoch": 0.29850746268656714,
      "grad_norm": 0.011762083508074284,
      "learning_rate": 1.801005230258962e-05,
      "loss": 0.0267,
      "step": 19500
    },
    {
      "epoch": 0.3061615001913509,
      "grad_norm": 0.08587345480918884,
      "learning_rate": 1.7959025385891057e-05,
      "loss": 0.0237,
      "step": 20000
    },
    {
      "epoch": 0.31381553769613474,
      "grad_norm": 0.02281419187784195,
      "learning_rate": 1.79079984691925e-05,
      "loss": 0.014,
      "step": 20500
    },
    {
      "epoch": 0.3214695752009185,
      "grad_norm": 0.010391081683337688,
      "learning_rate": 1.7856971552493943e-05,
      "loss": 0.0149,
      "step": 21000
    },
    {
      "epoch": 0.3291236127057023,
      "grad_norm": 0.014235551469027996,
      "learning_rate": 1.7805944635795384e-05,
      "loss": 0.0211,
      "step": 21500
    },
    {
      "epoch": 0.33677765021048606,
      "grad_norm": 0.2528054416179657,
      "learning_rate": 1.7754917719096825e-05,
      "loss": 0.0221,
      "step": 22000
    },
    {
      "epoch": 0.34443168771526983,
      "grad_norm": 0.01012225728482008,
      "learning_rate": 1.7703890802398266e-05,
      "loss": 0.0221,
      "step": 22500
    },
    {
      "epoch": 0.3520857252200536,
      "grad_norm": 0.10904286056756973,
      "learning_rate": 1.7652863885699707e-05,
      "loss": 0.0279,
      "step": 23000
    },
    {
      "epoch": 0.3597397627248374,
      "grad_norm": 0.01233335118740797,
      "learning_rate": 1.7601836969001148e-05,
      "loss": 0.0155,
      "step": 23500
    },
    {
      "epoch": 0.36739380022962115,
      "grad_norm": 0.015503888949751854,
      "learning_rate": 1.7550810052302593e-05,
      "loss": 0.0258,
      "step": 24000
    },
    {
      "epoch": 0.3750478377344049,
      "grad_norm": 0.015703734010457993,
      "learning_rate": 1.749978313560403e-05,
      "loss": 0.0174,
      "step": 24500
    },
    {
      "epoch": 0.3827018752391887,
      "grad_norm": 0.19040265679359436,
      "learning_rate": 1.7448756218905475e-05,
      "loss": 0.0227,
      "step": 25000
    },
    {
      "epoch": 0.39035591274397247,
      "grad_norm": 0.012883909046649933,
      "learning_rate": 1.7397729302206916e-05,
      "loss": 0.0212,
      "step": 25500
    },
    {
      "epoch": 0.39800995024875624,
      "grad_norm": 0.0007494608871638775,
      "learning_rate": 1.7346702385508357e-05,
      "loss": 0.0101,
      "step": 26000
    },
    {
      "epoch": 0.40566398775354,
      "grad_norm": 0.015614866279065609,
      "learning_rate": 1.7295675468809798e-05,
      "loss": 0.0123,
      "step": 26500
    },
    {
      "epoch": 0.4133180252583238,
      "grad_norm": 0.008249713107943535,
      "learning_rate": 1.724464855211124e-05,
      "loss": 0.017,
      "step": 27000
    },
    {
      "epoch": 0.42097206276310756,
      "grad_norm": 0.008806606754660606,
      "learning_rate": 1.719362163541268e-05,
      "loss": 0.0266,
      "step": 27500
    },
    {
      "epoch": 0.42862610026789133,
      "grad_norm": 0.002821953035891056,
      "learning_rate": 1.714259471871412e-05,
      "loss": 0.013,
      "step": 28000
    },
    {
      "epoch": 0.4362801377726751,
      "grad_norm": 0.033807989209890366,
      "learning_rate": 1.7091567802015566e-05,
      "loss": 0.0175,
      "step": 28500
    },
    {
      "epoch": 0.4439341752774589,
      "grad_norm": 0.012942887842655182,
      "learning_rate": 1.7040540885317003e-05,
      "loss": 0.0174,
      "step": 29000
    },
    {
      "epoch": 0.45158821278224265,
      "grad_norm": 0.42944949865341187,
      "learning_rate": 1.6989513968618448e-05,
      "loss": 0.0135,
      "step": 29500
    },
    {
      "epoch": 0.4592422502870264,
      "grad_norm": 0.007172738667577505,
      "learning_rate": 1.693848705191989e-05,
      "loss": 0.0217,
      "step": 30000
    },
    {
      "epoch": 0.4668962877918102,
      "grad_norm": 0.006976378615945578,
      "learning_rate": 1.688746013522133e-05,
      "loss": 0.0114,
      "step": 30500
    },
    {
      "epoch": 0.47455032529659397,
      "grad_norm": 0.028359705582261086,
      "learning_rate": 1.683643321852277e-05,
      "loss": 0.0217,
      "step": 31000
    },
    {
      "epoch": 0.48220436280137774,
      "grad_norm": 0.013388414867222309,
      "learning_rate": 1.6785406301824216e-05,
      "loss": 0.015,
      "step": 31500
    },
    {
      "epoch": 0.4898584003061615,
      "grad_norm": 0.012085537426173687,
      "learning_rate": 1.6734379385125653e-05,
      "loss": 0.0227,
      "step": 32000
    },
    {
      "epoch": 0.4975124378109453,
      "grad_norm": 0.02972428686916828,
      "learning_rate": 1.6683352468427098e-05,
      "loss": 0.0249,
      "step": 32500
    },
    {
      "epoch": 0.505166475315729,
      "grad_norm": 7.652212619781494,
      "learning_rate": 1.663232555172854e-05,
      "loss": 0.0175,
      "step": 33000
    },
    {
      "epoch": 0.5128205128205128,
      "grad_norm": 0.007570531219244003,
      "learning_rate": 1.658129863502998e-05,
      "loss": 0.02,
      "step": 33500
    },
    {
      "epoch": 0.5204745503252965,
      "grad_norm": 0.009036584757268429,
      "learning_rate": 1.653027171833142e-05,
      "loss": 0.0117,
      "step": 34000
    },
    {
      "epoch": 0.5281285878300803,
      "grad_norm": 0.013528013601899147,
      "learning_rate": 1.6479244801632862e-05,
      "loss": 0.0236,
      "step": 34500
    },
    {
      "epoch": 0.5357826253348641,
      "grad_norm": 0.03182655945420265,
      "learning_rate": 1.6428217884934303e-05,
      "loss": 0.017,
      "step": 35000
    },
    {
      "epoch": 0.5434366628396479,
      "grad_norm": 0.021973270922899246,
      "learning_rate": 1.6377190968235744e-05,
      "loss": 0.0162,
      "step": 35500
    },
    {
      "epoch": 0.5510907003444316,
      "grad_norm": 0.08238168805837631,
      "learning_rate": 1.632616405153719e-05,
      "loss": 0.0139,
      "step": 36000
    },
    {
      "epoch": 0.5587447378492154,
      "grad_norm": 0.00797649472951889,
      "learning_rate": 1.6275137134838627e-05,
      "loss": 0.0182,
      "step": 36500
    },
    {
      "epoch": 0.5663987753539992,
      "grad_norm": 0.012431775219738483,
      "learning_rate": 1.622411021814007e-05,
      "loss": 0.0176,
      "step": 37000
    },
    {
      "epoch": 0.574052812858783,
      "grad_norm": 13.869054794311523,
      "learning_rate": 1.6173083301441512e-05,
      "loss": 0.0276,
      "step": 37500
    },
    {
      "epoch": 0.5817068503635667,
      "grad_norm": 0.022639520466327667,
      "learning_rate": 1.6122056384742953e-05,
      "loss": 0.0149,
      "step": 38000
    },
    {
      "epoch": 0.5893608878683505,
      "grad_norm": 0.0008122010622173548,
      "learning_rate": 1.6071029468044394e-05,
      "loss": 0.0211,
      "step": 38500
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.010585172101855278,
      "learning_rate": 1.6020002551345835e-05,
      "loss": 0.0188,
      "step": 39000
    },
    {
      "epoch": 0.604668962877918,
      "grad_norm": 0.0024072814267128706,
      "learning_rate": 1.5968975634647277e-05,
      "loss": 0.0185,
      "step": 39500
    },
    {
      "epoch": 0.6123230003827018,
      "grad_norm": 0.0044095623306930065,
      "learning_rate": 1.5917948717948718e-05,
      "loss": 0.0097,
      "step": 40000
    },
    {
      "epoch": 0.6199770378874856,
      "grad_norm": 0.02492479234933853,
      "learning_rate": 1.5866921801250162e-05,
      "loss": 0.0177,
      "step": 40500
    },
    {
      "epoch": 0.6276310753922695,
      "grad_norm": 0.038104210048913956,
      "learning_rate": 1.58158948845516e-05,
      "loss": 0.0249,
      "step": 41000
    },
    {
      "epoch": 0.6352851128970533,
      "grad_norm": 0.03469916060566902,
      "learning_rate": 1.5764867967853044e-05,
      "loss": 0.0146,
      "step": 41500
    },
    {
      "epoch": 0.642939150401837,
      "grad_norm": 0.0063214427791535854,
      "learning_rate": 1.5713841051154485e-05,
      "loss": 0.0142,
      "step": 42000
    },
    {
      "epoch": 0.6505931879066208,
      "grad_norm": 0.0506884828209877,
      "learning_rate": 1.5662814134455927e-05,
      "loss": 0.0245,
      "step": 42500
    },
    {
      "epoch": 0.6582472254114046,
      "grad_norm": 0.007845889776945114,
      "learning_rate": 1.5611787217757368e-05,
      "loss": 0.0208,
      "step": 43000
    },
    {
      "epoch": 0.6659012629161883,
      "grad_norm": 0.01178748533129692,
      "learning_rate": 1.5560760301058812e-05,
      "loss": 0.0147,
      "step": 43500
    },
    {
      "epoch": 0.6735553004209721,
      "grad_norm": 0.0015787078300490975,
      "learning_rate": 1.550973338436025e-05,
      "loss": 0.0265,
      "step": 44000
    },
    {
      "epoch": 0.6812093379257559,
      "grad_norm": 0.04057019576430321,
      "learning_rate": 1.5458706467661694e-05,
      "loss": 0.0165,
      "step": 44500
    },
    {
      "epoch": 0.6888633754305397,
      "grad_norm": 0.006024716421961784,
      "learning_rate": 1.5407679550963135e-05,
      "loss": 0.0153,
      "step": 45000
    },
    {
      "epoch": 0.6965174129353234,
      "grad_norm": 0.0009105561766773462,
      "learning_rate": 1.5356652634264576e-05,
      "loss": 0.0183,
      "step": 45500
    },
    {
      "epoch": 0.7041714504401072,
      "grad_norm": 0.004054246004670858,
      "learning_rate": 1.5305625717566018e-05,
      "loss": 0.0136,
      "step": 46000
    },
    {
      "epoch": 0.711825487944891,
      "grad_norm": 0.00794429611414671,
      "learning_rate": 1.5254598800867459e-05,
      "loss": 0.011,
      "step": 46500
    },
    {
      "epoch": 0.7194795254496748,
      "grad_norm": 0.01418149285018444,
      "learning_rate": 1.52035718841689e-05,
      "loss": 0.0156,
      "step": 47000
    },
    {
      "epoch": 0.7271335629544585,
      "grad_norm": 0.0007489131530746818,
      "learning_rate": 1.5152544967470343e-05,
      "loss": 0.0196,
      "step": 47500
    },
    {
      "epoch": 0.7347876004592423,
      "grad_norm": 0.025610439479351044,
      "learning_rate": 1.5101518050771784e-05,
      "loss": 0.0167,
      "step": 48000
    },
    {
      "epoch": 0.7424416379640261,
      "grad_norm": 0.001481801737099886,
      "learning_rate": 1.5050491134073225e-05,
      "loss": 0.0255,
      "step": 48500
    },
    {
      "epoch": 0.7500956754688098,
      "grad_norm": 0.0106174536049366,
      "learning_rate": 1.4999464217374666e-05,
      "loss": 0.0174,
      "step": 49000
    },
    {
      "epoch": 0.7577497129735936,
      "grad_norm": 0.017857227474451065,
      "learning_rate": 1.4948437300676109e-05,
      "loss": 0.0098,
      "step": 49500
    },
    {
      "epoch": 0.7654037504783774,
      "grad_norm": 0.008759105578064919,
      "learning_rate": 1.4897410383977548e-05,
      "loss": 0.0183,
      "step": 50000
    },
    {
      "epoch": 0.7730577879831612,
      "grad_norm": 0.010550866834819317,
      "learning_rate": 1.484638346727899e-05,
      "loss": 0.013,
      "step": 50500
    },
    {
      "epoch": 0.7807118254879449,
      "grad_norm": 0.04928474873304367,
      "learning_rate": 1.4795356550580434e-05,
      "loss": 0.0199,
      "step": 51000
    },
    {
      "epoch": 0.7883658629927287,
      "grad_norm": 0.00809688214212656,
      "learning_rate": 1.4744329633881873e-05,
      "loss": 0.0179,
      "step": 51500
    },
    {
      "epoch": 0.7960199004975125,
      "grad_norm": 0.012599923647940159,
      "learning_rate": 1.4693302717183316e-05,
      "loss": 0.0165,
      "step": 52000
    },
    {
      "epoch": 0.8036739380022963,
      "grad_norm": 0.026071950793266296,
      "learning_rate": 1.4642275800484757e-05,
      "loss": 0.0148,
      "step": 52500
    },
    {
      "epoch": 0.81132797550708,
      "grad_norm": 0.02482771687209606,
      "learning_rate": 1.4591248883786198e-05,
      "loss": 0.0139,
      "step": 53000
    },
    {
      "epoch": 0.8189820130118638,
      "grad_norm": 0.015472248196601868,
      "learning_rate": 1.454022196708764e-05,
      "loss": 0.0169,
      "step": 53500
    },
    {
      "epoch": 0.8266360505166476,
      "grad_norm": 0.009918730705976486,
      "learning_rate": 1.4489195050389082e-05,
      "loss": 0.0136,
      "step": 54000
    },
    {
      "epoch": 0.8342900880214313,
      "grad_norm": 0.00666081765666604,
      "learning_rate": 1.4438168133690523e-05,
      "loss": 0.0226,
      "step": 54500
    },
    {
      "epoch": 0.8419441255262151,
      "grad_norm": 0.0008123498992063105,
      "learning_rate": 1.4387141216991964e-05,
      "loss": 0.0164,
      "step": 55000
    },
    {
      "epoch": 0.8495981630309989,
      "grad_norm": 0.01785323955118656,
      "learning_rate": 1.4336114300293407e-05,
      "loss": 0.0123,
      "step": 55500
    },
    {
      "epoch": 0.8572522005357827,
      "grad_norm": 0.0032733685802668333,
      "learning_rate": 1.4285087383594846e-05,
      "loss": 0.0185,
      "step": 56000
    },
    {
      "epoch": 0.8649062380405664,
      "grad_norm": 0.02396867796778679,
      "learning_rate": 1.4234060466896289e-05,
      "loss": 0.0159,
      "step": 56500
    },
    {
      "epoch": 0.8725602755453502,
      "grad_norm": 0.00957475882023573,
      "learning_rate": 1.4183033550197732e-05,
      "loss": 0.0127,
      "step": 57000
    },
    {
      "epoch": 0.880214313050134,
      "grad_norm": 0.002219774527475238,
      "learning_rate": 1.4132006633499171e-05,
      "loss": 0.013,
      "step": 57500
    },
    {
      "epoch": 0.8878683505549178,
      "grad_norm": 0.008410902693867683,
      "learning_rate": 1.4080979716800614e-05,
      "loss": 0.0264,
      "step": 58000
    },
    {
      "epoch": 0.8955223880597015,
      "grad_norm": 0.007397367153316736,
      "learning_rate": 1.4029952800102055e-05,
      "loss": 0.0117,
      "step": 58500
    },
    {
      "epoch": 0.9031764255644853,
      "grad_norm": 0.017827987670898438,
      "learning_rate": 1.3978925883403496e-05,
      "loss": 0.0182,
      "step": 59000
    },
    {
      "epoch": 0.9108304630692691,
      "grad_norm": 0.049954790621995926,
      "learning_rate": 1.3927898966704937e-05,
      "loss": 0.0105,
      "step": 59500
    },
    {
      "epoch": 0.9184845005740528,
      "grad_norm": 0.013370374217629433,
      "learning_rate": 1.387687205000638e-05,
      "loss": 0.0167,
      "step": 60000
    },
    {
      "epoch": 0.9261385380788366,
      "grad_norm": 0.03570948913693428,
      "learning_rate": 1.3825845133307821e-05,
      "loss": 0.0233,
      "step": 60500
    },
    {
      "epoch": 0.9337925755836204,
      "grad_norm": 0.004486114718019962,
      "learning_rate": 1.3774818216609262e-05,
      "loss": 0.0183,
      "step": 61000
    },
    {
      "epoch": 0.9414466130884042,
      "grad_norm": 0.00040499024908058345,
      "learning_rate": 1.3723791299910705e-05,
      "loss": 0.012,
      "step": 61500
    },
    {
      "epoch": 0.9491006505931879,
      "grad_norm": 0.04955380782485008,
      "learning_rate": 1.3672764383212144e-05,
      "loss": 0.0174,
      "step": 62000
    },
    {
      "epoch": 0.9567546880979717,
      "grad_norm": 0.010551507584750652,
      "learning_rate": 1.3621737466513587e-05,
      "loss": 0.0214,
      "step": 62500
    },
    {
      "epoch": 0.9644087256027555,
      "grad_norm": 0.021999867632985115,
      "learning_rate": 1.357071054981503e-05,
      "loss": 0.0179,
      "step": 63000
    },
    {
      "epoch": 0.9720627631075393,
      "grad_norm": 0.0017240315210074186,
      "learning_rate": 1.351968363311647e-05,
      "loss": 0.0136,
      "step": 63500
    },
    {
      "epoch": 0.979716800612323,
      "grad_norm": 0.04497075453400612,
      "learning_rate": 1.3468656716417912e-05,
      "loss": 0.0174,
      "step": 64000
    },
    {
      "epoch": 0.9873708381171068,
      "grad_norm": 0.007401459850370884,
      "learning_rate": 1.3417629799719353e-05,
      "loss": 0.0185,
      "step": 64500
    },
    {
      "epoch": 0.9950248756218906,
      "grad_norm": 24.008777618408203,
      "learning_rate": 1.3366602883020794e-05,
      "loss": 0.0144,
      "step": 65000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9973593570608495,
      "eval_f1": 0.997359345018777,
      "eval_loss": 0.01702837273478508,
      "eval_runtime": 587.9775,
      "eval_samples_per_second": 222.202,
      "eval_steps_per_second": 27.777,
      "step": 65325
    }
  ],
  "logging_steps": 500,
  "max_steps": 195975,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.542929680784806e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
