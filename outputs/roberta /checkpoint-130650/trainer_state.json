{
  "best_global_step": 130650,
  "best_metric": 0.9974665135859165,
  "best_model_checkpoint": "./roberta_output/checkpoint-130650",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 130650,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007654037504783774,
      "grad_norm": 3.2395949363708496,
      "learning_rate": 1.994907513713484e-05,
      "loss": 0.3221,
      "step": 500
    },
    {
      "epoch": 0.015308075009567547,
      "grad_norm": 0.014469440095126629,
      "learning_rate": 1.9898048220436282e-05,
      "loss": 0.0263,
      "step": 1000
    },
    {
      "epoch": 0.022962112514351322,
      "grad_norm": 0.006248740945011377,
      "learning_rate": 1.9847021303737723e-05,
      "loss": 0.0356,
      "step": 1500
    },
    {
      "epoch": 0.030616150019135095,
      "grad_norm": 0.015773477032780647,
      "learning_rate": 1.9795994387039164e-05,
      "loss": 0.0383,
      "step": 2000
    },
    {
      "epoch": 0.038270187523918864,
      "grad_norm": 0.017072847113013268,
      "learning_rate": 1.9744967470340605e-05,
      "loss": 0.0152,
      "step": 2500
    },
    {
      "epoch": 0.045924225028702644,
      "grad_norm": 0.03983291611075401,
      "learning_rate": 1.9693940553642046e-05,
      "loss": 0.0368,
      "step": 3000
    },
    {
      "epoch": 0.053578262533486416,
      "grad_norm": 0.015236363746225834,
      "learning_rate": 1.964291363694349e-05,
      "loss": 0.025,
      "step": 3500
    },
    {
      "epoch": 0.06123230003827019,
      "grad_norm": 0.010577350854873657,
      "learning_rate": 1.9591886720244932e-05,
      "loss": 0.0347,
      "step": 4000
    },
    {
      "epoch": 0.06888633754305395,
      "grad_norm": 0.031425055116415024,
      "learning_rate": 1.9540859803546373e-05,
      "loss": 0.0268,
      "step": 4500
    },
    {
      "epoch": 0.07654037504783773,
      "grad_norm": 0.042128462344408035,
      "learning_rate": 1.9489832886847814e-05,
      "loss": 0.0349,
      "step": 5000
    },
    {
      "epoch": 0.08419441255262151,
      "grad_norm": 0.04068923741579056,
      "learning_rate": 1.9438805970149255e-05,
      "loss": 0.0335,
      "step": 5500
    },
    {
      "epoch": 0.09184845005740529,
      "grad_norm": 0.01758774183690548,
      "learning_rate": 1.9387779053450696e-05,
      "loss": 0.0242,
      "step": 6000
    },
    {
      "epoch": 0.09950248756218906,
      "grad_norm": 0.001603434095159173,
      "learning_rate": 1.933675213675214e-05,
      "loss": 0.0239,
      "step": 6500
    },
    {
      "epoch": 0.10715652506697283,
      "grad_norm": 0.0024540319573134184,
      "learning_rate": 1.928572522005358e-05,
      "loss": 0.0268,
      "step": 7000
    },
    {
      "epoch": 0.1148105625717566,
      "grad_norm": 0.003460218431428075,
      "learning_rate": 1.9234698303355023e-05,
      "loss": 0.0154,
      "step": 7500
    },
    {
      "epoch": 0.12246460007654038,
      "grad_norm": 7.229053974151611,
      "learning_rate": 1.9183671386656464e-05,
      "loss": 0.0246,
      "step": 8000
    },
    {
      "epoch": 0.13011863758132414,
      "grad_norm": 0.008742266334593296,
      "learning_rate": 1.9132644469957905e-05,
      "loss": 0.0297,
      "step": 8500
    },
    {
      "epoch": 0.1377726750861079,
      "grad_norm": 0.016277043148875237,
      "learning_rate": 1.9081617553259346e-05,
      "loss": 0.0208,
      "step": 9000
    },
    {
      "epoch": 0.14542671259089168,
      "grad_norm": 0.013757850974798203,
      "learning_rate": 1.9030590636560787e-05,
      "loss": 0.0197,
      "step": 9500
    },
    {
      "epoch": 0.15308075009567546,
      "grad_norm": 0.048026442527770996,
      "learning_rate": 1.897956371986223e-05,
      "loss": 0.0348,
      "step": 10000
    },
    {
      "epoch": 0.16073478760045926,
      "grad_norm": 0.019974982365965843,
      "learning_rate": 1.892853680316367e-05,
      "loss": 0.0255,
      "step": 10500
    },
    {
      "epoch": 0.16838882510524303,
      "grad_norm": 0.003897551679983735,
      "learning_rate": 1.8877509886465114e-05,
      "loss": 0.0216,
      "step": 11000
    },
    {
      "epoch": 0.1760428626100268,
      "grad_norm": 0.0060216160491108894,
      "learning_rate": 1.8826482969766552e-05,
      "loss": 0.0227,
      "step": 11500
    },
    {
      "epoch": 0.18369690011481057,
      "grad_norm": 0.006329769734293222,
      "learning_rate": 1.8775456053067996e-05,
      "loss": 0.0262,
      "step": 12000
    },
    {
      "epoch": 0.19135093761959435,
      "grad_norm": 0.0236835815012455,
      "learning_rate": 1.8724429136369437e-05,
      "loss": 0.0243,
      "step": 12500
    },
    {
      "epoch": 0.19900497512437812,
      "grad_norm": 0.003404367947950959,
      "learning_rate": 1.867340221967088e-05,
      "loss": 0.0239,
      "step": 13000
    },
    {
      "epoch": 0.2066590126291619,
      "grad_norm": 0.019915513694286346,
      "learning_rate": 1.862237530297232e-05,
      "loss": 0.0219,
      "step": 13500
    },
    {
      "epoch": 0.21431305013394567,
      "grad_norm": 0.02339695207774639,
      "learning_rate": 1.857134838627376e-05,
      "loss": 0.0335,
      "step": 14000
    },
    {
      "epoch": 0.22196708763872944,
      "grad_norm": 0.01294382382184267,
      "learning_rate": 1.85203214695752e-05,
      "loss": 0.0179,
      "step": 14500
    },
    {
      "epoch": 0.2296211251435132,
      "grad_norm": 0.045531731098890305,
      "learning_rate": 1.8469294552876643e-05,
      "loss": 0.0255,
      "step": 15000
    },
    {
      "epoch": 0.23727516264829698,
      "grad_norm": 0.004110763780772686,
      "learning_rate": 1.8418267636178087e-05,
      "loss": 0.0109,
      "step": 15500
    },
    {
      "epoch": 0.24492920015308076,
      "grad_norm": 0.017721066251397133,
      "learning_rate": 1.8367240719479525e-05,
      "loss": 0.0184,
      "step": 16000
    },
    {
      "epoch": 0.2525832376578645,
      "grad_norm": 0.0064967782236635685,
      "learning_rate": 1.831621380278097e-05,
      "loss": 0.0233,
      "step": 16500
    },
    {
      "epoch": 0.2602372751626483,
      "grad_norm": 0.011525420472025871,
      "learning_rate": 1.826518688608241e-05,
      "loss": 0.0177,
      "step": 17000
    },
    {
      "epoch": 0.26789131266743205,
      "grad_norm": 0.0071679530665278435,
      "learning_rate": 1.821415996938385e-05,
      "loss": 0.0199,
      "step": 17500
    },
    {
      "epoch": 0.2755453501722158,
      "grad_norm": 0.014114764519035816,
      "learning_rate": 1.8163133052685293e-05,
      "loss": 0.021,
      "step": 18000
    },
    {
      "epoch": 0.2831993876769996,
      "grad_norm": 0.03036193922162056,
      "learning_rate": 1.8112106135986734e-05,
      "loss": 0.0253,
      "step": 18500
    },
    {
      "epoch": 0.29085342518178336,
      "grad_norm": 0.01967034488916397,
      "learning_rate": 1.8061079219288175e-05,
      "loss": 0.0153,
      "step": 19000
    },
    {
      "epoch": 0.29850746268656714,
      "grad_norm": 0.011762083508074284,
      "learning_rate": 1.801005230258962e-05,
      "loss": 0.0267,
      "step": 19500
    },
    {
      "epoch": 0.3061615001913509,
      "grad_norm": 0.08587345480918884,
      "learning_rate": 1.7959025385891057e-05,
      "loss": 0.0237,
      "step": 20000
    },
    {
      "epoch": 0.31381553769613474,
      "grad_norm": 0.02281419187784195,
      "learning_rate": 1.79079984691925e-05,
      "loss": 0.014,
      "step": 20500
    },
    {
      "epoch": 0.3214695752009185,
      "grad_norm": 0.010391081683337688,
      "learning_rate": 1.7856971552493943e-05,
      "loss": 0.0149,
      "step": 21000
    },
    {
      "epoch": 0.3291236127057023,
      "grad_norm": 0.014235551469027996,
      "learning_rate": 1.7805944635795384e-05,
      "loss": 0.0211,
      "step": 21500
    },
    {
      "epoch": 0.33677765021048606,
      "grad_norm": 0.2528054416179657,
      "learning_rate": 1.7754917719096825e-05,
      "loss": 0.0221,
      "step": 22000
    },
    {
      "epoch": 0.34443168771526983,
      "grad_norm": 0.01012225728482008,
      "learning_rate": 1.7703890802398266e-05,
      "loss": 0.0221,
      "step": 22500
    },
    {
      "epoch": 0.3520857252200536,
      "grad_norm": 0.10904286056756973,
      "learning_rate": 1.7652863885699707e-05,
      "loss": 0.0279,
      "step": 23000
    },
    {
      "epoch": 0.3597397627248374,
      "grad_norm": 0.01233335118740797,
      "learning_rate": 1.7601836969001148e-05,
      "loss": 0.0155,
      "step": 23500
    },
    {
      "epoch": 0.36739380022962115,
      "grad_norm": 0.015503888949751854,
      "learning_rate": 1.7550810052302593e-05,
      "loss": 0.0258,
      "step": 24000
    },
    {
      "epoch": 0.3750478377344049,
      "grad_norm": 0.015703734010457993,
      "learning_rate": 1.749978313560403e-05,
      "loss": 0.0174,
      "step": 24500
    },
    {
      "epoch": 0.3827018752391887,
      "grad_norm": 0.19040265679359436,
      "learning_rate": 1.7448756218905475e-05,
      "loss": 0.0227,
      "step": 25000
    },
    {
      "epoch": 0.39035591274397247,
      "grad_norm": 0.012883909046649933,
      "learning_rate": 1.7397729302206916e-05,
      "loss": 0.0212,
      "step": 25500
    },
    {
      "epoch": 0.39800995024875624,
      "grad_norm": 0.0007494608871638775,
      "learning_rate": 1.7346702385508357e-05,
      "loss": 0.0101,
      "step": 26000
    },
    {
      "epoch": 0.40566398775354,
      "grad_norm": 0.015614866279065609,
      "learning_rate": 1.7295675468809798e-05,
      "loss": 0.0123,
      "step": 26500
    },
    {
      "epoch": 0.4133180252583238,
      "grad_norm": 0.008249713107943535,
      "learning_rate": 1.724464855211124e-05,
      "loss": 0.017,
      "step": 27000
    },
    {
      "epoch": 0.42097206276310756,
      "grad_norm": 0.008806606754660606,
      "learning_rate": 1.719362163541268e-05,
      "loss": 0.0266,
      "step": 27500
    },
    {
      "epoch": 0.42862610026789133,
      "grad_norm": 0.002821953035891056,
      "learning_rate": 1.714259471871412e-05,
      "loss": 0.013,
      "step": 28000
    },
    {
      "epoch": 0.4362801377726751,
      "grad_norm": 0.033807989209890366,
      "learning_rate": 1.7091567802015566e-05,
      "loss": 0.0175,
      "step": 28500
    },
    {
      "epoch": 0.4439341752774589,
      "grad_norm": 0.012942887842655182,
      "learning_rate": 1.7040540885317003e-05,
      "loss": 0.0174,
      "step": 29000
    },
    {
      "epoch": 0.45158821278224265,
      "grad_norm": 0.42944949865341187,
      "learning_rate": 1.6989513968618448e-05,
      "loss": 0.0135,
      "step": 29500
    },
    {
      "epoch": 0.4592422502870264,
      "grad_norm": 0.007172738667577505,
      "learning_rate": 1.693848705191989e-05,
      "loss": 0.0217,
      "step": 30000
    },
    {
      "epoch": 0.4668962877918102,
      "grad_norm": 0.006976378615945578,
      "learning_rate": 1.688746013522133e-05,
      "loss": 0.0114,
      "step": 30500
    },
    {
      "epoch": 0.47455032529659397,
      "grad_norm": 0.028359705582261086,
      "learning_rate": 1.683643321852277e-05,
      "loss": 0.0217,
      "step": 31000
    },
    {
      "epoch": 0.48220436280137774,
      "grad_norm": 0.013388414867222309,
      "learning_rate": 1.6785406301824216e-05,
      "loss": 0.015,
      "step": 31500
    },
    {
      "epoch": 0.4898584003061615,
      "grad_norm": 0.012085537426173687,
      "learning_rate": 1.6734379385125653e-05,
      "loss": 0.0227,
      "step": 32000
    },
    {
      "epoch": 0.4975124378109453,
      "grad_norm": 0.02972428686916828,
      "learning_rate": 1.6683352468427098e-05,
      "loss": 0.0249,
      "step": 32500
    },
    {
      "epoch": 0.505166475315729,
      "grad_norm": 7.652212619781494,
      "learning_rate": 1.663232555172854e-05,
      "loss": 0.0175,
      "step": 33000
    },
    {
      "epoch": 0.5128205128205128,
      "grad_norm": 0.007570531219244003,
      "learning_rate": 1.658129863502998e-05,
      "loss": 0.02,
      "step": 33500
    },
    {
      "epoch": 0.5204745503252965,
      "grad_norm": 0.009036584757268429,
      "learning_rate": 1.653027171833142e-05,
      "loss": 0.0117,
      "step": 34000
    },
    {
      "epoch": 0.5281285878300803,
      "grad_norm": 0.013528013601899147,
      "learning_rate": 1.6479244801632862e-05,
      "loss": 0.0236,
      "step": 34500
    },
    {
      "epoch": 0.5357826253348641,
      "grad_norm": 0.03182655945420265,
      "learning_rate": 1.6428217884934303e-05,
      "loss": 0.017,
      "step": 35000
    },
    {
      "epoch": 0.5434366628396479,
      "grad_norm": 0.021973270922899246,
      "learning_rate": 1.6377190968235744e-05,
      "loss": 0.0162,
      "step": 35500
    },
    {
      "epoch": 0.5510907003444316,
      "grad_norm": 0.08238168805837631,
      "learning_rate": 1.632616405153719e-05,
      "loss": 0.0139,
      "step": 36000
    },
    {
      "epoch": 0.5587447378492154,
      "grad_norm": 0.00797649472951889,
      "learning_rate": 1.6275137134838627e-05,
      "loss": 0.0182,
      "step": 36500
    },
    {
      "epoch": 0.5663987753539992,
      "grad_norm": 0.012431775219738483,
      "learning_rate": 1.622411021814007e-05,
      "loss": 0.0176,
      "step": 37000
    },
    {
      "epoch": 0.574052812858783,
      "grad_norm": 13.869054794311523,
      "learning_rate": 1.6173083301441512e-05,
      "loss": 0.0276,
      "step": 37500
    },
    {
      "epoch": 0.5817068503635667,
      "grad_norm": 0.022639520466327667,
      "learning_rate": 1.6122056384742953e-05,
      "loss": 0.0149,
      "step": 38000
    },
    {
      "epoch": 0.5893608878683505,
      "grad_norm": 0.0008122010622173548,
      "learning_rate": 1.6071029468044394e-05,
      "loss": 0.0211,
      "step": 38500
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.010585172101855278,
      "learning_rate": 1.6020002551345835e-05,
      "loss": 0.0188,
      "step": 39000
    },
    {
      "epoch": 0.604668962877918,
      "grad_norm": 0.0024072814267128706,
      "learning_rate": 1.5968975634647277e-05,
      "loss": 0.0185,
      "step": 39500
    },
    {
      "epoch": 0.6123230003827018,
      "grad_norm": 0.0044095623306930065,
      "learning_rate": 1.5917948717948718e-05,
      "loss": 0.0097,
      "step": 40000
    },
    {
      "epoch": 0.6199770378874856,
      "grad_norm": 0.02492479234933853,
      "learning_rate": 1.5866921801250162e-05,
      "loss": 0.0177,
      "step": 40500
    },
    {
      "epoch": 0.6276310753922695,
      "grad_norm": 0.038104210048913956,
      "learning_rate": 1.58158948845516e-05,
      "loss": 0.0249,
      "step": 41000
    },
    {
      "epoch": 0.6352851128970533,
      "grad_norm": 0.03469916060566902,
      "learning_rate": 1.5764867967853044e-05,
      "loss": 0.0146,
      "step": 41500
    },
    {
      "epoch": 0.642939150401837,
      "grad_norm": 0.0063214427791535854,
      "learning_rate": 1.5713841051154485e-05,
      "loss": 0.0142,
      "step": 42000
    },
    {
      "epoch": 0.6505931879066208,
      "grad_norm": 0.0506884828209877,
      "learning_rate": 1.5662814134455927e-05,
      "loss": 0.0245,
      "step": 42500
    },
    {
      "epoch": 0.6582472254114046,
      "grad_norm": 0.007845889776945114,
      "learning_rate": 1.5611787217757368e-05,
      "loss": 0.0208,
      "step": 43000
    },
    {
      "epoch": 0.6659012629161883,
      "grad_norm": 0.01178748533129692,
      "learning_rate": 1.5560760301058812e-05,
      "loss": 0.0147,
      "step": 43500
    },
    {
      "epoch": 0.6735553004209721,
      "grad_norm": 0.0015787078300490975,
      "learning_rate": 1.550973338436025e-05,
      "loss": 0.0265,
      "step": 44000
    },
    {
      "epoch": 0.6812093379257559,
      "grad_norm": 0.04057019576430321,
      "learning_rate": 1.5458706467661694e-05,
      "loss": 0.0165,
      "step": 44500
    },
    {
      "epoch": 0.6888633754305397,
      "grad_norm": 0.006024716421961784,
      "learning_rate": 1.5407679550963135e-05,
      "loss": 0.0153,
      "step": 45000
    },
    {
      "epoch": 0.6965174129353234,
      "grad_norm": 0.0009105561766773462,
      "learning_rate": 1.5356652634264576e-05,
      "loss": 0.0183,
      "step": 45500
    },
    {
      "epoch": 0.7041714504401072,
      "grad_norm": 0.004054246004670858,
      "learning_rate": 1.5305625717566018e-05,
      "loss": 0.0136,
      "step": 46000
    },
    {
      "epoch": 0.711825487944891,
      "grad_norm": 0.00794429611414671,
      "learning_rate": 1.5254598800867459e-05,
      "loss": 0.011,
      "step": 46500
    },
    {
      "epoch": 0.7194795254496748,
      "grad_norm": 0.01418149285018444,
      "learning_rate": 1.52035718841689e-05,
      "loss": 0.0156,
      "step": 47000
    },
    {
      "epoch": 0.7271335629544585,
      "grad_norm": 0.0007489131530746818,
      "learning_rate": 1.5152544967470343e-05,
      "loss": 0.0196,
      "step": 47500
    },
    {
      "epoch": 0.7347876004592423,
      "grad_norm": 0.025610439479351044,
      "learning_rate": 1.5101518050771784e-05,
      "loss": 0.0167,
      "step": 48000
    },
    {
      "epoch": 0.7424416379640261,
      "grad_norm": 0.001481801737099886,
      "learning_rate": 1.5050491134073225e-05,
      "loss": 0.0255,
      "step": 48500
    },
    {
      "epoch": 0.7500956754688098,
      "grad_norm": 0.0106174536049366,
      "learning_rate": 1.4999464217374666e-05,
      "loss": 0.0174,
      "step": 49000
    },
    {
      "epoch": 0.7577497129735936,
      "grad_norm": 0.017857227474451065,
      "learning_rate": 1.4948437300676109e-05,
      "loss": 0.0098,
      "step": 49500
    },
    {
      "epoch": 0.7654037504783774,
      "grad_norm": 0.008759105578064919,
      "learning_rate": 1.4897410383977548e-05,
      "loss": 0.0183,
      "step": 50000
    },
    {
      "epoch": 0.7730577879831612,
      "grad_norm": 0.010550866834819317,
      "learning_rate": 1.484638346727899e-05,
      "loss": 0.013,
      "step": 50500
    },
    {
      "epoch": 0.7807118254879449,
      "grad_norm": 0.04928474873304367,
      "learning_rate": 1.4795356550580434e-05,
      "loss": 0.0199,
      "step": 51000
    },
    {
      "epoch": 0.7883658629927287,
      "grad_norm": 0.00809688214212656,
      "learning_rate": 1.4744329633881873e-05,
      "loss": 0.0179,
      "step": 51500
    },
    {
      "epoch": 0.7960199004975125,
      "grad_norm": 0.012599923647940159,
      "learning_rate": 1.4693302717183316e-05,
      "loss": 0.0165,
      "step": 52000
    },
    {
      "epoch": 0.8036739380022963,
      "grad_norm": 0.026071950793266296,
      "learning_rate": 1.4642275800484757e-05,
      "loss": 0.0148,
      "step": 52500
    },
    {
      "epoch": 0.81132797550708,
      "grad_norm": 0.02482771687209606,
      "learning_rate": 1.4591248883786198e-05,
      "loss": 0.0139,
      "step": 53000
    },
    {
      "epoch": 0.8189820130118638,
      "grad_norm": 0.015472248196601868,
      "learning_rate": 1.454022196708764e-05,
      "loss": 0.0169,
      "step": 53500
    },
    {
      "epoch": 0.8266360505166476,
      "grad_norm": 0.009918730705976486,
      "learning_rate": 1.4489195050389082e-05,
      "loss": 0.0136,
      "step": 54000
    },
    {
      "epoch": 0.8342900880214313,
      "grad_norm": 0.00666081765666604,
      "learning_rate": 1.4438168133690523e-05,
      "loss": 0.0226,
      "step": 54500
    },
    {
      "epoch": 0.8419441255262151,
      "grad_norm": 0.0008123498992063105,
      "learning_rate": 1.4387141216991964e-05,
      "loss": 0.0164,
      "step": 55000
    },
    {
      "epoch": 0.8495981630309989,
      "grad_norm": 0.01785323955118656,
      "learning_rate": 1.4336114300293407e-05,
      "loss": 0.0123,
      "step": 55500
    },
    {
      "epoch": 0.8572522005357827,
      "grad_norm": 0.0032733685802668333,
      "learning_rate": 1.4285087383594846e-05,
      "loss": 0.0185,
      "step": 56000
    },
    {
      "epoch": 0.8649062380405664,
      "grad_norm": 0.02396867796778679,
      "learning_rate": 1.4234060466896289e-05,
      "loss": 0.0159,
      "step": 56500
    },
    {
      "epoch": 0.8725602755453502,
      "grad_norm": 0.00957475882023573,
      "learning_rate": 1.4183033550197732e-05,
      "loss": 0.0127,
      "step": 57000
    },
    {
      "epoch": 0.880214313050134,
      "grad_norm": 0.002219774527475238,
      "learning_rate": 1.4132006633499171e-05,
      "loss": 0.013,
      "step": 57500
    },
    {
      "epoch": 0.8878683505549178,
      "grad_norm": 0.008410902693867683,
      "learning_rate": 1.4080979716800614e-05,
      "loss": 0.0264,
      "step": 58000
    },
    {
      "epoch": 0.8955223880597015,
      "grad_norm": 0.007397367153316736,
      "learning_rate": 1.4029952800102055e-05,
      "loss": 0.0117,
      "step": 58500
    },
    {
      "epoch": 0.9031764255644853,
      "grad_norm": 0.017827987670898438,
      "learning_rate": 1.3978925883403496e-05,
      "loss": 0.0182,
      "step": 59000
    },
    {
      "epoch": 0.9108304630692691,
      "grad_norm": 0.049954790621995926,
      "learning_rate": 1.3927898966704937e-05,
      "loss": 0.0105,
      "step": 59500
    },
    {
      "epoch": 0.9184845005740528,
      "grad_norm": 0.013370374217629433,
      "learning_rate": 1.387687205000638e-05,
      "loss": 0.0167,
      "step": 60000
    },
    {
      "epoch": 0.9261385380788366,
      "grad_norm": 0.03570948913693428,
      "learning_rate": 1.3825845133307821e-05,
      "loss": 0.0233,
      "step": 60500
    },
    {
      "epoch": 0.9337925755836204,
      "grad_norm": 0.004486114718019962,
      "learning_rate": 1.3774818216609262e-05,
      "loss": 0.0183,
      "step": 61000
    },
    {
      "epoch": 0.9414466130884042,
      "grad_norm": 0.00040499024908058345,
      "learning_rate": 1.3723791299910705e-05,
      "loss": 0.012,
      "step": 61500
    },
    {
      "epoch": 0.9491006505931879,
      "grad_norm": 0.04955380782485008,
      "learning_rate": 1.3672764383212144e-05,
      "loss": 0.0174,
      "step": 62000
    },
    {
      "epoch": 0.9567546880979717,
      "grad_norm": 0.010551507584750652,
      "learning_rate": 1.3621737466513587e-05,
      "loss": 0.0214,
      "step": 62500
    },
    {
      "epoch": 0.9644087256027555,
      "grad_norm": 0.021999867632985115,
      "learning_rate": 1.357071054981503e-05,
      "loss": 0.0179,
      "step": 63000
    },
    {
      "epoch": 0.9720627631075393,
      "grad_norm": 0.0017240315210074186,
      "learning_rate": 1.351968363311647e-05,
      "loss": 0.0136,
      "step": 63500
    },
    {
      "epoch": 0.979716800612323,
      "grad_norm": 0.04497075453400612,
      "learning_rate": 1.3468656716417912e-05,
      "loss": 0.0174,
      "step": 64000
    },
    {
      "epoch": 0.9873708381171068,
      "grad_norm": 0.007401459850370884,
      "learning_rate": 1.3417629799719353e-05,
      "loss": 0.0185,
      "step": 64500
    },
    {
      "epoch": 0.9950248756218906,
      "grad_norm": 24.008777618408203,
      "learning_rate": 1.3366602883020794e-05,
      "loss": 0.0144,
      "step": 65000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9973593570608495,
      "eval_f1": 0.997359345018777,
      "eval_loss": 0.01702837273478508,
      "eval_runtime": 587.9775,
      "eval_samples_per_second": 222.202,
      "eval_steps_per_second": 27.777,
      "step": 65325
    },
    {
      "epoch": 1.0026789131266742,
      "grad_norm": 0.007525982800871134,
      "learning_rate": 1.3315575966322235e-05,
      "loss": 0.0093,
      "step": 65500
    },
    {
      "epoch": 1.010332950631458,
      "grad_norm": 0.0006921470630913973,
      "learning_rate": 1.3264549049623678e-05,
      "loss": 0.0125,
      "step": 66000
    },
    {
      "epoch": 1.0179869881362418,
      "grad_norm": 0.014523839578032494,
      "learning_rate": 1.321352213292512e-05,
      "loss": 0.0219,
      "step": 66500
    },
    {
      "epoch": 1.0256410256410255,
      "grad_norm": 0.00911787524819374,
      "learning_rate": 1.316249521622656e-05,
      "loss": 0.0166,
      "step": 67000
    },
    {
      "epoch": 1.0332950631458093,
      "grad_norm": 0.0045684389770030975,
      "learning_rate": 1.3111468299528003e-05,
      "loss": 0.016,
      "step": 67500
    },
    {
      "epoch": 1.040949100650593,
      "grad_norm": 0.030417338013648987,
      "learning_rate": 1.3060441382829443e-05,
      "loss": 0.0173,
      "step": 68000
    },
    {
      "epoch": 1.0486031381553769,
      "grad_norm": 0.009309531189501286,
      "learning_rate": 1.3009414466130885e-05,
      "loss": 0.0095,
      "step": 68500
    },
    {
      "epoch": 1.0562571756601606,
      "grad_norm": 0.004370613489300013,
      "learning_rate": 1.2958387549432328e-05,
      "loss": 0.0161,
      "step": 69000
    },
    {
      "epoch": 1.0639112131649444,
      "grad_norm": 0.02108973264694214,
      "learning_rate": 1.2907360632733767e-05,
      "loss": 0.0215,
      "step": 69500
    },
    {
      "epoch": 1.0715652506697282,
      "grad_norm": 0.03863188996911049,
      "learning_rate": 1.285633371603521e-05,
      "loss": 0.0238,
      "step": 70000
    },
    {
      "epoch": 1.079219288174512,
      "grad_norm": 0.030021177604794502,
      "learning_rate": 1.2805306799336651e-05,
      "loss": 0.0204,
      "step": 70500
    },
    {
      "epoch": 1.0868733256792957,
      "grad_norm": 0.013968453742563725,
      "learning_rate": 1.2754279882638092e-05,
      "loss": 0.0114,
      "step": 71000
    },
    {
      "epoch": 1.0945273631840795,
      "grad_norm": 0.010086163878440857,
      "learning_rate": 1.2703252965939534e-05,
      "loss": 0.0105,
      "step": 71500
    },
    {
      "epoch": 1.1021814006888633,
      "grad_norm": 0.017095742747187614,
      "learning_rate": 1.2652226049240976e-05,
      "loss": 0.0178,
      "step": 72000
    },
    {
      "epoch": 1.109835438193647,
      "grad_norm": 0.0006034833495505154,
      "learning_rate": 1.2601199132542416e-05,
      "loss": 0.0223,
      "step": 72500
    },
    {
      "epoch": 1.1174894756984308,
      "grad_norm": 0.01862037181854248,
      "learning_rate": 1.2550172215843859e-05,
      "loss": 0.0239,
      "step": 73000
    },
    {
      "epoch": 1.1251435132032146,
      "grad_norm": 0.002598024904727936,
      "learning_rate": 1.2499145299145301e-05,
      "loss": 0.016,
      "step": 73500
    },
    {
      "epoch": 1.1327975507079984,
      "grad_norm": 0.0006981054320931435,
      "learning_rate": 1.244811838244674e-05,
      "loss": 0.0245,
      "step": 74000
    },
    {
      "epoch": 1.1404515882127821,
      "grad_norm": 0.0370415598154068,
      "learning_rate": 1.2397091465748183e-05,
      "loss": 0.0133,
      "step": 74500
    },
    {
      "epoch": 1.148105625717566,
      "grad_norm": 0.013898489996790886,
      "learning_rate": 1.2346064549049626e-05,
      "loss": 0.0149,
      "step": 75000
    },
    {
      "epoch": 1.1557596632223497,
      "grad_norm": 0.014648509211838245,
      "learning_rate": 1.2295037632351066e-05,
      "loss": 0.0158,
      "step": 75500
    },
    {
      "epoch": 1.1634137007271335,
      "grad_norm": 0.020234568044543266,
      "learning_rate": 1.2244010715652508e-05,
      "loss": 0.0151,
      "step": 76000
    },
    {
      "epoch": 1.1710677382319172,
      "grad_norm": 0.03056810051202774,
      "learning_rate": 1.219298379895395e-05,
      "loss": 0.0177,
      "step": 76500
    },
    {
      "epoch": 1.178721775736701,
      "grad_norm": 0.009762034751474857,
      "learning_rate": 1.214195688225539e-05,
      "loss": 0.0151,
      "step": 77000
    },
    {
      "epoch": 1.1863758132414848,
      "grad_norm": 0.013478045351803303,
      "learning_rate": 1.2090929965556832e-05,
      "loss": 0.0191,
      "step": 77500
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.01823304407298565,
      "learning_rate": 1.2039903048858275e-05,
      "loss": 0.014,
      "step": 78000
    },
    {
      "epoch": 1.2016838882510523,
      "grad_norm": 0.03018408641219139,
      "learning_rate": 1.1988876132159714e-05,
      "loss": 0.0217,
      "step": 78500
    },
    {
      "epoch": 1.209337925755836,
      "grad_norm": 0.0005245207576081157,
      "learning_rate": 1.1937849215461157e-05,
      "loss": 0.0085,
      "step": 79000
    },
    {
      "epoch": 1.2169919632606199,
      "grad_norm": 0.013507082127034664,
      "learning_rate": 1.18868222987626e-05,
      "loss": 0.0149,
      "step": 79500
    },
    {
      "epoch": 1.2246460007654036,
      "grad_norm": 0.011456447653472424,
      "learning_rate": 1.1835795382064039e-05,
      "loss": 0.0209,
      "step": 80000
    },
    {
      "epoch": 1.2323000382701874,
      "grad_norm": 0.010593488812446594,
      "learning_rate": 1.1784768465365482e-05,
      "loss": 0.0067,
      "step": 80500
    },
    {
      "epoch": 1.2399540757749712,
      "grad_norm": 0.018642237409949303,
      "learning_rate": 1.1733741548666924e-05,
      "loss": 0.0122,
      "step": 81000
    },
    {
      "epoch": 1.247608113279755,
      "grad_norm": 0.048807237297296524,
      "learning_rate": 1.1682714631968364e-05,
      "loss": 0.0168,
      "step": 81500
    },
    {
      "epoch": 1.2552621507845387,
      "grad_norm": 0.020721152424812317,
      "learning_rate": 1.1631687715269807e-05,
      "loss": 0.0202,
      "step": 82000
    },
    {
      "epoch": 1.2629161882893225,
      "grad_norm": 0.006206052377820015,
      "learning_rate": 1.1580660798571248e-05,
      "loss": 0.0125,
      "step": 82500
    },
    {
      "epoch": 1.2705702257941063,
      "grad_norm": 0.003759062848985195,
      "learning_rate": 1.1529633881872689e-05,
      "loss": 0.0098,
      "step": 83000
    },
    {
      "epoch": 1.27822426329889,
      "grad_norm": 0.015222888439893723,
      "learning_rate": 1.147860696517413e-05,
      "loss": 0.0183,
      "step": 83500
    },
    {
      "epoch": 1.2858783008036738,
      "grad_norm": 0.011027079075574875,
      "learning_rate": 1.1427580048475573e-05,
      "loss": 0.0145,
      "step": 84000
    },
    {
      "epoch": 1.2935323383084576,
      "grad_norm": 0.042579229921102524,
      "learning_rate": 1.1376553131777012e-05,
      "loss": 0.0219,
      "step": 84500
    },
    {
      "epoch": 1.3011863758132414,
      "grad_norm": 0.02250136435031891,
      "learning_rate": 1.1325526215078455e-05,
      "loss": 0.0196,
      "step": 85000
    },
    {
      "epoch": 1.3088404133180251,
      "grad_norm": 0.03543633967638016,
      "learning_rate": 1.1274499298379898e-05,
      "loss": 0.0218,
      "step": 85500
    },
    {
      "epoch": 1.316494450822809,
      "grad_norm": 0.030302628874778748,
      "learning_rate": 1.1223472381681337e-05,
      "loss": 0.0155,
      "step": 86000
    },
    {
      "epoch": 1.3241484883275927,
      "grad_norm": 0.03153447434306145,
      "learning_rate": 1.117244546498278e-05,
      "loss": 0.0137,
      "step": 86500
    },
    {
      "epoch": 1.3318025258323765,
      "grad_norm": 0.0013161883689463139,
      "learning_rate": 1.1121418548284223e-05,
      "loss": 0.019,
      "step": 87000
    },
    {
      "epoch": 1.3394565633371602,
      "grad_norm": 0.008223711512982845,
      "learning_rate": 1.1070391631585662e-05,
      "loss": 0.0132,
      "step": 87500
    },
    {
      "epoch": 1.347110600841944,
      "grad_norm": 0.00021367653971537948,
      "learning_rate": 1.1019364714887105e-05,
      "loss": 0.0118,
      "step": 88000
    },
    {
      "epoch": 1.3547646383467278,
      "grad_norm": 0.015283296816051006,
      "learning_rate": 1.0968337798188546e-05,
      "loss": 0.0189,
      "step": 88500
    },
    {
      "epoch": 1.3624186758515116,
      "grad_norm": 0.022599278017878532,
      "learning_rate": 1.0917310881489987e-05,
      "loss": 0.0129,
      "step": 89000
    },
    {
      "epoch": 1.3700727133562953,
      "grad_norm": 0.009539528749883175,
      "learning_rate": 1.0866283964791428e-05,
      "loss": 0.0186,
      "step": 89500
    },
    {
      "epoch": 1.377726750861079,
      "grad_norm": 0.0004947602865286171,
      "learning_rate": 1.0815257048092871e-05,
      "loss": 0.0208,
      "step": 90000
    },
    {
      "epoch": 1.3853807883658629,
      "grad_norm": 0.003854156704619527,
      "learning_rate": 1.076423013139431e-05,
      "loss": 0.0167,
      "step": 90500
    },
    {
      "epoch": 1.3930348258706466,
      "grad_norm": 0.003943365998566151,
      "learning_rate": 1.0713203214695753e-05,
      "loss": 0.0171,
      "step": 91000
    },
    {
      "epoch": 1.4006888633754304,
      "grad_norm": 3.9507999420166016,
      "learning_rate": 1.0662176297997196e-05,
      "loss": 0.0187,
      "step": 91500
    },
    {
      "epoch": 1.4083429008802142,
      "grad_norm": 3.9975836277008057,
      "learning_rate": 1.0611149381298635e-05,
      "loss": 0.0128,
      "step": 92000
    },
    {
      "epoch": 1.415996938384998,
      "grad_norm": 0.01696852035820484,
      "learning_rate": 1.0560122464600078e-05,
      "loss": 0.0166,
      "step": 92500
    },
    {
      "epoch": 1.4236509758897817,
      "grad_norm": 0.0006630304269492626,
      "learning_rate": 1.050909554790152e-05,
      "loss": 0.0112,
      "step": 93000
    },
    {
      "epoch": 1.4313050133945655,
      "grad_norm": 0.026279112324118614,
      "learning_rate": 1.045806863120296e-05,
      "loss": 0.0207,
      "step": 93500
    },
    {
      "epoch": 1.4389590508993493,
      "grad_norm": 0.00107383425347507,
      "learning_rate": 1.0407041714504403e-05,
      "loss": 0.0137,
      "step": 94000
    },
    {
      "epoch": 1.446613088404133,
      "grad_norm": 0.006281970534473658,
      "learning_rate": 1.0356014797805844e-05,
      "loss": 0.0139,
      "step": 94500
    },
    {
      "epoch": 1.4542671259089168,
      "grad_norm": 0.11723911762237549,
      "learning_rate": 1.0304987881107285e-05,
      "loss": 0.0171,
      "step": 95000
    },
    {
      "epoch": 1.4619211634137006,
      "grad_norm": 0.0003739020321518183,
      "learning_rate": 1.0253960964408726e-05,
      "loss": 0.0145,
      "step": 95500
    },
    {
      "epoch": 1.4695752009184844,
      "grad_norm": 0.003105662064626813,
      "learning_rate": 1.0202934047710169e-05,
      "loss": 0.0174,
      "step": 96000
    },
    {
      "epoch": 1.4772292384232681,
      "grad_norm": 0.03652188926935196,
      "learning_rate": 1.0151907131011608e-05,
      "loss": 0.0224,
      "step": 96500
    },
    {
      "epoch": 1.484883275928052,
      "grad_norm": 0.05786196142435074,
      "learning_rate": 1.0100880214313051e-05,
      "loss": 0.0191,
      "step": 97000
    },
    {
      "epoch": 1.4925373134328357,
      "grad_norm": 0.01863015443086624,
      "learning_rate": 1.0049853297614494e-05,
      "loss": 0.0161,
      "step": 97500
    },
    {
      "epoch": 1.5001913509376195,
      "grad_norm": 0.005836783442646265,
      "learning_rate": 9.998826380915933e-06,
      "loss": 0.0168,
      "step": 98000
    },
    {
      "epoch": 1.5078453884424032,
      "grad_norm": 0.010650859214365482,
      "learning_rate": 9.947799464217376e-06,
      "loss": 0.0123,
      "step": 98500
    },
    {
      "epoch": 1.515499425947187,
      "grad_norm": 0.00033427259768359363,
      "learning_rate": 9.896772547518817e-06,
      "loss": 0.0102,
      "step": 99000
    },
    {
      "epoch": 1.5231534634519708,
      "grad_norm": 0.007214593701064587,
      "learning_rate": 9.845745630820258e-06,
      "loss": 0.0235,
      "step": 99500
    },
    {
      "epoch": 1.5308075009567546,
      "grad_norm": 9.974594116210938,
      "learning_rate": 9.794718714121701e-06,
      "loss": 0.0174,
      "step": 100000
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 0.009372622705996037,
      "learning_rate": 9.743691797423142e-06,
      "loss": 0.017,
      "step": 100500
    },
    {
      "epoch": 1.546115575966322,
      "grad_norm": 0.056108128279447556,
      "learning_rate": 9.692664880724583e-06,
      "loss": 0.0207,
      "step": 101000
    },
    {
      "epoch": 1.5537696134711059,
      "grad_norm": 0.0065086460672318935,
      "learning_rate": 9.641637964026024e-06,
      "loss": 0.0071,
      "step": 101500
    },
    {
      "epoch": 1.5614236509758896,
      "grad_norm": 0.009320647455751896,
      "learning_rate": 9.590611047327466e-06,
      "loss": 0.0164,
      "step": 102000
    },
    {
      "epoch": 1.5690776884806734,
      "grad_norm": 0.010231515392661095,
      "learning_rate": 9.539584130628907e-06,
      "loss": 0.0085,
      "step": 102500
    },
    {
      "epoch": 1.5767317259854572,
      "grad_norm": 0.03383587673306465,
      "learning_rate": 9.48855721393035e-06,
      "loss": 0.0167,
      "step": 103000
    },
    {
      "epoch": 1.584385763490241,
      "grad_norm": 0.02057141624391079,
      "learning_rate": 9.43753029723179e-06,
      "loss": 0.015,
      "step": 103500
    },
    {
      "epoch": 1.5920398009950247,
      "grad_norm": 0.004016270861029625,
      "learning_rate": 9.386503380533232e-06,
      "loss": 0.0138,
      "step": 104000
    },
    {
      "epoch": 1.5996938384998085,
      "grad_norm": 0.010522682219743729,
      "learning_rate": 9.335476463834674e-06,
      "loss": 0.015,
      "step": 104500
    },
    {
      "epoch": 1.6073478760045923,
      "grad_norm": 0.0012820566771551967,
      "learning_rate": 9.284449547136115e-06,
      "loss": 0.0087,
      "step": 105000
    },
    {
      "epoch": 1.615001913509376,
      "grad_norm": 0.011592317372560501,
      "learning_rate": 9.233422630437557e-06,
      "loss": 0.0274,
      "step": 105500
    },
    {
      "epoch": 1.6226559510141598,
      "grad_norm": 0.01869588904082775,
      "learning_rate": 9.182395713738998e-06,
      "loss": 0.0175,
      "step": 106000
    },
    {
      "epoch": 1.6303099885189436,
      "grad_norm": 0.4322880208492279,
      "learning_rate": 9.13136879704044e-06,
      "loss": 0.0092,
      "step": 106500
    },
    {
      "epoch": 1.6379640260237274,
      "grad_norm": 0.004391665104776621,
      "learning_rate": 9.080341880341882e-06,
      "loss": 0.0101,
      "step": 107000
    },
    {
      "epoch": 1.6456180635285111,
      "grad_norm": 0.0061098490841686726,
      "learning_rate": 9.029314963643323e-06,
      "loss": 0.0102,
      "step": 107500
    },
    {
      "epoch": 1.653272101033295,
      "grad_norm": 0.0319940410554409,
      "learning_rate": 8.978288046944764e-06,
      "loss": 0.021,
      "step": 108000
    },
    {
      "epoch": 1.6609261385380787,
      "grad_norm": 0.0008171938825398684,
      "learning_rate": 8.927261130246205e-06,
      "loss": 0.0269,
      "step": 108500
    },
    {
      "epoch": 1.6685801760428625,
      "grad_norm": 0.0370413102209568,
      "learning_rate": 8.876234213547648e-06,
      "loss": 0.0147,
      "step": 109000
    },
    {
      "epoch": 1.6762342135476462,
      "grad_norm": 0.003233014838770032,
      "learning_rate": 8.825207296849089e-06,
      "loss": 0.0123,
      "step": 109500
    },
    {
      "epoch": 1.68388825105243,
      "grad_norm": 0.009816826321184635,
      "learning_rate": 8.77418038015053e-06,
      "loss": 0.0114,
      "step": 110000
    },
    {
      "epoch": 1.6915422885572138,
      "grad_norm": 0.003291777102276683,
      "learning_rate": 8.723153463451973e-06,
      "loss": 0.0091,
      "step": 110500
    },
    {
      "epoch": 1.6991963260619976,
      "grad_norm": 0.04011242091655731,
      "learning_rate": 8.672126546753414e-06,
      "loss": 0.015,
      "step": 111000
    },
    {
      "epoch": 1.7068503635667813,
      "grad_norm": 0.0008109760237857699,
      "learning_rate": 8.621099630054855e-06,
      "loss": 0.0214,
      "step": 111500
    },
    {
      "epoch": 1.714504401071565,
      "grad_norm": 0.0011500961845740676,
      "learning_rate": 8.570072713356296e-06,
      "loss": 0.018,
      "step": 112000
    },
    {
      "epoch": 1.7221584385763489,
      "grad_norm": 0.025118926540017128,
      "learning_rate": 8.519045796657737e-06,
      "loss": 0.022,
      "step": 112500
    },
    {
      "epoch": 1.7298124760811326,
      "grad_norm": 0.03050198405981064,
      "learning_rate": 8.46801887995918e-06,
      "loss": 0.0163,
      "step": 113000
    },
    {
      "epoch": 1.7374665135859164,
      "grad_norm": 0.009151952341198921,
      "learning_rate": 8.41699196326062e-06,
      "loss": 0.0158,
      "step": 113500
    },
    {
      "epoch": 1.7451205510907002,
      "grad_norm": 0.03300062566995621,
      "learning_rate": 8.365965046562062e-06,
      "loss": 0.0193,
      "step": 114000
    },
    {
      "epoch": 1.752774588595484,
      "grad_norm": 0.0061890436336398125,
      "learning_rate": 8.314938129863503e-06,
      "loss": 0.0216,
      "step": 114500
    },
    {
      "epoch": 1.7604286261002677,
      "grad_norm": 0.0041587213054299355,
      "learning_rate": 8.263911213164946e-06,
      "loss": 0.0109,
      "step": 115000
    },
    {
      "epoch": 1.7680826636050515,
      "grad_norm": 0.025689151138067245,
      "learning_rate": 8.212884296466387e-06,
      "loss": 0.0083,
      "step": 115500
    },
    {
      "epoch": 1.7757367011098353,
      "grad_norm": 0.014212292619049549,
      "learning_rate": 8.161857379767828e-06,
      "loss": 0.0069,
      "step": 116000
    },
    {
      "epoch": 1.783390738614619,
      "grad_norm": 0.033617425709962845,
      "learning_rate": 8.11083046306927e-06,
      "loss": 0.0115,
      "step": 116500
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.03381316363811493,
      "learning_rate": 8.059803546370712e-06,
      "loss": 0.022,
      "step": 117000
    },
    {
      "epoch": 1.7986988136241866,
      "grad_norm": 0.0009022735757753253,
      "learning_rate": 8.008776629672153e-06,
      "loss": 0.0185,
      "step": 117500
    },
    {
      "epoch": 1.8063528511289704,
      "grad_norm": 0.019447021186351776,
      "learning_rate": 7.957749712973594e-06,
      "loss": 0.0107,
      "step": 118000
    },
    {
      "epoch": 1.8140068886337541,
      "grad_norm": 0.00374153396114707,
      "learning_rate": 7.906722796275035e-06,
      "loss": 0.0156,
      "step": 118500
    },
    {
      "epoch": 1.821660926138538,
      "grad_norm": 0.028403282165527344,
      "learning_rate": 7.855695879576476e-06,
      "loss": 0.0108,
      "step": 119000
    },
    {
      "epoch": 1.8293149636433217,
      "grad_norm": 0.00861379038542509,
      "learning_rate": 7.804668962877919e-06,
      "loss": 0.019,
      "step": 119500
    },
    {
      "epoch": 1.8369690011481055,
      "grad_norm": 0.01606564410030842,
      "learning_rate": 7.75364204617936e-06,
      "loss": 0.0166,
      "step": 120000
    },
    {
      "epoch": 1.8446230386528892,
      "grad_norm": 0.0009824461303651333,
      "learning_rate": 7.702615129480801e-06,
      "loss": 0.018,
      "step": 120500
    },
    {
      "epoch": 1.852277076157673,
      "grad_norm": 0.010237845592200756,
      "learning_rate": 7.651588212782242e-06,
      "loss": 0.0136,
      "step": 121000
    },
    {
      "epoch": 1.8599311136624568,
      "grad_norm": 0.006204240955412388,
      "learning_rate": 7.600561296083685e-06,
      "loss": 0.0107,
      "step": 121500
    },
    {
      "epoch": 1.8675851511672406,
      "grad_norm": 0.0032175546512007713,
      "learning_rate": 7.549534379385126e-06,
      "loss": 0.0135,
      "step": 122000
    },
    {
      "epoch": 1.8752391886720245,
      "grad_norm": 0.007920161820948124,
      "learning_rate": 7.498507462686567e-06,
      "loss": 0.0157,
      "step": 122500
    },
    {
      "epoch": 1.8828932261768083,
      "grad_norm": 0.004214418586343527,
      "learning_rate": 7.447480545988009e-06,
      "loss": 0.016,
      "step": 123000
    },
    {
      "epoch": 1.890547263681592,
      "grad_norm": 0.012041901238262653,
      "learning_rate": 7.39645362928945e-06,
      "loss": 0.019,
      "step": 123500
    },
    {
      "epoch": 1.8982013011863759,
      "grad_norm": 0.016335323452949524,
      "learning_rate": 7.345426712590891e-06,
      "loss": 0.0081,
      "step": 124000
    },
    {
      "epoch": 1.9058553386911596,
      "grad_norm": 0.0661420226097107,
      "learning_rate": 7.294399795892334e-06,
      "loss": 0.0145,
      "step": 124500
    },
    {
      "epoch": 1.9135093761959434,
      "grad_norm": 0.001160134794190526,
      "learning_rate": 7.243372879193775e-06,
      "loss": 0.0154,
      "step": 125000
    },
    {
      "epoch": 1.9211634137007272,
      "grad_norm": 0.013036927208304405,
      "learning_rate": 7.192345962495216e-06,
      "loss": 0.0147,
      "step": 125500
    },
    {
      "epoch": 1.928817451205511,
      "grad_norm": 0.017445063218474388,
      "learning_rate": 7.141319045796658e-06,
      "loss": 0.011,
      "step": 126000
    },
    {
      "epoch": 1.9364714887102947,
      "grad_norm": 0.00024701544316485524,
      "learning_rate": 7.090292129098099e-06,
      "loss": 0.0122,
      "step": 126500
    },
    {
      "epoch": 1.9441255262150785,
      "grad_norm": 0.0004125712439417839,
      "learning_rate": 7.0392652123995405e-06,
      "loss": 0.0178,
      "step": 127000
    },
    {
      "epoch": 1.9517795637198623,
      "grad_norm": 0.03389019891619682,
      "learning_rate": 6.988238295700983e-06,
      "loss": 0.0168,
      "step": 127500
    },
    {
      "epoch": 1.959433601224646,
      "grad_norm": 0.0003631215076893568,
      "learning_rate": 6.937211379002424e-06,
      "loss": 0.0121,
      "step": 128000
    },
    {
      "epoch": 1.9670876387294298,
      "grad_norm": 0.011440320871770382,
      "learning_rate": 6.8861844623038654e-06,
      "loss": 0.0175,
      "step": 128500
    },
    {
      "epoch": 1.9747416762342136,
      "grad_norm": 0.01832323893904686,
      "learning_rate": 6.835157545605307e-06,
      "loss": 0.0148,
      "step": 129000
    },
    {
      "epoch": 1.9823957137389974,
      "grad_norm": 0.013129415921866894,
      "learning_rate": 6.7841306289067485e-06,
      "loss": 0.014,
      "step": 129500
    },
    {
      "epoch": 1.9900497512437811,
      "grad_norm": 0.03721408545970917,
      "learning_rate": 6.7331037122081896e-06,
      "loss": 0.0184,
      "step": 130000
    },
    {
      "epoch": 1.997703788748565,
      "grad_norm": 0.00492901261895895,
      "learning_rate": 6.682076795509632e-06,
      "loss": 0.014,
      "step": 130500
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9974665135859165,
      "eval_f1": 0.9974665018662731,
      "eval_loss": 0.014863661490380764,
      "eval_runtime": 588.7113,
      "eval_samples_per_second": 221.925,
      "eval_steps_per_second": 27.742,
      "step": 130650
    }
  ],
  "logging_steps": 500,
  "max_steps": 195975,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7080625714367053e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
