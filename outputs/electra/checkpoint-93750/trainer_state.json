{
  "best_global_step": 31250,
  "best_metric": 0.0019,
  "best_model_checkpoint": "./outputs/electra/checkpoint-31250",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 93750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00032,
      "grad_norm": 0.7916929125785828,
      "learning_rate": 1.9998080000000003e-05,
      "loss": 0.6855,
      "step": 10
    },
    {
      "epoch": 0.00064,
      "grad_norm": 0.6367036700248718,
      "learning_rate": 1.9995946666666668e-05,
      "loss": 0.6777,
      "step": 20
    },
    {
      "epoch": 0.00096,
      "grad_norm": 0.5531640648841858,
      "learning_rate": 1.9993813333333334e-05,
      "loss": 0.6718,
      "step": 30
    },
    {
      "epoch": 0.00128,
      "grad_norm": 0.7439553141593933,
      "learning_rate": 1.9991680000000002e-05,
      "loss": 0.6617,
      "step": 40
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.6936180591583252,
      "learning_rate": 1.9989546666666668e-05,
      "loss": 0.6527,
      "step": 50
    },
    {
      "epoch": 0.00192,
      "grad_norm": 0.5606865286827087,
      "learning_rate": 1.9987413333333333e-05,
      "loss": 0.6471,
      "step": 60
    },
    {
      "epoch": 0.00224,
      "grad_norm": 0.6535356044769287,
      "learning_rate": 1.9985280000000002e-05,
      "loss": 0.6314,
      "step": 70
    },
    {
      "epoch": 0.00256,
      "grad_norm": 0.7020179033279419,
      "learning_rate": 1.9983146666666667e-05,
      "loss": 0.6232,
      "step": 80
    },
    {
      "epoch": 0.00288,
      "grad_norm": 0.8060786128044128,
      "learning_rate": 1.9981013333333336e-05,
      "loss": 0.6119,
      "step": 90
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.7023255228996277,
      "learning_rate": 1.997888e-05,
      "loss": 0.6111,
      "step": 100
    },
    {
      "epoch": 0.00352,
      "grad_norm": 1.1846437454223633,
      "learning_rate": 1.997674666666667e-05,
      "loss": 0.5963,
      "step": 110
    },
    {
      "epoch": 0.00384,
      "grad_norm": 0.9604929089546204,
      "learning_rate": 1.9974613333333336e-05,
      "loss": 0.5682,
      "step": 120
    },
    {
      "epoch": 0.00416,
      "grad_norm": 1.0910801887512207,
      "learning_rate": 1.997248e-05,
      "loss": 0.5584,
      "step": 130
    },
    {
      "epoch": 0.00448,
      "grad_norm": 0.8442382216453552,
      "learning_rate": 1.997034666666667e-05,
      "loss": 0.5436,
      "step": 140
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.8255011439323425,
      "learning_rate": 1.9968213333333335e-05,
      "loss": 0.5112,
      "step": 150
    },
    {
      "epoch": 0.00512,
      "grad_norm": 1.1174322366714478,
      "learning_rate": 1.996608e-05,
      "loss": 0.5038,
      "step": 160
    },
    {
      "epoch": 0.00544,
      "grad_norm": 0.9626955389976501,
      "learning_rate": 1.996394666666667e-05,
      "loss": 0.4792,
      "step": 170
    },
    {
      "epoch": 0.00576,
      "grad_norm": 1.2871685028076172,
      "learning_rate": 1.9961813333333335e-05,
      "loss": 0.4449,
      "step": 180
    },
    {
      "epoch": 0.00608,
      "grad_norm": 1.2564787864685059,
      "learning_rate": 1.995968e-05,
      "loss": 0.4272,
      "step": 190
    },
    {
      "epoch": 0.0064,
      "grad_norm": 1.1323405504226685,
      "learning_rate": 1.995754666666667e-05,
      "loss": 0.4176,
      "step": 200
    },
    {
      "epoch": 0.00672,
      "grad_norm": 1.2523456811904907,
      "learning_rate": 1.9955413333333334e-05,
      "loss": 0.3743,
      "step": 210
    },
    {
      "epoch": 0.00704,
      "grad_norm": 1.2042423486709595,
      "learning_rate": 1.9953280000000003e-05,
      "loss": 0.3543,
      "step": 220
    },
    {
      "epoch": 0.00736,
      "grad_norm": 1.0259288549423218,
      "learning_rate": 1.9951146666666668e-05,
      "loss": 0.3325,
      "step": 230
    },
    {
      "epoch": 0.00768,
      "grad_norm": 1.1664756536483765,
      "learning_rate": 1.9949013333333337e-05,
      "loss": 0.3118,
      "step": 240
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.8863046765327454,
      "learning_rate": 1.9946880000000002e-05,
      "loss": 0.2734,
      "step": 250
    },
    {
      "epoch": 0.00832,
      "grad_norm": 1.117430329322815,
      "learning_rate": 1.9944746666666668e-05,
      "loss": 0.2526,
      "step": 260
    },
    {
      "epoch": 0.00864,
      "grad_norm": 0.8380494713783264,
      "learning_rate": 1.9942613333333337e-05,
      "loss": 0.2308,
      "step": 270
    },
    {
      "epoch": 0.00896,
      "grad_norm": 1.0463247299194336,
      "learning_rate": 1.9940480000000002e-05,
      "loss": 0.2128,
      "step": 280
    },
    {
      "epoch": 0.00928,
      "grad_norm": 1.1021043062210083,
      "learning_rate": 1.9938346666666667e-05,
      "loss": 0.1813,
      "step": 290
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.8993824124336243,
      "learning_rate": 1.9936213333333333e-05,
      "loss": 0.1628,
      "step": 300
    },
    {
      "epoch": 0.00992,
      "grad_norm": 0.8652257919311523,
      "learning_rate": 1.993408e-05,
      "loss": 0.1503,
      "step": 310
    },
    {
      "epoch": 0.01024,
      "grad_norm": 0.7799456715583801,
      "learning_rate": 1.9931946666666667e-05,
      "loss": 0.1449,
      "step": 320
    },
    {
      "epoch": 0.01056,
      "grad_norm": 1.0566357374191284,
      "learning_rate": 1.9929813333333336e-05,
      "loss": 0.1221,
      "step": 330
    },
    {
      "epoch": 0.01088,
      "grad_norm": 0.9498649835586548,
      "learning_rate": 1.992768e-05,
      "loss": 0.1425,
      "step": 340
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.6723945140838623,
      "learning_rate": 1.992554666666667e-05,
      "loss": 0.1356,
      "step": 350
    },
    {
      "epoch": 0.01152,
      "grad_norm": 0.6155112385749817,
      "learning_rate": 1.9923413333333335e-05,
      "loss": 0.1074,
      "step": 360
    },
    {
      "epoch": 0.01184,
      "grad_norm": 0.7830743789672852,
      "learning_rate": 1.992128e-05,
      "loss": 0.1123,
      "step": 370
    },
    {
      "epoch": 0.01216,
      "grad_norm": 0.6570001244544983,
      "learning_rate": 1.991914666666667e-05,
      "loss": 0.0707,
      "step": 380
    },
    {
      "epoch": 0.01248,
      "grad_norm": 0.7558070421218872,
      "learning_rate": 1.9917013333333335e-05,
      "loss": 0.1148,
      "step": 390
    },
    {
      "epoch": 0.0128,
      "grad_norm": 1.0655313730239868,
      "learning_rate": 1.991488e-05,
      "loss": 0.0865,
      "step": 400
    },
    {
      "epoch": 0.01312,
      "grad_norm": 0.7005034685134888,
      "learning_rate": 1.991274666666667e-05,
      "loss": 0.0879,
      "step": 410
    },
    {
      "epoch": 0.01344,
      "grad_norm": 0.5084385275840759,
      "learning_rate": 1.9910613333333334e-05,
      "loss": 0.0721,
      "step": 420
    },
    {
      "epoch": 0.01376,
      "grad_norm": 0.5171892046928406,
      "learning_rate": 1.990848e-05,
      "loss": 0.0432,
      "step": 430
    },
    {
      "epoch": 0.01408,
      "grad_norm": 0.4089224338531494,
      "learning_rate": 1.9906346666666668e-05,
      "loss": 0.063,
      "step": 440
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.6597904562950134,
      "learning_rate": 1.9904213333333337e-05,
      "loss": 0.0657,
      "step": 450
    },
    {
      "epoch": 0.01472,
      "grad_norm": 0.3183935880661011,
      "learning_rate": 1.9902080000000002e-05,
      "loss": 0.0421,
      "step": 460
    },
    {
      "epoch": 0.01504,
      "grad_norm": 1.065425157546997,
      "learning_rate": 1.9899946666666668e-05,
      "loss": 0.0629,
      "step": 470
    },
    {
      "epoch": 0.01536,
      "grad_norm": 0.540303111076355,
      "learning_rate": 1.9897813333333337e-05,
      "loss": 0.0383,
      "step": 480
    },
    {
      "epoch": 0.01568,
      "grad_norm": 0.2927817106246948,
      "learning_rate": 1.9895680000000002e-05,
      "loss": 0.0718,
      "step": 490
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.573326587677002,
      "learning_rate": 1.9893546666666667e-05,
      "loss": 0.0507,
      "step": 500
    },
    {
      "epoch": 0.01632,
      "grad_norm": 0.2342994511127472,
      "learning_rate": 1.9891413333333336e-05,
      "loss": 0.044,
      "step": 510
    },
    {
      "epoch": 0.01664,
      "grad_norm": 0.6105933785438538,
      "learning_rate": 1.988928e-05,
      "loss": 0.0532,
      "step": 520
    },
    {
      "epoch": 0.01696,
      "grad_norm": 0.23890171945095062,
      "learning_rate": 1.9887146666666667e-05,
      "loss": 0.0262,
      "step": 530
    },
    {
      "epoch": 0.01728,
      "grad_norm": 0.38232922554016113,
      "learning_rate": 1.9885013333333336e-05,
      "loss": 0.0331,
      "step": 540
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.2959619462490082,
      "learning_rate": 1.988288e-05,
      "loss": 0.0372,
      "step": 550
    },
    {
      "epoch": 0.01792,
      "grad_norm": 0.24557681381702423,
      "learning_rate": 1.988074666666667e-05,
      "loss": 0.0352,
      "step": 560
    },
    {
      "epoch": 0.01824,
      "grad_norm": 0.21641962230205536,
      "learning_rate": 1.9878613333333335e-05,
      "loss": 0.0271,
      "step": 570
    },
    {
      "epoch": 0.01856,
      "grad_norm": 0.534761905670166,
      "learning_rate": 1.9876480000000004e-05,
      "loss": 0.0301,
      "step": 580
    },
    {
      "epoch": 0.01888,
      "grad_norm": 0.18481729924678802,
      "learning_rate": 1.987434666666667e-05,
      "loss": 0.0237,
      "step": 590
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.15710829198360443,
      "learning_rate": 1.9872213333333335e-05,
      "loss": 0.0601,
      "step": 600
    },
    {
      "epoch": 0.01952,
      "grad_norm": 0.21682587265968323,
      "learning_rate": 1.9870080000000003e-05,
      "loss": 0.0893,
      "step": 610
    },
    {
      "epoch": 0.01984,
      "grad_norm": 0.9615607261657715,
      "learning_rate": 1.986794666666667e-05,
      "loss": 0.0311,
      "step": 620
    },
    {
      "epoch": 0.02016,
      "grad_norm": 0.15439552068710327,
      "learning_rate": 1.9865813333333334e-05,
      "loss": 0.0429,
      "step": 630
    },
    {
      "epoch": 0.02048,
      "grad_norm": 0.1854565590620041,
      "learning_rate": 1.986368e-05,
      "loss": 0.0319,
      "step": 640
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.15710142254829407,
      "learning_rate": 1.9861546666666668e-05,
      "loss": 0.0264,
      "step": 650
    },
    {
      "epoch": 0.02112,
      "grad_norm": 0.14998166263103485,
      "learning_rate": 1.9859413333333334e-05,
      "loss": 0.0485,
      "step": 660
    },
    {
      "epoch": 0.02144,
      "grad_norm": 0.1583578735589981,
      "learning_rate": 1.9857280000000002e-05,
      "loss": 0.0142,
      "step": 670
    },
    {
      "epoch": 0.02176,
      "grad_norm": 0.13278180360794067,
      "learning_rate": 1.9855146666666668e-05,
      "loss": 0.0218,
      "step": 680
    },
    {
      "epoch": 0.02208,
      "grad_norm": 0.5469614863395691,
      "learning_rate": 1.9853013333333337e-05,
      "loss": 0.0506,
      "step": 690
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.17524629831314087,
      "learning_rate": 1.9850880000000002e-05,
      "loss": 0.0751,
      "step": 700
    },
    {
      "epoch": 0.02272,
      "grad_norm": 0.18082690238952637,
      "learning_rate": 1.9848746666666667e-05,
      "loss": 0.0199,
      "step": 710
    },
    {
      "epoch": 0.02304,
      "grad_norm": 0.10059069842100143,
      "learning_rate": 1.9846613333333336e-05,
      "loss": 0.0222,
      "step": 720
    },
    {
      "epoch": 0.02336,
      "grad_norm": 0.12895303964614868,
      "learning_rate": 1.984448e-05,
      "loss": 0.0239,
      "step": 730
    },
    {
      "epoch": 0.02368,
      "grad_norm": 0.1213865727186203,
      "learning_rate": 1.9842346666666667e-05,
      "loss": 0.02,
      "step": 740
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.11431650817394257,
      "learning_rate": 1.9840213333333336e-05,
      "loss": 0.086,
      "step": 750
    },
    {
      "epoch": 0.02432,
      "grad_norm": 0.16331343352794647,
      "learning_rate": 1.983808e-05,
      "loss": 0.023,
      "step": 760
    },
    {
      "epoch": 0.02464,
      "grad_norm": 0.11230385303497314,
      "learning_rate": 1.9835946666666666e-05,
      "loss": 0.0249,
      "step": 770
    },
    {
      "epoch": 0.02496,
      "grad_norm": 0.11259903758764267,
      "learning_rate": 1.9833813333333335e-05,
      "loss": 0.012,
      "step": 780
    },
    {
      "epoch": 0.02528,
      "grad_norm": 0.14946231245994568,
      "learning_rate": 1.983168e-05,
      "loss": 0.0129,
      "step": 790
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.11629552394151688,
      "learning_rate": 1.982954666666667e-05,
      "loss": 0.0193,
      "step": 800
    },
    {
      "epoch": 0.02592,
      "grad_norm": 0.10262305289506912,
      "learning_rate": 1.9827413333333335e-05,
      "loss": 0.0171,
      "step": 810
    },
    {
      "epoch": 0.02624,
      "grad_norm": 0.12439712882041931,
      "learning_rate": 1.9825280000000003e-05,
      "loss": 0.0127,
      "step": 820
    },
    {
      "epoch": 0.02656,
      "grad_norm": 1.0495469570159912,
      "learning_rate": 1.982314666666667e-05,
      "loss": 0.0309,
      "step": 830
    },
    {
      "epoch": 0.02688,
      "grad_norm": 0.8894426822662354,
      "learning_rate": 1.9821013333333334e-05,
      "loss": 0.0134,
      "step": 840
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.10989057272672653,
      "learning_rate": 1.9818880000000003e-05,
      "loss": 0.0093,
      "step": 850
    },
    {
      "epoch": 0.02752,
      "grad_norm": 0.2434721291065216,
      "learning_rate": 1.981674666666667e-05,
      "loss": 0.0108,
      "step": 860
    },
    {
      "epoch": 0.02784,
      "grad_norm": 0.09293822944164276,
      "learning_rate": 1.9814613333333334e-05,
      "loss": 0.0103,
      "step": 870
    },
    {
      "epoch": 0.02816,
      "grad_norm": 0.088924340903759,
      "learning_rate": 1.981248e-05,
      "loss": 0.0112,
      "step": 880
    },
    {
      "epoch": 0.02848,
      "grad_norm": 0.09550101310014725,
      "learning_rate": 1.9810346666666668e-05,
      "loss": 0.0305,
      "step": 890
    },
    {
      "epoch": 0.0288,
      "grad_norm": 2.501784086227417,
      "learning_rate": 1.9808213333333333e-05,
      "loss": 0.0515,
      "step": 900
    },
    {
      "epoch": 0.02912,
      "grad_norm": 0.1589222401380539,
      "learning_rate": 1.9806080000000002e-05,
      "loss": 0.0065,
      "step": 910
    },
    {
      "epoch": 0.02944,
      "grad_norm": 0.10622238367795944,
      "learning_rate": 1.980394666666667e-05,
      "loss": 0.0248,
      "step": 920
    },
    {
      "epoch": 0.02976,
      "grad_norm": 0.867138147354126,
      "learning_rate": 1.9801813333333336e-05,
      "loss": 0.0084,
      "step": 930
    },
    {
      "epoch": 0.03008,
      "grad_norm": 0.07591813057661057,
      "learning_rate": 1.979968e-05,
      "loss": 0.0395,
      "step": 940
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.09684564173221588,
      "learning_rate": 1.979754666666667e-05,
      "loss": 0.006,
      "step": 950
    },
    {
      "epoch": 0.03072,
      "grad_norm": 0.06338373571634293,
      "learning_rate": 1.9795413333333336e-05,
      "loss": 0.0064,
      "step": 960
    },
    {
      "epoch": 0.03104,
      "grad_norm": 0.08325731754302979,
      "learning_rate": 1.979328e-05,
      "loss": 0.0179,
      "step": 970
    },
    {
      "epoch": 0.03136,
      "grad_norm": 0.16807758808135986,
      "learning_rate": 1.9791146666666666e-05,
      "loss": 0.005,
      "step": 980
    },
    {
      "epoch": 0.03168,
      "grad_norm": 0.05187332257628441,
      "learning_rate": 1.9789013333333335e-05,
      "loss": 0.0069,
      "step": 990
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.06093372777104378,
      "learning_rate": 1.978688e-05,
      "loss": 0.0053,
      "step": 1000
    },
    {
      "epoch": 0.03232,
      "grad_norm": 0.06739725172519684,
      "learning_rate": 1.978474666666667e-05,
      "loss": 0.0367,
      "step": 1010
    },
    {
      "epoch": 0.03264,
      "grad_norm": 0.09026747941970825,
      "learning_rate": 1.9782613333333335e-05,
      "loss": 0.0295,
      "step": 1020
    },
    {
      "epoch": 0.03296,
      "grad_norm": 0.08101535588502884,
      "learning_rate": 1.9780480000000003e-05,
      "loss": 0.0046,
      "step": 1030
    },
    {
      "epoch": 0.03328,
      "grad_norm": 0.136461541056633,
      "learning_rate": 1.977834666666667e-05,
      "loss": 0.006,
      "step": 1040
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.05358695238828659,
      "learning_rate": 1.9776213333333334e-05,
      "loss": 0.007,
      "step": 1050
    },
    {
      "epoch": 0.03392,
      "grad_norm": 0.16639935970306396,
      "learning_rate": 1.9774080000000003e-05,
      "loss": 0.004,
      "step": 1060
    },
    {
      "epoch": 0.03424,
      "grad_norm": 0.04723183438181877,
      "learning_rate": 1.977194666666667e-05,
      "loss": 0.0068,
      "step": 1070
    },
    {
      "epoch": 0.03456,
      "grad_norm": 0.0643983781337738,
      "learning_rate": 1.9769813333333334e-05,
      "loss": 0.0152,
      "step": 1080
    },
    {
      "epoch": 0.03488,
      "grad_norm": 0.12923018634319305,
      "learning_rate": 1.9767680000000002e-05,
      "loss": 0.009,
      "step": 1090
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.056507814675569534,
      "learning_rate": 1.9765546666666668e-05,
      "loss": 0.0092,
      "step": 1100
    },
    {
      "epoch": 0.03552,
      "grad_norm": 0.048775576055049896,
      "learning_rate": 1.9763413333333333e-05,
      "loss": 0.0177,
      "step": 1110
    },
    {
      "epoch": 0.03584,
      "grad_norm": 0.06011013686656952,
      "learning_rate": 1.9761280000000002e-05,
      "loss": 0.0113,
      "step": 1120
    },
    {
      "epoch": 0.03616,
      "grad_norm": 0.16646501421928406,
      "learning_rate": 1.9759146666666667e-05,
      "loss": 0.0042,
      "step": 1130
    },
    {
      "epoch": 0.03648,
      "grad_norm": 0.05489866062998772,
      "learning_rate": 1.9757013333333336e-05,
      "loss": 0.0032,
      "step": 1140
    },
    {
      "epoch": 0.0368,
      "grad_norm": 3.0370240211486816,
      "learning_rate": 1.975488e-05,
      "loss": 0.0605,
      "step": 1150
    },
    {
      "epoch": 0.03712,
      "grad_norm": 0.0499749481678009,
      "learning_rate": 1.975274666666667e-05,
      "loss": 0.0324,
      "step": 1160
    },
    {
      "epoch": 0.03744,
      "grad_norm": 0.06240769475698471,
      "learning_rate": 1.9750613333333336e-05,
      "loss": 0.0046,
      "step": 1170
    },
    {
      "epoch": 0.03776,
      "grad_norm": 0.05660512298345566,
      "learning_rate": 1.974848e-05,
      "loss": 0.0129,
      "step": 1180
    },
    {
      "epoch": 0.03808,
      "grad_norm": 0.06467002630233765,
      "learning_rate": 1.974634666666667e-05,
      "loss": 0.0513,
      "step": 1190
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.030954523012042046,
      "learning_rate": 1.9744213333333335e-05,
      "loss": 0.0138,
      "step": 1200
    },
    {
      "epoch": 0.03872,
      "grad_norm": 0.04721156135201454,
      "learning_rate": 1.974208e-05,
      "loss": 0.0429,
      "step": 1210
    },
    {
      "epoch": 0.03904,
      "grad_norm": 0.04443025961518288,
      "learning_rate": 1.9739946666666666e-05,
      "loss": 0.0134,
      "step": 1220
    },
    {
      "epoch": 0.03936,
      "grad_norm": 0.06255178153514862,
      "learning_rate": 1.9737813333333335e-05,
      "loss": 0.0155,
      "step": 1230
    },
    {
      "epoch": 0.03968,
      "grad_norm": 0.04579856991767883,
      "learning_rate": 1.973568e-05,
      "loss": 0.0602,
      "step": 1240
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.10220381617546082,
      "learning_rate": 1.973354666666667e-05,
      "loss": 0.0031,
      "step": 1250
    },
    {
      "epoch": 0.04032,
      "grad_norm": 0.08094366639852524,
      "learning_rate": 1.9731413333333334e-05,
      "loss": 0.0043,
      "step": 1260
    },
    {
      "epoch": 0.04064,
      "grad_norm": 0.3253929316997528,
      "learning_rate": 1.9729280000000003e-05,
      "loss": 0.0039,
      "step": 1270
    },
    {
      "epoch": 0.04096,
      "grad_norm": 0.04276096075773239,
      "learning_rate": 1.972714666666667e-05,
      "loss": 0.0037,
      "step": 1280
    },
    {
      "epoch": 0.04128,
      "grad_norm": 0.03864404186606407,
      "learning_rate": 1.9725013333333337e-05,
      "loss": 0.0328,
      "step": 1290
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.03268224000930786,
      "learning_rate": 1.9722880000000003e-05,
      "loss": 0.003,
      "step": 1300
    },
    {
      "epoch": 0.04192,
      "grad_norm": 0.03312001749873161,
      "learning_rate": 1.9720746666666668e-05,
      "loss": 0.0817,
      "step": 1310
    },
    {
      "epoch": 0.04224,
      "grad_norm": 0.03155967965722084,
      "learning_rate": 1.9718613333333333e-05,
      "loss": 0.0027,
      "step": 1320
    },
    {
      "epoch": 0.04256,
      "grad_norm": 0.04430381581187248,
      "learning_rate": 1.9716480000000002e-05,
      "loss": 0.0026,
      "step": 1330
    },
    {
      "epoch": 0.04288,
      "grad_norm": 0.027502775192260742,
      "learning_rate": 1.9714346666666667e-05,
      "loss": 0.0607,
      "step": 1340
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.2147568166255951,
      "learning_rate": 1.9712213333333333e-05,
      "loss": 0.0033,
      "step": 1350
    },
    {
      "epoch": 0.04352,
      "grad_norm": 0.8214404582977295,
      "learning_rate": 1.971008e-05,
      "loss": 0.005,
      "step": 1360
    },
    {
      "epoch": 0.04384,
      "grad_norm": 0.033950988203287125,
      "learning_rate": 1.9707946666666667e-05,
      "loss": 0.0121,
      "step": 1370
    },
    {
      "epoch": 0.04416,
      "grad_norm": 0.04016464576125145,
      "learning_rate": 1.9705813333333336e-05,
      "loss": 0.0026,
      "step": 1380
    },
    {
      "epoch": 0.04448,
      "grad_norm": 0.052217211574316025,
      "learning_rate": 1.970368e-05,
      "loss": 0.0198,
      "step": 1390
    },
    {
      "epoch": 0.0448,
      "grad_norm": 1.4847239255905151,
      "learning_rate": 1.970154666666667e-05,
      "loss": 0.0406,
      "step": 1400
    },
    {
      "epoch": 0.04512,
      "grad_norm": 0.028942754492163658,
      "learning_rate": 1.9699413333333335e-05,
      "loss": 0.0029,
      "step": 1410
    },
    {
      "epoch": 0.04544,
      "grad_norm": 0.0554317831993103,
      "learning_rate": 1.969728e-05,
      "loss": 0.0027,
      "step": 1420
    },
    {
      "epoch": 0.04576,
      "grad_norm": 0.03307592496275902,
      "learning_rate": 1.969514666666667e-05,
      "loss": 0.0025,
      "step": 1430
    },
    {
      "epoch": 0.04608,
      "grad_norm": 0.030631713569164276,
      "learning_rate": 1.9693013333333335e-05,
      "loss": 0.0023,
      "step": 1440
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.02598468028008938,
      "learning_rate": 1.969088e-05,
      "loss": 0.0041,
      "step": 1450
    },
    {
      "epoch": 0.04672,
      "grad_norm": 0.026790639385581017,
      "learning_rate": 1.968874666666667e-05,
      "loss": 0.0178,
      "step": 1460
    },
    {
      "epoch": 0.04704,
      "grad_norm": 0.028589995577931404,
      "learning_rate": 1.9686613333333334e-05,
      "loss": 0.0022,
      "step": 1470
    },
    {
      "epoch": 0.04736,
      "grad_norm": 1.8557308912277222,
      "learning_rate": 1.9684480000000003e-05,
      "loss": 0.0092,
      "step": 1480
    },
    {
      "epoch": 0.04768,
      "grad_norm": 0.026367047801613808,
      "learning_rate": 1.968234666666667e-05,
      "loss": 0.0037,
      "step": 1490
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.10071670264005661,
      "learning_rate": 1.9680213333333337e-05,
      "loss": 0.0027,
      "step": 1500
    },
    {
      "epoch": 0.04832,
      "grad_norm": 0.1565462052822113,
      "learning_rate": 1.9678080000000003e-05,
      "loss": 0.003,
      "step": 1510
    },
    {
      "epoch": 0.04864,
      "grad_norm": 0.11456327885389328,
      "learning_rate": 1.9675946666666668e-05,
      "loss": 0.0382,
      "step": 1520
    },
    {
      "epoch": 0.04896,
      "grad_norm": 1.3305660486221313,
      "learning_rate": 1.9673813333333337e-05,
      "loss": 0.0441,
      "step": 1530
    },
    {
      "epoch": 0.04928,
      "grad_norm": 0.7329368591308594,
      "learning_rate": 1.9671680000000002e-05,
      "loss": 0.0035,
      "step": 1540
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.02801179140806198,
      "learning_rate": 1.9669546666666667e-05,
      "loss": 0.0122,
      "step": 1550
    },
    {
      "epoch": 0.04992,
      "grad_norm": 1.0898000001907349,
      "learning_rate": 1.9667413333333333e-05,
      "loss": 0.0262,
      "step": 1560
    },
    {
      "epoch": 0.05024,
      "grad_norm": 0.032045938074588776,
      "learning_rate": 1.966528e-05,
      "loss": 0.0357,
      "step": 1570
    },
    {
      "epoch": 0.05056,
      "grad_norm": 0.028350071981549263,
      "learning_rate": 1.9663146666666667e-05,
      "loss": 0.003,
      "step": 1580
    },
    {
      "epoch": 0.05088,
      "grad_norm": 0.045762646943330765,
      "learning_rate": 1.9661013333333336e-05,
      "loss": 0.0153,
      "step": 1590
    },
    {
      "epoch": 0.0512,
      "grad_norm": 1.8240046501159668,
      "learning_rate": 1.965888e-05,
      "loss": 0.0209,
      "step": 1600
    },
    {
      "epoch": 0.05152,
      "grad_norm": 0.09187998622655869,
      "learning_rate": 1.965674666666667e-05,
      "loss": 0.0451,
      "step": 1610
    },
    {
      "epoch": 0.05184,
      "grad_norm": 0.03587576746940613,
      "learning_rate": 1.9654613333333335e-05,
      "loss": 0.0264,
      "step": 1620
    },
    {
      "epoch": 0.05216,
      "grad_norm": 1.4958360195159912,
      "learning_rate": 1.9652480000000004e-05,
      "loss": 0.0427,
      "step": 1630
    },
    {
      "epoch": 0.05248,
      "grad_norm": 0.03007919155061245,
      "learning_rate": 1.965034666666667e-05,
      "loss": 0.0454,
      "step": 1640
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.029834292829036713,
      "learning_rate": 1.9648213333333335e-05,
      "loss": 0.0019,
      "step": 1650
    },
    {
      "epoch": 0.05312,
      "grad_norm": 0.026264341548085213,
      "learning_rate": 1.964608e-05,
      "loss": 0.0034,
      "step": 1660
    },
    {
      "epoch": 0.05344,
      "grad_norm": 2.714628219604492,
      "learning_rate": 1.964394666666667e-05,
      "loss": 0.051,
      "step": 1670
    },
    {
      "epoch": 0.05376,
      "grad_norm": 0.03562018647789955,
      "learning_rate": 1.9641813333333334e-05,
      "loss": 0.0018,
      "step": 1680
    },
    {
      "epoch": 0.05408,
      "grad_norm": 0.27320829033851624,
      "learning_rate": 1.963968e-05,
      "loss": 0.0434,
      "step": 1690
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.024947062134742737,
      "learning_rate": 1.963754666666667e-05,
      "loss": 0.0302,
      "step": 1700
    },
    {
      "epoch": 0.05472,
      "grad_norm": 0.02544686198234558,
      "learning_rate": 1.9635413333333334e-05,
      "loss": 0.0261,
      "step": 1710
    },
    {
      "epoch": 0.05504,
      "grad_norm": 0.10593628138303757,
      "learning_rate": 1.9633280000000003e-05,
      "loss": 0.0146,
      "step": 1720
    },
    {
      "epoch": 0.05536,
      "grad_norm": 0.04619676619768143,
      "learning_rate": 1.9631146666666668e-05,
      "loss": 0.002,
      "step": 1730
    },
    {
      "epoch": 0.05568,
      "grad_norm": 0.03386460244655609,
      "learning_rate": 1.9629013333333337e-05,
      "loss": 0.0035,
      "step": 1740
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.0689447671175003,
      "learning_rate": 1.9626880000000002e-05,
      "loss": 0.0026,
      "step": 1750
    },
    {
      "epoch": 0.05632,
      "grad_norm": 0.03705627843737602,
      "learning_rate": 1.9624746666666667e-05,
      "loss": 0.0017,
      "step": 1760
    },
    {
      "epoch": 0.05664,
      "grad_norm": 0.022571515291929245,
      "learning_rate": 1.9622613333333336e-05,
      "loss": 0.0016,
      "step": 1770
    },
    {
      "epoch": 0.05696,
      "grad_norm": 0.022180834785103798,
      "learning_rate": 1.962048e-05,
      "loss": 0.0019,
      "step": 1780
    },
    {
      "epoch": 0.05728,
      "grad_norm": 0.025813333690166473,
      "learning_rate": 1.9618346666666667e-05,
      "loss": 0.0015,
      "step": 1790
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.022175097838044167,
      "learning_rate": 1.9616213333333336e-05,
      "loss": 0.0029,
      "step": 1800
    },
    {
      "epoch": 0.05792,
      "grad_norm": 0.03431910276412964,
      "learning_rate": 1.961408e-05,
      "loss": 0.0108,
      "step": 1810
    },
    {
      "epoch": 0.05824,
      "grad_norm": 0.021085456013679504,
      "learning_rate": 1.9611946666666666e-05,
      "loss": 0.0027,
      "step": 1820
    },
    {
      "epoch": 0.05856,
      "grad_norm": 0.020569713786244392,
      "learning_rate": 1.9609813333333335e-05,
      "loss": 0.0092,
      "step": 1830
    },
    {
      "epoch": 0.05888,
      "grad_norm": 0.05730370804667473,
      "learning_rate": 1.960768e-05,
      "loss": 0.0434,
      "step": 1840
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.03062245436012745,
      "learning_rate": 1.960554666666667e-05,
      "loss": 0.0062,
      "step": 1850
    },
    {
      "epoch": 0.05952,
      "grad_norm": 0.02148696593940258,
      "learning_rate": 1.9603413333333335e-05,
      "loss": 0.0447,
      "step": 1860
    },
    {
      "epoch": 0.05984,
      "grad_norm": 0.0285947285592556,
      "learning_rate": 1.9601280000000004e-05,
      "loss": 0.0251,
      "step": 1870
    },
    {
      "epoch": 0.06016,
      "grad_norm": 0.023342393338680267,
      "learning_rate": 1.959914666666667e-05,
      "loss": 0.0049,
      "step": 1880
    },
    {
      "epoch": 0.06048,
      "grad_norm": 0.02044246345758438,
      "learning_rate": 1.9597013333333334e-05,
      "loss": 0.0256,
      "step": 1890
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.033173032104969025,
      "learning_rate": 1.959488e-05,
      "loss": 0.0016,
      "step": 1900
    },
    {
      "epoch": 0.06112,
      "grad_norm": 0.022216416895389557,
      "learning_rate": 1.959274666666667e-05,
      "loss": 0.0013,
      "step": 1910
    },
    {
      "epoch": 0.06144,
      "grad_norm": 0.021020453423261642,
      "learning_rate": 1.9590613333333334e-05,
      "loss": 0.0119,
      "step": 1920
    },
    {
      "epoch": 0.06176,
      "grad_norm": 0.02025221660733223,
      "learning_rate": 1.958848e-05,
      "loss": 0.0071,
      "step": 1930
    },
    {
      "epoch": 0.06208,
      "grad_norm": 0.018366200849413872,
      "learning_rate": 1.9586346666666668e-05,
      "loss": 0.0013,
      "step": 1940
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.48489195108413696,
      "learning_rate": 1.9584213333333337e-05,
      "loss": 0.0025,
      "step": 1950
    },
    {
      "epoch": 0.06272,
      "grad_norm": 0.02514917589724064,
      "learning_rate": 1.9582080000000002e-05,
      "loss": 0.0015,
      "step": 1960
    },
    {
      "epoch": 0.06304,
      "grad_norm": 0.4726954996585846,
      "learning_rate": 1.957994666666667e-05,
      "loss": 0.0287,
      "step": 1970
    },
    {
      "epoch": 0.06336,
      "grad_norm": 0.018649538978934288,
      "learning_rate": 1.9577813333333336e-05,
      "loss": 0.0725,
      "step": 1980
    },
    {
      "epoch": 0.06368,
      "grad_norm": 0.020996803417801857,
      "learning_rate": 1.957568e-05,
      "loss": 0.0014,
      "step": 1990
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.02851143479347229,
      "learning_rate": 1.9573546666666667e-05,
      "loss": 0.0206,
      "step": 2000
    },
    {
      "epoch": 0.06432,
      "grad_norm": 0.020243721082806587,
      "learning_rate": 1.9571413333333336e-05,
      "loss": 0.0158,
      "step": 2010
    },
    {
      "epoch": 0.06464,
      "grad_norm": 0.024271873757243156,
      "learning_rate": 1.956928e-05,
      "loss": 0.0019,
      "step": 2020
    },
    {
      "epoch": 0.06496,
      "grad_norm": 0.02079026959836483,
      "learning_rate": 1.9567146666666667e-05,
      "loss": 0.0354,
      "step": 2030
    },
    {
      "epoch": 0.06528,
      "grad_norm": 3.10703182220459,
      "learning_rate": 1.9565013333333335e-05,
      "loss": 0.008,
      "step": 2040
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.018805956467986107,
      "learning_rate": 1.956288e-05,
      "loss": 0.0012,
      "step": 2050
    },
    {
      "epoch": 0.06592,
      "grad_norm": 0.2129475474357605,
      "learning_rate": 1.956074666666667e-05,
      "loss": 0.0157,
      "step": 2060
    },
    {
      "epoch": 0.06624,
      "grad_norm": 0.03252488747239113,
      "learning_rate": 1.9558613333333335e-05,
      "loss": 0.0016,
      "step": 2070
    },
    {
      "epoch": 0.06656,
      "grad_norm": 0.019005000591278076,
      "learning_rate": 1.9556480000000004e-05,
      "loss": 0.0015,
      "step": 2080
    },
    {
      "epoch": 0.06688,
      "grad_norm": 0.017800966277718544,
      "learning_rate": 1.955434666666667e-05,
      "loss": 0.0047,
      "step": 2090
    },
    {
      "epoch": 0.0672,
      "grad_norm": 1.3967605829238892,
      "learning_rate": 1.9552213333333334e-05,
      "loss": 0.1294,
      "step": 2100
    },
    {
      "epoch": 0.06752,
      "grad_norm": 0.13879798352718353,
      "learning_rate": 1.9550080000000003e-05,
      "loss": 0.003,
      "step": 2110
    },
    {
      "epoch": 0.06784,
      "grad_norm": 0.029326362535357475,
      "learning_rate": 1.954794666666667e-05,
      "loss": 0.022,
      "step": 2120
    },
    {
      "epoch": 0.06816,
      "grad_norm": 0.015684882178902626,
      "learning_rate": 1.9545813333333334e-05,
      "loss": 0.0153,
      "step": 2130
    },
    {
      "epoch": 0.06848,
      "grad_norm": 0.023718884214758873,
      "learning_rate": 1.9543680000000003e-05,
      "loss": 0.031,
      "step": 2140
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.027170952409505844,
      "learning_rate": 1.9541546666666668e-05,
      "loss": 0.0013,
      "step": 2150
    },
    {
      "epoch": 0.06912,
      "grad_norm": 0.019600331783294678,
      "learning_rate": 1.9539413333333333e-05,
      "loss": 0.0018,
      "step": 2160
    },
    {
      "epoch": 0.06944,
      "grad_norm": 3.593268394470215,
      "learning_rate": 1.9537280000000002e-05,
      "loss": 0.0375,
      "step": 2170
    },
    {
      "epoch": 0.06976,
      "grad_norm": 2.4364092350006104,
      "learning_rate": 1.9535146666666667e-05,
      "loss": 0.0482,
      "step": 2180
    },
    {
      "epoch": 0.07008,
      "grad_norm": 0.022774280980229378,
      "learning_rate": 1.9533013333333336e-05,
      "loss": 0.0014,
      "step": 2190
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.01800515130162239,
      "learning_rate": 1.953088e-05,
      "loss": 0.0367,
      "step": 2200
    },
    {
      "epoch": 0.07072,
      "grad_norm": 0.01736893691122532,
      "learning_rate": 1.952874666666667e-05,
      "loss": 0.0438,
      "step": 2210
    },
    {
      "epoch": 0.07104,
      "grad_norm": 0.02732962556183338,
      "learning_rate": 1.9526613333333336e-05,
      "loss": 0.0021,
      "step": 2220
    },
    {
      "epoch": 0.07136,
      "grad_norm": 0.02033153548836708,
      "learning_rate": 1.952448e-05,
      "loss": 0.0031,
      "step": 2230
    },
    {
      "epoch": 0.07168,
      "grad_norm": 0.02399560809135437,
      "learning_rate": 1.9522346666666667e-05,
      "loss": 0.0468,
      "step": 2240
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.041499972343444824,
      "learning_rate": 1.9520213333333335e-05,
      "loss": 0.0014,
      "step": 2250
    },
    {
      "epoch": 0.07232,
      "grad_norm": 0.020224029198288918,
      "learning_rate": 1.951808e-05,
      "loss": 0.0014,
      "step": 2260
    },
    {
      "epoch": 0.07264,
      "grad_norm": 0.024076426401734352,
      "learning_rate": 1.9515946666666666e-05,
      "loss": 0.0021,
      "step": 2270
    },
    {
      "epoch": 0.07296,
      "grad_norm": 0.030600812286138535,
      "learning_rate": 1.9513813333333335e-05,
      "loss": 0.0035,
      "step": 2280
    },
    {
      "epoch": 0.07328,
      "grad_norm": 0.028086500242352486,
      "learning_rate": 1.951168e-05,
      "loss": 0.0114,
      "step": 2290
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.025126054883003235,
      "learning_rate": 1.950954666666667e-05,
      "loss": 0.0013,
      "step": 2300
    },
    {
      "epoch": 0.07392,
      "grad_norm": 0.01625993475317955,
      "learning_rate": 1.9507413333333334e-05,
      "loss": 0.0021,
      "step": 2310
    },
    {
      "epoch": 0.07424,
      "grad_norm": 0.019603373482823372,
      "learning_rate": 1.9505280000000003e-05,
      "loss": 0.0013,
      "step": 2320
    },
    {
      "epoch": 0.07456,
      "grad_norm": 0.014337198808789253,
      "learning_rate": 1.950314666666667e-05,
      "loss": 0.0116,
      "step": 2330
    },
    {
      "epoch": 0.07488,
      "grad_norm": 0.267394095659256,
      "learning_rate": 1.9501013333333334e-05,
      "loss": 0.0021,
      "step": 2340
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.01450668927282095,
      "learning_rate": 1.9498880000000003e-05,
      "loss": 0.0012,
      "step": 2350
    },
    {
      "epoch": 0.07552,
      "grad_norm": 0.01691528968513012,
      "learning_rate": 1.9496746666666668e-05,
      "loss": 0.0021,
      "step": 2360
    },
    {
      "epoch": 0.07584,
      "grad_norm": 0.9844909906387329,
      "learning_rate": 1.9494613333333333e-05,
      "loss": 0.0149,
      "step": 2370
    },
    {
      "epoch": 0.07616,
      "grad_norm": 0.014073741622269154,
      "learning_rate": 1.9492480000000002e-05,
      "loss": 0.0078,
      "step": 2380
    },
    {
      "epoch": 0.07648,
      "grad_norm": 0.01599431410431862,
      "learning_rate": 1.9490346666666668e-05,
      "loss": 0.0008,
      "step": 2390
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.3826262354850769,
      "learning_rate": 1.9488213333333333e-05,
      "loss": 0.0304,
      "step": 2400
    },
    {
      "epoch": 0.07712,
      "grad_norm": 0.020896732807159424,
      "learning_rate": 1.948608e-05,
      "loss": 0.0009,
      "step": 2410
    },
    {
      "epoch": 0.07744,
      "grad_norm": 0.021991008892655373,
      "learning_rate": 1.948394666666667e-05,
      "loss": 0.0014,
      "step": 2420
    },
    {
      "epoch": 0.07776,
      "grad_norm": 0.01794723980128765,
      "learning_rate": 1.9481813333333336e-05,
      "loss": 0.0012,
      "step": 2430
    },
    {
      "epoch": 0.07808,
      "grad_norm": 0.02222856692969799,
      "learning_rate": 1.947968e-05,
      "loss": 0.0478,
      "step": 2440
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.029876623302698135,
      "learning_rate": 1.947754666666667e-05,
      "loss": 0.0013,
      "step": 2450
    },
    {
      "epoch": 0.07872,
      "grad_norm": 0.019566306844353676,
      "learning_rate": 1.9475413333333335e-05,
      "loss": 0.0072,
      "step": 2460
    },
    {
      "epoch": 0.07904,
      "grad_norm": 0.016232606023550034,
      "learning_rate": 1.947328e-05,
      "loss": 0.0011,
      "step": 2470
    },
    {
      "epoch": 0.07936,
      "grad_norm": 0.019533345475792885,
      "learning_rate": 1.947114666666667e-05,
      "loss": 0.001,
      "step": 2480
    },
    {
      "epoch": 0.07968,
      "grad_norm": 0.014436978846788406,
      "learning_rate": 1.9469013333333335e-05,
      "loss": 0.0814,
      "step": 2490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.014742105267941952,
      "learning_rate": 1.946688e-05,
      "loss": 0.0014,
      "step": 2500
    },
    {
      "epoch": 0.08032,
      "grad_norm": 0.017053669318556786,
      "learning_rate": 1.946474666666667e-05,
      "loss": 0.0013,
      "step": 2510
    },
    {
      "epoch": 0.08064,
      "grad_norm": 3.3552310466766357,
      "learning_rate": 1.9462613333333334e-05,
      "loss": 0.0406,
      "step": 2520
    },
    {
      "epoch": 0.08096,
      "grad_norm": 0.019815027713775635,
      "learning_rate": 1.9460480000000003e-05,
      "loss": 0.001,
      "step": 2530
    },
    {
      "epoch": 0.08128,
      "grad_norm": 0.016407696530222893,
      "learning_rate": 1.945834666666667e-05,
      "loss": 0.0011,
      "step": 2540
    },
    {
      "epoch": 0.0816,
      "grad_norm": 4.430270195007324,
      "learning_rate": 1.9456213333333337e-05,
      "loss": 0.0492,
      "step": 2550
    },
    {
      "epoch": 0.08192,
      "grad_norm": 0.017054356634616852,
      "learning_rate": 1.9454080000000003e-05,
      "loss": 0.0299,
      "step": 2560
    },
    {
      "epoch": 0.08224,
      "grad_norm": 0.01564926840364933,
      "learning_rate": 1.9451946666666668e-05,
      "loss": 0.001,
      "step": 2570
    },
    {
      "epoch": 0.08256,
      "grad_norm": 0.01211266778409481,
      "learning_rate": 1.9449813333333333e-05,
      "loss": 0.0028,
      "step": 2580
    },
    {
      "epoch": 0.08288,
      "grad_norm": 0.030441977083683014,
      "learning_rate": 1.9447680000000002e-05,
      "loss": 0.0009,
      "step": 2590
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.014259523712098598,
      "learning_rate": 1.9445546666666668e-05,
      "loss": 0.0019,
      "step": 2600
    },
    {
      "epoch": 0.08352,
      "grad_norm": 0.01486736349761486,
      "learning_rate": 1.9443413333333333e-05,
      "loss": 0.0008,
      "step": 2610
    },
    {
      "epoch": 0.08384,
      "grad_norm": 0.019732998684048653,
      "learning_rate": 1.944128e-05,
      "loss": 0.001,
      "step": 2620
    },
    {
      "epoch": 0.08416,
      "grad_norm": 1.4757492542266846,
      "learning_rate": 1.9439146666666667e-05,
      "loss": 0.0445,
      "step": 2630
    },
    {
      "epoch": 0.08448,
      "grad_norm": 0.029808634892106056,
      "learning_rate": 1.9437013333333336e-05,
      "loss": 0.0017,
      "step": 2640
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.23286180198192596,
      "learning_rate": 1.943488e-05,
      "loss": 0.0013,
      "step": 2650
    },
    {
      "epoch": 0.08512,
      "grad_norm": 0.011966769583523273,
      "learning_rate": 1.943274666666667e-05,
      "loss": 0.0019,
      "step": 2660
    },
    {
      "epoch": 0.08544,
      "grad_norm": 2.5691494941711426,
      "learning_rate": 1.9430613333333335e-05,
      "loss": 0.0385,
      "step": 2670
    },
    {
      "epoch": 0.08576,
      "grad_norm": 0.02186424657702446,
      "learning_rate": 1.942848e-05,
      "loss": 0.0009,
      "step": 2680
    },
    {
      "epoch": 0.08608,
      "grad_norm": 0.03607962653040886,
      "learning_rate": 1.942634666666667e-05,
      "loss": 0.0011,
      "step": 2690
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.01252079475671053,
      "learning_rate": 1.9424213333333335e-05,
      "loss": 0.0161,
      "step": 2700
    },
    {
      "epoch": 0.08672,
      "grad_norm": 0.017989100888371468,
      "learning_rate": 1.942208e-05,
      "loss": 0.0009,
      "step": 2710
    },
    {
      "epoch": 0.08704,
      "grad_norm": 3.917572021484375,
      "learning_rate": 1.941994666666667e-05,
      "loss": 0.0382,
      "step": 2720
    },
    {
      "epoch": 0.08736,
      "grad_norm": 0.015097854658961296,
      "learning_rate": 1.9417813333333334e-05,
      "loss": 0.0011,
      "step": 2730
    },
    {
      "epoch": 0.08768,
      "grad_norm": 0.01161735225468874,
      "learning_rate": 1.941568e-05,
      "loss": 0.0568,
      "step": 2740
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.012353873811662197,
      "learning_rate": 1.941354666666667e-05,
      "loss": 0.0008,
      "step": 2750
    },
    {
      "epoch": 0.08832,
      "grad_norm": 0.049385812133550644,
      "learning_rate": 1.9411413333333334e-05,
      "loss": 0.001,
      "step": 2760
    },
    {
      "epoch": 0.08864,
      "grad_norm": 0.01681501418352127,
      "learning_rate": 1.9409280000000003e-05,
      "loss": 0.0778,
      "step": 2770
    },
    {
      "epoch": 0.08896,
      "grad_norm": 0.02983606420457363,
      "learning_rate": 1.9407146666666668e-05,
      "loss": 0.0089,
      "step": 2780
    },
    {
      "epoch": 0.08928,
      "grad_norm": 0.013694224879145622,
      "learning_rate": 1.9405013333333337e-05,
      "loss": 0.0009,
      "step": 2790
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.25652027130126953,
      "learning_rate": 1.9402880000000002e-05,
      "loss": 0.0014,
      "step": 2800
    },
    {
      "epoch": 0.08992,
      "grad_norm": 0.015488237142562866,
      "learning_rate": 1.9400746666666668e-05,
      "loss": 0.0291,
      "step": 2810
    },
    {
      "epoch": 0.09024,
      "grad_norm": 0.014464844018220901,
      "learning_rate": 1.9398613333333336e-05,
      "loss": 0.0087,
      "step": 2820
    },
    {
      "epoch": 0.09056,
      "grad_norm": 0.01193463709205389,
      "learning_rate": 1.9396480000000002e-05,
      "loss": 0.0009,
      "step": 2830
    },
    {
      "epoch": 0.09088,
      "grad_norm": 0.016522545367479324,
      "learning_rate": 1.9394346666666667e-05,
      "loss": 0.0023,
      "step": 2840
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.023564953356981277,
      "learning_rate": 1.9392213333333332e-05,
      "loss": 0.0011,
      "step": 2850
    },
    {
      "epoch": 0.09152,
      "grad_norm": 0.08449429273605347,
      "learning_rate": 1.939008e-05,
      "loss": 0.0154,
      "step": 2860
    },
    {
      "epoch": 0.09184,
      "grad_norm": 0.04277126118540764,
      "learning_rate": 1.9387946666666667e-05,
      "loss": 0.0436,
      "step": 2870
    },
    {
      "epoch": 0.09216,
      "grad_norm": 0.019305989146232605,
      "learning_rate": 1.9385813333333335e-05,
      "loss": 0.0012,
      "step": 2880
    },
    {
      "epoch": 0.09248,
      "grad_norm": 0.03153853490948677,
      "learning_rate": 1.9383680000000004e-05,
      "loss": 0.0011,
      "step": 2890
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.018239088356494904,
      "learning_rate": 1.938154666666667e-05,
      "loss": 0.0157,
      "step": 2900
    },
    {
      "epoch": 0.09312,
      "grad_norm": 0.014986611902713776,
      "learning_rate": 1.9379413333333335e-05,
      "loss": 0.001,
      "step": 2910
    },
    {
      "epoch": 0.09344,
      "grad_norm": 0.012768237851560116,
      "learning_rate": 1.937728e-05,
      "loss": 0.0008,
      "step": 2920
    },
    {
      "epoch": 0.09376,
      "grad_norm": 0.01731283590197563,
      "learning_rate": 1.937514666666667e-05,
      "loss": 0.0351,
      "step": 2930
    },
    {
      "epoch": 0.09408,
      "grad_norm": 0.015671897679567337,
      "learning_rate": 1.9373013333333334e-05,
      "loss": 0.0934,
      "step": 2940
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.05321209877729416,
      "learning_rate": 1.937088e-05,
      "loss": 0.0801,
      "step": 2950
    },
    {
      "epoch": 0.09472,
      "grad_norm": 0.017692752182483673,
      "learning_rate": 1.936874666666667e-05,
      "loss": 0.0013,
      "step": 2960
    },
    {
      "epoch": 0.09504,
      "grad_norm": 0.09218592196702957,
      "learning_rate": 1.9366613333333334e-05,
      "loss": 0.0152,
      "step": 2970
    },
    {
      "epoch": 0.09536,
      "grad_norm": 0.017870090901851654,
      "learning_rate": 1.9364480000000003e-05,
      "loss": 0.0171,
      "step": 2980
    },
    {
      "epoch": 0.09568,
      "grad_norm": 0.01710447110235691,
      "learning_rate": 1.9362346666666668e-05,
      "loss": 0.0266,
      "step": 2990
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.014733674935996532,
      "learning_rate": 1.9360213333333337e-05,
      "loss": 0.0018,
      "step": 3000
    },
    {
      "epoch": 0.09632,
      "grad_norm": 0.14311084151268005,
      "learning_rate": 1.9358080000000002e-05,
      "loss": 0.0056,
      "step": 3010
    },
    {
      "epoch": 0.09664,
      "grad_norm": 0.017600873485207558,
      "learning_rate": 1.9355946666666668e-05,
      "loss": 0.0044,
      "step": 3020
    },
    {
      "epoch": 0.09696,
      "grad_norm": 0.12305125594139099,
      "learning_rate": 1.9353813333333336e-05,
      "loss": 0.0011,
      "step": 3030
    },
    {
      "epoch": 0.09728,
      "grad_norm": 0.012698938138782978,
      "learning_rate": 1.9351680000000002e-05,
      "loss": 0.0408,
      "step": 3040
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.02009478025138378,
      "learning_rate": 1.9349546666666667e-05,
      "loss": 0.0012,
      "step": 3050
    },
    {
      "epoch": 0.09792,
      "grad_norm": 0.04575204476714134,
      "learning_rate": 1.9347413333333336e-05,
      "loss": 0.0046,
      "step": 3060
    },
    {
      "epoch": 0.09824,
      "grad_norm": 0.014317670837044716,
      "learning_rate": 1.934528e-05,
      "loss": 0.0009,
      "step": 3070
    },
    {
      "epoch": 0.09856,
      "grad_norm": 0.019528962671756744,
      "learning_rate": 1.9343146666666667e-05,
      "loss": 0.0013,
      "step": 3080
    },
    {
      "epoch": 0.09888,
      "grad_norm": 0.013560838997364044,
      "learning_rate": 1.9341013333333335e-05,
      "loss": 0.0427,
      "step": 3090
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.012026614509522915,
      "learning_rate": 1.933888e-05,
      "loss": 0.0013,
      "step": 3100
    },
    {
      "epoch": 0.09952,
      "grad_norm": 0.009441059082746506,
      "learning_rate": 1.933674666666667e-05,
      "loss": 0.0017,
      "step": 3110
    },
    {
      "epoch": 0.09984,
      "grad_norm": 0.0209751408547163,
      "learning_rate": 1.9334613333333335e-05,
      "loss": 0.0014,
      "step": 3120
    },
    {
      "epoch": 0.10016,
      "grad_norm": 0.014394662342965603,
      "learning_rate": 1.9332480000000004e-05,
      "loss": 0.011,
      "step": 3130
    },
    {
      "epoch": 0.10048,
      "grad_norm": 0.01588975265622139,
      "learning_rate": 1.933034666666667e-05,
      "loss": 0.0061,
      "step": 3140
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.012282738462090492,
      "learning_rate": 1.9328213333333334e-05,
      "loss": 0.001,
      "step": 3150
    },
    {
      "epoch": 0.10112,
      "grad_norm": 0.5272453427314758,
      "learning_rate": 1.9326080000000003e-05,
      "loss": 0.0016,
      "step": 3160
    },
    {
      "epoch": 0.10144,
      "grad_norm": 3.2377259731292725,
      "learning_rate": 1.932394666666667e-05,
      "loss": 0.0314,
      "step": 3170
    },
    {
      "epoch": 0.10176,
      "grad_norm": 0.05356232449412346,
      "learning_rate": 1.9321813333333334e-05,
      "loss": 0.0018,
      "step": 3180
    },
    {
      "epoch": 0.10208,
      "grad_norm": 0.010169320739805698,
      "learning_rate": 1.931968e-05,
      "loss": 0.0011,
      "step": 3190
    },
    {
      "epoch": 0.1024,
      "grad_norm": 1.9736777544021606,
      "learning_rate": 1.9317546666666668e-05,
      "loss": 0.0996,
      "step": 3200
    },
    {
      "epoch": 0.10272,
      "grad_norm": 0.6840440630912781,
      "learning_rate": 1.9315413333333333e-05,
      "loss": 0.0288,
      "step": 3210
    },
    {
      "epoch": 0.10304,
      "grad_norm": 0.0148963937535882,
      "learning_rate": 1.9313280000000002e-05,
      "loss": 0.001,
      "step": 3220
    },
    {
      "epoch": 0.10336,
      "grad_norm": 0.015321355313062668,
      "learning_rate": 1.9311146666666668e-05,
      "loss": 0.002,
      "step": 3230
    },
    {
      "epoch": 0.10368,
      "grad_norm": 0.041885700076818466,
      "learning_rate": 1.9309013333333336e-05,
      "loss": 0.002,
      "step": 3240
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.015026849694550037,
      "learning_rate": 1.9306880000000002e-05,
      "loss": 0.0021,
      "step": 3250
    },
    {
      "epoch": 0.10432,
      "grad_norm": 0.02544761635363102,
      "learning_rate": 1.9304746666666667e-05,
      "loss": 0.0484,
      "step": 3260
    },
    {
      "epoch": 0.10464,
      "grad_norm": 2.227743148803711,
      "learning_rate": 1.9302613333333336e-05,
      "loss": 0.0494,
      "step": 3270
    },
    {
      "epoch": 0.10496,
      "grad_norm": 0.09536506235599518,
      "learning_rate": 1.930048e-05,
      "loss": 0.0174,
      "step": 3280
    },
    {
      "epoch": 0.10528,
      "grad_norm": 0.01407875120639801,
      "learning_rate": 1.9298346666666667e-05,
      "loss": 0.0009,
      "step": 3290
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.017034783959388733,
      "learning_rate": 1.9296213333333335e-05,
      "loss": 0.0056,
      "step": 3300
    },
    {
      "epoch": 0.10592,
      "grad_norm": 0.011828585527837276,
      "learning_rate": 1.929408e-05,
      "loss": 0.0019,
      "step": 3310
    },
    {
      "epoch": 0.10624,
      "grad_norm": 1.084553837776184,
      "learning_rate": 1.9291946666666666e-05,
      "loss": 0.0396,
      "step": 3320
    },
    {
      "epoch": 0.10656,
      "grad_norm": 0.01272534765303135,
      "learning_rate": 1.9289813333333335e-05,
      "loss": 0.0334,
      "step": 3330
    },
    {
      "epoch": 0.10688,
      "grad_norm": 0.01315750740468502,
      "learning_rate": 1.928768e-05,
      "loss": 0.0008,
      "step": 3340
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.014063986018300056,
      "learning_rate": 1.928554666666667e-05,
      "loss": 0.0009,
      "step": 3350
    },
    {
      "epoch": 0.10752,
      "grad_norm": 0.012583457864820957,
      "learning_rate": 1.9283413333333334e-05,
      "loss": 0.0009,
      "step": 3360
    },
    {
      "epoch": 0.10784,
      "grad_norm": 0.03834505379199982,
      "learning_rate": 1.9281280000000003e-05,
      "loss": 0.0008,
      "step": 3370
    },
    {
      "epoch": 0.10816,
      "grad_norm": 0.015290760435163975,
      "learning_rate": 1.927914666666667e-05,
      "loss": 0.0008,
      "step": 3380
    },
    {
      "epoch": 0.10848,
      "grad_norm": 0.011664429679512978,
      "learning_rate": 1.9277013333333334e-05,
      "loss": 0.0008,
      "step": 3390
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.013687803409993649,
      "learning_rate": 1.9274880000000003e-05,
      "loss": 0.001,
      "step": 3400
    },
    {
      "epoch": 0.10912,
      "grad_norm": 0.0182949285954237,
      "learning_rate": 1.9272746666666668e-05,
      "loss": 0.0012,
      "step": 3410
    },
    {
      "epoch": 0.10944,
      "grad_norm": 0.021601198241114616,
      "learning_rate": 1.9270613333333333e-05,
      "loss": 0.0444,
      "step": 3420
    },
    {
      "epoch": 0.10976,
      "grad_norm": 0.013630070723593235,
      "learning_rate": 1.926848e-05,
      "loss": 0.0535,
      "step": 3430
    },
    {
      "epoch": 0.11008,
      "grad_norm": 0.012996857054531574,
      "learning_rate": 1.9266346666666668e-05,
      "loss": 0.0422,
      "step": 3440
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.010983143001794815,
      "learning_rate": 1.9264213333333336e-05,
      "loss": 0.001,
      "step": 3450
    },
    {
      "epoch": 0.11072,
      "grad_norm": 0.02275318093597889,
      "learning_rate": 1.9262080000000002e-05,
      "loss": 0.0391,
      "step": 3460
    },
    {
      "epoch": 0.11104,
      "grad_norm": 0.018055113032460213,
      "learning_rate": 1.925994666666667e-05,
      "loss": 0.0008,
      "step": 3470
    },
    {
      "epoch": 0.11136,
      "grad_norm": 0.013459241949021816,
      "learning_rate": 1.9257813333333336e-05,
      "loss": 0.0009,
      "step": 3480
    },
    {
      "epoch": 0.11168,
      "grad_norm": 0.012654428370296955,
      "learning_rate": 1.925568e-05,
      "loss": 0.0716,
      "step": 3490
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.0442107692360878,
      "learning_rate": 1.925354666666667e-05,
      "loss": 0.0022,
      "step": 3500
    },
    {
      "epoch": 0.11232,
      "grad_norm": 0.014325421303510666,
      "learning_rate": 1.9251413333333335e-05,
      "loss": 0.0315,
      "step": 3510
    },
    {
      "epoch": 0.11264,
      "grad_norm": 0.12585346400737762,
      "learning_rate": 1.924928e-05,
      "loss": 0.0191,
      "step": 3520
    },
    {
      "epoch": 0.11296,
      "grad_norm": 0.013416189700365067,
      "learning_rate": 1.9247146666666666e-05,
      "loss": 0.0008,
      "step": 3530
    },
    {
      "epoch": 0.11328,
      "grad_norm": 0.017522621899843216,
      "learning_rate": 1.9245013333333335e-05,
      "loss": 0.001,
      "step": 3540
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.01495642401278019,
      "learning_rate": 1.924288e-05,
      "loss": 0.0013,
      "step": 3550
    },
    {
      "epoch": 0.11392,
      "grad_norm": 0.014770668931305408,
      "learning_rate": 1.924074666666667e-05,
      "loss": 0.0009,
      "step": 3560
    },
    {
      "epoch": 0.11424,
      "grad_norm": 0.03951260820031166,
      "learning_rate": 1.9238613333333334e-05,
      "loss": 0.0018,
      "step": 3570
    },
    {
      "epoch": 0.11456,
      "grad_norm": 0.013061142526566982,
      "learning_rate": 1.9236480000000003e-05,
      "loss": 0.001,
      "step": 3580
    },
    {
      "epoch": 0.11488,
      "grad_norm": 0.01886535808444023,
      "learning_rate": 1.923434666666667e-05,
      "loss": 0.0012,
      "step": 3590
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.012524654157459736,
      "learning_rate": 1.9232213333333334e-05,
      "loss": 0.0403,
      "step": 3600
    },
    {
      "epoch": 0.11552,
      "grad_norm": 0.01056541409343481,
      "learning_rate": 1.9230080000000003e-05,
      "loss": 0.0013,
      "step": 3610
    },
    {
      "epoch": 0.11584,
      "grad_norm": 0.010470299050211906,
      "learning_rate": 1.9227946666666668e-05,
      "loss": 0.0022,
      "step": 3620
    },
    {
      "epoch": 0.11616,
      "grad_norm": 2.6682677268981934,
      "learning_rate": 1.9225813333333334e-05,
      "loss": 0.0411,
      "step": 3630
    },
    {
      "epoch": 0.11648,
      "grad_norm": 0.7591625452041626,
      "learning_rate": 1.9223680000000002e-05,
      "loss": 0.0019,
      "step": 3640
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.014426324516534805,
      "learning_rate": 1.9221546666666668e-05,
      "loss": 0.0008,
      "step": 3650
    },
    {
      "epoch": 0.11712,
      "grad_norm": 0.019675105810165405,
      "learning_rate": 1.9219413333333333e-05,
      "loss": 0.0009,
      "step": 3660
    },
    {
      "epoch": 0.11744,
      "grad_norm": 0.012126483023166656,
      "learning_rate": 1.9217280000000002e-05,
      "loss": 0.0009,
      "step": 3670
    },
    {
      "epoch": 0.11776,
      "grad_norm": 0.02272062934935093,
      "learning_rate": 1.9215146666666667e-05,
      "loss": 0.0378,
      "step": 3680
    },
    {
      "epoch": 0.11808,
      "grad_norm": 0.012526815757155418,
      "learning_rate": 1.9213013333333336e-05,
      "loss": 0.0009,
      "step": 3690
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.011790083721280098,
      "learning_rate": 1.921088e-05,
      "loss": 0.0118,
      "step": 3700
    },
    {
      "epoch": 0.11872,
      "grad_norm": 0.016206638887524605,
      "learning_rate": 1.920874666666667e-05,
      "loss": 0.0401,
      "step": 3710
    },
    {
      "epoch": 0.11904,
      "grad_norm": 0.011415896005928516,
      "learning_rate": 1.9206613333333335e-05,
      "loss": 0.0058,
      "step": 3720
    },
    {
      "epoch": 0.11936,
      "grad_norm": 0.014375370927155018,
      "learning_rate": 1.920448e-05,
      "loss": 0.0014,
      "step": 3730
    },
    {
      "epoch": 0.11968,
      "grad_norm": 0.01867886818945408,
      "learning_rate": 1.920234666666667e-05,
      "loss": 0.0011,
      "step": 3740
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.17132313549518585,
      "learning_rate": 1.9200213333333335e-05,
      "loss": 0.0172,
      "step": 3750
    },
    {
      "epoch": 0.12032,
      "grad_norm": 0.009436918422579765,
      "learning_rate": 1.919808e-05,
      "loss": 0.0013,
      "step": 3760
    },
    {
      "epoch": 0.12064,
      "grad_norm": 0.024956367909908295,
      "learning_rate": 1.9195946666666666e-05,
      "loss": 0.0009,
      "step": 3770
    },
    {
      "epoch": 0.12096,
      "grad_norm": 0.015241304412484169,
      "learning_rate": 1.9193813333333334e-05,
      "loss": 0.0035,
      "step": 3780
    },
    {
      "epoch": 0.12128,
      "grad_norm": 0.01376679539680481,
      "learning_rate": 1.919168e-05,
      "loss": 0.0047,
      "step": 3790
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.0543520413339138,
      "learning_rate": 1.918954666666667e-05,
      "loss": 0.0008,
      "step": 3800
    },
    {
      "epoch": 0.12192,
      "grad_norm": 0.04199269786477089,
      "learning_rate": 1.9187413333333334e-05,
      "loss": 0.0007,
      "step": 3810
    },
    {
      "epoch": 0.12224,
      "grad_norm": 0.009722684510052204,
      "learning_rate": 1.9185280000000003e-05,
      "loss": 0.0008,
      "step": 3820
    },
    {
      "epoch": 0.12256,
      "grad_norm": 0.013754372484982014,
      "learning_rate": 1.9183146666666668e-05,
      "loss": 0.0331,
      "step": 3830
    },
    {
      "epoch": 0.12288,
      "grad_norm": 0.00970597192645073,
      "learning_rate": 1.9181013333333337e-05,
      "loss": 0.0183,
      "step": 3840
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.01324492134153843,
      "learning_rate": 1.9178880000000002e-05,
      "loss": 0.0007,
      "step": 3850
    },
    {
      "epoch": 0.12352,
      "grad_norm": 0.011564616113901138,
      "learning_rate": 1.9176746666666668e-05,
      "loss": 0.0018,
      "step": 3860
    },
    {
      "epoch": 0.12384,
      "grad_norm": 0.009027182124555111,
      "learning_rate": 1.9174613333333333e-05,
      "loss": 0.0007,
      "step": 3870
    },
    {
      "epoch": 0.12416,
      "grad_norm": 0.009678618051111698,
      "learning_rate": 1.9172480000000002e-05,
      "loss": 0.0012,
      "step": 3880
    },
    {
      "epoch": 0.12448,
      "grad_norm": 0.008538701571524143,
      "learning_rate": 1.9170346666666667e-05,
      "loss": 0.0403,
      "step": 3890
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.011363436467945576,
      "learning_rate": 1.9168213333333333e-05,
      "loss": 0.0006,
      "step": 3900
    },
    {
      "epoch": 0.12512,
      "grad_norm": 0.2754180133342743,
      "learning_rate": 1.916608e-05,
      "loss": 0.0011,
      "step": 3910
    },
    {
      "epoch": 0.12544,
      "grad_norm": 0.01673790253698826,
      "learning_rate": 1.916394666666667e-05,
      "loss": 0.0356,
      "step": 3920
    },
    {
      "epoch": 0.12576,
      "grad_norm": 0.011110962368547916,
      "learning_rate": 1.9161813333333335e-05,
      "loss": 0.0013,
      "step": 3930
    },
    {
      "epoch": 0.12608,
      "grad_norm": 0.01339410524815321,
      "learning_rate": 1.915968e-05,
      "loss": 0.0203,
      "step": 3940
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.01309211179614067,
      "learning_rate": 1.915754666666667e-05,
      "loss": 0.0008,
      "step": 3950
    },
    {
      "epoch": 0.12672,
      "grad_norm": 0.010012488812208176,
      "learning_rate": 1.9155413333333335e-05,
      "loss": 0.0007,
      "step": 3960
    },
    {
      "epoch": 0.12704,
      "grad_norm": 0.012013301253318787,
      "learning_rate": 1.915328e-05,
      "loss": 0.0038,
      "step": 3970
    },
    {
      "epoch": 0.12736,
      "grad_norm": 0.010777464136481285,
      "learning_rate": 1.915114666666667e-05,
      "loss": 0.0124,
      "step": 3980
    },
    {
      "epoch": 0.12768,
      "grad_norm": 0.009369203820824623,
      "learning_rate": 1.9149013333333335e-05,
      "loss": 0.0532,
      "step": 3990
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.01418366190046072,
      "learning_rate": 1.914688e-05,
      "loss": 0.0069,
      "step": 4000
    },
    {
      "epoch": 0.12832,
      "grad_norm": 0.1346595585346222,
      "learning_rate": 1.914474666666667e-05,
      "loss": 0.0008,
      "step": 4010
    },
    {
      "epoch": 0.12864,
      "grad_norm": 0.013993201777338982,
      "learning_rate": 1.9142613333333334e-05,
      "loss": 0.0018,
      "step": 4020
    },
    {
      "epoch": 0.12896,
      "grad_norm": 0.01193622499704361,
      "learning_rate": 1.9140480000000003e-05,
      "loss": 0.0007,
      "step": 4030
    },
    {
      "epoch": 0.12928,
      "grad_norm": 0.009360873140394688,
      "learning_rate": 1.9138346666666668e-05,
      "loss": 0.0017,
      "step": 4040
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.01367641519755125,
      "learning_rate": 1.9136213333333337e-05,
      "loss": 0.0023,
      "step": 4050
    },
    {
      "epoch": 0.12992,
      "grad_norm": 0.008618846535682678,
      "learning_rate": 1.9134080000000002e-05,
      "loss": 0.0007,
      "step": 4060
    },
    {
      "epoch": 0.13024,
      "grad_norm": 0.009142361581325531,
      "learning_rate": 1.9131946666666668e-05,
      "loss": 0.0005,
      "step": 4070
    },
    {
      "epoch": 0.13056,
      "grad_norm": 0.11472068727016449,
      "learning_rate": 1.9129813333333336e-05,
      "loss": 0.0499,
      "step": 4080
    },
    {
      "epoch": 0.13088,
      "grad_norm": 0.009256130084395409,
      "learning_rate": 1.9127680000000002e-05,
      "loss": 0.0326,
      "step": 4090
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.09868277609348297,
      "learning_rate": 1.9125546666666667e-05,
      "loss": 0.0008,
      "step": 4100
    },
    {
      "epoch": 0.13152,
      "grad_norm": 0.008244799450039864,
      "learning_rate": 1.9123413333333333e-05,
      "loss": 0.0006,
      "step": 4110
    },
    {
      "epoch": 0.13184,
      "grad_norm": 0.008881298825144768,
      "learning_rate": 1.912128e-05,
      "loss": 0.0006,
      "step": 4120
    },
    {
      "epoch": 0.13216,
      "grad_norm": 0.011022304184734821,
      "learning_rate": 1.9119146666666667e-05,
      "loss": 0.0101,
      "step": 4130
    },
    {
      "epoch": 0.13248,
      "grad_norm": 0.009541013278067112,
      "learning_rate": 1.9117013333333335e-05,
      "loss": 0.0006,
      "step": 4140
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.012051057070493698,
      "learning_rate": 1.911488e-05,
      "loss": 0.0006,
      "step": 4150
    },
    {
      "epoch": 0.13312,
      "grad_norm": 0.07206650823354721,
      "learning_rate": 1.911274666666667e-05,
      "loss": 0.0009,
      "step": 4160
    },
    {
      "epoch": 0.13344,
      "grad_norm": 0.009906981140375137,
      "learning_rate": 1.9110613333333335e-05,
      "loss": 0.0181,
      "step": 4170
    },
    {
      "epoch": 0.13376,
      "grad_norm": 0.009692716412246227,
      "learning_rate": 1.9108480000000004e-05,
      "loss": 0.0865,
      "step": 4180
    },
    {
      "epoch": 0.13408,
      "grad_norm": 0.01656736619770527,
      "learning_rate": 1.910634666666667e-05,
      "loss": 0.0397,
      "step": 4190
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.016213612630963326,
      "learning_rate": 1.9104213333333335e-05,
      "loss": 0.0009,
      "step": 4200
    },
    {
      "epoch": 0.13472,
      "grad_norm": 0.01378156803548336,
      "learning_rate": 1.910208e-05,
      "loss": 0.1067,
      "step": 4210
    },
    {
      "epoch": 0.13504,
      "grad_norm": 0.011042499914765358,
      "learning_rate": 1.909994666666667e-05,
      "loss": 0.0007,
      "step": 4220
    },
    {
      "epoch": 0.13536,
      "grad_norm": 0.0095926932990551,
      "learning_rate": 1.9097813333333334e-05,
      "loss": 0.0292,
      "step": 4230
    },
    {
      "epoch": 0.13568,
      "grad_norm": 0.02116072177886963,
      "learning_rate": 1.909568e-05,
      "loss": 0.0016,
      "step": 4240
    },
    {
      "epoch": 0.136,
      "grad_norm": 6.92732572555542,
      "learning_rate": 1.9093546666666668e-05,
      "loss": 0.0389,
      "step": 4250
    },
    {
      "epoch": 0.13632,
      "grad_norm": 0.011611947789788246,
      "learning_rate": 1.9091413333333334e-05,
      "loss": 0.0436,
      "step": 4260
    },
    {
      "epoch": 0.13664,
      "grad_norm": 0.023988043889403343,
      "learning_rate": 1.9089280000000002e-05,
      "loss": 0.0012,
      "step": 4270
    },
    {
      "epoch": 0.13696,
      "grad_norm": 0.011538496240973473,
      "learning_rate": 1.9087146666666668e-05,
      "loss": 0.003,
      "step": 4280
    },
    {
      "epoch": 0.13728,
      "grad_norm": 0.028609566390514374,
      "learning_rate": 1.9085013333333336e-05,
      "loss": 0.0204,
      "step": 4290
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.010492000728845596,
      "learning_rate": 1.9082880000000002e-05,
      "loss": 0.0529,
      "step": 4300
    },
    {
      "epoch": 0.13792,
      "grad_norm": 0.013111530803143978,
      "learning_rate": 1.9080746666666667e-05,
      "loss": 0.0015,
      "step": 4310
    },
    {
      "epoch": 0.13824,
      "grad_norm": 0.01385561004281044,
      "learning_rate": 1.9078613333333336e-05,
      "loss": 0.0038,
      "step": 4320
    },
    {
      "epoch": 0.13856,
      "grad_norm": 0.013101456686854362,
      "learning_rate": 1.907648e-05,
      "loss": 0.0008,
      "step": 4330
    },
    {
      "epoch": 0.13888,
      "grad_norm": 0.17053508758544922,
      "learning_rate": 1.9074346666666667e-05,
      "loss": 0.001,
      "step": 4340
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.10530995577573776,
      "learning_rate": 1.9072213333333336e-05,
      "loss": 0.0111,
      "step": 4350
    },
    {
      "epoch": 0.13952,
      "grad_norm": 0.014895162545144558,
      "learning_rate": 1.907008e-05,
      "loss": 0.0008,
      "step": 4360
    },
    {
      "epoch": 0.13984,
      "grad_norm": 0.013369599357247353,
      "learning_rate": 1.9067946666666666e-05,
      "loss": 0.0007,
      "step": 4370
    },
    {
      "epoch": 0.14016,
      "grad_norm": 0.01005355454981327,
      "learning_rate": 1.9065813333333335e-05,
      "loss": 0.0211,
      "step": 4380
    },
    {
      "epoch": 0.14048,
      "grad_norm": 0.01119307428598404,
      "learning_rate": 1.9063680000000004e-05,
      "loss": 0.0007,
      "step": 4390
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.014994697645306587,
      "learning_rate": 1.906154666666667e-05,
      "loss": 0.0193,
      "step": 4400
    },
    {
      "epoch": 0.14112,
      "grad_norm": 0.008200296200811863,
      "learning_rate": 1.9059413333333335e-05,
      "loss": 0.128,
      "step": 4410
    },
    {
      "epoch": 0.14144,
      "grad_norm": 0.012093023397028446,
      "learning_rate": 1.9057280000000003e-05,
      "loss": 0.0008,
      "step": 4420
    },
    {
      "epoch": 0.14176,
      "grad_norm": 0.09112424403429031,
      "learning_rate": 1.905514666666667e-05,
      "loss": 0.001,
      "step": 4430
    },
    {
      "epoch": 0.14208,
      "grad_norm": 0.013001708313822746,
      "learning_rate": 1.9053013333333334e-05,
      "loss": 0.0161,
      "step": 4440
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.014246242120862007,
      "learning_rate": 1.905088e-05,
      "loss": 0.0027,
      "step": 4450
    },
    {
      "epoch": 0.14272,
      "grad_norm": 0.014889758080244064,
      "learning_rate": 1.9048746666666668e-05,
      "loss": 0.0723,
      "step": 4460
    },
    {
      "epoch": 0.14304,
      "grad_norm": 0.01332076359540224,
      "learning_rate": 1.9046613333333334e-05,
      "loss": 0.0016,
      "step": 4470
    },
    {
      "epoch": 0.14336,
      "grad_norm": 0.024048427119851112,
      "learning_rate": 1.9044480000000002e-05,
      "loss": 0.0015,
      "step": 4480
    },
    {
      "epoch": 0.14368,
      "grad_norm": 0.012438712641596794,
      "learning_rate": 1.9042346666666668e-05,
      "loss": 0.0288,
      "step": 4490
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.040944620966911316,
      "learning_rate": 1.9040213333333336e-05,
      "loss": 0.0009,
      "step": 4500
    },
    {
      "epoch": 0.14432,
      "grad_norm": 0.013902955688536167,
      "learning_rate": 1.9038080000000002e-05,
      "loss": 0.0435,
      "step": 4510
    },
    {
      "epoch": 0.14464,
      "grad_norm": 0.014224138110876083,
      "learning_rate": 1.903594666666667e-05,
      "loss": 0.0238,
      "step": 4520
    },
    {
      "epoch": 0.14496,
      "grad_norm": 0.017486024647951126,
      "learning_rate": 1.9033813333333336e-05,
      "loss": 0.001,
      "step": 4530
    },
    {
      "epoch": 0.14528,
      "grad_norm": 0.009709840640425682,
      "learning_rate": 1.903168e-05,
      "loss": 0.001,
      "step": 4540
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.04448901489377022,
      "learning_rate": 1.9029546666666667e-05,
      "loss": 0.0467,
      "step": 4550
    },
    {
      "epoch": 0.14592,
      "grad_norm": 0.04009890556335449,
      "learning_rate": 1.9027413333333336e-05,
      "loss": 0.0017,
      "step": 4560
    },
    {
      "epoch": 0.14624,
      "grad_norm": 0.009858323261141777,
      "learning_rate": 1.902528e-05,
      "loss": 0.0028,
      "step": 4570
    },
    {
      "epoch": 0.14656,
      "grad_norm": 0.030166568234562874,
      "learning_rate": 1.9023146666666666e-05,
      "loss": 0.0036,
      "step": 4580
    },
    {
      "epoch": 0.14688,
      "grad_norm": 0.012244784273207188,
      "learning_rate": 1.9021013333333335e-05,
      "loss": 0.0121,
      "step": 4590
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.01401909813284874,
      "learning_rate": 1.901888e-05,
      "loss": 0.0008,
      "step": 4600
    },
    {
      "epoch": 0.14752,
      "grad_norm": 0.24404166638851166,
      "learning_rate": 1.901674666666667e-05,
      "loss": 0.0011,
      "step": 4610
    },
    {
      "epoch": 0.14784,
      "grad_norm": 0.014216175302863121,
      "learning_rate": 1.9014613333333335e-05,
      "loss": 0.0007,
      "step": 4620
    },
    {
      "epoch": 0.14816,
      "grad_norm": 0.023671060800552368,
      "learning_rate": 1.9012480000000003e-05,
      "loss": 0.0446,
      "step": 4630
    },
    {
      "epoch": 0.14848,
      "grad_norm": 0.010805711150169373,
      "learning_rate": 1.901034666666667e-05,
      "loss": 0.0009,
      "step": 4640
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.011452623642981052,
      "learning_rate": 1.9008213333333334e-05,
      "loss": 0.043,
      "step": 4650
    },
    {
      "epoch": 0.14912,
      "grad_norm": 0.014041547663509846,
      "learning_rate": 1.9006080000000003e-05,
      "loss": 0.001,
      "step": 4660
    },
    {
      "epoch": 0.14944,
      "grad_norm": 0.017004694789648056,
      "learning_rate": 1.9003946666666668e-05,
      "loss": 0.004,
      "step": 4670
    },
    {
      "epoch": 0.14976,
      "grad_norm": 0.010026865638792515,
      "learning_rate": 1.9001813333333334e-05,
      "loss": 0.0426,
      "step": 4680
    },
    {
      "epoch": 0.15008,
      "grad_norm": 0.015140442177653313,
      "learning_rate": 1.8999680000000002e-05,
      "loss": 0.0019,
      "step": 4690
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.01383129507303238,
      "learning_rate": 1.8997546666666668e-05,
      "loss": 0.001,
      "step": 4700
    },
    {
      "epoch": 0.15072,
      "grad_norm": 0.012156283482909203,
      "learning_rate": 1.8995413333333333e-05,
      "loss": 0.0034,
      "step": 4710
    },
    {
      "epoch": 0.15104,
      "grad_norm": 0.013748071156442165,
      "learning_rate": 1.8993280000000002e-05,
      "loss": 0.0007,
      "step": 4720
    },
    {
      "epoch": 0.15136,
      "grad_norm": 0.009313388727605343,
      "learning_rate": 1.8991146666666667e-05,
      "loss": 0.0007,
      "step": 4730
    },
    {
      "epoch": 0.15168,
      "grad_norm": 0.010158812627196312,
      "learning_rate": 1.8989013333333336e-05,
      "loss": 0.014,
      "step": 4740
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.011728961952030659,
      "learning_rate": 1.898688e-05,
      "loss": 0.0006,
      "step": 4750
    },
    {
      "epoch": 0.15232,
      "grad_norm": 0.015517588704824448,
      "learning_rate": 1.898474666666667e-05,
      "loss": 0.0057,
      "step": 4760
    },
    {
      "epoch": 0.15264,
      "grad_norm": 0.014008892700076103,
      "learning_rate": 1.8982613333333336e-05,
      "loss": 0.052,
      "step": 4770
    },
    {
      "epoch": 0.15296,
      "grad_norm": 0.012266370467841625,
      "learning_rate": 1.898048e-05,
      "loss": 0.0008,
      "step": 4780
    },
    {
      "epoch": 0.15328,
      "grad_norm": 0.012522137723863125,
      "learning_rate": 1.8978346666666666e-05,
      "loss": 0.001,
      "step": 4790
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.013853286392986774,
      "learning_rate": 1.8976213333333335e-05,
      "loss": 0.056,
      "step": 4800
    },
    {
      "epoch": 0.15392,
      "grad_norm": 0.017286596819758415,
      "learning_rate": 1.897408e-05,
      "loss": 0.0189,
      "step": 4810
    },
    {
      "epoch": 0.15424,
      "grad_norm": 0.012709285132586956,
      "learning_rate": 1.8971946666666666e-05,
      "loss": 0.0007,
      "step": 4820
    },
    {
      "epoch": 0.15456,
      "grad_norm": 0.017370199784636497,
      "learning_rate": 1.8969813333333335e-05,
      "loss": 0.0051,
      "step": 4830
    },
    {
      "epoch": 0.15488,
      "grad_norm": 0.014568101614713669,
      "learning_rate": 1.896768e-05,
      "loss": 0.0802,
      "step": 4840
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.011279163882136345,
      "learning_rate": 1.896554666666667e-05,
      "loss": 0.0009,
      "step": 4850
    },
    {
      "epoch": 0.15552,
      "grad_norm": 0.01662246137857437,
      "learning_rate": 1.8963413333333338e-05,
      "loss": 0.0007,
      "step": 4860
    },
    {
      "epoch": 0.15584,
      "grad_norm": 0.011363079771399498,
      "learning_rate": 1.8961280000000003e-05,
      "loss": 0.0008,
      "step": 4870
    },
    {
      "epoch": 0.15616,
      "grad_norm": 0.020556127652525902,
      "learning_rate": 1.8959146666666668e-05,
      "loss": 0.0649,
      "step": 4880
    },
    {
      "epoch": 0.15648,
      "grad_norm": 0.01622570864856243,
      "learning_rate": 1.8957013333333334e-05,
      "loss": 0.0008,
      "step": 4890
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.017159679904580116,
      "learning_rate": 1.8954880000000002e-05,
      "loss": 0.0669,
      "step": 4900
    },
    {
      "epoch": 0.15712,
      "grad_norm": 0.023529257625341415,
      "learning_rate": 1.8952746666666668e-05,
      "loss": 0.0016,
      "step": 4910
    },
    {
      "epoch": 0.15744,
      "grad_norm": 0.016577223315835,
      "learning_rate": 1.8950613333333333e-05,
      "loss": 0.001,
      "step": 4920
    },
    {
      "epoch": 0.15776,
      "grad_norm": 0.014284626580774784,
      "learning_rate": 1.8948480000000002e-05,
      "loss": 0.0173,
      "step": 4930
    },
    {
      "epoch": 0.15808,
      "grad_norm": 0.008010131306946278,
      "learning_rate": 1.8946346666666667e-05,
      "loss": 0.0065,
      "step": 4940
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.01773514598608017,
      "learning_rate": 1.8944213333333336e-05,
      "loss": 0.0008,
      "step": 4950
    },
    {
      "epoch": 0.15872,
      "grad_norm": 0.012220294214785099,
      "learning_rate": 1.894208e-05,
      "loss": 0.0012,
      "step": 4960
    },
    {
      "epoch": 0.15904,
      "grad_norm": 0.015466342680156231,
      "learning_rate": 1.893994666666667e-05,
      "loss": 0.0008,
      "step": 4970
    },
    {
      "epoch": 0.15936,
      "grad_norm": 0.0279720276594162,
      "learning_rate": 1.8937813333333336e-05,
      "loss": 0.0011,
      "step": 4980
    },
    {
      "epoch": 0.15968,
      "grad_norm": 0.012684555724263191,
      "learning_rate": 1.893568e-05,
      "loss": 0.0445,
      "step": 4990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.019743837416172028,
      "learning_rate": 1.893354666666667e-05,
      "loss": 0.0319,
      "step": 5000
    },
    {
      "epoch": 0.16032,
      "grad_norm": 0.014445101842284203,
      "learning_rate": 1.8931413333333335e-05,
      "loss": 0.0008,
      "step": 5010
    },
    {
      "epoch": 0.16064,
      "grad_norm": 0.02929217927157879,
      "learning_rate": 1.892928e-05,
      "loss": 0.0008,
      "step": 5020
    },
    {
      "epoch": 0.16096,
      "grad_norm": 0.010190240107476711,
      "learning_rate": 1.892714666666667e-05,
      "loss": 0.0008,
      "step": 5030
    },
    {
      "epoch": 0.16128,
      "grad_norm": 0.013015330769121647,
      "learning_rate": 1.8925013333333335e-05,
      "loss": 0.0009,
      "step": 5040
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.012124591507017612,
      "learning_rate": 1.892288e-05,
      "loss": 0.0008,
      "step": 5050
    },
    {
      "epoch": 0.16192,
      "grad_norm": 0.012755735777318478,
      "learning_rate": 1.892074666666667e-05,
      "loss": 0.0008,
      "step": 5060
    },
    {
      "epoch": 0.16224,
      "grad_norm": 0.016579948365688324,
      "learning_rate": 1.8918613333333334e-05,
      "loss": 0.0261,
      "step": 5070
    },
    {
      "epoch": 0.16256,
      "grad_norm": 0.008914419449865818,
      "learning_rate": 1.8916480000000003e-05,
      "loss": 0.0007,
      "step": 5080
    },
    {
      "epoch": 0.16288,
      "grad_norm": 0.41336825489997864,
      "learning_rate": 1.8914346666666668e-05,
      "loss": 0.0009,
      "step": 5090
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.010652138851583004,
      "learning_rate": 1.8912213333333337e-05,
      "loss": 0.0491,
      "step": 5100
    },
    {
      "epoch": 0.16352,
      "grad_norm": 0.012573949992656708,
      "learning_rate": 1.8910080000000002e-05,
      "loss": 0.0025,
      "step": 5110
    },
    {
      "epoch": 0.16384,
      "grad_norm": 0.012860496528446674,
      "learning_rate": 1.8907946666666668e-05,
      "loss": 0.0438,
      "step": 5120
    },
    {
      "epoch": 0.16416,
      "grad_norm": 0.011690928600728512,
      "learning_rate": 1.8905813333333333e-05,
      "loss": 0.0055,
      "step": 5130
    },
    {
      "epoch": 0.16448,
      "grad_norm": 2.9718592166900635,
      "learning_rate": 1.8903680000000002e-05,
      "loss": 0.0955,
      "step": 5140
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.0116634052246809,
      "learning_rate": 1.8901546666666667e-05,
      "loss": 0.0301,
      "step": 5150
    },
    {
      "epoch": 0.16512,
      "grad_norm": 0.015655191615223885,
      "learning_rate": 1.8899413333333333e-05,
      "loss": 0.0021,
      "step": 5160
    },
    {
      "epoch": 0.16544,
      "grad_norm": 4.034626007080078,
      "learning_rate": 1.889728e-05,
      "loss": 0.0086,
      "step": 5170
    },
    {
      "epoch": 0.16576,
      "grad_norm": 0.015045978128910065,
      "learning_rate": 1.8895146666666667e-05,
      "loss": 0.0008,
      "step": 5180
    },
    {
      "epoch": 0.16608,
      "grad_norm": 0.013718795962631702,
      "learning_rate": 1.8893013333333336e-05,
      "loss": 0.0835,
      "step": 5190
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.015414410270750523,
      "learning_rate": 1.889088e-05,
      "loss": 0.001,
      "step": 5200
    },
    {
      "epoch": 0.16672,
      "grad_norm": 0.019194073975086212,
      "learning_rate": 1.888874666666667e-05,
      "loss": 0.0011,
      "step": 5210
    },
    {
      "epoch": 0.16704,
      "grad_norm": 0.04273061454296112,
      "learning_rate": 1.8886613333333335e-05,
      "loss": 0.001,
      "step": 5220
    },
    {
      "epoch": 0.16736,
      "grad_norm": 0.016930684447288513,
      "learning_rate": 1.888448e-05,
      "loss": 0.0376,
      "step": 5230
    },
    {
      "epoch": 0.16768,
      "grad_norm": 3.4336893558502197,
      "learning_rate": 1.888234666666667e-05,
      "loss": 0.0544,
      "step": 5240
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.02131585404276848,
      "learning_rate": 1.8880213333333335e-05,
      "loss": 0.0012,
      "step": 5250
    },
    {
      "epoch": 0.16832,
      "grad_norm": 2.089156150817871,
      "learning_rate": 1.887808e-05,
      "loss": 0.0391,
      "step": 5260
    },
    {
      "epoch": 0.16864,
      "grad_norm": 0.013091759756207466,
      "learning_rate": 1.887594666666667e-05,
      "loss": 0.0017,
      "step": 5270
    },
    {
      "epoch": 0.16896,
      "grad_norm": 0.019329536706209183,
      "learning_rate": 1.8873813333333334e-05,
      "loss": 0.0138,
      "step": 5280
    },
    {
      "epoch": 0.16928,
      "grad_norm": 1.4214521646499634,
      "learning_rate": 1.887168e-05,
      "loss": 0.0421,
      "step": 5290
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.01613146811723709,
      "learning_rate": 1.8869546666666668e-05,
      "loss": 0.0012,
      "step": 5300
    },
    {
      "epoch": 0.16992,
      "grad_norm": 0.032315876334905624,
      "learning_rate": 1.8867413333333334e-05,
      "loss": 0.0011,
      "step": 5310
    },
    {
      "epoch": 0.17024,
      "grad_norm": 0.019463077187538147,
      "learning_rate": 1.8865280000000002e-05,
      "loss": 0.0016,
      "step": 5320
    },
    {
      "epoch": 0.17056,
      "grad_norm": 0.015419808216392994,
      "learning_rate": 1.8863146666666668e-05,
      "loss": 0.001,
      "step": 5330
    },
    {
      "epoch": 0.17088,
      "grad_norm": 0.012268057093024254,
      "learning_rate": 1.8861013333333337e-05,
      "loss": 0.0013,
      "step": 5340
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.020548120141029358,
      "learning_rate": 1.8858880000000002e-05,
      "loss": 0.001,
      "step": 5350
    },
    {
      "epoch": 0.17152,
      "grad_norm": 0.01703333482146263,
      "learning_rate": 1.8856746666666667e-05,
      "loss": 0.0009,
      "step": 5360
    },
    {
      "epoch": 0.17184,
      "grad_norm": 0.017605893313884735,
      "learning_rate": 1.8854613333333336e-05,
      "loss": 0.0011,
      "step": 5370
    },
    {
      "epoch": 0.17216,
      "grad_norm": 0.01186282467097044,
      "learning_rate": 1.885248e-05,
      "loss": 0.0169,
      "step": 5380
    },
    {
      "epoch": 0.17248,
      "grad_norm": 0.013081502169370651,
      "learning_rate": 1.8850346666666667e-05,
      "loss": 0.0247,
      "step": 5390
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.015822185203433037,
      "learning_rate": 1.8848213333333332e-05,
      "loss": 0.0011,
      "step": 5400
    },
    {
      "epoch": 0.17312,
      "grad_norm": 0.013102156110107899,
      "learning_rate": 1.884608e-05,
      "loss": 0.001,
      "step": 5410
    },
    {
      "epoch": 0.17344,
      "grad_norm": 0.0168608445674181,
      "learning_rate": 1.884394666666667e-05,
      "loss": 0.0012,
      "step": 5420
    },
    {
      "epoch": 0.17376,
      "grad_norm": 0.31779107451438904,
      "learning_rate": 1.8841813333333335e-05,
      "loss": 0.079,
      "step": 5430
    },
    {
      "epoch": 0.17408,
      "grad_norm": 0.011563828215003014,
      "learning_rate": 1.8839680000000004e-05,
      "loss": 0.0009,
      "step": 5440
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.022702539339661598,
      "learning_rate": 1.883754666666667e-05,
      "loss": 0.0011,
      "step": 5450
    },
    {
      "epoch": 0.17472,
      "grad_norm": 0.01712968945503235,
      "learning_rate": 1.8835413333333335e-05,
      "loss": 0.0009,
      "step": 5460
    },
    {
      "epoch": 0.17504,
      "grad_norm": 0.013675606809556484,
      "learning_rate": 1.883328e-05,
      "loss": 0.0013,
      "step": 5470
    },
    {
      "epoch": 0.17536,
      "grad_norm": 0.012006298638880253,
      "learning_rate": 1.883114666666667e-05,
      "loss": 0.0011,
      "step": 5480
    },
    {
      "epoch": 0.17568,
      "grad_norm": 0.020036080852150917,
      "learning_rate": 1.8829013333333334e-05,
      "loss": 0.0296,
      "step": 5490
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.009775791317224503,
      "learning_rate": 1.882688e-05,
      "loss": 0.0384,
      "step": 5500
    },
    {
      "epoch": 0.17632,
      "grad_norm": 0.0121321314945817,
      "learning_rate": 1.882474666666667e-05,
      "loss": 0.0022,
      "step": 5510
    },
    {
      "epoch": 0.17664,
      "grad_norm": 0.013275284320116043,
      "learning_rate": 1.8822613333333334e-05,
      "loss": 0.0032,
      "step": 5520
    },
    {
      "epoch": 0.17696,
      "grad_norm": 0.022237256169319153,
      "learning_rate": 1.8820480000000002e-05,
      "loss": 0.0009,
      "step": 5530
    },
    {
      "epoch": 0.17728,
      "grad_norm": 0.011242887005209923,
      "learning_rate": 1.8818346666666668e-05,
      "loss": 0.0421,
      "step": 5540
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.03762132674455643,
      "learning_rate": 1.8816213333333337e-05,
      "loss": 0.001,
      "step": 5550
    },
    {
      "epoch": 0.17792,
      "grad_norm": 0.014987519942224026,
      "learning_rate": 1.8814080000000002e-05,
      "loss": 0.0008,
      "step": 5560
    },
    {
      "epoch": 0.17824,
      "grad_norm": 0.032974306493997574,
      "learning_rate": 1.8811946666666667e-05,
      "loss": 0.001,
      "step": 5570
    },
    {
      "epoch": 0.17856,
      "grad_norm": 0.03549875691533089,
      "learning_rate": 1.8809813333333336e-05,
      "loss": 0.0331,
      "step": 5580
    },
    {
      "epoch": 0.17888,
      "grad_norm": 0.01492819469422102,
      "learning_rate": 1.880768e-05,
      "loss": 0.0032,
      "step": 5590
    },
    {
      "epoch": 0.1792,
      "grad_norm": 5.146620750427246,
      "learning_rate": 1.8805546666666667e-05,
      "loss": 0.004,
      "step": 5600
    },
    {
      "epoch": 0.17952,
      "grad_norm": 0.013011186383664608,
      "learning_rate": 1.8803413333333336e-05,
      "loss": 0.0473,
      "step": 5610
    },
    {
      "epoch": 0.17984,
      "grad_norm": 0.016501514241099358,
      "learning_rate": 1.880128e-05,
      "loss": 0.0008,
      "step": 5620
    },
    {
      "epoch": 0.18016,
      "grad_norm": 0.01953929103910923,
      "learning_rate": 1.8799146666666666e-05,
      "loss": 0.0456,
      "step": 5630
    },
    {
      "epoch": 0.18048,
      "grad_norm": 0.013664749450981617,
      "learning_rate": 1.8797013333333335e-05,
      "loss": 0.0094,
      "step": 5640
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.016140863299369812,
      "learning_rate": 1.879488e-05,
      "loss": 0.0013,
      "step": 5650
    },
    {
      "epoch": 0.18112,
      "grad_norm": 0.016713446006178856,
      "learning_rate": 1.879274666666667e-05,
      "loss": 0.0009,
      "step": 5660
    },
    {
      "epoch": 0.18144,
      "grad_norm": 0.016268495470285416,
      "learning_rate": 1.8790613333333335e-05,
      "loss": 0.0031,
      "step": 5670
    },
    {
      "epoch": 0.18176,
      "grad_norm": 0.017929868772625923,
      "learning_rate": 1.8788480000000003e-05,
      "loss": 0.043,
      "step": 5680
    },
    {
      "epoch": 0.18208,
      "grad_norm": 0.018819577991962433,
      "learning_rate": 1.878634666666667e-05,
      "loss": 0.0853,
      "step": 5690
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.015494370833039284,
      "learning_rate": 1.8784213333333334e-05,
      "loss": 0.0014,
      "step": 5700
    },
    {
      "epoch": 0.18272,
      "grad_norm": 0.01870099827647209,
      "learning_rate": 1.8782080000000003e-05,
      "loss": 0.0019,
      "step": 5710
    },
    {
      "epoch": 0.18304,
      "grad_norm": 0.015474306419491768,
      "learning_rate": 1.877994666666667e-05,
      "loss": 0.001,
      "step": 5720
    },
    {
      "epoch": 0.18336,
      "grad_norm": 0.02614145167171955,
      "learning_rate": 1.8777813333333334e-05,
      "loss": 0.001,
      "step": 5730
    },
    {
      "epoch": 0.18368,
      "grad_norm": 0.13463996350765228,
      "learning_rate": 1.877568e-05,
      "loss": 0.0011,
      "step": 5740
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.01327267661690712,
      "learning_rate": 1.8773546666666668e-05,
      "loss": 0.0012,
      "step": 5750
    },
    {
      "epoch": 0.18432,
      "grad_norm": 0.009857195429503918,
      "learning_rate": 1.8771413333333333e-05,
      "loss": 0.001,
      "step": 5760
    },
    {
      "epoch": 0.18464,
      "grad_norm": 0.012592168524861336,
      "learning_rate": 1.8769280000000002e-05,
      "loss": 0.036,
      "step": 5770
    },
    {
      "epoch": 0.18496,
      "grad_norm": 0.0405525378882885,
      "learning_rate": 1.8767146666666667e-05,
      "loss": 0.055,
      "step": 5780
    },
    {
      "epoch": 0.18528,
      "grad_norm": 0.014821508899331093,
      "learning_rate": 1.8765013333333336e-05,
      "loss": 0.0013,
      "step": 5790
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.028733810409903526,
      "learning_rate": 1.876288e-05,
      "loss": 0.001,
      "step": 5800
    },
    {
      "epoch": 0.18592,
      "grad_norm": 0.017356036230921745,
      "learning_rate": 1.8760746666666667e-05,
      "loss": 0.0532,
      "step": 5810
    },
    {
      "epoch": 0.18624,
      "grad_norm": 0.02428591065108776,
      "learning_rate": 1.8758613333333336e-05,
      "loss": 0.0166,
      "step": 5820
    },
    {
      "epoch": 0.18656,
      "grad_norm": 1.9126763343811035,
      "learning_rate": 1.875648e-05,
      "loss": 0.0551,
      "step": 5830
    },
    {
      "epoch": 0.18688,
      "grad_norm": 0.02294897846877575,
      "learning_rate": 1.8754346666666666e-05,
      "loss": 0.0362,
      "step": 5840
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.015659837052226067,
      "learning_rate": 1.8752213333333335e-05,
      "loss": 0.0013,
      "step": 5850
    },
    {
      "epoch": 0.18752,
      "grad_norm": 0.018301736563444138,
      "learning_rate": 1.875008e-05,
      "loss": 0.0013,
      "step": 5860
    },
    {
      "epoch": 0.18784,
      "grad_norm": 0.09197096526622772,
      "learning_rate": 1.8747946666666666e-05,
      "loss": 0.0019,
      "step": 5870
    },
    {
      "epoch": 0.18816,
      "grad_norm": 0.014968120492994785,
      "learning_rate": 1.8745813333333335e-05,
      "loss": 0.0427,
      "step": 5880
    },
    {
      "epoch": 0.18848,
      "grad_norm": 0.01564418151974678,
      "learning_rate": 1.8743680000000003e-05,
      "loss": 0.0016,
      "step": 5890
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.12335055321455002,
      "learning_rate": 1.874154666666667e-05,
      "loss": 0.0017,
      "step": 5900
    },
    {
      "epoch": 0.18912,
      "grad_norm": 0.014961476437747478,
      "learning_rate": 1.8739413333333334e-05,
      "loss": 0.05,
      "step": 5910
    },
    {
      "epoch": 0.18944,
      "grad_norm": 0.016898155212402344,
      "learning_rate": 1.8737280000000003e-05,
      "loss": 0.0404,
      "step": 5920
    },
    {
      "epoch": 0.18976,
      "grad_norm": 0.019011005759239197,
      "learning_rate": 1.873514666666667e-05,
      "loss": 0.0027,
      "step": 5930
    },
    {
      "epoch": 0.19008,
      "grad_norm": 5.371581554412842,
      "learning_rate": 1.8733013333333334e-05,
      "loss": 0.0113,
      "step": 5940
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.01939959079027176,
      "learning_rate": 1.8730880000000003e-05,
      "loss": 0.0011,
      "step": 5950
    },
    {
      "epoch": 0.19072,
      "grad_norm": 0.024274185299873352,
      "learning_rate": 1.8728746666666668e-05,
      "loss": 0.0012,
      "step": 5960
    },
    {
      "epoch": 0.19104,
      "grad_norm": 0.014375131577253342,
      "learning_rate": 1.8726613333333333e-05,
      "loss": 0.0011,
      "step": 5970
    },
    {
      "epoch": 0.19136,
      "grad_norm": 0.017972173169255257,
      "learning_rate": 1.8724480000000002e-05,
      "loss": 0.0011,
      "step": 5980
    },
    {
      "epoch": 0.19168,
      "grad_norm": 0.01702638529241085,
      "learning_rate": 1.8722346666666667e-05,
      "loss": 0.0015,
      "step": 5990
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.011488769203424454,
      "learning_rate": 1.8720213333333336e-05,
      "loss": 0.0009,
      "step": 6000
    },
    {
      "epoch": 0.19232,
      "grad_norm": 0.016104068607091904,
      "learning_rate": 1.871808e-05,
      "loss": 0.001,
      "step": 6010
    },
    {
      "epoch": 0.19264,
      "grad_norm": 0.01848243921995163,
      "learning_rate": 1.871594666666667e-05,
      "loss": 0.0008,
      "step": 6020
    },
    {
      "epoch": 0.19296,
      "grad_norm": 0.016385717317461967,
      "learning_rate": 1.8713813333333336e-05,
      "loss": 0.0703,
      "step": 6030
    },
    {
      "epoch": 0.19328,
      "grad_norm": 0.02126343920826912,
      "learning_rate": 1.871168e-05,
      "loss": 0.0011,
      "step": 6040
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.012689868919551373,
      "learning_rate": 1.870954666666667e-05,
      "loss": 0.0009,
      "step": 6050
    },
    {
      "epoch": 0.19392,
      "grad_norm": 2.672502040863037,
      "learning_rate": 1.8707413333333335e-05,
      "loss": 0.0319,
      "step": 6060
    },
    {
      "epoch": 0.19424,
      "grad_norm": 0.034612011164426804,
      "learning_rate": 1.870528e-05,
      "loss": 0.0014,
      "step": 6070
    },
    {
      "epoch": 0.19456,
      "grad_norm": 0.017206775024533272,
      "learning_rate": 1.8703146666666666e-05,
      "loss": 0.0011,
      "step": 6080
    },
    {
      "epoch": 0.19488,
      "grad_norm": 0.0920615866780281,
      "learning_rate": 1.8701013333333335e-05,
      "loss": 0.0011,
      "step": 6090
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.015893401578068733,
      "learning_rate": 1.869888e-05,
      "loss": 0.0414,
      "step": 6100
    },
    {
      "epoch": 0.19552,
      "grad_norm": 0.023245418444275856,
      "learning_rate": 1.869674666666667e-05,
      "loss": 0.0009,
      "step": 6110
    },
    {
      "epoch": 0.19584,
      "grad_norm": 0.025656649842858315,
      "learning_rate": 1.8694613333333334e-05,
      "loss": 0.0014,
      "step": 6120
    },
    {
      "epoch": 0.19616,
      "grad_norm": 0.11538698524236679,
      "learning_rate": 1.8692480000000003e-05,
      "loss": 0.0458,
      "step": 6130
    },
    {
      "epoch": 0.19648,
      "grad_norm": 0.053066436201334,
      "learning_rate": 1.869034666666667e-05,
      "loss": 0.0011,
      "step": 6140
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.012972396798431873,
      "learning_rate": 1.8688213333333334e-05,
      "loss": 0.001,
      "step": 6150
    },
    {
      "epoch": 0.19712,
      "grad_norm": 0.019023824483156204,
      "learning_rate": 1.8686080000000003e-05,
      "loss": 0.0694,
      "step": 6160
    },
    {
      "epoch": 0.19744,
      "grad_norm": 0.012247047387063503,
      "learning_rate": 1.8683946666666668e-05,
      "loss": 0.0009,
      "step": 6170
    },
    {
      "epoch": 0.19776,
      "grad_norm": 0.013529389165341854,
      "learning_rate": 1.8681813333333333e-05,
      "loss": 0.0011,
      "step": 6180
    },
    {
      "epoch": 0.19808,
      "grad_norm": 0.11785906553268433,
      "learning_rate": 1.8679680000000002e-05,
      "loss": 0.0372,
      "step": 6190
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.013980538584291935,
      "learning_rate": 1.8677546666666667e-05,
      "loss": 0.001,
      "step": 6200
    },
    {
      "epoch": 0.19872,
      "grad_norm": 0.015504005365073681,
      "learning_rate": 1.8675413333333333e-05,
      "loss": 0.0009,
      "step": 6210
    },
    {
      "epoch": 0.19904,
      "grad_norm": 0.014634723775088787,
      "learning_rate": 1.867328e-05,
      "loss": 0.0011,
      "step": 6220
    },
    {
      "epoch": 0.19936,
      "grad_norm": 0.01592906005680561,
      "learning_rate": 1.8671146666666667e-05,
      "loss": 0.0012,
      "step": 6230
    },
    {
      "epoch": 0.19968,
      "grad_norm": 0.014759295620024204,
      "learning_rate": 1.8669013333333336e-05,
      "loss": 0.0015,
      "step": 6240
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013494090177118778,
      "learning_rate": 1.866688e-05,
      "loss": 0.0018,
      "step": 6250
    },
    {
      "epoch": 0.20032,
      "grad_norm": 0.01777702011168003,
      "learning_rate": 1.866474666666667e-05,
      "loss": 0.0259,
      "step": 6260
    },
    {
      "epoch": 0.20064,
      "grad_norm": 0.0181883592158556,
      "learning_rate": 1.8662613333333335e-05,
      "loss": 0.0008,
      "step": 6270
    },
    {
      "epoch": 0.20096,
      "grad_norm": 0.013691424392163754,
      "learning_rate": 1.866048e-05,
      "loss": 0.0018,
      "step": 6280
    },
    {
      "epoch": 0.20128,
      "grad_norm": 0.009037603624165058,
      "learning_rate": 1.865834666666667e-05,
      "loss": 0.0009,
      "step": 6290
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.011652316898107529,
      "learning_rate": 1.8656213333333335e-05,
      "loss": 0.0008,
      "step": 6300
    },
    {
      "epoch": 0.20192,
      "grad_norm": 0.030700527131557465,
      "learning_rate": 1.865408e-05,
      "loss": 0.0009,
      "step": 6310
    },
    {
      "epoch": 0.20224,
      "grad_norm": 0.010087214410305023,
      "learning_rate": 1.8651946666666666e-05,
      "loss": 0.001,
      "step": 6320
    },
    {
      "epoch": 0.20256,
      "grad_norm": 0.012015323154628277,
      "learning_rate": 1.8649813333333334e-05,
      "loss": 0.0007,
      "step": 6330
    },
    {
      "epoch": 0.20288,
      "grad_norm": 0.012083220295608044,
      "learning_rate": 1.864768e-05,
      "loss": 0.0009,
      "step": 6340
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.01329009234905243,
      "learning_rate": 1.864554666666667e-05,
      "loss": 0.0431,
      "step": 6350
    },
    {
      "epoch": 0.20352,
      "grad_norm": 0.015000591054558754,
      "learning_rate": 1.8643413333333337e-05,
      "loss": 0.004,
      "step": 6360
    },
    {
      "epoch": 0.20384,
      "grad_norm": 0.019017210230231285,
      "learning_rate": 1.8641280000000003e-05,
      "loss": 0.0008,
      "step": 6370
    },
    {
      "epoch": 0.20416,
      "grad_norm": 0.021234171465039253,
      "learning_rate": 1.8639146666666668e-05,
      "loss": 0.0393,
      "step": 6380
    },
    {
      "epoch": 0.20448,
      "grad_norm": 0.0223312396556139,
      "learning_rate": 1.8637013333333337e-05,
      "loss": 0.0047,
      "step": 6390
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.017704838886857033,
      "learning_rate": 1.8634880000000002e-05,
      "loss": 0.0008,
      "step": 6400
    },
    {
      "epoch": 0.20512,
      "grad_norm": 0.009270865470170975,
      "learning_rate": 1.8632746666666667e-05,
      "loss": 0.0489,
      "step": 6410
    },
    {
      "epoch": 0.20544,
      "grad_norm": 0.01346875261515379,
      "learning_rate": 1.8630613333333333e-05,
      "loss": 0.0007,
      "step": 6420
    },
    {
      "epoch": 0.20576,
      "grad_norm": 0.011383750475943089,
      "learning_rate": 1.862848e-05,
      "loss": 0.0015,
      "step": 6430
    },
    {
      "epoch": 0.20608,
      "grad_norm": 0.01774086430668831,
      "learning_rate": 1.8626346666666667e-05,
      "loss": 0.0008,
      "step": 6440
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.009388706646859646,
      "learning_rate": 1.8624213333333336e-05,
      "loss": 0.001,
      "step": 6450
    },
    {
      "epoch": 0.20672,
      "grad_norm": 0.024346306920051575,
      "learning_rate": 1.862208e-05,
      "loss": 0.0009,
      "step": 6460
    },
    {
      "epoch": 0.20704,
      "grad_norm": 0.014060691930353642,
      "learning_rate": 1.861994666666667e-05,
      "loss": 0.0035,
      "step": 6470
    },
    {
      "epoch": 0.20736,
      "grad_norm": 0.010324268601834774,
      "learning_rate": 1.8617813333333335e-05,
      "loss": 0.0403,
      "step": 6480
    },
    {
      "epoch": 0.20768,
      "grad_norm": 0.014220931567251682,
      "learning_rate": 1.861568e-05,
      "loss": 0.003,
      "step": 6490
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.013675177469849586,
      "learning_rate": 1.861354666666667e-05,
      "loss": 0.0318,
      "step": 6500
    },
    {
      "epoch": 0.20832,
      "grad_norm": 0.01147227268666029,
      "learning_rate": 1.8611413333333335e-05,
      "loss": 0.0007,
      "step": 6510
    },
    {
      "epoch": 0.20864,
      "grad_norm": 0.01064699050039053,
      "learning_rate": 1.860928e-05,
      "loss": 0.0451,
      "step": 6520
    },
    {
      "epoch": 0.20896,
      "grad_norm": 0.01511277537792921,
      "learning_rate": 1.860714666666667e-05,
      "loss": 0.0008,
      "step": 6530
    },
    {
      "epoch": 0.20928,
      "grad_norm": 0.012834655120968819,
      "learning_rate": 1.8605013333333334e-05,
      "loss": 0.0204,
      "step": 6540
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.013746336102485657,
      "learning_rate": 1.860288e-05,
      "loss": 0.0472,
      "step": 6550
    },
    {
      "epoch": 0.20992,
      "grad_norm": 0.02001991681754589,
      "learning_rate": 1.860074666666667e-05,
      "loss": 0.001,
      "step": 6560
    },
    {
      "epoch": 0.21024,
      "grad_norm": 0.014207073487341404,
      "learning_rate": 1.8598613333333334e-05,
      "loss": 0.0009,
      "step": 6570
    },
    {
      "epoch": 0.21056,
      "grad_norm": 0.08100861310958862,
      "learning_rate": 1.8596480000000003e-05,
      "loss": 0.0073,
      "step": 6580
    },
    {
      "epoch": 0.21088,
      "grad_norm": 0.08723237365484238,
      "learning_rate": 1.8594346666666668e-05,
      "loss": 0.0404,
      "step": 6590
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.01184940803796053,
      "learning_rate": 1.8592213333333337e-05,
      "loss": 0.0086,
      "step": 6600
    },
    {
      "epoch": 0.21152,
      "grad_norm": 0.013857252895832062,
      "learning_rate": 1.8590080000000002e-05,
      "loss": 0.0007,
      "step": 6610
    },
    {
      "epoch": 0.21184,
      "grad_norm": 0.01261989027261734,
      "learning_rate": 1.8587946666666667e-05,
      "loss": 0.0014,
      "step": 6620
    },
    {
      "epoch": 0.21216,
      "grad_norm": 0.029336893931031227,
      "learning_rate": 1.8585813333333336e-05,
      "loss": 0.0008,
      "step": 6630
    },
    {
      "epoch": 0.21248,
      "grad_norm": 0.011364058591425419,
      "learning_rate": 1.858368e-05,
      "loss": 0.0304,
      "step": 6640
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.012700757011771202,
      "learning_rate": 1.8581546666666667e-05,
      "loss": 0.0374,
      "step": 6650
    },
    {
      "epoch": 0.21312,
      "grad_norm": 0.01629664935171604,
      "learning_rate": 1.8579413333333332e-05,
      "loss": 0.001,
      "step": 6660
    },
    {
      "epoch": 0.21344,
      "grad_norm": 0.025881705805659294,
      "learning_rate": 1.857728e-05,
      "loss": 0.0009,
      "step": 6670
    },
    {
      "epoch": 0.21376,
      "grad_norm": 0.7157048583030701,
      "learning_rate": 1.8575146666666667e-05,
      "loss": 0.0015,
      "step": 6680
    },
    {
      "epoch": 0.21408,
      "grad_norm": 0.014259207993745804,
      "learning_rate": 1.8573013333333335e-05,
      "loss": 0.0152,
      "step": 6690
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.015881983563303947,
      "learning_rate": 1.857088e-05,
      "loss": 0.0009,
      "step": 6700
    },
    {
      "epoch": 0.21472,
      "grad_norm": 0.016986293718218803,
      "learning_rate": 1.856874666666667e-05,
      "loss": 0.0008,
      "step": 6710
    },
    {
      "epoch": 0.21504,
      "grad_norm": 0.014926637522876263,
      "learning_rate": 1.8566613333333335e-05,
      "loss": 0.0026,
      "step": 6720
    },
    {
      "epoch": 0.21536,
      "grad_norm": 0.01957686059176922,
      "learning_rate": 1.8564480000000004e-05,
      "loss": 0.0504,
      "step": 6730
    },
    {
      "epoch": 0.21568,
      "grad_norm": 0.015327328816056252,
      "learning_rate": 1.856234666666667e-05,
      "loss": 0.0427,
      "step": 6740
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.018411464989185333,
      "learning_rate": 1.8560213333333334e-05,
      "loss": 0.0008,
      "step": 6750
    },
    {
      "epoch": 0.21632,
      "grad_norm": 0.01795404776930809,
      "learning_rate": 1.855808e-05,
      "loss": 0.0009,
      "step": 6760
    },
    {
      "epoch": 0.21664,
      "grad_norm": 0.012368252500891685,
      "learning_rate": 1.855594666666667e-05,
      "loss": 0.0372,
      "step": 6770
    },
    {
      "epoch": 0.21696,
      "grad_norm": 0.01222479622811079,
      "learning_rate": 1.8553813333333334e-05,
      "loss": 0.0012,
      "step": 6780
    },
    {
      "epoch": 0.21728,
      "grad_norm": 0.015854710713028908,
      "learning_rate": 1.855168e-05,
      "loss": 0.0064,
      "step": 6790
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.012125053443014622,
      "learning_rate": 1.8549546666666668e-05,
      "loss": 0.0446,
      "step": 6800
    },
    {
      "epoch": 0.21792,
      "grad_norm": 1.5387568473815918,
      "learning_rate": 1.8547413333333333e-05,
      "loss": 0.0438,
      "step": 6810
    },
    {
      "epoch": 0.21824,
      "grad_norm": 0.01808055117726326,
      "learning_rate": 1.8545280000000002e-05,
      "loss": 0.001,
      "step": 6820
    },
    {
      "epoch": 0.21856,
      "grad_norm": 0.01623850129544735,
      "learning_rate": 1.854314666666667e-05,
      "loss": 0.001,
      "step": 6830
    },
    {
      "epoch": 0.21888,
      "grad_norm": 0.026055000722408295,
      "learning_rate": 1.8541013333333336e-05,
      "loss": 0.001,
      "step": 6840
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.022765841335058212,
      "learning_rate": 1.853888e-05,
      "loss": 0.0378,
      "step": 6850
    },
    {
      "epoch": 0.21952,
      "grad_norm": 0.03486842289566994,
      "learning_rate": 1.8536746666666667e-05,
      "loss": 0.0012,
      "step": 6860
    },
    {
      "epoch": 0.21984,
      "grad_norm": 0.012332581914961338,
      "learning_rate": 1.8534613333333336e-05,
      "loss": 0.0013,
      "step": 6870
    },
    {
      "epoch": 0.22016,
      "grad_norm": 0.016575094312429428,
      "learning_rate": 1.853248e-05,
      "loss": 0.034,
      "step": 6880
    },
    {
      "epoch": 0.22048,
      "grad_norm": 0.015045282430946827,
      "learning_rate": 1.8530346666666667e-05,
      "loss": 0.0013,
      "step": 6890
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.01541348360478878,
      "learning_rate": 1.8528213333333335e-05,
      "loss": 0.0008,
      "step": 6900
    },
    {
      "epoch": 0.22112,
      "grad_norm": 0.012714040465652943,
      "learning_rate": 1.852608e-05,
      "loss": 0.001,
      "step": 6910
    },
    {
      "epoch": 0.22144,
      "grad_norm": 0.016826488077640533,
      "learning_rate": 1.852394666666667e-05,
      "loss": 0.0012,
      "step": 6920
    },
    {
      "epoch": 0.22176,
      "grad_norm": 0.012820756994187832,
      "learning_rate": 1.8521813333333335e-05,
      "loss": 0.0011,
      "step": 6930
    },
    {
      "epoch": 0.22208,
      "grad_norm": 0.6787649989128113,
      "learning_rate": 1.8519680000000004e-05,
      "loss": 0.002,
      "step": 6940
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.013678504154086113,
      "learning_rate": 1.851754666666667e-05,
      "loss": 0.035,
      "step": 6950
    },
    {
      "epoch": 0.22272,
      "grad_norm": 0.05862214043736458,
      "learning_rate": 1.8515413333333334e-05,
      "loss": 0.0464,
      "step": 6960
    },
    {
      "epoch": 0.22304,
      "grad_norm": 1.9213449954986572,
      "learning_rate": 1.8513280000000003e-05,
      "loss": 0.1021,
      "step": 6970
    },
    {
      "epoch": 0.22336,
      "grad_norm": 0.025995301082730293,
      "learning_rate": 1.851114666666667e-05,
      "loss": 0.001,
      "step": 6980
    },
    {
      "epoch": 0.22368,
      "grad_norm": 0.016536695882678032,
      "learning_rate": 1.8509013333333334e-05,
      "loss": 0.001,
      "step": 6990
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.016655899584293365,
      "learning_rate": 1.8506880000000003e-05,
      "loss": 0.0667,
      "step": 7000
    },
    {
      "epoch": 0.22432,
      "grad_norm": 0.018597543239593506,
      "learning_rate": 1.8504746666666668e-05,
      "loss": 0.034,
      "step": 7010
    },
    {
      "epoch": 0.22464,
      "grad_norm": 0.06577430665493011,
      "learning_rate": 1.8502613333333333e-05,
      "loss": 0.0016,
      "step": 7020
    },
    {
      "epoch": 0.22496,
      "grad_norm": 0.018903469666838646,
      "learning_rate": 1.8500480000000002e-05,
      "loss": 0.034,
      "step": 7030
    },
    {
      "epoch": 0.22528,
      "grad_norm": 0.020318137481808662,
      "learning_rate": 1.8498346666666668e-05,
      "loss": 0.0011,
      "step": 7040
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.018525080755352974,
      "learning_rate": 1.8496213333333336e-05,
      "loss": 0.0018,
      "step": 7050
    },
    {
      "epoch": 0.22592,
      "grad_norm": 0.01731640286743641,
      "learning_rate": 1.849408e-05,
      "loss": 0.0012,
      "step": 7060
    },
    {
      "epoch": 0.22624,
      "grad_norm": 0.014930018223822117,
      "learning_rate": 1.849194666666667e-05,
      "loss": 0.039,
      "step": 7070
    },
    {
      "epoch": 0.22656,
      "grad_norm": 0.01782337948679924,
      "learning_rate": 1.8489813333333336e-05,
      "loss": 0.0011,
      "step": 7080
    },
    {
      "epoch": 0.22688,
      "grad_norm": 2.0377626419067383,
      "learning_rate": 1.848768e-05,
      "loss": 0.0046,
      "step": 7090
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.016846418380737305,
      "learning_rate": 1.8485546666666667e-05,
      "loss": 0.0058,
      "step": 7100
    },
    {
      "epoch": 0.22752,
      "grad_norm": 0.01622588001191616,
      "learning_rate": 1.8483413333333335e-05,
      "loss": 0.001,
      "step": 7110
    },
    {
      "epoch": 0.22784,
      "grad_norm": 0.013302594423294067,
      "learning_rate": 1.848128e-05,
      "loss": 0.0009,
      "step": 7120
    },
    {
      "epoch": 0.22816,
      "grad_norm": 0.014042803086340427,
      "learning_rate": 1.8479146666666666e-05,
      "loss": 0.0011,
      "step": 7130
    },
    {
      "epoch": 0.22848,
      "grad_norm": 0.016394643113017082,
      "learning_rate": 1.8477013333333335e-05,
      "loss": 0.0013,
      "step": 7140
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.016015155240893364,
      "learning_rate": 1.847488e-05,
      "loss": 0.0331,
      "step": 7150
    },
    {
      "epoch": 0.22912,
      "grad_norm": 0.016357216984033585,
      "learning_rate": 1.847274666666667e-05,
      "loss": 0.0009,
      "step": 7160
    },
    {
      "epoch": 0.22944,
      "grad_norm": 0.014463548548519611,
      "learning_rate": 1.8470613333333334e-05,
      "loss": 0.0189,
      "step": 7170
    },
    {
      "epoch": 0.22976,
      "grad_norm": 0.017036035656929016,
      "learning_rate": 1.8468480000000003e-05,
      "loss": 0.0009,
      "step": 7180
    },
    {
      "epoch": 0.23008,
      "grad_norm": 0.014512368477880955,
      "learning_rate": 1.846634666666667e-05,
      "loss": 0.0338,
      "step": 7190
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.021089205518364906,
      "learning_rate": 1.8464213333333334e-05,
      "loss": 0.001,
      "step": 7200
    },
    {
      "epoch": 0.23072,
      "grad_norm": 0.014500470831990242,
      "learning_rate": 1.8462080000000003e-05,
      "loss": 0.0009,
      "step": 7210
    },
    {
      "epoch": 0.23104,
      "grad_norm": 0.01576385833323002,
      "learning_rate": 1.8459946666666668e-05,
      "loss": 0.0412,
      "step": 7220
    },
    {
      "epoch": 0.23136,
      "grad_norm": 0.0782882496714592,
      "learning_rate": 1.8457813333333333e-05,
      "loss": 0.0011,
      "step": 7230
    },
    {
      "epoch": 0.23168,
      "grad_norm": 0.0181248988956213,
      "learning_rate": 1.8455680000000002e-05,
      "loss": 0.0019,
      "step": 7240
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.01772579364478588,
      "learning_rate": 1.8453546666666668e-05,
      "loss": 0.0422,
      "step": 7250
    },
    {
      "epoch": 0.23232,
      "grad_norm": 0.01688922569155693,
      "learning_rate": 1.8451413333333333e-05,
      "loss": 0.0013,
      "step": 7260
    },
    {
      "epoch": 0.23264,
      "grad_norm": 0.015533646568655968,
      "learning_rate": 1.844928e-05,
      "loss": 0.0007,
      "step": 7270
    },
    {
      "epoch": 0.23296,
      "grad_norm": 0.013400312513113022,
      "learning_rate": 1.844714666666667e-05,
      "loss": 0.001,
      "step": 7280
    },
    {
      "epoch": 0.23328,
      "grad_norm": 0.010228677652776241,
      "learning_rate": 1.8445013333333336e-05,
      "loss": 0.001,
      "step": 7290
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.009540154598653316,
      "learning_rate": 1.844288e-05,
      "loss": 0.0008,
      "step": 7300
    },
    {
      "epoch": 0.23392,
      "grad_norm": 0.013501010835170746,
      "learning_rate": 1.844074666666667e-05,
      "loss": 0.0007,
      "step": 7310
    },
    {
      "epoch": 0.23424,
      "grad_norm": 0.013313518837094307,
      "learning_rate": 1.8438613333333335e-05,
      "loss": 0.0025,
      "step": 7320
    },
    {
      "epoch": 0.23456,
      "grad_norm": 0.013576619327068329,
      "learning_rate": 1.843648e-05,
      "loss": 0.0008,
      "step": 7330
    },
    {
      "epoch": 0.23488,
      "grad_norm": 0.010073835961520672,
      "learning_rate": 1.843434666666667e-05,
      "loss": 0.0236,
      "step": 7340
    },
    {
      "epoch": 0.2352,
      "grad_norm": 8.850625991821289,
      "learning_rate": 1.8432213333333335e-05,
      "loss": 0.0182,
      "step": 7350
    },
    {
      "epoch": 0.23552,
      "grad_norm": 0.012541649863123894,
      "learning_rate": 1.843008e-05,
      "loss": 0.0009,
      "step": 7360
    },
    {
      "epoch": 0.23584,
      "grad_norm": 0.05540666729211807,
      "learning_rate": 1.842794666666667e-05,
      "loss": 0.0008,
      "step": 7370
    },
    {
      "epoch": 0.23616,
      "grad_norm": 0.014432534575462341,
      "learning_rate": 1.8425813333333334e-05,
      "loss": 0.0007,
      "step": 7380
    },
    {
      "epoch": 0.23648,
      "grad_norm": 0.06493335217237473,
      "learning_rate": 1.8423680000000003e-05,
      "loss": 0.047,
      "step": 7390
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.015484709292650223,
      "learning_rate": 1.842154666666667e-05,
      "loss": 0.0082,
      "step": 7400
    },
    {
      "epoch": 0.23712,
      "grad_norm": 0.010026250965893269,
      "learning_rate": 1.8419413333333337e-05,
      "loss": 0.0012,
      "step": 7410
    },
    {
      "epoch": 0.23744,
      "grad_norm": 0.014566494151949883,
      "learning_rate": 1.8417280000000003e-05,
      "loss": 0.0439,
      "step": 7420
    },
    {
      "epoch": 0.23776,
      "grad_norm": 0.22515371441841125,
      "learning_rate": 1.8415146666666668e-05,
      "loss": 0.1039,
      "step": 7430
    },
    {
      "epoch": 0.23808,
      "grad_norm": 0.016894623637199402,
      "learning_rate": 1.8413013333333333e-05,
      "loss": 0.0009,
      "step": 7440
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.06199127063155174,
      "learning_rate": 1.8410880000000002e-05,
      "loss": 0.0011,
      "step": 7450
    },
    {
      "epoch": 0.23872,
      "grad_norm": 0.013127243146300316,
      "learning_rate": 1.8408746666666668e-05,
      "loss": 0.001,
      "step": 7460
    },
    {
      "epoch": 0.23904,
      "grad_norm": 0.017334269359707832,
      "learning_rate": 1.8406613333333333e-05,
      "loss": 0.001,
      "step": 7470
    },
    {
      "epoch": 0.23936,
      "grad_norm": 0.016636019572615623,
      "learning_rate": 1.8404480000000002e-05,
      "loss": 0.0009,
      "step": 7480
    },
    {
      "epoch": 0.23968,
      "grad_norm": 0.017373595386743546,
      "learning_rate": 1.8402346666666667e-05,
      "loss": 0.0099,
      "step": 7490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.011616725474596024,
      "learning_rate": 1.8400213333333336e-05,
      "loss": 0.0083,
      "step": 7500
    },
    {
      "epoch": 0.24032,
      "grad_norm": 0.009896433912217617,
      "learning_rate": 1.839808e-05,
      "loss": 0.0007,
      "step": 7510
    },
    {
      "epoch": 0.24064,
      "grad_norm": 0.011732887476682663,
      "learning_rate": 1.839594666666667e-05,
      "loss": 0.0023,
      "step": 7520
    },
    {
      "epoch": 0.24096,
      "grad_norm": 0.008180314674973488,
      "learning_rate": 1.8393813333333335e-05,
      "loss": 0.0007,
      "step": 7530
    },
    {
      "epoch": 0.24128,
      "grad_norm": 0.015001093968749046,
      "learning_rate": 1.839168e-05,
      "loss": 0.0093,
      "step": 7540
    },
    {
      "epoch": 0.2416,
      "grad_norm": 4.653637409210205,
      "learning_rate": 1.838954666666667e-05,
      "loss": 0.0641,
      "step": 7550
    },
    {
      "epoch": 0.24192,
      "grad_norm": 0.012928985059261322,
      "learning_rate": 1.8387413333333335e-05,
      "loss": 0.0006,
      "step": 7560
    },
    {
      "epoch": 0.24224,
      "grad_norm": 0.01288622710853815,
      "learning_rate": 1.838528e-05,
      "loss": 0.0008,
      "step": 7570
    },
    {
      "epoch": 0.24256,
      "grad_norm": 0.009365463629364967,
      "learning_rate": 1.838314666666667e-05,
      "loss": 0.0008,
      "step": 7580
    },
    {
      "epoch": 0.24288,
      "grad_norm": 0.01781223528087139,
      "learning_rate": 1.8381013333333334e-05,
      "loss": 0.0007,
      "step": 7590
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.010215471498668194,
      "learning_rate": 1.837888e-05,
      "loss": 0.0513,
      "step": 7600
    },
    {
      "epoch": 0.24352,
      "grad_norm": 0.012288845144212246,
      "learning_rate": 1.837674666666667e-05,
      "loss": 0.0276,
      "step": 7610
    },
    {
      "epoch": 0.24384,
      "grad_norm": 2.4743528366088867,
      "learning_rate": 1.8374613333333334e-05,
      "loss": 0.0376,
      "step": 7620
    },
    {
      "epoch": 0.24416,
      "grad_norm": 0.013902253471314907,
      "learning_rate": 1.8372480000000003e-05,
      "loss": 0.0009,
      "step": 7630
    },
    {
      "epoch": 0.24448,
      "grad_norm": 0.014695166610181332,
      "learning_rate": 1.8370346666666668e-05,
      "loss": 0.0473,
      "step": 7640
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.015051341615617275,
      "learning_rate": 1.8368213333333337e-05,
      "loss": 0.008,
      "step": 7650
    },
    {
      "epoch": 0.24512,
      "grad_norm": 0.1002437099814415,
      "learning_rate": 1.8366080000000002e-05,
      "loss": 0.0013,
      "step": 7660
    },
    {
      "epoch": 0.24544,
      "grad_norm": 0.010538077913224697,
      "learning_rate": 1.8363946666666668e-05,
      "loss": 0.0269,
      "step": 7670
    },
    {
      "epoch": 0.24576,
      "grad_norm": 0.019417930394411087,
      "learning_rate": 1.8361813333333336e-05,
      "loss": 0.0009,
      "step": 7680
    },
    {
      "epoch": 0.24608,
      "grad_norm": 0.014761609956622124,
      "learning_rate": 1.8359680000000002e-05,
      "loss": 0.0008,
      "step": 7690
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.019964298233389854,
      "learning_rate": 1.8357546666666667e-05,
      "loss": 0.0008,
      "step": 7700
    },
    {
      "epoch": 0.24672,
      "grad_norm": 0.021599365398287773,
      "learning_rate": 1.8355413333333332e-05,
      "loss": 0.0028,
      "step": 7710
    },
    {
      "epoch": 0.24704,
      "grad_norm": 0.01168910413980484,
      "learning_rate": 1.835328e-05,
      "loss": 0.0009,
      "step": 7720
    },
    {
      "epoch": 0.24736,
      "grad_norm": 0.01155680138617754,
      "learning_rate": 1.8351146666666667e-05,
      "loss": 0.0468,
      "step": 7730
    },
    {
      "epoch": 0.24768,
      "grad_norm": 0.02801225520670414,
      "learning_rate": 1.8349013333333335e-05,
      "loss": 0.0454,
      "step": 7740
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.015452500432729721,
      "learning_rate": 1.8346880000000004e-05,
      "loss": 0.0457,
      "step": 7750
    },
    {
      "epoch": 0.24832,
      "grad_norm": 0.03690032660961151,
      "learning_rate": 1.834474666666667e-05,
      "loss": 0.001,
      "step": 7760
    },
    {
      "epoch": 0.24864,
      "grad_norm": 0.01181389857083559,
      "learning_rate": 1.8342613333333335e-05,
      "loss": 0.0043,
      "step": 7770
    },
    {
      "epoch": 0.24896,
      "grad_norm": 0.016518210992217064,
      "learning_rate": 1.834048e-05,
      "loss": 0.001,
      "step": 7780
    },
    {
      "epoch": 0.24928,
      "grad_norm": 0.0180876012891531,
      "learning_rate": 1.833834666666667e-05,
      "loss": 0.0009,
      "step": 7790
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.07785242050886154,
      "learning_rate": 1.8336213333333334e-05,
      "loss": 0.0275,
      "step": 7800
    },
    {
      "epoch": 0.24992,
      "grad_norm": 2.2512013912200928,
      "learning_rate": 1.833408e-05,
      "loss": 0.0464,
      "step": 7810
    },
    {
      "epoch": 0.25024,
      "grad_norm": 0.01604864001274109,
      "learning_rate": 1.833194666666667e-05,
      "loss": 0.0016,
      "step": 7820
    },
    {
      "epoch": 0.25056,
      "grad_norm": 0.01401561126112938,
      "learning_rate": 1.8329813333333334e-05,
      "loss": 0.1122,
      "step": 7830
    },
    {
      "epoch": 0.25088,
      "grad_norm": 0.022033408284187317,
      "learning_rate": 1.8327680000000003e-05,
      "loss": 0.0581,
      "step": 7840
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.05883118137717247,
      "learning_rate": 1.8325546666666668e-05,
      "loss": 0.0012,
      "step": 7850
    },
    {
      "epoch": 0.25152,
      "grad_norm": 0.038594987243413925,
      "learning_rate": 1.8323413333333337e-05,
      "loss": 0.0012,
      "step": 7860
    },
    {
      "epoch": 0.25184,
      "grad_norm": 0.01711317151784897,
      "learning_rate": 1.8321280000000002e-05,
      "loss": 0.0011,
      "step": 7870
    },
    {
      "epoch": 0.25216,
      "grad_norm": 0.025843558833003044,
      "learning_rate": 1.8319146666666668e-05,
      "loss": 0.001,
      "step": 7880
    },
    {
      "epoch": 0.25248,
      "grad_norm": 0.03395116329193115,
      "learning_rate": 1.8317013333333336e-05,
      "loss": 0.0265,
      "step": 7890
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.018218815326690674,
      "learning_rate": 1.8314880000000002e-05,
      "loss": 0.0014,
      "step": 7900
    },
    {
      "epoch": 0.25312,
      "grad_norm": 0.019253401085734367,
      "learning_rate": 1.8312746666666667e-05,
      "loss": 0.0197,
      "step": 7910
    },
    {
      "epoch": 0.25344,
      "grad_norm": 0.03292231261730194,
      "learning_rate": 1.8310613333333336e-05,
      "loss": 0.0868,
      "step": 7920
    },
    {
      "epoch": 0.25376,
      "grad_norm": 0.031032999977469444,
      "learning_rate": 1.830848e-05,
      "loss": 0.0016,
      "step": 7930
    },
    {
      "epoch": 0.25408,
      "grad_norm": 0.01936955936253071,
      "learning_rate": 1.8306346666666667e-05,
      "loss": 0.0014,
      "step": 7940
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.25303134322166443,
      "learning_rate": 1.8304213333333335e-05,
      "loss": 0.0022,
      "step": 7950
    },
    {
      "epoch": 0.25472,
      "grad_norm": 0.019432099536061287,
      "learning_rate": 1.830208e-05,
      "loss": 0.0024,
      "step": 7960
    },
    {
      "epoch": 0.25504,
      "grad_norm": 0.016125857830047607,
      "learning_rate": 1.829994666666667e-05,
      "loss": 0.0029,
      "step": 7970
    },
    {
      "epoch": 0.25536,
      "grad_norm": 0.013155598193407059,
      "learning_rate": 1.8297813333333335e-05,
      "loss": 0.001,
      "step": 7980
    },
    {
      "epoch": 0.25568,
      "grad_norm": 0.2020081728696823,
      "learning_rate": 1.8295680000000004e-05,
      "loss": 0.0015,
      "step": 7990
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.016541847959160805,
      "learning_rate": 1.829354666666667e-05,
      "loss": 0.0012,
      "step": 8000
    },
    {
      "epoch": 0.25632,
      "grad_norm": 0.02456858567893505,
      "learning_rate": 1.8291413333333334e-05,
      "loss": 0.0011,
      "step": 8010
    },
    {
      "epoch": 0.25664,
      "grad_norm": 0.015290910378098488,
      "learning_rate": 1.8289280000000003e-05,
      "loss": 0.0614,
      "step": 8020
    },
    {
      "epoch": 0.25696,
      "grad_norm": 0.015507384203374386,
      "learning_rate": 1.828714666666667e-05,
      "loss": 0.0009,
      "step": 8030
    },
    {
      "epoch": 0.25728,
      "grad_norm": 0.014907082542777061,
      "learning_rate": 1.8285013333333334e-05,
      "loss": 0.0389,
      "step": 8040
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.020184168592095375,
      "learning_rate": 1.828288e-05,
      "loss": 0.007,
      "step": 8050
    },
    {
      "epoch": 0.25792,
      "grad_norm": 0.02085847407579422,
      "learning_rate": 1.8280746666666668e-05,
      "loss": 0.02,
      "step": 8060
    },
    {
      "epoch": 0.25824,
      "grad_norm": 0.013037963770329952,
      "learning_rate": 1.8278613333333333e-05,
      "loss": 0.0009,
      "step": 8070
    },
    {
      "epoch": 0.25856,
      "grad_norm": 0.017520740628242493,
      "learning_rate": 1.8276480000000002e-05,
      "loss": 0.0008,
      "step": 8080
    },
    {
      "epoch": 0.25888,
      "grad_norm": 0.021610897034406662,
      "learning_rate": 1.8274346666666668e-05,
      "loss": 0.0164,
      "step": 8090
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.015508253127336502,
      "learning_rate": 1.8272213333333336e-05,
      "loss": 0.0013,
      "step": 8100
    },
    {
      "epoch": 0.25952,
      "grad_norm": 0.06334029883146286,
      "learning_rate": 1.8270080000000002e-05,
      "loss": 0.0024,
      "step": 8110
    },
    {
      "epoch": 0.25984,
      "grad_norm": 0.018496479839086533,
      "learning_rate": 1.8267946666666667e-05,
      "loss": 0.0016,
      "step": 8120
    },
    {
      "epoch": 0.26016,
      "grad_norm": 0.01226548757404089,
      "learning_rate": 1.8265813333333336e-05,
      "loss": 0.0374,
      "step": 8130
    },
    {
      "epoch": 0.26048,
      "grad_norm": 0.017135273665189743,
      "learning_rate": 1.826368e-05,
      "loss": 0.001,
      "step": 8140
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.015599432401359081,
      "learning_rate": 1.8261546666666667e-05,
      "loss": 0.001,
      "step": 8150
    },
    {
      "epoch": 0.26112,
      "grad_norm": 0.01026055309921503,
      "learning_rate": 1.8259413333333335e-05,
      "loss": 0.0009,
      "step": 8160
    },
    {
      "epoch": 0.26144,
      "grad_norm": 0.022116558626294136,
      "learning_rate": 1.825728e-05,
      "loss": 0.0009,
      "step": 8170
    },
    {
      "epoch": 0.26176,
      "grad_norm": 0.011732812039554119,
      "learning_rate": 1.8255146666666666e-05,
      "loss": 0.0009,
      "step": 8180
    },
    {
      "epoch": 0.26208,
      "grad_norm": 0.011883600614964962,
      "learning_rate": 1.8253013333333335e-05,
      "loss": 0.0202,
      "step": 8190
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.0140531649813056,
      "learning_rate": 1.825088e-05,
      "loss": 0.0362,
      "step": 8200
    },
    {
      "epoch": 0.26272,
      "grad_norm": 0.016312330961227417,
      "learning_rate": 1.824874666666667e-05,
      "loss": 0.0008,
      "step": 8210
    },
    {
      "epoch": 0.26304,
      "grad_norm": 0.013644118793308735,
      "learning_rate": 1.8246613333333334e-05,
      "loss": 0.0011,
      "step": 8220
    },
    {
      "epoch": 0.26336,
      "grad_norm": 0.010496675036847591,
      "learning_rate": 1.8244480000000003e-05,
      "loss": 0.0461,
      "step": 8230
    },
    {
      "epoch": 0.26368,
      "grad_norm": 0.012479076161980629,
      "learning_rate": 1.824234666666667e-05,
      "loss": 0.0009,
      "step": 8240
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.022016361355781555,
      "learning_rate": 1.8240213333333334e-05,
      "loss": 0.0011,
      "step": 8250
    },
    {
      "epoch": 0.26432,
      "grad_norm": 0.017272626981139183,
      "learning_rate": 1.8238080000000003e-05,
      "loss": 0.0007,
      "step": 8260
    },
    {
      "epoch": 0.26464,
      "grad_norm": 0.012399519793689251,
      "learning_rate": 1.8235946666666668e-05,
      "loss": 0.0375,
      "step": 8270
    },
    {
      "epoch": 0.26496,
      "grad_norm": 0.3189760744571686,
      "learning_rate": 1.8233813333333334e-05,
      "loss": 0.0454,
      "step": 8280
    },
    {
      "epoch": 0.26528,
      "grad_norm": 0.01061607152223587,
      "learning_rate": 1.823168e-05,
      "loss": 0.0009,
      "step": 8290
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.01298239640891552,
      "learning_rate": 1.8229546666666668e-05,
      "loss": 0.0011,
      "step": 8300
    },
    {
      "epoch": 0.26592,
      "grad_norm": 0.01992214098572731,
      "learning_rate": 1.8227413333333336e-05,
      "loss": 0.0045,
      "step": 8310
    },
    {
      "epoch": 0.26624,
      "grad_norm": 0.014972729608416557,
      "learning_rate": 1.8225280000000002e-05,
      "loss": 0.001,
      "step": 8320
    },
    {
      "epoch": 0.26656,
      "grad_norm": 0.015244918875396252,
      "learning_rate": 1.822314666666667e-05,
      "loss": 0.0274,
      "step": 8330
    },
    {
      "epoch": 0.26688,
      "grad_norm": 0.012296877801418304,
      "learning_rate": 1.8221013333333336e-05,
      "loss": 0.0007,
      "step": 8340
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.01213189959526062,
      "learning_rate": 1.821888e-05,
      "loss": 0.0011,
      "step": 8350
    },
    {
      "epoch": 0.26752,
      "grad_norm": 0.013435259461402893,
      "learning_rate": 1.821674666666667e-05,
      "loss": 0.0008,
      "step": 8360
    },
    {
      "epoch": 0.26784,
      "grad_norm": 0.018481088802218437,
      "learning_rate": 1.8214613333333335e-05,
      "loss": 0.0008,
      "step": 8370
    },
    {
      "epoch": 0.26816,
      "grad_norm": 0.019470004364848137,
      "learning_rate": 1.821248e-05,
      "loss": 0.0344,
      "step": 8380
    },
    {
      "epoch": 0.26848,
      "grad_norm": 0.05148095265030861,
      "learning_rate": 1.8210346666666666e-05,
      "loss": 0.0009,
      "step": 8390
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.009242893196642399,
      "learning_rate": 1.8208213333333335e-05,
      "loss": 0.001,
      "step": 8400
    },
    {
      "epoch": 0.26912,
      "grad_norm": 0.018918626010417938,
      "learning_rate": 1.820608e-05,
      "loss": 0.0009,
      "step": 8410
    },
    {
      "epoch": 0.26944,
      "grad_norm": 0.011537933722138405,
      "learning_rate": 1.820394666666667e-05,
      "loss": 0.0008,
      "step": 8420
    },
    {
      "epoch": 0.26976,
      "grad_norm": 0.014798865653574467,
      "learning_rate": 1.8201813333333334e-05,
      "loss": 0.0009,
      "step": 8430
    },
    {
      "epoch": 0.27008,
      "grad_norm": 0.0262212585657835,
      "learning_rate": 1.8199680000000003e-05,
      "loss": 0.001,
      "step": 8440
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.010057034902274609,
      "learning_rate": 1.819754666666667e-05,
      "loss": 0.0381,
      "step": 8450
    },
    {
      "epoch": 0.27072,
      "grad_norm": 0.015742700546979904,
      "learning_rate": 1.8195413333333334e-05,
      "loss": 0.0007,
      "step": 8460
    },
    {
      "epoch": 0.27104,
      "grad_norm": 0.015678303316235542,
      "learning_rate": 1.8193280000000003e-05,
      "loss": 0.001,
      "step": 8470
    },
    {
      "epoch": 0.27136,
      "grad_norm": 0.012296962551772594,
      "learning_rate": 1.8191146666666668e-05,
      "loss": 0.0018,
      "step": 8480
    },
    {
      "epoch": 0.27168,
      "grad_norm": 0.015253410674631596,
      "learning_rate": 1.8189013333333334e-05,
      "loss": 0.0034,
      "step": 8490
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.011324802413582802,
      "learning_rate": 1.8186880000000002e-05,
      "loss": 0.0019,
      "step": 8500
    },
    {
      "epoch": 0.27232,
      "grad_norm": 0.02228526957333088,
      "learning_rate": 1.8184746666666668e-05,
      "loss": 0.0008,
      "step": 8510
    },
    {
      "epoch": 0.27264,
      "grad_norm": 0.010559534654021263,
      "learning_rate": 1.8182613333333333e-05,
      "loss": 0.0006,
      "step": 8520
    },
    {
      "epoch": 0.27296,
      "grad_norm": 0.01508819218724966,
      "learning_rate": 1.8180480000000002e-05,
      "loss": 0.0368,
      "step": 8530
    },
    {
      "epoch": 0.27328,
      "grad_norm": 0.5454279184341431,
      "learning_rate": 1.8178346666666667e-05,
      "loss": 0.0009,
      "step": 8540
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.008257110603153706,
      "learning_rate": 1.8176213333333336e-05,
      "loss": 0.0007,
      "step": 8550
    },
    {
      "epoch": 0.27392,
      "grad_norm": 0.012576010078191757,
      "learning_rate": 1.817408e-05,
      "loss": 0.0046,
      "step": 8560
    },
    {
      "epoch": 0.27424,
      "grad_norm": 0.01253475621342659,
      "learning_rate": 1.817194666666667e-05,
      "loss": 0.0006,
      "step": 8570
    },
    {
      "epoch": 0.27456,
      "grad_norm": 0.009083990938961506,
      "learning_rate": 1.8169813333333335e-05,
      "loss": 0.0006,
      "step": 8580
    },
    {
      "epoch": 0.27488,
      "grad_norm": 0.012244456447660923,
      "learning_rate": 1.816768e-05,
      "loss": 0.0441,
      "step": 8590
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.016239294782280922,
      "learning_rate": 1.816554666666667e-05,
      "loss": 0.0455,
      "step": 8600
    },
    {
      "epoch": 0.27552,
      "grad_norm": 0.01076887920498848,
      "learning_rate": 1.8163413333333335e-05,
      "loss": 0.0008,
      "step": 8610
    },
    {
      "epoch": 0.27584,
      "grad_norm": 0.009681246243417263,
      "learning_rate": 1.816128e-05,
      "loss": 0.0008,
      "step": 8620
    },
    {
      "epoch": 0.27616,
      "grad_norm": 0.014525544829666615,
      "learning_rate": 1.8159146666666666e-05,
      "loss": 0.0008,
      "step": 8630
    },
    {
      "epoch": 0.27648,
      "grad_norm": 0.008180443197488785,
      "learning_rate": 1.8157013333333335e-05,
      "loss": 0.0006,
      "step": 8640
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.01227589137852192,
      "learning_rate": 1.815488e-05,
      "loss": 0.001,
      "step": 8650
    },
    {
      "epoch": 0.27712,
      "grad_norm": 0.027664244174957275,
      "learning_rate": 1.815274666666667e-05,
      "loss": 0.0009,
      "step": 8660
    },
    {
      "epoch": 0.27744,
      "grad_norm": 0.00966903381049633,
      "learning_rate": 1.8150613333333334e-05,
      "loss": 0.0006,
      "step": 8670
    },
    {
      "epoch": 0.27776,
      "grad_norm": 0.01146615482866764,
      "learning_rate": 1.8148480000000003e-05,
      "loss": 0.0263,
      "step": 8680
    },
    {
      "epoch": 0.27808,
      "grad_norm": 0.012406609021127224,
      "learning_rate": 1.8146346666666668e-05,
      "loss": 0.001,
      "step": 8690
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.007298704236745834,
      "learning_rate": 1.8144213333333337e-05,
      "loss": 0.0008,
      "step": 8700
    },
    {
      "epoch": 0.27872,
      "grad_norm": 0.11569627374410629,
      "learning_rate": 1.8142080000000002e-05,
      "loss": 0.006,
      "step": 8710
    },
    {
      "epoch": 0.27904,
      "grad_norm": 0.010786066763103008,
      "learning_rate": 1.8139946666666668e-05,
      "loss": 0.0011,
      "step": 8720
    },
    {
      "epoch": 0.27936,
      "grad_norm": 0.010169816203415394,
      "learning_rate": 1.8137813333333333e-05,
      "loss": 0.0005,
      "step": 8730
    },
    {
      "epoch": 0.27968,
      "grad_norm": 0.022527482360601425,
      "learning_rate": 1.8135680000000002e-05,
      "loss": 0.0008,
      "step": 8740
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.011328103952109814,
      "learning_rate": 1.8133546666666667e-05,
      "loss": 0.0385,
      "step": 8750
    },
    {
      "epoch": 0.28032,
      "grad_norm": 0.010186822153627872,
      "learning_rate": 1.8131413333333333e-05,
      "loss": 0.0006,
      "step": 8760
    },
    {
      "epoch": 0.28064,
      "grad_norm": 0.00952532235532999,
      "learning_rate": 1.812928e-05,
      "loss": 0.0126,
      "step": 8770
    },
    {
      "epoch": 0.28096,
      "grad_norm": 0.01761050708591938,
      "learning_rate": 1.812714666666667e-05,
      "loss": 0.0008,
      "step": 8780
    },
    {
      "epoch": 0.28128,
      "grad_norm": 0.012900258414447308,
      "learning_rate": 1.8125013333333336e-05,
      "loss": 0.0207,
      "step": 8790
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.01302098948508501,
      "learning_rate": 1.812288e-05,
      "loss": 0.0044,
      "step": 8800
    },
    {
      "epoch": 0.28192,
      "grad_norm": 0.008363012224435806,
      "learning_rate": 1.812074666666667e-05,
      "loss": 0.0545,
      "step": 8810
    },
    {
      "epoch": 0.28224,
      "grad_norm": 0.00755480257794261,
      "learning_rate": 1.8118613333333335e-05,
      "loss": 0.0008,
      "step": 8820
    },
    {
      "epoch": 0.28256,
      "grad_norm": 0.011252795346081257,
      "learning_rate": 1.811648e-05,
      "loss": 0.0007,
      "step": 8830
    },
    {
      "epoch": 0.28288,
      "grad_norm": 0.01563456282019615,
      "learning_rate": 1.811434666666667e-05,
      "loss": 0.0019,
      "step": 8840
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.009202785789966583,
      "learning_rate": 1.8112213333333335e-05,
      "loss": 0.002,
      "step": 8850
    },
    {
      "epoch": 0.28352,
      "grad_norm": 0.013272244483232498,
      "learning_rate": 1.811008e-05,
      "loss": 0.0006,
      "step": 8860
    },
    {
      "epoch": 0.28384,
      "grad_norm": 0.010647696442902088,
      "learning_rate": 1.810794666666667e-05,
      "loss": 0.009,
      "step": 8870
    },
    {
      "epoch": 0.28416,
      "grad_norm": 0.010721257887780666,
      "learning_rate": 1.8105813333333334e-05,
      "loss": 0.0418,
      "step": 8880
    },
    {
      "epoch": 0.28448,
      "grad_norm": 1.5472720861434937,
      "learning_rate": 1.8103680000000003e-05,
      "loss": 0.0029,
      "step": 8890
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.008179563097655773,
      "learning_rate": 1.8101546666666668e-05,
      "loss": 0.0006,
      "step": 8900
    },
    {
      "epoch": 0.28512,
      "grad_norm": 0.008372670039534569,
      "learning_rate": 1.8099413333333337e-05,
      "loss": 0.0011,
      "step": 8910
    },
    {
      "epoch": 0.28544,
      "grad_norm": 0.006771743763238192,
      "learning_rate": 1.8097280000000002e-05,
      "loss": 0.0005,
      "step": 8920
    },
    {
      "epoch": 0.28576,
      "grad_norm": 0.009221990592777729,
      "learning_rate": 1.8095146666666668e-05,
      "loss": 0.0004,
      "step": 8930
    },
    {
      "epoch": 0.28608,
      "grad_norm": 0.006990415975451469,
      "learning_rate": 1.8093013333333336e-05,
      "loss": 0.0005,
      "step": 8940
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.05360626056790352,
      "learning_rate": 1.8090880000000002e-05,
      "loss": 0.0005,
      "step": 8950
    },
    {
      "epoch": 0.28672,
      "grad_norm": 0.005384601652622223,
      "learning_rate": 1.8088746666666667e-05,
      "loss": 0.0004,
      "step": 8960
    },
    {
      "epoch": 0.28704,
      "grad_norm": 0.023290513083338737,
      "learning_rate": 1.8086613333333333e-05,
      "loss": 0.0005,
      "step": 8970
    },
    {
      "epoch": 0.28736,
      "grad_norm": 0.0070243836380541325,
      "learning_rate": 1.808448e-05,
      "loss": 0.0005,
      "step": 8980
    },
    {
      "epoch": 0.28768,
      "grad_norm": 0.005761704873293638,
      "learning_rate": 1.8082346666666667e-05,
      "loss": 0.0533,
      "step": 8990
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.016574891284108162,
      "learning_rate": 1.8080213333333336e-05,
      "loss": 0.0004,
      "step": 9000
    },
    {
      "epoch": 0.28832,
      "grad_norm": 0.006181399337947369,
      "learning_rate": 1.807808e-05,
      "loss": 0.0007,
      "step": 9010
    },
    {
      "epoch": 0.28864,
      "grad_norm": 0.005782547406852245,
      "learning_rate": 1.807594666666667e-05,
      "loss": 0.0005,
      "step": 9020
    },
    {
      "epoch": 0.28896,
      "grad_norm": 0.005326234269887209,
      "learning_rate": 1.8073813333333335e-05,
      "loss": 0.0257,
      "step": 9030
    },
    {
      "epoch": 0.28928,
      "grad_norm": 0.01973559334874153,
      "learning_rate": 1.8071680000000004e-05,
      "loss": 0.0787,
      "step": 9040
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.006603119894862175,
      "learning_rate": 1.806954666666667e-05,
      "loss": 0.0019,
      "step": 9050
    },
    {
      "epoch": 0.28992,
      "grad_norm": 0.007660111878067255,
      "learning_rate": 1.8067413333333335e-05,
      "loss": 0.0005,
      "step": 9060
    },
    {
      "epoch": 0.29024,
      "grad_norm": 0.021467043086886406,
      "learning_rate": 1.806528e-05,
      "loss": 0.031,
      "step": 9070
    },
    {
      "epoch": 0.29056,
      "grad_norm": 0.007504719775170088,
      "learning_rate": 1.806314666666667e-05,
      "loss": 0.0389,
      "step": 9080
    },
    {
      "epoch": 0.29088,
      "grad_norm": 0.009476685896515846,
      "learning_rate": 1.8061013333333334e-05,
      "loss": 0.0007,
      "step": 9090
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.01812693104147911,
      "learning_rate": 1.805888e-05,
      "loss": 0.0006,
      "step": 9100
    },
    {
      "epoch": 0.29152,
      "grad_norm": 8.879613876342773,
      "learning_rate": 1.8056746666666668e-05,
      "loss": 0.0367,
      "step": 9110
    },
    {
      "epoch": 0.29184,
      "grad_norm": 0.00880496483296156,
      "learning_rate": 1.8054613333333334e-05,
      "loss": 0.0012,
      "step": 9120
    },
    {
      "epoch": 0.29216,
      "grad_norm": 0.010009925812482834,
      "learning_rate": 1.8052480000000002e-05,
      "loss": 0.052,
      "step": 9130
    },
    {
      "epoch": 0.29248,
      "grad_norm": 5.910918712615967,
      "learning_rate": 1.8050346666666668e-05,
      "loss": 0.055,
      "step": 9140
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.048922907561063766,
      "learning_rate": 1.8048213333333337e-05,
      "loss": 0.045,
      "step": 9150
    },
    {
      "epoch": 0.29312,
      "grad_norm": 0.015064203180372715,
      "learning_rate": 1.8046080000000002e-05,
      "loss": 0.0042,
      "step": 9160
    },
    {
      "epoch": 0.29344,
      "grad_norm": 0.010060291737318039,
      "learning_rate": 1.8043946666666667e-05,
      "loss": 0.0008,
      "step": 9170
    },
    {
      "epoch": 0.29376,
      "grad_norm": 0.011587255634367466,
      "learning_rate": 1.8041813333333336e-05,
      "loss": 0.0379,
      "step": 9180
    },
    {
      "epoch": 0.29408,
      "grad_norm": 4.250293731689453,
      "learning_rate": 1.803968e-05,
      "loss": 0.0249,
      "step": 9190
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.20601093769073486,
      "learning_rate": 1.8037546666666667e-05,
      "loss": 0.001,
      "step": 9200
    },
    {
      "epoch": 0.29472,
      "grad_norm": 0.01288458053022623,
      "learning_rate": 1.8035413333333336e-05,
      "loss": 0.005,
      "step": 9210
    },
    {
      "epoch": 0.29504,
      "grad_norm": 0.05976207181811333,
      "learning_rate": 1.803328e-05,
      "loss": 0.0413,
      "step": 9220
    },
    {
      "epoch": 0.29536,
      "grad_norm": 0.014874270185828209,
      "learning_rate": 1.8031146666666666e-05,
      "loss": 0.0007,
      "step": 9230
    },
    {
      "epoch": 0.29568,
      "grad_norm": 0.0500958189368248,
      "learning_rate": 1.8029013333333335e-05,
      "loss": 0.0009,
      "step": 9240
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.017396777868270874,
      "learning_rate": 1.8026880000000004e-05,
      "loss": 0.0054,
      "step": 9250
    },
    {
      "epoch": 0.29632,
      "grad_norm": 0.018863698467612267,
      "learning_rate": 1.802474666666667e-05,
      "loss": 0.0018,
      "step": 9260
    },
    {
      "epoch": 0.29664,
      "grad_norm": 0.023914914578199387,
      "learning_rate": 1.8022613333333335e-05,
      "loss": 0.0006,
      "step": 9270
    },
    {
      "epoch": 0.29696,
      "grad_norm": 0.0191776342689991,
      "learning_rate": 1.8020480000000003e-05,
      "loss": 0.0006,
      "step": 9280
    },
    {
      "epoch": 0.29728,
      "grad_norm": 0.01488747913390398,
      "learning_rate": 1.801834666666667e-05,
      "loss": 0.0009,
      "step": 9290
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.01456903014332056,
      "learning_rate": 1.8016213333333334e-05,
      "loss": 0.0007,
      "step": 9300
    },
    {
      "epoch": 0.29792,
      "grad_norm": 0.010529788210988045,
      "learning_rate": 1.801408e-05,
      "loss": 0.0009,
      "step": 9310
    },
    {
      "epoch": 0.29824,
      "grad_norm": 0.009910511784255505,
      "learning_rate": 1.8011946666666668e-05,
      "loss": 0.0005,
      "step": 9320
    },
    {
      "epoch": 0.29856,
      "grad_norm": 0.013269186019897461,
      "learning_rate": 1.8009813333333334e-05,
      "loss": 0.0777,
      "step": 9330
    },
    {
      "epoch": 0.29888,
      "grad_norm": 0.02306765876710415,
      "learning_rate": 1.8007680000000002e-05,
      "loss": 0.001,
      "step": 9340
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.006847233511507511,
      "learning_rate": 1.8005546666666668e-05,
      "loss": 0.0007,
      "step": 9350
    },
    {
      "epoch": 0.29952,
      "grad_norm": 0.017428332939743996,
      "learning_rate": 1.8003413333333337e-05,
      "loss": 0.0008,
      "step": 9360
    },
    {
      "epoch": 0.29984,
      "grad_norm": 0.011709990911185741,
      "learning_rate": 1.8001280000000002e-05,
      "loss": 0.0363,
      "step": 9370
    },
    {
      "epoch": 0.30016,
      "grad_norm": 0.008783354423940182,
      "learning_rate": 1.799914666666667e-05,
      "loss": 0.0021,
      "step": 9380
    },
    {
      "epoch": 0.30048,
      "grad_norm": 0.01023195218294859,
      "learning_rate": 1.7997013333333336e-05,
      "loss": 0.0008,
      "step": 9390
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.009786791168153286,
      "learning_rate": 1.799488e-05,
      "loss": 0.0006,
      "step": 9400
    },
    {
      "epoch": 0.30112,
      "grad_norm": 0.005632072687149048,
      "learning_rate": 1.7992746666666667e-05,
      "loss": 0.0147,
      "step": 9410
    },
    {
      "epoch": 0.30144,
      "grad_norm": 0.009031067602336407,
      "learning_rate": 1.7990613333333336e-05,
      "loss": 0.0005,
      "step": 9420
    },
    {
      "epoch": 0.30176,
      "grad_norm": 0.014463589526712894,
      "learning_rate": 1.798848e-05,
      "loss": 0.0597,
      "step": 9430
    },
    {
      "epoch": 0.30208,
      "grad_norm": 0.018680285662412643,
      "learning_rate": 1.7986346666666666e-05,
      "loss": 0.0006,
      "step": 9440
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.011661360040307045,
      "learning_rate": 1.7984213333333335e-05,
      "loss": 0.0007,
      "step": 9450
    },
    {
      "epoch": 0.30272,
      "grad_norm": 0.008800679817795753,
      "learning_rate": 1.798208e-05,
      "loss": 0.0008,
      "step": 9460
    },
    {
      "epoch": 0.30304,
      "grad_norm": 0.007793119177222252,
      "learning_rate": 1.797994666666667e-05,
      "loss": 0.0101,
      "step": 9470
    },
    {
      "epoch": 0.30336,
      "grad_norm": 0.013172116130590439,
      "learning_rate": 1.7977813333333335e-05,
      "loss": 0.0133,
      "step": 9480
    },
    {
      "epoch": 0.30368,
      "grad_norm": 0.03179018199443817,
      "learning_rate": 1.7975680000000003e-05,
      "loss": 0.0011,
      "step": 9490
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.015742182731628418,
      "learning_rate": 1.797354666666667e-05,
      "loss": 0.0006,
      "step": 9500
    },
    {
      "epoch": 0.30432,
      "grad_norm": 0.011642538011074066,
      "learning_rate": 1.7971413333333334e-05,
      "loss": 0.0367,
      "step": 9510
    },
    {
      "epoch": 0.30464,
      "grad_norm": 0.007691424805670977,
      "learning_rate": 1.7969280000000003e-05,
      "loss": 0.0015,
      "step": 9520
    },
    {
      "epoch": 0.30496,
      "grad_norm": 0.01479515340179205,
      "learning_rate": 1.7967146666666668e-05,
      "loss": 0.001,
      "step": 9530
    },
    {
      "epoch": 0.30528,
      "grad_norm": 0.0059833708219230175,
      "learning_rate": 1.7965013333333334e-05,
      "loss": 0.0104,
      "step": 9540
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.008918813429772854,
      "learning_rate": 1.7962880000000002e-05,
      "loss": 0.0007,
      "step": 9550
    },
    {
      "epoch": 0.30592,
      "grad_norm": 0.04489763453602791,
      "learning_rate": 1.7960746666666668e-05,
      "loss": 0.0007,
      "step": 9560
    },
    {
      "epoch": 0.30624,
      "grad_norm": 0.013471495360136032,
      "learning_rate": 1.7958613333333333e-05,
      "loss": 0.0006,
      "step": 9570
    },
    {
      "epoch": 0.30656,
      "grad_norm": 0.012504005804657936,
      "learning_rate": 1.7956480000000002e-05,
      "loss": 0.0006,
      "step": 9580
    },
    {
      "epoch": 0.30688,
      "grad_norm": 0.006920603569597006,
      "learning_rate": 1.7954346666666667e-05,
      "loss": 0.004,
      "step": 9590
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.024140257388353348,
      "learning_rate": 1.7952213333333336e-05,
      "loss": 0.0009,
      "step": 9600
    },
    {
      "epoch": 0.30752,
      "grad_norm": 0.006875442806631327,
      "learning_rate": 1.795008e-05,
      "loss": 0.0116,
      "step": 9610
    },
    {
      "epoch": 0.30784,
      "grad_norm": 2.586869478225708,
      "learning_rate": 1.794794666666667e-05,
      "loss": 0.0551,
      "step": 9620
    },
    {
      "epoch": 0.30816,
      "grad_norm": 0.010179772041738033,
      "learning_rate": 1.7945813333333336e-05,
      "loss": 0.0333,
      "step": 9630
    },
    {
      "epoch": 0.30848,
      "grad_norm": 0.14723548293113708,
      "learning_rate": 1.794368e-05,
      "loss": 0.0007,
      "step": 9640
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.011190100573003292,
      "learning_rate": 1.7941546666666666e-05,
      "loss": 0.0005,
      "step": 9650
    },
    {
      "epoch": 0.30912,
      "grad_norm": 0.011749454773962498,
      "learning_rate": 1.7939413333333335e-05,
      "loss": 0.0006,
      "step": 9660
    },
    {
      "epoch": 0.30944,
      "grad_norm": 0.0065137180499732494,
      "learning_rate": 1.793728e-05,
      "loss": 0.0078,
      "step": 9670
    },
    {
      "epoch": 0.30976,
      "grad_norm": 0.011584085412323475,
      "learning_rate": 1.7935146666666666e-05,
      "loss": 0.0007,
      "step": 9680
    },
    {
      "epoch": 0.31008,
      "grad_norm": 0.006478185299783945,
      "learning_rate": 1.7933013333333335e-05,
      "loss": 0.0008,
      "step": 9690
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.005813052412122488,
      "learning_rate": 1.793088e-05,
      "loss": 0.0451,
      "step": 9700
    },
    {
      "epoch": 0.31072,
      "grad_norm": 0.01395932026207447,
      "learning_rate": 1.792874666666667e-05,
      "loss": 0.0009,
      "step": 9710
    },
    {
      "epoch": 0.31104,
      "grad_norm": 0.010224108584225178,
      "learning_rate": 1.7926613333333338e-05,
      "loss": 0.0005,
      "step": 9720
    },
    {
      "epoch": 0.31136,
      "grad_norm": 0.009826562367379665,
      "learning_rate": 1.7924480000000003e-05,
      "loss": 0.0012,
      "step": 9730
    },
    {
      "epoch": 0.31168,
      "grad_norm": 0.02015615627169609,
      "learning_rate": 1.7922346666666668e-05,
      "loss": 0.0339,
      "step": 9740
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.029129551723599434,
      "learning_rate": 1.7920213333333334e-05,
      "loss": 0.0367,
      "step": 9750
    },
    {
      "epoch": 0.31232,
      "grad_norm": 0.04070575162768364,
      "learning_rate": 1.7918080000000002e-05,
      "loss": 0.0008,
      "step": 9760
    },
    {
      "epoch": 0.31264,
      "grad_norm": 0.0119944978505373,
      "learning_rate": 1.7915946666666668e-05,
      "loss": 0.0687,
      "step": 9770
    },
    {
      "epoch": 0.31296,
      "grad_norm": 0.013286378234624863,
      "learning_rate": 1.7913813333333333e-05,
      "loss": 0.0009,
      "step": 9780
    },
    {
      "epoch": 0.31328,
      "grad_norm": 0.011638452298939228,
      "learning_rate": 1.7911680000000002e-05,
      "loss": 0.0011,
      "step": 9790
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.028750529512763023,
      "learning_rate": 1.7909546666666667e-05,
      "loss": 0.0008,
      "step": 9800
    },
    {
      "epoch": 0.31392,
      "grad_norm": 0.01498456858098507,
      "learning_rate": 1.7907413333333336e-05,
      "loss": 0.0096,
      "step": 9810
    },
    {
      "epoch": 0.31424,
      "grad_norm": 0.007244874257594347,
      "learning_rate": 1.790528e-05,
      "loss": 0.0453,
      "step": 9820
    },
    {
      "epoch": 0.31456,
      "grad_norm": 1.6098273992538452,
      "learning_rate": 1.790314666666667e-05,
      "loss": 0.0137,
      "step": 9830
    },
    {
      "epoch": 0.31488,
      "grad_norm": 0.013499421067535877,
      "learning_rate": 1.7901013333333336e-05,
      "loss": 0.0008,
      "step": 9840
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.00921422615647316,
      "learning_rate": 1.789888e-05,
      "loss": 0.0116,
      "step": 9850
    },
    {
      "epoch": 0.31552,
      "grad_norm": 0.015651347115635872,
      "learning_rate": 1.789674666666667e-05,
      "loss": 0.0067,
      "step": 9860
    },
    {
      "epoch": 0.31584,
      "grad_norm": 2.196742296218872,
      "learning_rate": 1.7894613333333335e-05,
      "loss": 0.0431,
      "step": 9870
    },
    {
      "epoch": 0.31616,
      "grad_norm": 0.017191315069794655,
      "learning_rate": 1.789248e-05,
      "loss": 0.0008,
      "step": 9880
    },
    {
      "epoch": 0.31648,
      "grad_norm": 0.03477264195680618,
      "learning_rate": 1.789034666666667e-05,
      "loss": 0.0008,
      "step": 9890
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.00988746527582407,
      "learning_rate": 1.7888213333333335e-05,
      "loss": 0.002,
      "step": 9900
    },
    {
      "epoch": 0.31712,
      "grad_norm": 0.03911914303898811,
      "learning_rate": 1.788608e-05,
      "loss": 0.0014,
      "step": 9910
    },
    {
      "epoch": 0.31744,
      "grad_norm": 0.009347968734800816,
      "learning_rate": 1.788394666666667e-05,
      "loss": 0.0006,
      "step": 9920
    },
    {
      "epoch": 0.31776,
      "grad_norm": 3.311494827270508,
      "learning_rate": 1.7881813333333334e-05,
      "loss": 0.0493,
      "step": 9930
    },
    {
      "epoch": 0.31808,
      "grad_norm": 0.011528233997523785,
      "learning_rate": 1.7879680000000003e-05,
      "loss": 0.0005,
      "step": 9940
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.01074986532330513,
      "learning_rate": 1.787754666666667e-05,
      "loss": 0.0007,
      "step": 9950
    },
    {
      "epoch": 0.31872,
      "grad_norm": 0.010189306922256947,
      "learning_rate": 1.7875413333333337e-05,
      "loss": 0.0007,
      "step": 9960
    },
    {
      "epoch": 0.31904,
      "grad_norm": 0.01057327538728714,
      "learning_rate": 1.7873280000000002e-05,
      "loss": 0.0248,
      "step": 9970
    },
    {
      "epoch": 0.31936,
      "grad_norm": 0.010033686645328999,
      "learning_rate": 1.7871146666666668e-05,
      "loss": 0.0357,
      "step": 9980
    },
    {
      "epoch": 0.31968,
      "grad_norm": 0.011175466701388359,
      "learning_rate": 1.7869013333333333e-05,
      "loss": 0.0008,
      "step": 9990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.019763849675655365,
      "learning_rate": 1.7866880000000002e-05,
      "loss": 0.0306,
      "step": 10000
    },
    {
      "epoch": 0.32032,
      "grad_norm": 0.009119502268731594,
      "learning_rate": 1.7864746666666667e-05,
      "loss": 0.0006,
      "step": 10010
    },
    {
      "epoch": 0.32064,
      "grad_norm": 0.07093638181686401,
      "learning_rate": 1.7862613333333333e-05,
      "loss": 0.0007,
      "step": 10020
    },
    {
      "epoch": 0.32096,
      "grad_norm": 0.014538097195327282,
      "learning_rate": 1.786048e-05,
      "loss": 0.001,
      "step": 10030
    },
    {
      "epoch": 0.32128,
      "grad_norm": 0.009405138902366161,
      "learning_rate": 1.7858346666666667e-05,
      "loss": 0.0005,
      "step": 10040
    },
    {
      "epoch": 0.3216,
      "grad_norm": 3.6171586513519287,
      "learning_rate": 1.7856213333333336e-05,
      "loss": 0.0104,
      "step": 10050
    },
    {
      "epoch": 0.32192,
      "grad_norm": 0.015168445184826851,
      "learning_rate": 1.785408e-05,
      "loss": 0.0017,
      "step": 10060
    },
    {
      "epoch": 0.32224,
      "grad_norm": 0.00813467800617218,
      "learning_rate": 1.785194666666667e-05,
      "loss": 0.0008,
      "step": 10070
    },
    {
      "epoch": 0.32256,
      "grad_norm": 0.009513678960502148,
      "learning_rate": 1.7849813333333335e-05,
      "loss": 0.0015,
      "step": 10080
    },
    {
      "epoch": 0.32288,
      "grad_norm": 0.008744065649807453,
      "learning_rate": 1.784768e-05,
      "loss": 0.0412,
      "step": 10090
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.02194775640964508,
      "learning_rate": 1.784554666666667e-05,
      "loss": 0.0234,
      "step": 10100
    },
    {
      "epoch": 0.32352,
      "grad_norm": 0.017422253265976906,
      "learning_rate": 1.7843413333333335e-05,
      "loss": 0.0005,
      "step": 10110
    },
    {
      "epoch": 0.32384,
      "grad_norm": 0.05214283615350723,
      "learning_rate": 1.784128e-05,
      "loss": 0.0014,
      "step": 10120
    },
    {
      "epoch": 0.32416,
      "grad_norm": 0.010259195230901241,
      "learning_rate": 1.783914666666667e-05,
      "loss": 0.0074,
      "step": 10130
    },
    {
      "epoch": 0.32448,
      "grad_norm": 0.008810552768409252,
      "learning_rate": 1.7837013333333334e-05,
      "loss": 0.0006,
      "step": 10140
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.01977057009935379,
      "learning_rate": 1.783488e-05,
      "loss": 0.001,
      "step": 10150
    },
    {
      "epoch": 0.32512,
      "grad_norm": 0.009232227690517902,
      "learning_rate": 1.783274666666667e-05,
      "loss": 0.0376,
      "step": 10160
    },
    {
      "epoch": 0.32544,
      "grad_norm": 0.01550916489213705,
      "learning_rate": 1.7830613333333334e-05,
      "loss": 0.0006,
      "step": 10170
    },
    {
      "epoch": 0.32576,
      "grad_norm": 0.015560993924736977,
      "learning_rate": 1.7828480000000002e-05,
      "loss": 0.0006,
      "step": 10180
    },
    {
      "epoch": 0.32608,
      "grad_norm": 0.02389448508620262,
      "learning_rate": 1.7826346666666668e-05,
      "loss": 0.0196,
      "step": 10190
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.009181271307170391,
      "learning_rate": 1.7824213333333337e-05,
      "loss": 0.0008,
      "step": 10200
    },
    {
      "epoch": 0.32672,
      "grad_norm": 1.5833287239074707,
      "learning_rate": 1.7822080000000002e-05,
      "loss": 0.048,
      "step": 10210
    },
    {
      "epoch": 0.32704,
      "grad_norm": 0.007141382433474064,
      "learning_rate": 1.7819946666666667e-05,
      "loss": 0.0061,
      "step": 10220
    },
    {
      "epoch": 0.32736,
      "grad_norm": 0.015512075275182724,
      "learning_rate": 1.7817813333333336e-05,
      "loss": 0.0007,
      "step": 10230
    },
    {
      "epoch": 0.32768,
      "grad_norm": 0.015314163640141487,
      "learning_rate": 1.781568e-05,
      "loss": 0.0005,
      "step": 10240
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.012465113773941994,
      "learning_rate": 1.7813546666666667e-05,
      "loss": 0.0007,
      "step": 10250
    },
    {
      "epoch": 0.32832,
      "grad_norm": 0.017504900693893433,
      "learning_rate": 1.7811413333333332e-05,
      "loss": 0.0041,
      "step": 10260
    },
    {
      "epoch": 0.32864,
      "grad_norm": 0.01389902550727129,
      "learning_rate": 1.780928e-05,
      "loss": 0.0005,
      "step": 10270
    },
    {
      "epoch": 0.32896,
      "grad_norm": 1.639737606048584,
      "learning_rate": 1.780714666666667e-05,
      "loss": 0.0647,
      "step": 10280
    },
    {
      "epoch": 0.32928,
      "grad_norm": 0.008342080749571323,
      "learning_rate": 1.7805013333333335e-05,
      "loss": 0.0189,
      "step": 10290
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.01014170702546835,
      "learning_rate": 1.7802880000000004e-05,
      "loss": 0.0008,
      "step": 10300
    },
    {
      "epoch": 0.32992,
      "grad_norm": 0.010290407575666904,
      "learning_rate": 1.780074666666667e-05,
      "loss": 0.0007,
      "step": 10310
    },
    {
      "epoch": 0.33024,
      "grad_norm": 0.009855672717094421,
      "learning_rate": 1.7798613333333335e-05,
      "loss": 0.0352,
      "step": 10320
    },
    {
      "epoch": 0.33056,
      "grad_norm": 0.006934575270861387,
      "learning_rate": 1.779648e-05,
      "loss": 0.0252,
      "step": 10330
    },
    {
      "epoch": 0.33088,
      "grad_norm": 0.011725564487278461,
      "learning_rate": 1.779434666666667e-05,
      "loss": 0.0131,
      "step": 10340
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.06306350231170654,
      "learning_rate": 1.7792213333333334e-05,
      "loss": 0.0009,
      "step": 10350
    },
    {
      "epoch": 0.33152,
      "grad_norm": 0.008170269429683685,
      "learning_rate": 1.779008e-05,
      "loss": 0.0007,
      "step": 10360
    },
    {
      "epoch": 0.33184,
      "grad_norm": 0.017380153760313988,
      "learning_rate": 1.778794666666667e-05,
      "loss": 0.0241,
      "step": 10370
    },
    {
      "epoch": 0.33216,
      "grad_norm": 2.2087762355804443,
      "learning_rate": 1.7785813333333334e-05,
      "loss": 0.0027,
      "step": 10380
    },
    {
      "epoch": 0.33248,
      "grad_norm": 2.6424639225006104,
      "learning_rate": 1.7783680000000003e-05,
      "loss": 0.0087,
      "step": 10390
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.093222476541996,
      "learning_rate": 1.7781546666666668e-05,
      "loss": 0.0015,
      "step": 10400
    },
    {
      "epoch": 0.33312,
      "grad_norm": 0.045642267912626266,
      "learning_rate": 1.7779413333333337e-05,
      "loss": 0.0008,
      "step": 10410
    },
    {
      "epoch": 0.33344,
      "grad_norm": 0.007722908165305853,
      "learning_rate": 1.7777280000000002e-05,
      "loss": 0.001,
      "step": 10420
    },
    {
      "epoch": 0.33376,
      "grad_norm": 0.014591931365430355,
      "learning_rate": 1.7775146666666667e-05,
      "loss": 0.0009,
      "step": 10430
    },
    {
      "epoch": 0.33408,
      "grad_norm": 0.008734344504773617,
      "learning_rate": 1.7773013333333336e-05,
      "loss": 0.0006,
      "step": 10440
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.0126578314229846,
      "learning_rate": 1.777088e-05,
      "loss": 0.0592,
      "step": 10450
    },
    {
      "epoch": 0.33472,
      "grad_norm": 0.006779494229704142,
      "learning_rate": 1.7768746666666667e-05,
      "loss": 0.0008,
      "step": 10460
    },
    {
      "epoch": 0.33504,
      "grad_norm": 0.009004006162285805,
      "learning_rate": 1.7766613333333336e-05,
      "loss": 0.0006,
      "step": 10470
    },
    {
      "epoch": 0.33536,
      "grad_norm": 0.020327633246779442,
      "learning_rate": 1.776448e-05,
      "loss": 0.0097,
      "step": 10480
    },
    {
      "epoch": 0.33568,
      "grad_norm": 0.013353477232158184,
      "learning_rate": 1.7762346666666666e-05,
      "loss": 0.0007,
      "step": 10490
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.00868749339133501,
      "learning_rate": 1.7760213333333335e-05,
      "loss": 0.0006,
      "step": 10500
    },
    {
      "epoch": 0.33632,
      "grad_norm": 2.5695481300354004,
      "learning_rate": 1.775808e-05,
      "loss": 0.0264,
      "step": 10510
    },
    {
      "epoch": 0.33664,
      "grad_norm": 0.05643496662378311,
      "learning_rate": 1.775594666666667e-05,
      "loss": 0.0008,
      "step": 10520
    },
    {
      "epoch": 0.33696,
      "grad_norm": 0.009333359077572823,
      "learning_rate": 1.7753813333333335e-05,
      "loss": 0.0005,
      "step": 10530
    },
    {
      "epoch": 0.33728,
      "grad_norm": 0.00950753502547741,
      "learning_rate": 1.7751680000000003e-05,
      "loss": 0.0005,
      "step": 10540
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.013230777345597744,
      "learning_rate": 1.774954666666667e-05,
      "loss": 0.0475,
      "step": 10550
    },
    {
      "epoch": 0.33792,
      "grad_norm": 0.008997565135359764,
      "learning_rate": 1.7747413333333334e-05,
      "loss": 0.0007,
      "step": 10560
    },
    {
      "epoch": 0.33824,
      "grad_norm": 0.012470993213355541,
      "learning_rate": 1.7745280000000003e-05,
      "loss": 0.0304,
      "step": 10570
    },
    {
      "epoch": 0.33856,
      "grad_norm": 4.310108184814453,
      "learning_rate": 1.774314666666667e-05,
      "loss": 0.0089,
      "step": 10580
    },
    {
      "epoch": 0.33888,
      "grad_norm": 0.005190744996070862,
      "learning_rate": 1.7741013333333334e-05,
      "loss": 0.0014,
      "step": 10590
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.02755558304488659,
      "learning_rate": 1.773888e-05,
      "loss": 0.0006,
      "step": 10600
    },
    {
      "epoch": 0.33952,
      "grad_norm": 0.774299681186676,
      "learning_rate": 1.7736746666666668e-05,
      "loss": 0.0036,
      "step": 10610
    },
    {
      "epoch": 0.33984,
      "grad_norm": 0.07568878680467606,
      "learning_rate": 1.7734613333333333e-05,
      "loss": 0.0006,
      "step": 10620
    },
    {
      "epoch": 0.34016,
      "grad_norm": 0.009037436917424202,
      "learning_rate": 1.7732480000000002e-05,
      "loss": 0.0019,
      "step": 10630
    },
    {
      "epoch": 0.34048,
      "grad_norm": 0.00790478102862835,
      "learning_rate": 1.7730346666666667e-05,
      "loss": 0.0475,
      "step": 10640
    },
    {
      "epoch": 0.3408,
      "grad_norm": 3.134976863861084,
      "learning_rate": 1.7728213333333336e-05,
      "loss": 0.0098,
      "step": 10650
    },
    {
      "epoch": 0.34112,
      "grad_norm": 0.012634359300136566,
      "learning_rate": 1.772608e-05,
      "loss": 0.0142,
      "step": 10660
    },
    {
      "epoch": 0.34144,
      "grad_norm": 0.9700695872306824,
      "learning_rate": 1.7723946666666667e-05,
      "loss": 0.053,
      "step": 10670
    },
    {
      "epoch": 0.34176,
      "grad_norm": 0.01317228190600872,
      "learning_rate": 1.7721813333333336e-05,
      "loss": 0.0035,
      "step": 10680
    },
    {
      "epoch": 0.34208,
      "grad_norm": 0.009687676094472408,
      "learning_rate": 1.771968e-05,
      "loss": 0.001,
      "step": 10690
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.017491944134235382,
      "learning_rate": 1.7717546666666666e-05,
      "loss": 0.0576,
      "step": 10700
    },
    {
      "epoch": 0.34272,
      "grad_norm": 0.015554684214293957,
      "learning_rate": 1.7715413333333335e-05,
      "loss": 0.0456,
      "step": 10710
    },
    {
      "epoch": 0.34304,
      "grad_norm": 0.010079876519739628,
      "learning_rate": 1.771328e-05,
      "loss": 0.0442,
      "step": 10720
    },
    {
      "epoch": 0.34336,
      "grad_norm": 0.009611496701836586,
      "learning_rate": 1.7711146666666666e-05,
      "loss": 0.0199,
      "step": 10730
    },
    {
      "epoch": 0.34368,
      "grad_norm": 0.012011874467134476,
      "learning_rate": 1.7709013333333335e-05,
      "loss": 0.0011,
      "step": 10740
    },
    {
      "epoch": 0.344,
      "grad_norm": 3.2937307357788086,
      "learning_rate": 1.7706880000000004e-05,
      "loss": 0.0123,
      "step": 10750
    },
    {
      "epoch": 0.34432,
      "grad_norm": 0.01069863885641098,
      "learning_rate": 1.770474666666667e-05,
      "loss": 0.0117,
      "step": 10760
    },
    {
      "epoch": 0.34464,
      "grad_norm": 0.01735256426036358,
      "learning_rate": 1.7702613333333334e-05,
      "loss": 0.0006,
      "step": 10770
    },
    {
      "epoch": 0.34496,
      "grad_norm": 0.01044926792383194,
      "learning_rate": 1.7700480000000003e-05,
      "loss": 0.0007,
      "step": 10780
    },
    {
      "epoch": 0.34528,
      "grad_norm": 0.016232561320066452,
      "learning_rate": 1.769834666666667e-05,
      "loss": 0.0011,
      "step": 10790
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.13840921223163605,
      "learning_rate": 1.7696213333333334e-05,
      "loss": 0.0225,
      "step": 10800
    },
    {
      "epoch": 0.34592,
      "grad_norm": 0.009413224644958973,
      "learning_rate": 1.7694080000000003e-05,
      "loss": 0.0327,
      "step": 10810
    },
    {
      "epoch": 0.34624,
      "grad_norm": 0.01238393597304821,
      "learning_rate": 1.7691946666666668e-05,
      "loss": 0.005,
      "step": 10820
    },
    {
      "epoch": 0.34656,
      "grad_norm": 3.3667218685150146,
      "learning_rate": 1.7689813333333333e-05,
      "loss": 0.0335,
      "step": 10830
    },
    {
      "epoch": 0.34688,
      "grad_norm": 0.01692638359963894,
      "learning_rate": 1.7687680000000002e-05,
      "loss": 0.0015,
      "step": 10840
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.013044202700257301,
      "learning_rate": 1.7685546666666667e-05,
      "loss": 0.0006,
      "step": 10850
    },
    {
      "epoch": 0.34752,
      "grad_norm": 0.010652105323970318,
      "learning_rate": 1.7683413333333336e-05,
      "loss": 0.0011,
      "step": 10860
    },
    {
      "epoch": 0.34784,
      "grad_norm": 0.4824065566062927,
      "learning_rate": 1.768128e-05,
      "loss": 0.0013,
      "step": 10870
    },
    {
      "epoch": 0.34816,
      "grad_norm": 0.018305746838450432,
      "learning_rate": 1.767914666666667e-05,
      "loss": 0.0147,
      "step": 10880
    },
    {
      "epoch": 0.34848,
      "grad_norm": 0.020441856235265732,
      "learning_rate": 1.7677013333333336e-05,
      "loss": 0.0007,
      "step": 10890
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.012222527526319027,
      "learning_rate": 1.767488e-05,
      "loss": 0.0018,
      "step": 10900
    },
    {
      "epoch": 0.34912,
      "grad_norm": 0.022530455142259598,
      "learning_rate": 1.767274666666667e-05,
      "loss": 0.0005,
      "step": 10910
    },
    {
      "epoch": 0.34944,
      "grad_norm": 0.025695741176605225,
      "learning_rate": 1.7670613333333335e-05,
      "loss": 0.0408,
      "step": 10920
    },
    {
      "epoch": 0.34976,
      "grad_norm": 0.008775588124990463,
      "learning_rate": 1.766848e-05,
      "loss": 0.0708,
      "step": 10930
    },
    {
      "epoch": 0.35008,
      "grad_norm": 0.016619492322206497,
      "learning_rate": 1.7666346666666666e-05,
      "loss": 0.0072,
      "step": 10940
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.0232002642005682,
      "learning_rate": 1.7664213333333335e-05,
      "loss": 0.0868,
      "step": 10950
    },
    {
      "epoch": 0.35072,
      "grad_norm": 0.014643318951129913,
      "learning_rate": 1.766208e-05,
      "loss": 0.0065,
      "step": 10960
    },
    {
      "epoch": 0.35104,
      "grad_norm": 0.07097438722848892,
      "learning_rate": 1.765994666666667e-05,
      "loss": 0.002,
      "step": 10970
    },
    {
      "epoch": 0.35136,
      "grad_norm": 0.010223585180938244,
      "learning_rate": 1.7657813333333334e-05,
      "loss": 0.0026,
      "step": 10980
    },
    {
      "epoch": 0.35168,
      "grad_norm": 0.19426696002483368,
      "learning_rate": 1.7655680000000003e-05,
      "loss": 0.0011,
      "step": 10990
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.04466282203793526,
      "learning_rate": 1.765354666666667e-05,
      "loss": 0.0423,
      "step": 11000
    },
    {
      "epoch": 0.35232,
      "grad_norm": 0.014287491329014301,
      "learning_rate": 1.7651413333333334e-05,
      "loss": 0.0015,
      "step": 11010
    },
    {
      "epoch": 0.35264,
      "grad_norm": 0.013077052310109138,
      "learning_rate": 1.7649280000000003e-05,
      "loss": 0.0008,
      "step": 11020
    },
    {
      "epoch": 0.35296,
      "grad_norm": 0.017945056781172752,
      "learning_rate": 1.7647146666666668e-05,
      "loss": 0.0008,
      "step": 11030
    },
    {
      "epoch": 0.35328,
      "grad_norm": 0.009553219191730022,
      "learning_rate": 1.7645013333333333e-05,
      "loss": 0.0451,
      "step": 11040
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.014004301279783249,
      "learning_rate": 1.7642880000000002e-05,
      "loss": 0.0315,
      "step": 11050
    },
    {
      "epoch": 0.35392,
      "grad_norm": 0.012502899393439293,
      "learning_rate": 1.7640746666666667e-05,
      "loss": 0.0013,
      "step": 11060
    },
    {
      "epoch": 0.35424,
      "grad_norm": 0.02038756012916565,
      "learning_rate": 1.7638613333333333e-05,
      "loss": 0.0009,
      "step": 11070
    },
    {
      "epoch": 0.35456,
      "grad_norm": 0.00908675231039524,
      "learning_rate": 1.763648e-05,
      "loss": 0.0514,
      "step": 11080
    },
    {
      "epoch": 0.35488,
      "grad_norm": 0.010684257373213768,
      "learning_rate": 1.7634346666666667e-05,
      "loss": 0.0011,
      "step": 11090
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.013046495616436005,
      "learning_rate": 1.7632213333333336e-05,
      "loss": 0.0053,
      "step": 11100
    },
    {
      "epoch": 0.35552,
      "grad_norm": 0.015234282240271568,
      "learning_rate": 1.763008e-05,
      "loss": 0.0022,
      "step": 11110
    },
    {
      "epoch": 0.35584,
      "grad_norm": 0.011145645752549171,
      "learning_rate": 1.762794666666667e-05,
      "loss": 0.0026,
      "step": 11120
    },
    {
      "epoch": 0.35616,
      "grad_norm": 0.00791477132588625,
      "learning_rate": 1.7625813333333335e-05,
      "loss": 0.0008,
      "step": 11130
    },
    {
      "epoch": 0.35648,
      "grad_norm": 0.014909174293279648,
      "learning_rate": 1.762368e-05,
      "loss": 0.0017,
      "step": 11140
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.032801851630210876,
      "learning_rate": 1.762154666666667e-05,
      "loss": 0.0008,
      "step": 11150
    },
    {
      "epoch": 0.35712,
      "grad_norm": 0.02016628161072731,
      "learning_rate": 1.7619413333333335e-05,
      "loss": 0.0395,
      "step": 11160
    },
    {
      "epoch": 0.35744,
      "grad_norm": 0.012865121476352215,
      "learning_rate": 1.761728e-05,
      "loss": 0.0487,
      "step": 11170
    },
    {
      "epoch": 0.35776,
      "grad_norm": 0.005310886539518833,
      "learning_rate": 1.7615146666666666e-05,
      "loss": 0.0009,
      "step": 11180
    },
    {
      "epoch": 0.35808,
      "grad_norm": 0.8722326755523682,
      "learning_rate": 1.7613013333333334e-05,
      "loss": 0.0019,
      "step": 11190
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.015612819232046604,
      "learning_rate": 1.761088e-05,
      "loss": 0.0011,
      "step": 11200
    },
    {
      "epoch": 0.35872,
      "grad_norm": 0.009023301303386688,
      "learning_rate": 1.760874666666667e-05,
      "loss": 0.0016,
      "step": 11210
    },
    {
      "epoch": 0.35904,
      "grad_norm": 0.019108839333057404,
      "learning_rate": 1.7606613333333337e-05,
      "loss": 0.0345,
      "step": 11220
    },
    {
      "epoch": 0.35936,
      "grad_norm": 0.027196545153856277,
      "learning_rate": 1.7604480000000003e-05,
      "loss": 0.0009,
      "step": 11230
    },
    {
      "epoch": 0.35968,
      "grad_norm": 0.015347884967923164,
      "learning_rate": 1.7602346666666668e-05,
      "loss": 0.0008,
      "step": 11240
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.006846523843705654,
      "learning_rate": 1.7600213333333337e-05,
      "loss": 0.0006,
      "step": 11250
    },
    {
      "epoch": 0.36032,
      "grad_norm": 0.011721514165401459,
      "learning_rate": 1.7598080000000002e-05,
      "loss": 0.0008,
      "step": 11260
    },
    {
      "epoch": 0.36064,
      "grad_norm": 0.017214976251125336,
      "learning_rate": 1.7595946666666667e-05,
      "loss": 0.0008,
      "step": 11270
    },
    {
      "epoch": 0.36096,
      "grad_norm": 0.01471620798110962,
      "learning_rate": 1.7593813333333333e-05,
      "loss": 0.0007,
      "step": 11280
    },
    {
      "epoch": 0.36128,
      "grad_norm": 0.028447549790143967,
      "learning_rate": 1.759168e-05,
      "loss": 0.0303,
      "step": 11290
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.01987057365477085,
      "learning_rate": 1.7589546666666667e-05,
      "loss": 0.001,
      "step": 11300
    },
    {
      "epoch": 0.36192,
      "grad_norm": 0.017892595380544662,
      "learning_rate": 1.7587413333333336e-05,
      "loss": 0.0009,
      "step": 11310
    },
    {
      "epoch": 0.36224,
      "grad_norm": 0.02868492156267166,
      "learning_rate": 1.758528e-05,
      "loss": 0.0008,
      "step": 11320
    },
    {
      "epoch": 0.36256,
      "grad_norm": 0.012513590045273304,
      "learning_rate": 1.758314666666667e-05,
      "loss": 0.0007,
      "step": 11330
    },
    {
      "epoch": 0.36288,
      "grad_norm": 0.013646194711327553,
      "learning_rate": 1.7581013333333335e-05,
      "loss": 0.0008,
      "step": 11340
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.013695377856492996,
      "learning_rate": 1.757888e-05,
      "loss": 0.0013,
      "step": 11350
    },
    {
      "epoch": 0.36352,
      "grad_norm": 0.009932029992341995,
      "learning_rate": 1.757674666666667e-05,
      "loss": 0.0112,
      "step": 11360
    },
    {
      "epoch": 0.36384,
      "grad_norm": 0.01689772494137287,
      "learning_rate": 1.7574613333333335e-05,
      "loss": 0.0491,
      "step": 11370
    },
    {
      "epoch": 0.36416,
      "grad_norm": 1.6426585912704468,
      "learning_rate": 1.757248e-05,
      "loss": 0.0694,
      "step": 11380
    },
    {
      "epoch": 0.36448,
      "grad_norm": 0.016701575368642807,
      "learning_rate": 1.757034666666667e-05,
      "loss": 0.0299,
      "step": 11390
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.015338825061917305,
      "learning_rate": 1.7568213333333334e-05,
      "loss": 0.0008,
      "step": 11400
    },
    {
      "epoch": 0.36512,
      "grad_norm": 0.024391070008277893,
      "learning_rate": 1.756608e-05,
      "loss": 0.0007,
      "step": 11410
    },
    {
      "epoch": 0.36544,
      "grad_norm": 0.0076783145777881145,
      "learning_rate": 1.756394666666667e-05,
      "loss": 0.0037,
      "step": 11420
    },
    {
      "epoch": 0.36576,
      "grad_norm": 0.02783491648733616,
      "learning_rate": 1.7561813333333334e-05,
      "loss": 0.0007,
      "step": 11430
    },
    {
      "epoch": 0.36608,
      "grad_norm": 0.013499618507921696,
      "learning_rate": 1.7559680000000003e-05,
      "loss": 0.0267,
      "step": 11440
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.02280469425022602,
      "learning_rate": 1.7557546666666668e-05,
      "loss": 0.0006,
      "step": 11450
    },
    {
      "epoch": 0.36672,
      "grad_norm": 0.007440593093633652,
      "learning_rate": 1.7555413333333337e-05,
      "loss": 0.0011,
      "step": 11460
    },
    {
      "epoch": 0.36704,
      "grad_norm": 0.018247142434120178,
      "learning_rate": 1.7553280000000002e-05,
      "loss": 0.0007,
      "step": 11470
    },
    {
      "epoch": 0.36736,
      "grad_norm": 0.013172319158911705,
      "learning_rate": 1.7551146666666668e-05,
      "loss": 0.0007,
      "step": 11480
    },
    {
      "epoch": 0.36768,
      "grad_norm": 0.009087967686355114,
      "learning_rate": 1.7549013333333336e-05,
      "loss": 0.0006,
      "step": 11490
    },
    {
      "epoch": 0.368,
      "grad_norm": 3.556331157684326,
      "learning_rate": 1.754688e-05,
      "loss": 0.0549,
      "step": 11500
    },
    {
      "epoch": 0.36832,
      "grad_norm": 0.0375165157020092,
      "learning_rate": 1.7544746666666667e-05,
      "loss": 0.0008,
      "step": 11510
    },
    {
      "epoch": 0.36864,
      "grad_norm": 0.04417283087968826,
      "learning_rate": 1.7542613333333332e-05,
      "loss": 0.0462,
      "step": 11520
    },
    {
      "epoch": 0.36896,
      "grad_norm": 0.013614664785563946,
      "learning_rate": 1.754048e-05,
      "loss": 0.0007,
      "step": 11530
    },
    {
      "epoch": 0.36928,
      "grad_norm": 0.019875196740031242,
      "learning_rate": 1.7538346666666667e-05,
      "loss": 0.0396,
      "step": 11540
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.02415260672569275,
      "learning_rate": 1.7536213333333335e-05,
      "loss": 0.0016,
      "step": 11550
    },
    {
      "epoch": 0.36992,
      "grad_norm": 0.052679501473903656,
      "learning_rate": 1.753408e-05,
      "loss": 0.0014,
      "step": 11560
    },
    {
      "epoch": 0.37024,
      "grad_norm": 0.03974354639649391,
      "learning_rate": 1.753194666666667e-05,
      "loss": 0.0023,
      "step": 11570
    },
    {
      "epoch": 0.37056,
      "grad_norm": 0.009712415747344494,
      "learning_rate": 1.7529813333333335e-05,
      "loss": 0.0029,
      "step": 11580
    },
    {
      "epoch": 0.37088,
      "grad_norm": 0.0170810054987669,
      "learning_rate": 1.7527680000000004e-05,
      "loss": 0.0007,
      "step": 11590
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.021527407690882683,
      "learning_rate": 1.752554666666667e-05,
      "loss": 0.0648,
      "step": 11600
    },
    {
      "epoch": 0.37152,
      "grad_norm": 0.008708837442100048,
      "learning_rate": 1.7523413333333334e-05,
      "loss": 0.0271,
      "step": 11610
    },
    {
      "epoch": 0.37184,
      "grad_norm": 0.03208906948566437,
      "learning_rate": 1.752128e-05,
      "loss": 0.0054,
      "step": 11620
    },
    {
      "epoch": 0.37216,
      "grad_norm": 0.014554869383573532,
      "learning_rate": 1.751914666666667e-05,
      "loss": 0.0377,
      "step": 11630
    },
    {
      "epoch": 0.37248,
      "grad_norm": 5.256735324859619,
      "learning_rate": 1.7517013333333334e-05,
      "loss": 0.0309,
      "step": 11640
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.41352760791778564,
      "learning_rate": 1.751488e-05,
      "loss": 0.0015,
      "step": 11650
    },
    {
      "epoch": 0.37312,
      "grad_norm": 1.8866970539093018,
      "learning_rate": 1.7512746666666668e-05,
      "loss": 0.0275,
      "step": 11660
    },
    {
      "epoch": 0.37344,
      "grad_norm": 0.027204405516386032,
      "learning_rate": 1.7510613333333333e-05,
      "loss": 0.005,
      "step": 11670
    },
    {
      "epoch": 0.37376,
      "grad_norm": 0.011724417097866535,
      "learning_rate": 1.7508480000000002e-05,
      "loss": 0.0008,
      "step": 11680
    },
    {
      "epoch": 0.37408,
      "grad_norm": 0.017847836017608643,
      "learning_rate": 1.7506346666666668e-05,
      "loss": 0.0009,
      "step": 11690
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.010670260526239872,
      "learning_rate": 1.7504213333333336e-05,
      "loss": 0.0212,
      "step": 11700
    },
    {
      "epoch": 0.37472,
      "grad_norm": 0.03557910770177841,
      "learning_rate": 1.750208e-05,
      "loss": 0.0008,
      "step": 11710
    },
    {
      "epoch": 0.37504,
      "grad_norm": 0.012569589540362358,
      "learning_rate": 1.7499946666666667e-05,
      "loss": 0.0007,
      "step": 11720
    },
    {
      "epoch": 0.37536,
      "grad_norm": 0.010234774090349674,
      "learning_rate": 1.7497813333333336e-05,
      "loss": 0.0006,
      "step": 11730
    },
    {
      "epoch": 0.37568,
      "grad_norm": 0.00708372425287962,
      "learning_rate": 1.749568e-05,
      "loss": 0.0005,
      "step": 11740
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.009743710048496723,
      "learning_rate": 1.7493546666666667e-05,
      "loss": 0.0005,
      "step": 11750
    },
    {
      "epoch": 0.37632,
      "grad_norm": 0.011501161381602287,
      "learning_rate": 1.7491413333333335e-05,
      "loss": 0.0006,
      "step": 11760
    },
    {
      "epoch": 0.37664,
      "grad_norm": 0.01086992584168911,
      "learning_rate": 1.748928e-05,
      "loss": 0.0016,
      "step": 11770
    },
    {
      "epoch": 0.37696,
      "grad_norm": 0.008074103854596615,
      "learning_rate": 1.748714666666667e-05,
      "loss": 0.0008,
      "step": 11780
    },
    {
      "epoch": 0.37728,
      "grad_norm": 0.009480386041104794,
      "learning_rate": 1.7485013333333335e-05,
      "loss": 0.0008,
      "step": 11790
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.005953383632004261,
      "learning_rate": 1.7482880000000004e-05,
      "loss": 0.0215,
      "step": 11800
    },
    {
      "epoch": 0.37792,
      "grad_norm": 0.008625131100416183,
      "learning_rate": 1.748074666666667e-05,
      "loss": 0.0005,
      "step": 11810
    },
    {
      "epoch": 0.37824,
      "grad_norm": 0.010254351422190666,
      "learning_rate": 1.7478613333333334e-05,
      "loss": 0.0007,
      "step": 11820
    },
    {
      "epoch": 0.37856,
      "grad_norm": 0.03551212325692177,
      "learning_rate": 1.7476480000000003e-05,
      "loss": 0.0005,
      "step": 11830
    },
    {
      "epoch": 0.37888,
      "grad_norm": 0.1835552304983139,
      "learning_rate": 1.747434666666667e-05,
      "loss": 0.0013,
      "step": 11840
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.00625599967315793,
      "learning_rate": 1.7472213333333334e-05,
      "loss": 0.0329,
      "step": 11850
    },
    {
      "epoch": 0.37952,
      "grad_norm": 0.0071921623311936855,
      "learning_rate": 1.747008e-05,
      "loss": 0.0008,
      "step": 11860
    },
    {
      "epoch": 0.37984,
      "grad_norm": 0.006857291795313358,
      "learning_rate": 1.7467946666666668e-05,
      "loss": 0.0033,
      "step": 11870
    },
    {
      "epoch": 0.38016,
      "grad_norm": 0.008169515989720821,
      "learning_rate": 1.7465813333333333e-05,
      "loss": 0.0006,
      "step": 11880
    },
    {
      "epoch": 0.38048,
      "grad_norm": 0.007464664056897163,
      "learning_rate": 1.7463680000000002e-05,
      "loss": 0.0015,
      "step": 11890
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.024675114080309868,
      "learning_rate": 1.7461546666666668e-05,
      "loss": 0.0389,
      "step": 11900
    },
    {
      "epoch": 0.38112,
      "grad_norm": 0.004974062088876963,
      "learning_rate": 1.7459413333333336e-05,
      "loss": 0.0005,
      "step": 11910
    },
    {
      "epoch": 0.38144,
      "grad_norm": 0.014561295509338379,
      "learning_rate": 1.7457280000000002e-05,
      "loss": 0.0007,
      "step": 11920
    },
    {
      "epoch": 0.38176,
      "grad_norm": 0.0823957696557045,
      "learning_rate": 1.745514666666667e-05,
      "loss": 0.0007,
      "step": 11930
    },
    {
      "epoch": 0.38208,
      "grad_norm": 0.008495111018419266,
      "learning_rate": 1.7453013333333336e-05,
      "loss": 0.0006,
      "step": 11940
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.006199325900524855,
      "learning_rate": 1.745088e-05,
      "loss": 0.0006,
      "step": 11950
    },
    {
      "epoch": 0.38272,
      "grad_norm": 0.00857715867459774,
      "learning_rate": 1.7448746666666667e-05,
      "loss": 0.0006,
      "step": 11960
    },
    {
      "epoch": 0.38304,
      "grad_norm": 0.015650320798158646,
      "learning_rate": 1.7446613333333335e-05,
      "loss": 0.056,
      "step": 11970
    },
    {
      "epoch": 0.38336,
      "grad_norm": 0.11861825734376907,
      "learning_rate": 1.744448e-05,
      "loss": 0.0008,
      "step": 11980
    },
    {
      "epoch": 0.38368,
      "grad_norm": 0.03848975896835327,
      "learning_rate": 1.7442346666666666e-05,
      "loss": 0.0006,
      "step": 11990
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.00937273632735014,
      "learning_rate": 1.7440213333333335e-05,
      "loss": 0.0402,
      "step": 12000
    },
    {
      "epoch": 0.38432,
      "grad_norm": 0.0068262554705142975,
      "learning_rate": 1.743808e-05,
      "loss": 0.0005,
      "step": 12010
    },
    {
      "epoch": 0.38464,
      "grad_norm": 0.048057060688734055,
      "learning_rate": 1.743594666666667e-05,
      "loss": 0.001,
      "step": 12020
    },
    {
      "epoch": 0.38496,
      "grad_norm": 0.00751206511631608,
      "learning_rate": 1.7433813333333334e-05,
      "loss": 0.0004,
      "step": 12030
    },
    {
      "epoch": 0.38528,
      "grad_norm": 0.006061531137675047,
      "learning_rate": 1.7431680000000003e-05,
      "loss": 0.0004,
      "step": 12040
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.004560729023069143,
      "learning_rate": 1.742954666666667e-05,
      "loss": 0.0004,
      "step": 12050
    },
    {
      "epoch": 0.38592,
      "grad_norm": 0.010259401053190231,
      "learning_rate": 1.7427413333333334e-05,
      "loss": 0.0431,
      "step": 12060
    },
    {
      "epoch": 0.38624,
      "grad_norm": 0.011313781142234802,
      "learning_rate": 1.7425280000000003e-05,
      "loss": 0.0006,
      "step": 12070
    },
    {
      "epoch": 0.38656,
      "grad_norm": 0.008254694752395153,
      "learning_rate": 1.7423146666666668e-05,
      "loss": 0.0006,
      "step": 12080
    },
    {
      "epoch": 0.38688,
      "grad_norm": 0.0041076126508414745,
      "learning_rate": 1.7421013333333333e-05,
      "loss": 0.0004,
      "step": 12090
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.13191281259059906,
      "learning_rate": 1.7418880000000002e-05,
      "loss": 0.0063,
      "step": 12100
    },
    {
      "epoch": 0.38752,
      "grad_norm": 0.00514968391507864,
      "learning_rate": 1.7416746666666668e-05,
      "loss": 0.0324,
      "step": 12110
    },
    {
      "epoch": 0.38784,
      "grad_norm": 0.013993477448821068,
      "learning_rate": 1.7414613333333333e-05,
      "loss": 0.0006,
      "step": 12120
    },
    {
      "epoch": 0.38816,
      "grad_norm": 0.015838174149394035,
      "learning_rate": 1.7412480000000002e-05,
      "loss": 0.0005,
      "step": 12130
    },
    {
      "epoch": 0.38848,
      "grad_norm": 0.008196058683097363,
      "learning_rate": 1.7410346666666667e-05,
      "loss": 0.0006,
      "step": 12140
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.01128754299134016,
      "learning_rate": 1.7408213333333336e-05,
      "loss": 0.0005,
      "step": 12150
    },
    {
      "epoch": 0.38912,
      "grad_norm": 0.014594568870961666,
      "learning_rate": 1.740608e-05,
      "loss": 0.0288,
      "step": 12160
    },
    {
      "epoch": 0.38944,
      "grad_norm": 0.007591784931719303,
      "learning_rate": 1.740394666666667e-05,
      "loss": 0.0005,
      "step": 12170
    },
    {
      "epoch": 0.38976,
      "grad_norm": 0.005614378489553928,
      "learning_rate": 1.7401813333333335e-05,
      "loss": 0.0004,
      "step": 12180
    },
    {
      "epoch": 0.39008,
      "grad_norm": 0.008192607201635838,
      "learning_rate": 1.739968e-05,
      "loss": 0.0328,
      "step": 12190
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.008241203613579273,
      "learning_rate": 1.7397546666666666e-05,
      "loss": 0.0004,
      "step": 12200
    },
    {
      "epoch": 0.39072,
      "grad_norm": 0.007715418003499508,
      "learning_rate": 1.7395413333333335e-05,
      "loss": 0.0005,
      "step": 12210
    },
    {
      "epoch": 0.39104,
      "grad_norm": 0.006529762875288725,
      "learning_rate": 1.739328e-05,
      "loss": 0.035,
      "step": 12220
    },
    {
      "epoch": 0.39136,
      "grad_norm": 0.006758309435099363,
      "learning_rate": 1.7391146666666666e-05,
      "loss": 0.0005,
      "step": 12230
    },
    {
      "epoch": 0.39168,
      "grad_norm": 0.2709008753299713,
      "learning_rate": 1.7389013333333334e-05,
      "loss": 0.0046,
      "step": 12240
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.008228345774114132,
      "learning_rate": 1.7386880000000003e-05,
      "loss": 0.001,
      "step": 12250
    },
    {
      "epoch": 0.39232,
      "grad_norm": 0.004141829442232847,
      "learning_rate": 1.738474666666667e-05,
      "loss": 0.0008,
      "step": 12260
    },
    {
      "epoch": 0.39264,
      "grad_norm": 0.00596437556669116,
      "learning_rate": 1.7382613333333337e-05,
      "loss": 0.0018,
      "step": 12270
    },
    {
      "epoch": 0.39296,
      "grad_norm": 0.007076850160956383,
      "learning_rate": 1.7380480000000003e-05,
      "loss": 0.003,
      "step": 12280
    },
    {
      "epoch": 0.39328,
      "grad_norm": 0.009071297012269497,
      "learning_rate": 1.7378346666666668e-05,
      "loss": 0.0554,
      "step": 12290
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.00656267162412405,
      "learning_rate": 1.7376213333333333e-05,
      "loss": 0.0005,
      "step": 12300
    },
    {
      "epoch": 0.39392,
      "grad_norm": 0.005881811026483774,
      "learning_rate": 1.7374080000000002e-05,
      "loss": 0.0004,
      "step": 12310
    },
    {
      "epoch": 0.39424,
      "grad_norm": 0.008448881097137928,
      "learning_rate": 1.7371946666666668e-05,
      "loss": 0.0006,
      "step": 12320
    },
    {
      "epoch": 0.39456,
      "grad_norm": 1.4168862104415894,
      "learning_rate": 1.7369813333333333e-05,
      "loss": 0.0031,
      "step": 12330
    },
    {
      "epoch": 0.39488,
      "grad_norm": 0.010071733966469765,
      "learning_rate": 1.7367680000000002e-05,
      "loss": 0.0007,
      "step": 12340
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.12462688982486725,
      "learning_rate": 1.7365546666666667e-05,
      "loss": 0.0011,
      "step": 12350
    },
    {
      "epoch": 0.39552,
      "grad_norm": 0.011766903102397919,
      "learning_rate": 1.7363413333333336e-05,
      "loss": 0.0011,
      "step": 12360
    },
    {
      "epoch": 0.39584,
      "grad_norm": 0.011753398925065994,
      "learning_rate": 1.736128e-05,
      "loss": 0.0414,
      "step": 12370
    },
    {
      "epoch": 0.39616,
      "grad_norm": 0.0035607449244707823,
      "learning_rate": 1.735914666666667e-05,
      "loss": 0.0004,
      "step": 12380
    },
    {
      "epoch": 0.39648,
      "grad_norm": 0.014316799119114876,
      "learning_rate": 1.7357013333333335e-05,
      "loss": 0.0004,
      "step": 12390
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.006766144186258316,
      "learning_rate": 1.735488e-05,
      "loss": 0.0494,
      "step": 12400
    },
    {
      "epoch": 0.39712,
      "grad_norm": 0.0061914571560919285,
      "learning_rate": 1.735274666666667e-05,
      "loss": 0.0019,
      "step": 12410
    },
    {
      "epoch": 0.39744,
      "grad_norm": 0.006337607279419899,
      "learning_rate": 1.7350613333333335e-05,
      "loss": 0.0004,
      "step": 12420
    },
    {
      "epoch": 0.39776,
      "grad_norm": 0.006073700729757547,
      "learning_rate": 1.734848e-05,
      "loss": 0.0149,
      "step": 12430
    },
    {
      "epoch": 0.39808,
      "grad_norm": 0.018972909078001976,
      "learning_rate": 1.734634666666667e-05,
      "loss": 0.0004,
      "step": 12440
    },
    {
      "epoch": 0.3984,
      "grad_norm": 5.813806533813477,
      "learning_rate": 1.7344213333333334e-05,
      "loss": 0.0691,
      "step": 12450
    },
    {
      "epoch": 0.39872,
      "grad_norm": 0.012654098682105541,
      "learning_rate": 1.734208e-05,
      "loss": 0.0005,
      "step": 12460
    },
    {
      "epoch": 0.39904,
      "grad_norm": 0.010635405778884888,
      "learning_rate": 1.733994666666667e-05,
      "loss": 0.0538,
      "step": 12470
    },
    {
      "epoch": 0.39936,
      "grad_norm": 0.011842232197523117,
      "learning_rate": 1.7337813333333334e-05,
      "loss": 0.0072,
      "step": 12480
    },
    {
      "epoch": 0.39968,
      "grad_norm": 0.013700840063393116,
      "learning_rate": 1.7335680000000003e-05,
      "loss": 0.0042,
      "step": 12490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.007350377272814512,
      "learning_rate": 1.7333546666666668e-05,
      "loss": 0.0533,
      "step": 12500
    },
    {
      "epoch": 0.40032,
      "grad_norm": 0.008180978707969189,
      "learning_rate": 1.7331413333333337e-05,
      "loss": 0.0004,
      "step": 12510
    },
    {
      "epoch": 0.40064,
      "grad_norm": 0.010626583360135555,
      "learning_rate": 1.7329280000000002e-05,
      "loss": 0.0004,
      "step": 12520
    },
    {
      "epoch": 0.40096,
      "grad_norm": 0.017023293301463127,
      "learning_rate": 1.7327146666666668e-05,
      "loss": 0.0005,
      "step": 12530
    },
    {
      "epoch": 0.40128,
      "grad_norm": 0.009923236444592476,
      "learning_rate": 1.7325013333333333e-05,
      "loss": 0.0013,
      "step": 12540
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.018213139846920967,
      "learning_rate": 1.7322880000000002e-05,
      "loss": 0.0005,
      "step": 12550
    },
    {
      "epoch": 0.40192,
      "grad_norm": 0.01149255782365799,
      "learning_rate": 1.7320746666666667e-05,
      "loss": 0.0006,
      "step": 12560
    },
    {
      "epoch": 0.40224,
      "grad_norm": 0.005521408747881651,
      "learning_rate": 1.7318613333333333e-05,
      "loss": 0.0375,
      "step": 12570
    },
    {
      "epoch": 0.40256,
      "grad_norm": 0.00966549851000309,
      "learning_rate": 1.731648e-05,
      "loss": 0.001,
      "step": 12580
    },
    {
      "epoch": 0.40288,
      "grad_norm": 0.008871260099112988,
      "learning_rate": 1.7314346666666667e-05,
      "loss": 0.0015,
      "step": 12590
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.06737606227397919,
      "learning_rate": 1.7312213333333335e-05,
      "loss": 0.0008,
      "step": 12600
    },
    {
      "epoch": 0.40352,
      "grad_norm": 0.004478059709072113,
      "learning_rate": 1.731008e-05,
      "loss": 0.0005,
      "step": 12610
    },
    {
      "epoch": 0.40384,
      "grad_norm": 0.009572776965796947,
      "learning_rate": 1.730794666666667e-05,
      "loss": 0.0006,
      "step": 12620
    },
    {
      "epoch": 0.40416,
      "grad_norm": 0.009767154231667519,
      "learning_rate": 1.7305813333333335e-05,
      "loss": 0.0005,
      "step": 12630
    },
    {
      "epoch": 0.40448,
      "grad_norm": 0.004553661681711674,
      "learning_rate": 1.730368e-05,
      "loss": 0.0098,
      "step": 12640
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.009359195828437805,
      "learning_rate": 1.730154666666667e-05,
      "loss": 0.0005,
      "step": 12650
    },
    {
      "epoch": 0.40512,
      "grad_norm": 0.007642036769539118,
      "learning_rate": 1.7299413333333334e-05,
      "loss": 0.0006,
      "step": 12660
    },
    {
      "epoch": 0.40544,
      "grad_norm": 0.012060403823852539,
      "learning_rate": 1.729728e-05,
      "loss": 0.0007,
      "step": 12670
    },
    {
      "epoch": 0.40576,
      "grad_norm": 0.016073888167738914,
      "learning_rate": 1.729514666666667e-05,
      "loss": 0.0005,
      "step": 12680
    },
    {
      "epoch": 0.40608,
      "grad_norm": 0.008721111342310905,
      "learning_rate": 1.7293013333333334e-05,
      "loss": 0.0004,
      "step": 12690
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.009232782758772373,
      "learning_rate": 1.729088e-05,
      "loss": 0.0006,
      "step": 12700
    },
    {
      "epoch": 0.40672,
      "grad_norm": 0.010651377029716969,
      "learning_rate": 1.7288746666666668e-05,
      "loss": 0.0475,
      "step": 12710
    },
    {
      "epoch": 0.40704,
      "grad_norm": 0.016758104786276817,
      "learning_rate": 1.7286613333333337e-05,
      "loss": 0.0412,
      "step": 12720
    },
    {
      "epoch": 0.40736,
      "grad_norm": 0.014250409789383411,
      "learning_rate": 1.7284480000000002e-05,
      "loss": 0.0005,
      "step": 12730
    },
    {
      "epoch": 0.40768,
      "grad_norm": 0.2832396924495697,
      "learning_rate": 1.7282346666666668e-05,
      "loss": 0.0011,
      "step": 12740
    },
    {
      "epoch": 0.408,
      "grad_norm": 2.585874080657959,
      "learning_rate": 1.7280213333333336e-05,
      "loss": 0.0275,
      "step": 12750
    },
    {
      "epoch": 0.40832,
      "grad_norm": 0.01254101563245058,
      "learning_rate": 1.7278080000000002e-05,
      "loss": 0.0005,
      "step": 12760
    },
    {
      "epoch": 0.40864,
      "grad_norm": 0.00872084591537714,
      "learning_rate": 1.7275946666666667e-05,
      "loss": 0.0004,
      "step": 12770
    },
    {
      "epoch": 0.40896,
      "grad_norm": 0.019103894010186195,
      "learning_rate": 1.7273813333333336e-05,
      "loss": 0.0037,
      "step": 12780
    },
    {
      "epoch": 0.40928,
      "grad_norm": 0.01202671229839325,
      "learning_rate": 1.727168e-05,
      "loss": 0.0121,
      "step": 12790
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.010255626402795315,
      "learning_rate": 1.7269546666666667e-05,
      "loss": 0.0398,
      "step": 12800
    },
    {
      "epoch": 0.40992,
      "grad_norm": 0.006993463728576899,
      "learning_rate": 1.7267413333333335e-05,
      "loss": 0.0008,
      "step": 12810
    },
    {
      "epoch": 0.41024,
      "grad_norm": 0.008626963943243027,
      "learning_rate": 1.726528e-05,
      "loss": 0.0005,
      "step": 12820
    },
    {
      "epoch": 0.41056,
      "grad_norm": 0.0052812471985816956,
      "learning_rate": 1.726314666666667e-05,
      "loss": 0.0004,
      "step": 12830
    },
    {
      "epoch": 0.41088,
      "grad_norm": 0.019118554890155792,
      "learning_rate": 1.7261013333333335e-05,
      "loss": 0.0013,
      "step": 12840
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.00520979193970561,
      "learning_rate": 1.7258880000000004e-05,
      "loss": 0.0011,
      "step": 12850
    },
    {
      "epoch": 0.41152,
      "grad_norm": 6.110079765319824,
      "learning_rate": 1.725674666666667e-05,
      "loss": 0.0591,
      "step": 12860
    },
    {
      "epoch": 0.41184,
      "grad_norm": 0.007083439733833075,
      "learning_rate": 1.7254613333333334e-05,
      "loss": 0.0003,
      "step": 12870
    },
    {
      "epoch": 0.41216,
      "grad_norm": 0.017947152256965637,
      "learning_rate": 1.725248e-05,
      "loss": 0.0639,
      "step": 12880
    },
    {
      "epoch": 0.41248,
      "grad_norm": 0.007665681187063456,
      "learning_rate": 1.725034666666667e-05,
      "loss": 0.0009,
      "step": 12890
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.013867934234440327,
      "learning_rate": 1.7248213333333334e-05,
      "loss": 0.0005,
      "step": 12900
    },
    {
      "epoch": 0.41312,
      "grad_norm": 0.04273097589612007,
      "learning_rate": 1.724608e-05,
      "loss": 0.0012,
      "step": 12910
    },
    {
      "epoch": 0.41344,
      "grad_norm": 0.02750726416707039,
      "learning_rate": 1.7243946666666668e-05,
      "loss": 0.0005,
      "step": 12920
    },
    {
      "epoch": 0.41376,
      "grad_norm": 0.007678601425141096,
      "learning_rate": 1.7241813333333334e-05,
      "loss": 0.0303,
      "step": 12930
    },
    {
      "epoch": 0.41408,
      "grad_norm": 0.00685703894123435,
      "learning_rate": 1.7239680000000002e-05,
      "loss": 0.0005,
      "step": 12940
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.013370264321565628,
      "learning_rate": 1.7237546666666668e-05,
      "loss": 0.0011,
      "step": 12950
    },
    {
      "epoch": 0.41472,
      "grad_norm": 0.010306967422366142,
      "learning_rate": 1.7235413333333336e-05,
      "loss": 0.0015,
      "step": 12960
    },
    {
      "epoch": 0.41504,
      "grad_norm": 0.00278691784478724,
      "learning_rate": 1.7233280000000002e-05,
      "loss": 0.0068,
      "step": 12970
    },
    {
      "epoch": 0.41536,
      "grad_norm": 0.10180266201496124,
      "learning_rate": 1.7231146666666667e-05,
      "loss": 0.0417,
      "step": 12980
    },
    {
      "epoch": 0.41568,
      "grad_norm": 0.00922079011797905,
      "learning_rate": 1.7229013333333336e-05,
      "loss": 0.0005,
      "step": 12990
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.005045522470027208,
      "learning_rate": 1.722688e-05,
      "loss": 0.001,
      "step": 13000
    },
    {
      "epoch": 0.41632,
      "grad_norm": 0.18171019852161407,
      "learning_rate": 1.7224746666666667e-05,
      "loss": 0.001,
      "step": 13010
    },
    {
      "epoch": 0.41664,
      "grad_norm": 0.006718629505485296,
      "learning_rate": 1.7222613333333335e-05,
      "loss": 0.0352,
      "step": 13020
    },
    {
      "epoch": 0.41696,
      "grad_norm": 0.005986672360450029,
      "learning_rate": 1.722048e-05,
      "loss": 0.0004,
      "step": 13030
    },
    {
      "epoch": 0.41728,
      "grad_norm": 0.03363415598869324,
      "learning_rate": 1.7218346666666666e-05,
      "loss": 0.0004,
      "step": 13040
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.0072080111131072044,
      "learning_rate": 1.7216213333333335e-05,
      "loss": 0.0515,
      "step": 13050
    },
    {
      "epoch": 0.41792,
      "grad_norm": 0.008387708105146885,
      "learning_rate": 1.721408e-05,
      "loss": 0.0203,
      "step": 13060
    },
    {
      "epoch": 0.41824,
      "grad_norm": 0.011594259180128574,
      "learning_rate": 1.721194666666667e-05,
      "loss": 0.0169,
      "step": 13070
    },
    {
      "epoch": 0.41856,
      "grad_norm": 0.010209847241640091,
      "learning_rate": 1.7209813333333335e-05,
      "loss": 0.0006,
      "step": 13080
    },
    {
      "epoch": 0.41888,
      "grad_norm": 0.009392749518156052,
      "learning_rate": 1.7207680000000003e-05,
      "loss": 0.0007,
      "step": 13090
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.007963147014379501,
      "learning_rate": 1.720554666666667e-05,
      "loss": 0.0473,
      "step": 13100
    },
    {
      "epoch": 0.41952,
      "grad_norm": 0.008937707170844078,
      "learning_rate": 1.7203413333333334e-05,
      "loss": 0.0004,
      "step": 13110
    },
    {
      "epoch": 0.41984,
      "grad_norm": 0.13711808621883392,
      "learning_rate": 1.7201280000000003e-05,
      "loss": 0.0335,
      "step": 13120
    },
    {
      "epoch": 0.42016,
      "grad_norm": 0.06676702201366425,
      "learning_rate": 1.7199146666666668e-05,
      "loss": 0.0023,
      "step": 13130
    },
    {
      "epoch": 0.42048,
      "grad_norm": 0.021293533965945244,
      "learning_rate": 1.7197013333333334e-05,
      "loss": 0.0011,
      "step": 13140
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.022218558937311172,
      "learning_rate": 1.719488e-05,
      "loss": 0.0099,
      "step": 13150
    },
    {
      "epoch": 0.42112,
      "grad_norm": 0.017757922410964966,
      "learning_rate": 1.7192746666666668e-05,
      "loss": 0.0006,
      "step": 13160
    },
    {
      "epoch": 0.42144,
      "grad_norm": 0.011883702129125595,
      "learning_rate": 1.7190613333333333e-05,
      "loss": 0.0008,
      "step": 13170
    },
    {
      "epoch": 0.42176,
      "grad_norm": 0.011768647469580173,
      "learning_rate": 1.7188480000000002e-05,
      "loss": 0.0025,
      "step": 13180
    },
    {
      "epoch": 0.42208,
      "grad_norm": 0.012160522863268852,
      "learning_rate": 1.718634666666667e-05,
      "loss": 0.0708,
      "step": 13190
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.01670631766319275,
      "learning_rate": 1.7184213333333336e-05,
      "loss": 0.0161,
      "step": 13200
    },
    {
      "epoch": 0.42272,
      "grad_norm": 0.023184454068541527,
      "learning_rate": 1.718208e-05,
      "loss": 0.0324,
      "step": 13210
    },
    {
      "epoch": 0.42304,
      "grad_norm": 0.0134544987231493,
      "learning_rate": 1.7179946666666667e-05,
      "loss": 0.0007,
      "step": 13220
    },
    {
      "epoch": 0.42336,
      "grad_norm": 0.018373413011431694,
      "learning_rate": 1.7177813333333335e-05,
      "loss": 0.0008,
      "step": 13230
    },
    {
      "epoch": 0.42368,
      "grad_norm": 0.019447747617959976,
      "learning_rate": 1.717568e-05,
      "loss": 0.0479,
      "step": 13240
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.011625695042312145,
      "learning_rate": 1.7173546666666666e-05,
      "loss": 0.0009,
      "step": 13250
    },
    {
      "epoch": 0.42432,
      "grad_norm": 0.011941472068428993,
      "learning_rate": 1.7171413333333335e-05,
      "loss": 0.0015,
      "step": 13260
    },
    {
      "epoch": 0.42464,
      "grad_norm": 0.02708425000309944,
      "learning_rate": 1.716928e-05,
      "loss": 0.0012,
      "step": 13270
    },
    {
      "epoch": 0.42496,
      "grad_norm": 0.010618358850479126,
      "learning_rate": 1.716714666666667e-05,
      "loss": 0.0006,
      "step": 13280
    },
    {
      "epoch": 0.42528,
      "grad_norm": 0.01052806619554758,
      "learning_rate": 1.7165013333333335e-05,
      "loss": 0.0005,
      "step": 13290
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.00861913338303566,
      "learning_rate": 1.7162880000000003e-05,
      "loss": 0.0004,
      "step": 13300
    },
    {
      "epoch": 0.42592,
      "grad_norm": 0.0059344954788684845,
      "learning_rate": 1.716074666666667e-05,
      "loss": 0.0005,
      "step": 13310
    },
    {
      "epoch": 0.42624,
      "grad_norm": 0.013078402727842331,
      "learning_rate": 1.7158613333333334e-05,
      "loss": 0.0009,
      "step": 13320
    },
    {
      "epoch": 0.42656,
      "grad_norm": 0.029359519481658936,
      "learning_rate": 1.7156480000000003e-05,
      "loss": 0.0224,
      "step": 13330
    },
    {
      "epoch": 0.42688,
      "grad_norm": 0.031884822994470596,
      "learning_rate": 1.7154346666666668e-05,
      "loss": 0.0005,
      "step": 13340
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.008484943769872189,
      "learning_rate": 1.7152213333333334e-05,
      "loss": 0.0005,
      "step": 13350
    },
    {
      "epoch": 0.42752,
      "grad_norm": 6.573117256164551,
      "learning_rate": 1.7150080000000002e-05,
      "loss": 0.0234,
      "step": 13360
    },
    {
      "epoch": 0.42784,
      "grad_norm": 0.013360208831727505,
      "learning_rate": 1.7147946666666668e-05,
      "loss": 0.0459,
      "step": 13370
    },
    {
      "epoch": 0.42816,
      "grad_norm": 0.0959644466638565,
      "learning_rate": 1.7145813333333333e-05,
      "loss": 0.0348,
      "step": 13380
    },
    {
      "epoch": 0.42848,
      "grad_norm": 0.014804783277213573,
      "learning_rate": 1.7143680000000002e-05,
      "loss": 0.0006,
      "step": 13390
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.010993297211825848,
      "learning_rate": 1.7141546666666667e-05,
      "loss": 0.0464,
      "step": 13400
    },
    {
      "epoch": 0.42912,
      "grad_norm": 0.026036854833364487,
      "learning_rate": 1.7139413333333336e-05,
      "loss": 0.0021,
      "step": 13410
    },
    {
      "epoch": 0.42944,
      "grad_norm": 0.009119867347180843,
      "learning_rate": 1.713728e-05,
      "loss": 0.0005,
      "step": 13420
    },
    {
      "epoch": 0.42976,
      "grad_norm": 0.3615550696849823,
      "learning_rate": 1.713514666666667e-05,
      "loss": 0.0018,
      "step": 13430
    },
    {
      "epoch": 0.43008,
      "grad_norm": 0.010735736228525639,
      "learning_rate": 1.7133013333333336e-05,
      "loss": 0.0007,
      "step": 13440
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.008325694128870964,
      "learning_rate": 1.713088e-05,
      "loss": 0.0005,
      "step": 13450
    },
    {
      "epoch": 0.43072,
      "grad_norm": 0.007398797664791346,
      "learning_rate": 1.712874666666667e-05,
      "loss": 0.0006,
      "step": 13460
    },
    {
      "epoch": 0.43104,
      "grad_norm": 0.011467262171208858,
      "learning_rate": 1.7126613333333335e-05,
      "loss": 0.0005,
      "step": 13470
    },
    {
      "epoch": 0.43136,
      "grad_norm": 0.014885909855365753,
      "learning_rate": 1.712448e-05,
      "loss": 0.0222,
      "step": 13480
    },
    {
      "epoch": 0.43168,
      "grad_norm": 0.015946967527270317,
      "learning_rate": 1.7122346666666666e-05,
      "loss": 0.0007,
      "step": 13490
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.011922948062419891,
      "learning_rate": 1.7120213333333335e-05,
      "loss": 0.0196,
      "step": 13500
    },
    {
      "epoch": 0.43232,
      "grad_norm": 0.013069205917418003,
      "learning_rate": 1.711808e-05,
      "loss": 0.0007,
      "step": 13510
    },
    {
      "epoch": 0.43264,
      "grad_norm": 0.008803286589682102,
      "learning_rate": 1.711594666666667e-05,
      "loss": 0.0026,
      "step": 13520
    },
    {
      "epoch": 0.43296,
      "grad_norm": 0.016223175451159477,
      "learning_rate": 1.7113813333333334e-05,
      "loss": 0.0006,
      "step": 13530
    },
    {
      "epoch": 0.43328,
      "grad_norm": 0.030202198773622513,
      "learning_rate": 1.7111680000000003e-05,
      "loss": 0.0025,
      "step": 13540
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.007557382341474295,
      "learning_rate": 1.7109546666666668e-05,
      "loss": 0.0005,
      "step": 13550
    },
    {
      "epoch": 0.43392,
      "grad_norm": 0.023318370804190636,
      "learning_rate": 1.7107413333333334e-05,
      "loss": 0.1074,
      "step": 13560
    },
    {
      "epoch": 0.43424,
      "grad_norm": 0.00759853795170784,
      "learning_rate": 1.7105280000000002e-05,
      "loss": 0.0007,
      "step": 13570
    },
    {
      "epoch": 0.43456,
      "grad_norm": 0.10031681507825851,
      "learning_rate": 1.7103146666666668e-05,
      "loss": 0.0007,
      "step": 13580
    },
    {
      "epoch": 0.43488,
      "grad_norm": 0.0631759911775589,
      "learning_rate": 1.7101013333333333e-05,
      "loss": 0.0009,
      "step": 13590
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.006336469203233719,
      "learning_rate": 1.7098880000000002e-05,
      "loss": 0.0091,
      "step": 13600
    },
    {
      "epoch": 0.43552,
      "grad_norm": 0.009955315850675106,
      "learning_rate": 1.7096746666666667e-05,
      "loss": 0.0005,
      "step": 13610
    },
    {
      "epoch": 0.43584,
      "grad_norm": 0.007749568205326796,
      "learning_rate": 1.7094613333333333e-05,
      "loss": 0.0187,
      "step": 13620
    },
    {
      "epoch": 0.43616,
      "grad_norm": 0.023904504254460335,
      "learning_rate": 1.709248e-05,
      "loss": 0.004,
      "step": 13630
    },
    {
      "epoch": 0.43648,
      "grad_norm": 0.016131237149238586,
      "learning_rate": 1.7090346666666667e-05,
      "loss": 0.0208,
      "step": 13640
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.01507010217756033,
      "learning_rate": 1.7088213333333336e-05,
      "loss": 0.0005,
      "step": 13650
    },
    {
      "epoch": 0.43712,
      "grad_norm": 0.03336549550294876,
      "learning_rate": 1.708608e-05,
      "loss": 0.0007,
      "step": 13660
    },
    {
      "epoch": 0.43744,
      "grad_norm": 3.0537564754486084,
      "learning_rate": 1.708394666666667e-05,
      "loss": 0.0319,
      "step": 13670
    },
    {
      "epoch": 0.43776,
      "grad_norm": 0.019104376435279846,
      "learning_rate": 1.7081813333333335e-05,
      "loss": 0.0255,
      "step": 13680
    },
    {
      "epoch": 0.43808,
      "grad_norm": 0.01117165107280016,
      "learning_rate": 1.707968e-05,
      "loss": 0.0008,
      "step": 13690
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.011012101545929909,
      "learning_rate": 1.707754666666667e-05,
      "loss": 0.0463,
      "step": 13700
    },
    {
      "epoch": 0.43872,
      "grad_norm": 0.011508856900036335,
      "learning_rate": 1.7075413333333335e-05,
      "loss": 0.0338,
      "step": 13710
    },
    {
      "epoch": 0.43904,
      "grad_norm": 0.013765359297394753,
      "learning_rate": 1.707328e-05,
      "loss": 0.0011,
      "step": 13720
    },
    {
      "epoch": 0.43936,
      "grad_norm": 0.00782830361276865,
      "learning_rate": 1.7071146666666665e-05,
      "loss": 0.0008,
      "step": 13730
    },
    {
      "epoch": 0.43968,
      "grad_norm": 0.010371344164013863,
      "learning_rate": 1.7069013333333334e-05,
      "loss": 0.0795,
      "step": 13740
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.042106226086616516,
      "learning_rate": 1.7066880000000003e-05,
      "loss": 0.0072,
      "step": 13750
    },
    {
      "epoch": 0.44032,
      "grad_norm": 4.29000997543335,
      "learning_rate": 1.7064746666666668e-05,
      "loss": 0.0224,
      "step": 13760
    },
    {
      "epoch": 0.44064,
      "grad_norm": 0.01144799031317234,
      "learning_rate": 1.7062613333333337e-05,
      "loss": 0.0007,
      "step": 13770
    },
    {
      "epoch": 0.44096,
      "grad_norm": 0.010286947712302208,
      "learning_rate": 1.7060480000000002e-05,
      "loss": 0.0559,
      "step": 13780
    },
    {
      "epoch": 0.44128,
      "grad_norm": 0.022360704839229584,
      "learning_rate": 1.7058346666666668e-05,
      "loss": 0.0037,
      "step": 13790
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.00873252097517252,
      "learning_rate": 1.7056213333333337e-05,
      "loss": 0.0006,
      "step": 13800
    },
    {
      "epoch": 0.44192,
      "grad_norm": 0.008449516259133816,
      "learning_rate": 1.7054080000000002e-05,
      "loss": 0.0013,
      "step": 13810
    },
    {
      "epoch": 0.44224,
      "grad_norm": 0.013778419233858585,
      "learning_rate": 1.7051946666666667e-05,
      "loss": 0.0006,
      "step": 13820
    },
    {
      "epoch": 0.44256,
      "grad_norm": 0.010539640672504902,
      "learning_rate": 1.7049813333333333e-05,
      "loss": 0.0143,
      "step": 13830
    },
    {
      "epoch": 0.44288,
      "grad_norm": 0.0074028377421200275,
      "learning_rate": 1.704768e-05,
      "loss": 0.018,
      "step": 13840
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.00877142883837223,
      "learning_rate": 1.7045546666666667e-05,
      "loss": 0.0005,
      "step": 13850
    },
    {
      "epoch": 0.44352,
      "grad_norm": 0.007773030083626509,
      "learning_rate": 1.7043413333333336e-05,
      "loss": 0.0088,
      "step": 13860
    },
    {
      "epoch": 0.44384,
      "grad_norm": 0.010547989048063755,
      "learning_rate": 1.704128e-05,
      "loss": 0.0059,
      "step": 13870
    },
    {
      "epoch": 0.44416,
      "grad_norm": 0.0645798072218895,
      "learning_rate": 1.703914666666667e-05,
      "loss": 0.0028,
      "step": 13880
    },
    {
      "epoch": 0.44448,
      "grad_norm": 0.009443581104278564,
      "learning_rate": 1.7037013333333335e-05,
      "loss": 0.0007,
      "step": 13890
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.08116009831428528,
      "learning_rate": 1.703488e-05,
      "loss": 0.0009,
      "step": 13900
    },
    {
      "epoch": 0.44512,
      "grad_norm": 0.005731511395424604,
      "learning_rate": 1.703274666666667e-05,
      "loss": 0.0011,
      "step": 13910
    },
    {
      "epoch": 0.44544,
      "grad_norm": 0.013022098690271378,
      "learning_rate": 1.7030613333333335e-05,
      "loss": 0.023,
      "step": 13920
    },
    {
      "epoch": 0.44576,
      "grad_norm": 0.011640859767794609,
      "learning_rate": 1.702848e-05,
      "loss": 0.0013,
      "step": 13930
    },
    {
      "epoch": 0.44608,
      "grad_norm": 0.00977246928960085,
      "learning_rate": 1.702634666666667e-05,
      "loss": 0.0005,
      "step": 13940
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.04734214395284653,
      "learning_rate": 1.7024213333333334e-05,
      "loss": 0.0035,
      "step": 13950
    },
    {
      "epoch": 0.44672,
      "grad_norm": 0.006231475155800581,
      "learning_rate": 1.702208e-05,
      "loss": 0.0004,
      "step": 13960
    },
    {
      "epoch": 0.44704,
      "grad_norm": 0.0425589457154274,
      "learning_rate": 1.7019946666666668e-05,
      "loss": 0.0005,
      "step": 13970
    },
    {
      "epoch": 0.44736,
      "grad_norm": 0.007320611272007227,
      "learning_rate": 1.7017813333333334e-05,
      "loss": 0.0224,
      "step": 13980
    },
    {
      "epoch": 0.44768,
      "grad_norm": 0.009577610529959202,
      "learning_rate": 1.7015680000000002e-05,
      "loss": 0.0006,
      "step": 13990
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.008090577088296413,
      "learning_rate": 1.7013546666666668e-05,
      "loss": 0.0009,
      "step": 14000
    },
    {
      "epoch": 0.44832,
      "grad_norm": 0.005303142126649618,
      "learning_rate": 1.7011413333333337e-05,
      "loss": 0.0009,
      "step": 14010
    },
    {
      "epoch": 0.44864,
      "grad_norm": 0.06404197216033936,
      "learning_rate": 1.7009280000000002e-05,
      "loss": 0.0037,
      "step": 14020
    },
    {
      "epoch": 0.44896,
      "grad_norm": 4.002235412597656,
      "learning_rate": 1.7007146666666667e-05,
      "loss": 0.0223,
      "step": 14030
    },
    {
      "epoch": 0.44928,
      "grad_norm": 1.6439979076385498,
      "learning_rate": 1.7005013333333336e-05,
      "loss": 0.0477,
      "step": 14040
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.010302078910171986,
      "learning_rate": 1.700288e-05,
      "loss": 0.0006,
      "step": 14050
    },
    {
      "epoch": 0.44992,
      "grad_norm": 0.010733149945735931,
      "learning_rate": 1.7000746666666667e-05,
      "loss": 0.0005,
      "step": 14060
    },
    {
      "epoch": 0.45024,
      "grad_norm": 0.00629138108342886,
      "learning_rate": 1.6998613333333332e-05,
      "loss": 0.05,
      "step": 14070
    },
    {
      "epoch": 0.45056,
      "grad_norm": 0.004443335812538862,
      "learning_rate": 1.699648e-05,
      "loss": 0.0462,
      "step": 14080
    },
    {
      "epoch": 0.45088,
      "grad_norm": 0.014525237493216991,
      "learning_rate": 1.6994346666666666e-05,
      "loss": 0.0008,
      "step": 14090
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.014713136479258537,
      "learning_rate": 1.6992213333333335e-05,
      "loss": 0.0005,
      "step": 14100
    },
    {
      "epoch": 0.45152,
      "grad_norm": 0.008635635487735271,
      "learning_rate": 1.699008e-05,
      "loss": 0.0008,
      "step": 14110
    },
    {
      "epoch": 0.45184,
      "grad_norm": 0.07763684540987015,
      "learning_rate": 1.698794666666667e-05,
      "loss": 0.0018,
      "step": 14120
    },
    {
      "epoch": 0.45216,
      "grad_norm": 0.006837877444922924,
      "learning_rate": 1.6985813333333335e-05,
      "loss": 0.0005,
      "step": 14130
    },
    {
      "epoch": 0.45248,
      "grad_norm": 0.00970817357301712,
      "learning_rate": 1.6983680000000003e-05,
      "loss": 0.0009,
      "step": 14140
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.8857409954071045,
      "learning_rate": 1.698154666666667e-05,
      "loss": 0.0019,
      "step": 14150
    },
    {
      "epoch": 0.45312,
      "grad_norm": 0.007872057147324085,
      "learning_rate": 1.6979413333333334e-05,
      "loss": 0.019,
      "step": 14160
    },
    {
      "epoch": 0.45344,
      "grad_norm": 0.009481414221227169,
      "learning_rate": 1.697728e-05,
      "loss": 0.0004,
      "step": 14170
    },
    {
      "epoch": 0.45376,
      "grad_norm": 0.011831511743366718,
      "learning_rate": 1.6975146666666668e-05,
      "loss": 0.0005,
      "step": 14180
    },
    {
      "epoch": 0.45408,
      "grad_norm": 0.013529636897146702,
      "learning_rate": 1.6973013333333334e-05,
      "loss": 0.0006,
      "step": 14190
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.00881804246455431,
      "learning_rate": 1.697088e-05,
      "loss": 0.05,
      "step": 14200
    },
    {
      "epoch": 0.45472,
      "grad_norm": 0.008989338763058186,
      "learning_rate": 1.6968746666666668e-05,
      "loss": 0.0006,
      "step": 14210
    },
    {
      "epoch": 0.45504,
      "grad_norm": 0.010258730500936508,
      "learning_rate": 1.6966613333333337e-05,
      "loss": 0.0007,
      "step": 14220
    },
    {
      "epoch": 0.45536,
      "grad_norm": 2.3138625621795654,
      "learning_rate": 1.6964480000000002e-05,
      "loss": 0.0351,
      "step": 14230
    },
    {
      "epoch": 0.45568,
      "grad_norm": 0.03986072540283203,
      "learning_rate": 1.6962346666666667e-05,
      "loss": 0.0288,
      "step": 14240
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.013075202703475952,
      "learning_rate": 1.6960213333333336e-05,
      "loss": 0.0056,
      "step": 14250
    },
    {
      "epoch": 0.45632,
      "grad_norm": 0.006960568483918905,
      "learning_rate": 1.695808e-05,
      "loss": 0.0005,
      "step": 14260
    },
    {
      "epoch": 0.45664,
      "grad_norm": 0.01851433515548706,
      "learning_rate": 1.6955946666666667e-05,
      "loss": 0.0033,
      "step": 14270
    },
    {
      "epoch": 0.45696,
      "grad_norm": 0.011884596198797226,
      "learning_rate": 1.6953813333333336e-05,
      "loss": 0.0018,
      "step": 14280
    },
    {
      "epoch": 0.45728,
      "grad_norm": 0.021474529057741165,
      "learning_rate": 1.695168e-05,
      "loss": 0.0046,
      "step": 14290
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.010416376404464245,
      "learning_rate": 1.6949546666666666e-05,
      "loss": 0.0679,
      "step": 14300
    },
    {
      "epoch": 0.45792,
      "grad_norm": 0.013778452761471272,
      "learning_rate": 1.6947413333333335e-05,
      "loss": 0.0006,
      "step": 14310
    },
    {
      "epoch": 0.45824,
      "grad_norm": 0.009030384011566639,
      "learning_rate": 1.694528e-05,
      "loss": 0.0203,
      "step": 14320
    },
    {
      "epoch": 0.45856,
      "grad_norm": 0.009698254987597466,
      "learning_rate": 1.694314666666667e-05,
      "loss": 0.0234,
      "step": 14330
    },
    {
      "epoch": 0.45888,
      "grad_norm": 0.015627430751919746,
      "learning_rate": 1.6941013333333335e-05,
      "loss": 0.0006,
      "step": 14340
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.013385046273469925,
      "learning_rate": 1.6938880000000003e-05,
      "loss": 0.0006,
      "step": 14350
    },
    {
      "epoch": 0.45952,
      "grad_norm": 0.008848795667290688,
      "learning_rate": 1.693674666666667e-05,
      "loss": 0.0009,
      "step": 14360
    },
    {
      "epoch": 0.45984,
      "grad_norm": 2.813544988632202,
      "learning_rate": 1.6934613333333334e-05,
      "loss": 0.0102,
      "step": 14370
    },
    {
      "epoch": 0.46016,
      "grad_norm": 0.018457572907209396,
      "learning_rate": 1.6932480000000003e-05,
      "loss": 0.0275,
      "step": 14380
    },
    {
      "epoch": 0.46048,
      "grad_norm": 0.012754974886775017,
      "learning_rate": 1.6930346666666668e-05,
      "loss": 0.0005,
      "step": 14390
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.05321265012025833,
      "learning_rate": 1.6928213333333334e-05,
      "loss": 0.0007,
      "step": 14400
    },
    {
      "epoch": 0.46112,
      "grad_norm": 0.009709497913718224,
      "learning_rate": 1.692608e-05,
      "loss": 0.0468,
      "step": 14410
    },
    {
      "epoch": 0.46144,
      "grad_norm": 0.0059446473605930805,
      "learning_rate": 1.6923946666666668e-05,
      "loss": 0.0006,
      "step": 14420
    },
    {
      "epoch": 0.46176,
      "grad_norm": 0.026576707139611244,
      "learning_rate": 1.6921813333333333e-05,
      "loss": 0.0006,
      "step": 14430
    },
    {
      "epoch": 0.46208,
      "grad_norm": 0.018319623544812202,
      "learning_rate": 1.6919680000000002e-05,
      "loss": 0.0011,
      "step": 14440
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.009928855113685131,
      "learning_rate": 1.6917546666666667e-05,
      "loss": 0.0511,
      "step": 14450
    },
    {
      "epoch": 0.46272,
      "grad_norm": 0.022359240800142288,
      "learning_rate": 1.6915413333333336e-05,
      "loss": 0.001,
      "step": 14460
    },
    {
      "epoch": 0.46304,
      "grad_norm": 0.010329388082027435,
      "learning_rate": 1.691328e-05,
      "loss": 0.0011,
      "step": 14470
    },
    {
      "epoch": 0.46336,
      "grad_norm": 0.010620098561048508,
      "learning_rate": 1.691114666666667e-05,
      "loss": 0.0338,
      "step": 14480
    },
    {
      "epoch": 0.46368,
      "grad_norm": 0.013814193196594715,
      "learning_rate": 1.6909013333333336e-05,
      "loss": 0.0007,
      "step": 14490
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.025365985929965973,
      "learning_rate": 1.690688e-05,
      "loss": 0.0008,
      "step": 14500
    },
    {
      "epoch": 0.46432,
      "grad_norm": 0.011362857185304165,
      "learning_rate": 1.6904746666666666e-05,
      "loss": 0.0008,
      "step": 14510
    },
    {
      "epoch": 0.46464,
      "grad_norm": 0.004900929518043995,
      "learning_rate": 1.6902613333333335e-05,
      "loss": 0.001,
      "step": 14520
    },
    {
      "epoch": 0.46496,
      "grad_norm": 0.010996158234775066,
      "learning_rate": 1.690048e-05,
      "loss": 0.0039,
      "step": 14530
    },
    {
      "epoch": 0.46528,
      "grad_norm": 0.011533014476299286,
      "learning_rate": 1.6898346666666666e-05,
      "loss": 0.0013,
      "step": 14540
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.010677730664610863,
      "learning_rate": 1.6896213333333335e-05,
      "loss": 0.0005,
      "step": 14550
    },
    {
      "epoch": 0.46592,
      "grad_norm": 0.009561535902321339,
      "learning_rate": 1.689408e-05,
      "loss": 0.0005,
      "step": 14560
    },
    {
      "epoch": 0.46624,
      "grad_norm": 0.0163420457392931,
      "learning_rate": 1.689194666666667e-05,
      "loss": 0.0514,
      "step": 14570
    },
    {
      "epoch": 0.46656,
      "grad_norm": 0.020564598962664604,
      "learning_rate": 1.6889813333333334e-05,
      "loss": 0.001,
      "step": 14580
    },
    {
      "epoch": 0.46688,
      "grad_norm": 0.005023443140089512,
      "learning_rate": 1.6887680000000003e-05,
      "loss": 0.0305,
      "step": 14590
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.009403156116604805,
      "learning_rate": 1.688554666666667e-05,
      "loss": 0.0007,
      "step": 14600
    },
    {
      "epoch": 0.46752,
      "grad_norm": 0.0341993123292923,
      "learning_rate": 1.6883413333333334e-05,
      "loss": 0.001,
      "step": 14610
    },
    {
      "epoch": 0.46784,
      "grad_norm": 0.0061044637113809586,
      "learning_rate": 1.6881280000000002e-05,
      "loss": 0.0006,
      "step": 14620
    },
    {
      "epoch": 0.46816,
      "grad_norm": 0.007403102237731218,
      "learning_rate": 1.6879146666666668e-05,
      "loss": 0.0011,
      "step": 14630
    },
    {
      "epoch": 0.46848,
      "grad_norm": 0.006709361914545298,
      "learning_rate": 1.6877013333333333e-05,
      "loss": 0.0005,
      "step": 14640
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.007873901166021824,
      "learning_rate": 1.6874880000000002e-05,
      "loss": 0.0006,
      "step": 14650
    },
    {
      "epoch": 0.46912,
      "grad_norm": 0.11933339387178421,
      "learning_rate": 1.6872746666666667e-05,
      "loss": 0.0141,
      "step": 14660
    },
    {
      "epoch": 0.46944,
      "grad_norm": 0.012887490913271904,
      "learning_rate": 1.6870613333333333e-05,
      "loss": 0.0314,
      "step": 14670
    },
    {
      "epoch": 0.46976,
      "grad_norm": 0.01252682600170374,
      "learning_rate": 1.686848e-05,
      "loss": 0.0006,
      "step": 14680
    },
    {
      "epoch": 0.47008,
      "grad_norm": 0.00880528800189495,
      "learning_rate": 1.686634666666667e-05,
      "loss": 0.0007,
      "step": 14690
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.03926606848835945,
      "learning_rate": 1.6864213333333336e-05,
      "loss": 0.0008,
      "step": 14700
    },
    {
      "epoch": 0.47072,
      "grad_norm": 0.009189624339342117,
      "learning_rate": 1.686208e-05,
      "loss": 0.001,
      "step": 14710
    },
    {
      "epoch": 0.47104,
      "grad_norm": 0.009210755117237568,
      "learning_rate": 1.685994666666667e-05,
      "loss": 0.0006,
      "step": 14720
    },
    {
      "epoch": 0.47136,
      "grad_norm": 0.012050008401274681,
      "learning_rate": 1.6857813333333335e-05,
      "loss": 0.0005,
      "step": 14730
    },
    {
      "epoch": 0.47168,
      "grad_norm": 0.00962317269295454,
      "learning_rate": 1.685568e-05,
      "loss": 0.0005,
      "step": 14740
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.018747884780168533,
      "learning_rate": 1.6853546666666666e-05,
      "loss": 0.0006,
      "step": 14750
    },
    {
      "epoch": 0.47232,
      "grad_norm": 0.008730529807507992,
      "learning_rate": 1.6851413333333335e-05,
      "loss": 0.0004,
      "step": 14760
    },
    {
      "epoch": 0.47264,
      "grad_norm": 0.01068638265132904,
      "learning_rate": 1.684928e-05,
      "loss": 0.0098,
      "step": 14770
    },
    {
      "epoch": 0.47296,
      "grad_norm": 0.12145819514989853,
      "learning_rate": 1.684714666666667e-05,
      "loss": 0.0008,
      "step": 14780
    },
    {
      "epoch": 0.47328,
      "grad_norm": 0.015807097777724266,
      "learning_rate": 1.6845013333333334e-05,
      "loss": 0.0244,
      "step": 14790
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.01852148212492466,
      "learning_rate": 1.6842880000000003e-05,
      "loss": 0.0007,
      "step": 14800
    },
    {
      "epoch": 0.47392,
      "grad_norm": 0.006820995360612869,
      "learning_rate": 1.684074666666667e-05,
      "loss": 0.0005,
      "step": 14810
    },
    {
      "epoch": 0.47424,
      "grad_norm": 0.007255211006850004,
      "learning_rate": 1.6838613333333337e-05,
      "loss": 0.0375,
      "step": 14820
    },
    {
      "epoch": 0.47456,
      "grad_norm": 0.018960611894726753,
      "learning_rate": 1.6836480000000002e-05,
      "loss": 0.0682,
      "step": 14830
    },
    {
      "epoch": 0.47488,
      "grad_norm": 0.01070430688560009,
      "learning_rate": 1.6834346666666668e-05,
      "loss": 0.0006,
      "step": 14840
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.006733170710504055,
      "learning_rate": 1.6832213333333333e-05,
      "loss": 0.0006,
      "step": 14850
    },
    {
      "epoch": 0.47552,
      "grad_norm": 0.006819766480475664,
      "learning_rate": 1.6830080000000002e-05,
      "loss": 0.0006,
      "step": 14860
    },
    {
      "epoch": 0.47584,
      "grad_norm": 0.006489396560937166,
      "learning_rate": 1.6827946666666667e-05,
      "loss": 0.0034,
      "step": 14870
    },
    {
      "epoch": 0.47616,
      "grad_norm": 0.17252598702907562,
      "learning_rate": 1.6825813333333333e-05,
      "loss": 0.001,
      "step": 14880
    },
    {
      "epoch": 0.47648,
      "grad_norm": 0.014808598905801773,
      "learning_rate": 1.682368e-05,
      "loss": 0.0005,
      "step": 14890
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.12523788213729858,
      "learning_rate": 1.6821546666666667e-05,
      "loss": 0.0286,
      "step": 14900
    },
    {
      "epoch": 0.47712,
      "grad_norm": 0.0093322042375803,
      "learning_rate": 1.6819413333333336e-05,
      "loss": 0.0004,
      "step": 14910
    },
    {
      "epoch": 0.47744,
      "grad_norm": 0.006327819544821978,
      "learning_rate": 1.681728e-05,
      "loss": 0.0004,
      "step": 14920
    },
    {
      "epoch": 0.47776,
      "grad_norm": 0.0061303093098104,
      "learning_rate": 1.681514666666667e-05,
      "loss": 0.001,
      "step": 14930
    },
    {
      "epoch": 0.47808,
      "grad_norm": 0.01181675773113966,
      "learning_rate": 1.6813013333333335e-05,
      "loss": 0.0003,
      "step": 14940
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.016534455120563507,
      "learning_rate": 1.681088e-05,
      "loss": 0.0007,
      "step": 14950
    },
    {
      "epoch": 0.47872,
      "grad_norm": 0.07288584113121033,
      "learning_rate": 1.680874666666667e-05,
      "loss": 0.0008,
      "step": 14960
    },
    {
      "epoch": 0.47904,
      "grad_norm": 0.007706455420702696,
      "learning_rate": 1.6806613333333335e-05,
      "loss": 0.0014,
      "step": 14970
    },
    {
      "epoch": 0.47936,
      "grad_norm": 0.008441716432571411,
      "learning_rate": 1.680448e-05,
      "loss": 0.022,
      "step": 14980
    },
    {
      "epoch": 0.47968,
      "grad_norm": 0.007204986643046141,
      "learning_rate": 1.680234666666667e-05,
      "loss": 0.0046,
      "step": 14990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.009668241254985332,
      "learning_rate": 1.6800213333333334e-05,
      "loss": 0.0005,
      "step": 15000
    },
    {
      "epoch": 0.48032,
      "grad_norm": 0.010958350263535976,
      "learning_rate": 1.679808e-05,
      "loss": 0.0004,
      "step": 15010
    },
    {
      "epoch": 0.48064,
      "grad_norm": 0.008582535199820995,
      "learning_rate": 1.679594666666667e-05,
      "loss": 0.0091,
      "step": 15020
    },
    {
      "epoch": 0.48096,
      "grad_norm": 0.010254628956317902,
      "learning_rate": 1.6793813333333334e-05,
      "loss": 0.0003,
      "step": 15030
    },
    {
      "epoch": 0.48128,
      "grad_norm": 0.0059555210173130035,
      "learning_rate": 1.6791680000000003e-05,
      "loss": 0.0007,
      "step": 15040
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.004661965649574995,
      "learning_rate": 1.6789546666666668e-05,
      "loss": 0.0004,
      "step": 15050
    },
    {
      "epoch": 0.48192,
      "grad_norm": 0.028407985344529152,
      "learning_rate": 1.6787413333333337e-05,
      "loss": 0.0004,
      "step": 15060
    },
    {
      "epoch": 0.48224,
      "grad_norm": 0.017747992649674416,
      "learning_rate": 1.6785280000000002e-05,
      "loss": 0.0005,
      "step": 15070
    },
    {
      "epoch": 0.48256,
      "grad_norm": 0.011002443730831146,
      "learning_rate": 1.6783146666666667e-05,
      "loss": 0.0004,
      "step": 15080
    },
    {
      "epoch": 0.48288,
      "grad_norm": 0.007071537431329489,
      "learning_rate": 1.6781013333333333e-05,
      "loss": 0.0011,
      "step": 15090
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.009463144466280937,
      "learning_rate": 1.677888e-05,
      "loss": 0.0076,
      "step": 15100
    },
    {
      "epoch": 0.48352,
      "grad_norm": 0.005015865433961153,
      "learning_rate": 1.6776746666666667e-05,
      "loss": 0.0005,
      "step": 15110
    },
    {
      "epoch": 0.48384,
      "grad_norm": 0.010052614845335484,
      "learning_rate": 1.6774613333333332e-05,
      "loss": 0.0003,
      "step": 15120
    },
    {
      "epoch": 0.48416,
      "grad_norm": 0.005630084779113531,
      "learning_rate": 1.677248e-05,
      "loss": 0.0004,
      "step": 15130
    },
    {
      "epoch": 0.48448,
      "grad_norm": 0.005937451496720314,
      "learning_rate": 1.6770346666666666e-05,
      "loss": 0.0089,
      "step": 15140
    },
    {
      "epoch": 0.4848,
      "grad_norm": 3.479497194290161,
      "learning_rate": 1.6768213333333335e-05,
      "loss": 0.011,
      "step": 15150
    },
    {
      "epoch": 0.48512,
      "grad_norm": 1.6123161315917969,
      "learning_rate": 1.6766080000000004e-05,
      "loss": 0.0527,
      "step": 15160
    },
    {
      "epoch": 0.48544,
      "grad_norm": 0.006521038245409727,
      "learning_rate": 1.676394666666667e-05,
      "loss": 0.0003,
      "step": 15170
    },
    {
      "epoch": 0.48576,
      "grad_norm": 0.004858335014432669,
      "learning_rate": 1.6761813333333335e-05,
      "loss": 0.0004,
      "step": 15180
    },
    {
      "epoch": 0.48608,
      "grad_norm": 0.010810723528265953,
      "learning_rate": 1.675968e-05,
      "loss": 0.0003,
      "step": 15190
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.008574801497161388,
      "learning_rate": 1.675754666666667e-05,
      "loss": 0.0013,
      "step": 15200
    },
    {
      "epoch": 0.48672,
      "grad_norm": 0.007132408674806356,
      "learning_rate": 1.6755413333333334e-05,
      "loss": 0.0007,
      "step": 15210
    },
    {
      "epoch": 0.48704,
      "grad_norm": 0.00928807444870472,
      "learning_rate": 1.675328e-05,
      "loss": 0.0715,
      "step": 15220
    },
    {
      "epoch": 0.48736,
      "grad_norm": 0.007409196346998215,
      "learning_rate": 1.675114666666667e-05,
      "loss": 0.0011,
      "step": 15230
    },
    {
      "epoch": 0.48768,
      "grad_norm": 0.006588369142264128,
      "learning_rate": 1.6749013333333334e-05,
      "loss": 0.0005,
      "step": 15240
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.014666147530078888,
      "learning_rate": 1.6746880000000003e-05,
      "loss": 0.0006,
      "step": 15250
    },
    {
      "epoch": 0.48832,
      "grad_norm": 0.008545545861124992,
      "learning_rate": 1.6744746666666668e-05,
      "loss": 0.0004,
      "step": 15260
    },
    {
      "epoch": 0.48864,
      "grad_norm": 0.018001457676291466,
      "learning_rate": 1.6742613333333337e-05,
      "loss": 0.0461,
      "step": 15270
    },
    {
      "epoch": 0.48896,
      "grad_norm": 0.0035122232511639595,
      "learning_rate": 1.6740480000000002e-05,
      "loss": 0.0126,
      "step": 15280
    },
    {
      "epoch": 0.48928,
      "grad_norm": 0.009322572499513626,
      "learning_rate": 1.6738346666666667e-05,
      "loss": 0.0006,
      "step": 15290
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.003581420984119177,
      "learning_rate": 1.6736213333333336e-05,
      "loss": 0.0198,
      "step": 15300
    },
    {
      "epoch": 0.48992,
      "grad_norm": 0.018964465707540512,
      "learning_rate": 1.673408e-05,
      "loss": 0.0028,
      "step": 15310
    },
    {
      "epoch": 0.49024,
      "grad_norm": 0.006694371346384287,
      "learning_rate": 1.6731946666666667e-05,
      "loss": 0.0044,
      "step": 15320
    },
    {
      "epoch": 0.49056,
      "grad_norm": 0.011470415629446507,
      "learning_rate": 1.6729813333333336e-05,
      "loss": 0.0124,
      "step": 15330
    },
    {
      "epoch": 0.49088,
      "grad_norm": 0.007973075844347477,
      "learning_rate": 1.672768e-05,
      "loss": 0.0006,
      "step": 15340
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.007313998881727457,
      "learning_rate": 1.6725546666666666e-05,
      "loss": 0.0004,
      "step": 15350
    },
    {
      "epoch": 0.49152,
      "grad_norm": 0.022447960451245308,
      "learning_rate": 1.6723413333333335e-05,
      "loss": 0.0005,
      "step": 15360
    },
    {
      "epoch": 0.49184,
      "grad_norm": 0.00832937192171812,
      "learning_rate": 1.672128e-05,
      "loss": 0.0547,
      "step": 15370
    },
    {
      "epoch": 0.49216,
      "grad_norm": 0.004530587699264288,
      "learning_rate": 1.671914666666667e-05,
      "loss": 0.0005,
      "step": 15380
    },
    {
      "epoch": 0.49248,
      "grad_norm": 0.009006605483591557,
      "learning_rate": 1.6717013333333335e-05,
      "loss": 0.0006,
      "step": 15390
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.00567994499579072,
      "learning_rate": 1.6714880000000004e-05,
      "loss": 0.0004,
      "step": 15400
    },
    {
      "epoch": 0.49312,
      "grad_norm": 0.010037272237241268,
      "learning_rate": 1.671274666666667e-05,
      "loss": 0.0006,
      "step": 15410
    },
    {
      "epoch": 0.49344,
      "grad_norm": 0.01298694871366024,
      "learning_rate": 1.6710613333333334e-05,
      "loss": 0.0011,
      "step": 15420
    },
    {
      "epoch": 0.49376,
      "grad_norm": 0.0076503693126142025,
      "learning_rate": 1.6708480000000003e-05,
      "loss": 0.0017,
      "step": 15430
    },
    {
      "epoch": 0.49408,
      "grad_norm": 0.009372192434966564,
      "learning_rate": 1.670634666666667e-05,
      "loss": 0.0461,
      "step": 15440
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.009101237170398235,
      "learning_rate": 1.6704213333333334e-05,
      "loss": 0.0004,
      "step": 15450
    },
    {
      "epoch": 0.49472,
      "grad_norm": 0.025440478697419167,
      "learning_rate": 1.670208e-05,
      "loss": 0.0005,
      "step": 15460
    },
    {
      "epoch": 0.49504,
      "grad_norm": 0.012817516922950745,
      "learning_rate": 1.6699946666666668e-05,
      "loss": 0.001,
      "step": 15470
    },
    {
      "epoch": 0.49536,
      "grad_norm": 0.009114973247051239,
      "learning_rate": 1.6697813333333333e-05,
      "loss": 0.0005,
      "step": 15480
    },
    {
      "epoch": 0.49568,
      "grad_norm": 0.006581297609955072,
      "learning_rate": 1.6695680000000002e-05,
      "loss": 0.017,
      "step": 15490
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.04003908112645149,
      "learning_rate": 1.6693546666666667e-05,
      "loss": 0.0004,
      "step": 15500
    },
    {
      "epoch": 0.49632,
      "grad_norm": 0.007248455658555031,
      "learning_rate": 1.6691413333333336e-05,
      "loss": 0.0504,
      "step": 15510
    },
    {
      "epoch": 0.49664,
      "grad_norm": 0.009357291273772717,
      "learning_rate": 1.668928e-05,
      "loss": 0.0395,
      "step": 15520
    },
    {
      "epoch": 0.49696,
      "grad_norm": 0.025481654331088066,
      "learning_rate": 1.6687146666666667e-05,
      "loss": 0.0097,
      "step": 15530
    },
    {
      "epoch": 0.49728,
      "grad_norm": 0.007140530738979578,
      "learning_rate": 1.6685013333333336e-05,
      "loss": 0.0004,
      "step": 15540
    },
    {
      "epoch": 0.4976,
      "grad_norm": 2.9134838581085205,
      "learning_rate": 1.668288e-05,
      "loss": 0.0295,
      "step": 15550
    },
    {
      "epoch": 0.49792,
      "grad_norm": 0.004155518487095833,
      "learning_rate": 1.6680746666666666e-05,
      "loss": 0.0006,
      "step": 15560
    },
    {
      "epoch": 0.49824,
      "grad_norm": 0.018664482980966568,
      "learning_rate": 1.6678613333333335e-05,
      "loss": 0.0004,
      "step": 15570
    },
    {
      "epoch": 0.49856,
      "grad_norm": 0.008965153247117996,
      "learning_rate": 1.667648e-05,
      "loss": 0.0005,
      "step": 15580
    },
    {
      "epoch": 0.49888,
      "grad_norm": 0.008553759194910526,
      "learning_rate": 1.6674346666666666e-05,
      "loss": 0.0449,
      "step": 15590
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.0076292650774121284,
      "learning_rate": 1.6672213333333335e-05,
      "loss": 0.0004,
      "step": 15600
    },
    {
      "epoch": 0.49952,
      "grad_norm": 0.021941306069493294,
      "learning_rate": 1.667008e-05,
      "loss": 0.0006,
      "step": 15610
    },
    {
      "epoch": 0.49984,
      "grad_norm": 0.016787368804216385,
      "learning_rate": 1.666794666666667e-05,
      "loss": 0.0006,
      "step": 15620
    },
    {
      "epoch": 0.50016,
      "grad_norm": 0.012165706604719162,
      "learning_rate": 1.6665813333333334e-05,
      "loss": 0.0286,
      "step": 15630
    },
    {
      "epoch": 0.50048,
      "grad_norm": 0.009444166906177998,
      "learning_rate": 1.6663680000000003e-05,
      "loss": 0.0006,
      "step": 15640
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.012014218606054783,
      "learning_rate": 1.666154666666667e-05,
      "loss": 0.0007,
      "step": 15650
    },
    {
      "epoch": 0.50112,
      "grad_norm": 0.004837926011532545,
      "learning_rate": 1.6659413333333334e-05,
      "loss": 0.0143,
      "step": 15660
    },
    {
      "epoch": 0.50144,
      "grad_norm": 0.007448794785887003,
      "learning_rate": 1.6657280000000003e-05,
      "loss": 0.0009,
      "step": 15670
    },
    {
      "epoch": 0.50176,
      "grad_norm": 0.015588238835334778,
      "learning_rate": 1.6655146666666668e-05,
      "loss": 0.0005,
      "step": 15680
    },
    {
      "epoch": 0.50208,
      "grad_norm": 0.008955066092312336,
      "learning_rate": 1.6653013333333333e-05,
      "loss": 0.0062,
      "step": 15690
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.008835074491798878,
      "learning_rate": 1.665088e-05,
      "loss": 0.0565,
      "step": 15700
    },
    {
      "epoch": 0.50272,
      "grad_norm": 0.008672750554978848,
      "learning_rate": 1.6648746666666667e-05,
      "loss": 0.0006,
      "step": 15710
    },
    {
      "epoch": 0.50304,
      "grad_norm": 0.005411512218415737,
      "learning_rate": 1.6646613333333336e-05,
      "loss": 0.001,
      "step": 15720
    },
    {
      "epoch": 0.50336,
      "grad_norm": 0.012405527755618095,
      "learning_rate": 1.664448e-05,
      "loss": 0.0233,
      "step": 15730
    },
    {
      "epoch": 0.50368,
      "grad_norm": 0.006954175420105457,
      "learning_rate": 1.664234666666667e-05,
      "loss": 0.0354,
      "step": 15740
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.021947689354419708,
      "learning_rate": 1.6640213333333336e-05,
      "loss": 0.0006,
      "step": 15750
    },
    {
      "epoch": 0.50432,
      "grad_norm": 0.008964603766798973,
      "learning_rate": 1.663808e-05,
      "loss": 0.0149,
      "step": 15760
    },
    {
      "epoch": 0.50464,
      "grad_norm": 0.008128819055855274,
      "learning_rate": 1.663594666666667e-05,
      "loss": 0.0006,
      "step": 15770
    },
    {
      "epoch": 0.50496,
      "grad_norm": 0.6806969046592712,
      "learning_rate": 1.6633813333333335e-05,
      "loss": 0.0239,
      "step": 15780
    },
    {
      "epoch": 0.50528,
      "grad_norm": 0.008370463736355305,
      "learning_rate": 1.663168e-05,
      "loss": 0.0009,
      "step": 15790
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.008708340115845203,
      "learning_rate": 1.6629546666666666e-05,
      "loss": 0.0042,
      "step": 15800
    },
    {
      "epoch": 0.50592,
      "grad_norm": 0.023999515920877457,
      "learning_rate": 1.6627413333333335e-05,
      "loss": 0.0004,
      "step": 15810
    },
    {
      "epoch": 0.50624,
      "grad_norm": 0.02924046479165554,
      "learning_rate": 1.662528e-05,
      "loss": 0.0265,
      "step": 15820
    },
    {
      "epoch": 0.50656,
      "grad_norm": 0.010226327925920486,
      "learning_rate": 1.662314666666667e-05,
      "loss": 0.0006,
      "step": 15830
    },
    {
      "epoch": 0.50688,
      "grad_norm": 0.012180779129266739,
      "learning_rate": 1.6621013333333334e-05,
      "loss": 0.0006,
      "step": 15840
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.016560040414333344,
      "learning_rate": 1.6618880000000003e-05,
      "loss": 0.0006,
      "step": 15850
    },
    {
      "epoch": 0.50752,
      "grad_norm": 0.020702406764030457,
      "learning_rate": 1.661674666666667e-05,
      "loss": 0.0006,
      "step": 15860
    },
    {
      "epoch": 0.50784,
      "grad_norm": 0.006248245947062969,
      "learning_rate": 1.6614613333333334e-05,
      "loss": 0.0006,
      "step": 15870
    },
    {
      "epoch": 0.50816,
      "grad_norm": 0.007577529642730951,
      "learning_rate": 1.6612480000000003e-05,
      "loss": 0.0005,
      "step": 15880
    },
    {
      "epoch": 0.50848,
      "grad_norm": 0.00874683540314436,
      "learning_rate": 1.6610346666666668e-05,
      "loss": 0.0493,
      "step": 15890
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.02148962952196598,
      "learning_rate": 1.6608213333333333e-05,
      "loss": 0.0471,
      "step": 15900
    },
    {
      "epoch": 0.50912,
      "grad_norm": 0.03802371770143509,
      "learning_rate": 1.6606080000000002e-05,
      "loss": 0.0007,
      "step": 15910
    },
    {
      "epoch": 0.50944,
      "grad_norm": 0.03131064400076866,
      "learning_rate": 1.6603946666666667e-05,
      "loss": 0.001,
      "step": 15920
    },
    {
      "epoch": 0.50976,
      "grad_norm": 0.014754248782992363,
      "learning_rate": 1.6601813333333333e-05,
      "loss": 0.0065,
      "step": 15930
    },
    {
      "epoch": 0.51008,
      "grad_norm": 0.01264454610645771,
      "learning_rate": 1.659968e-05,
      "loss": 0.0005,
      "step": 15940
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.02803925983607769,
      "learning_rate": 1.6597546666666667e-05,
      "loss": 0.0144,
      "step": 15950
    },
    {
      "epoch": 0.51072,
      "grad_norm": 0.014864684082567692,
      "learning_rate": 1.6595413333333336e-05,
      "loss": 0.0511,
      "step": 15960
    },
    {
      "epoch": 0.51104,
      "grad_norm": 0.01787485182285309,
      "learning_rate": 1.659328e-05,
      "loss": 0.0007,
      "step": 15970
    },
    {
      "epoch": 0.51136,
      "grad_norm": 0.0063425954431295395,
      "learning_rate": 1.659114666666667e-05,
      "loss": 0.0008,
      "step": 15980
    },
    {
      "epoch": 0.51168,
      "grad_norm": 0.01797250285744667,
      "learning_rate": 1.6589013333333335e-05,
      "loss": 0.0007,
      "step": 15990
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.006293045356869698,
      "learning_rate": 1.658688e-05,
      "loss": 0.0004,
      "step": 16000
    },
    {
      "epoch": 0.51232,
      "grad_norm": 0.00760841928422451,
      "learning_rate": 1.658474666666667e-05,
      "loss": 0.0431,
      "step": 16010
    },
    {
      "epoch": 0.51264,
      "grad_norm": 0.03618704155087471,
      "learning_rate": 1.6582613333333335e-05,
      "loss": 0.0008,
      "step": 16020
    },
    {
      "epoch": 0.51296,
      "grad_norm": 0.006916073616594076,
      "learning_rate": 1.658048e-05,
      "loss": 0.0008,
      "step": 16030
    },
    {
      "epoch": 0.51328,
      "grad_norm": 0.007720166817307472,
      "learning_rate": 1.6578346666666666e-05,
      "loss": 0.0006,
      "step": 16040
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.050419460982084274,
      "learning_rate": 1.6576213333333334e-05,
      "loss": 0.0006,
      "step": 16050
    },
    {
      "epoch": 0.51392,
      "grad_norm": 0.008027644827961922,
      "learning_rate": 1.657408e-05,
      "loss": 0.0186,
      "step": 16060
    },
    {
      "epoch": 0.51424,
      "grad_norm": 0.0076301670633256435,
      "learning_rate": 1.657194666666667e-05,
      "loss": 0.0005,
      "step": 16070
    },
    {
      "epoch": 0.51456,
      "grad_norm": 0.009769988246262074,
      "learning_rate": 1.6569813333333334e-05,
      "loss": 0.0191,
      "step": 16080
    },
    {
      "epoch": 0.51488,
      "grad_norm": 0.013529240153729916,
      "learning_rate": 1.6567680000000003e-05,
      "loss": 0.0005,
      "step": 16090
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.011153902858495712,
      "learning_rate": 1.6565546666666668e-05,
      "loss": 0.0092,
      "step": 16100
    },
    {
      "epoch": 0.51552,
      "grad_norm": 0.014069945551455021,
      "learning_rate": 1.6563413333333337e-05,
      "loss": 0.0009,
      "step": 16110
    },
    {
      "epoch": 0.51584,
      "grad_norm": 0.009013408794999123,
      "learning_rate": 1.6561280000000002e-05,
      "loss": 0.0007,
      "step": 16120
    },
    {
      "epoch": 0.51616,
      "grad_norm": 0.006428043358027935,
      "learning_rate": 1.6559146666666668e-05,
      "loss": 0.0006,
      "step": 16130
    },
    {
      "epoch": 0.51648,
      "grad_norm": 0.08254373073577881,
      "learning_rate": 1.6557013333333333e-05,
      "loss": 0.0713,
      "step": 16140
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.013099506497383118,
      "learning_rate": 1.655488e-05,
      "loss": 0.0094,
      "step": 16150
    },
    {
      "epoch": 0.51712,
      "grad_norm": 0.009414001367986202,
      "learning_rate": 1.6552746666666667e-05,
      "loss": 0.0004,
      "step": 16160
    },
    {
      "epoch": 0.51744,
      "grad_norm": 0.004849375691264868,
      "learning_rate": 1.6550613333333332e-05,
      "loss": 0.0006,
      "step": 16170
    },
    {
      "epoch": 0.51776,
      "grad_norm": 0.007160808425396681,
      "learning_rate": 1.654848e-05,
      "loss": 0.0008,
      "step": 16180
    },
    {
      "epoch": 0.51808,
      "grad_norm": 0.02427312172949314,
      "learning_rate": 1.654634666666667e-05,
      "loss": 0.0611,
      "step": 16190
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.025341179221868515,
      "learning_rate": 1.6544213333333335e-05,
      "loss": 0.0004,
      "step": 16200
    },
    {
      "epoch": 0.51872,
      "grad_norm": 0.020960360765457153,
      "learning_rate": 1.654208e-05,
      "loss": 0.0042,
      "step": 16210
    },
    {
      "epoch": 0.51904,
      "grad_norm": 1.7069436311721802,
      "learning_rate": 1.653994666666667e-05,
      "loss": 0.0027,
      "step": 16220
    },
    {
      "epoch": 0.51936,
      "grad_norm": 0.008468563668429852,
      "learning_rate": 1.6537813333333335e-05,
      "loss": 0.0219,
      "step": 16230
    },
    {
      "epoch": 0.51968,
      "grad_norm": 0.005741333123296499,
      "learning_rate": 1.653568e-05,
      "loss": 0.0374,
      "step": 16240
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.05520280450582504,
      "learning_rate": 1.653354666666667e-05,
      "loss": 0.0005,
      "step": 16250
    },
    {
      "epoch": 0.52032,
      "grad_norm": 0.006684124935418367,
      "learning_rate": 1.6531413333333334e-05,
      "loss": 0.0005,
      "step": 16260
    },
    {
      "epoch": 0.52064,
      "grad_norm": 0.06199348345398903,
      "learning_rate": 1.652928e-05,
      "loss": 0.0029,
      "step": 16270
    },
    {
      "epoch": 0.52096,
      "grad_norm": 0.0197795107960701,
      "learning_rate": 1.652714666666667e-05,
      "loss": 0.0007,
      "step": 16280
    },
    {
      "epoch": 0.52128,
      "grad_norm": 0.006254140753298998,
      "learning_rate": 1.6525013333333334e-05,
      "loss": 0.0007,
      "step": 16290
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.007585591170936823,
      "learning_rate": 1.6522880000000003e-05,
      "loss": 0.0004,
      "step": 16300
    },
    {
      "epoch": 0.52192,
      "grad_norm": 0.14864322543144226,
      "learning_rate": 1.6520746666666668e-05,
      "loss": 0.0096,
      "step": 16310
    },
    {
      "epoch": 0.52224,
      "grad_norm": 0.11790836602449417,
      "learning_rate": 1.6518613333333337e-05,
      "loss": 0.0008,
      "step": 16320
    },
    {
      "epoch": 0.52256,
      "grad_norm": 0.005242343060672283,
      "learning_rate": 1.6516480000000002e-05,
      "loss": 0.0004,
      "step": 16330
    },
    {
      "epoch": 0.52288,
      "grad_norm": 0.007900950498878956,
      "learning_rate": 1.6514346666666668e-05,
      "loss": 0.0034,
      "step": 16340
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.008792420849204063,
      "learning_rate": 1.6512213333333336e-05,
      "loss": 0.0012,
      "step": 16350
    },
    {
      "epoch": 0.52352,
      "grad_norm": 0.0823657214641571,
      "learning_rate": 1.651008e-05,
      "loss": 0.0007,
      "step": 16360
    },
    {
      "epoch": 0.52384,
      "grad_norm": 0.003863989608362317,
      "learning_rate": 1.6507946666666667e-05,
      "loss": 0.0003,
      "step": 16370
    },
    {
      "epoch": 0.52416,
      "grad_norm": 0.0098470663651824,
      "learning_rate": 1.6505813333333332e-05,
      "loss": 0.0558,
      "step": 16380
    },
    {
      "epoch": 0.52448,
      "grad_norm": 0.17699892818927765,
      "learning_rate": 1.650368e-05,
      "loss": 0.0009,
      "step": 16390
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.0056085665710270405,
      "learning_rate": 1.6501546666666667e-05,
      "loss": 0.0003,
      "step": 16400
    },
    {
      "epoch": 0.52512,
      "grad_norm": 0.00898407306522131,
      "learning_rate": 1.6499413333333335e-05,
      "loss": 0.0337,
      "step": 16410
    },
    {
      "epoch": 0.52544,
      "grad_norm": 0.012412551790475845,
      "learning_rate": 1.649728e-05,
      "loss": 0.0018,
      "step": 16420
    },
    {
      "epoch": 0.52576,
      "grad_norm": 0.06839016079902649,
      "learning_rate": 1.649514666666667e-05,
      "loss": 0.008,
      "step": 16430
    },
    {
      "epoch": 0.52608,
      "grad_norm": 0.006187459919601679,
      "learning_rate": 1.6493013333333335e-05,
      "loss": 0.0481,
      "step": 16440
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.00996171310544014,
      "learning_rate": 1.6490880000000004e-05,
      "loss": 0.0007,
      "step": 16450
    },
    {
      "epoch": 0.52672,
      "grad_norm": 0.004227558150887489,
      "learning_rate": 1.648874666666667e-05,
      "loss": 0.0008,
      "step": 16460
    },
    {
      "epoch": 0.52704,
      "grad_norm": 0.028133511543273926,
      "learning_rate": 1.6486613333333334e-05,
      "loss": 0.0007,
      "step": 16470
    },
    {
      "epoch": 0.52736,
      "grad_norm": 0.017372077330946922,
      "learning_rate": 1.648448e-05,
      "loss": 0.0008,
      "step": 16480
    },
    {
      "epoch": 0.52768,
      "grad_norm": 0.013264352455735207,
      "learning_rate": 1.648234666666667e-05,
      "loss": 0.0012,
      "step": 16490
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.011651508510112762,
      "learning_rate": 1.6480213333333334e-05,
      "loss": 0.0065,
      "step": 16500
    },
    {
      "epoch": 0.52832,
      "grad_norm": 0.006269482430070639,
      "learning_rate": 1.647808e-05,
      "loss": 0.003,
      "step": 16510
    },
    {
      "epoch": 0.52864,
      "grad_norm": 0.010473573580384254,
      "learning_rate": 1.6475946666666668e-05,
      "loss": 0.047,
      "step": 16520
    },
    {
      "epoch": 0.52896,
      "grad_norm": 0.012331679463386536,
      "learning_rate": 1.6473813333333333e-05,
      "loss": 0.0005,
      "step": 16530
    },
    {
      "epoch": 0.52928,
      "grad_norm": 0.008601754903793335,
      "learning_rate": 1.6471680000000002e-05,
      "loss": 0.0006,
      "step": 16540
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.00580970523878932,
      "learning_rate": 1.6469546666666668e-05,
      "loss": 0.0004,
      "step": 16550
    },
    {
      "epoch": 0.52992,
      "grad_norm": 0.004670686554163694,
      "learning_rate": 1.6467413333333336e-05,
      "loss": 0.0646,
      "step": 16560
    },
    {
      "epoch": 0.53024,
      "grad_norm": 0.0063790143467485905,
      "learning_rate": 1.6465280000000002e-05,
      "loss": 0.0008,
      "step": 16570
    },
    {
      "epoch": 0.53056,
      "grad_norm": 0.005650926847010851,
      "learning_rate": 1.6463146666666667e-05,
      "loss": 0.0006,
      "step": 16580
    },
    {
      "epoch": 0.53088,
      "grad_norm": 0.004978883545845747,
      "learning_rate": 1.6461013333333336e-05,
      "loss": 0.0103,
      "step": 16590
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.017972702160477638,
      "learning_rate": 1.645888e-05,
      "loss": 0.0007,
      "step": 16600
    },
    {
      "epoch": 0.53152,
      "grad_norm": 0.005401307251304388,
      "learning_rate": 1.6456746666666667e-05,
      "loss": 0.0005,
      "step": 16610
    },
    {
      "epoch": 0.53184,
      "grad_norm": 0.00806860625743866,
      "learning_rate": 1.6454613333333335e-05,
      "loss": 0.0004,
      "step": 16620
    },
    {
      "epoch": 0.53216,
      "grad_norm": 0.01709476299583912,
      "learning_rate": 1.645248e-05,
      "loss": 0.0012,
      "step": 16630
    },
    {
      "epoch": 0.53248,
      "grad_norm": 0.008746933192014694,
      "learning_rate": 1.6450346666666666e-05,
      "loss": 0.0039,
      "step": 16640
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.010770902037620544,
      "learning_rate": 1.6448213333333335e-05,
      "loss": 0.0004,
      "step": 16650
    },
    {
      "epoch": 0.53312,
      "grad_norm": 0.011954624205827713,
      "learning_rate": 1.6446080000000004e-05,
      "loss": 0.0004,
      "step": 16660
    },
    {
      "epoch": 0.53344,
      "grad_norm": 0.008293809369206429,
      "learning_rate": 1.644394666666667e-05,
      "loss": 0.0006,
      "step": 16670
    },
    {
      "epoch": 0.53376,
      "grad_norm": 0.0057153161615133286,
      "learning_rate": 1.6441813333333334e-05,
      "loss": 0.0003,
      "step": 16680
    },
    {
      "epoch": 0.53408,
      "grad_norm": 1.9493955373764038,
      "learning_rate": 1.6439680000000003e-05,
      "loss": 0.0288,
      "step": 16690
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.010040847584605217,
      "learning_rate": 1.643754666666667e-05,
      "loss": 0.001,
      "step": 16700
    },
    {
      "epoch": 0.53472,
      "grad_norm": 0.007364096585661173,
      "learning_rate": 1.6435413333333334e-05,
      "loss": 0.0004,
      "step": 16710
    },
    {
      "epoch": 0.53504,
      "grad_norm": 1.0101820230484009,
      "learning_rate": 1.643328e-05,
      "loss": 0.049,
      "step": 16720
    },
    {
      "epoch": 0.53536,
      "grad_norm": 0.08620171248912811,
      "learning_rate": 1.6431146666666668e-05,
      "loss": 0.0466,
      "step": 16730
    },
    {
      "epoch": 0.53568,
      "grad_norm": 0.01428964827209711,
      "learning_rate": 1.6429013333333333e-05,
      "loss": 0.0005,
      "step": 16740
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.02808183990418911,
      "learning_rate": 1.6426880000000002e-05,
      "loss": 0.0007,
      "step": 16750
    },
    {
      "epoch": 0.53632,
      "grad_norm": 0.01794525608420372,
      "learning_rate": 1.6424746666666668e-05,
      "loss": 0.0009,
      "step": 16760
    },
    {
      "epoch": 0.53664,
      "grad_norm": 0.01461729034781456,
      "learning_rate": 1.6422613333333336e-05,
      "loss": 0.0495,
      "step": 16770
    },
    {
      "epoch": 0.53696,
      "grad_norm": 0.006425789091736078,
      "learning_rate": 1.6420480000000002e-05,
      "loss": 0.001,
      "step": 16780
    },
    {
      "epoch": 0.53728,
      "grad_norm": 0.004753948654979467,
      "learning_rate": 1.641834666666667e-05,
      "loss": 0.0005,
      "step": 16790
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.0166563019156456,
      "learning_rate": 1.6416213333333336e-05,
      "loss": 0.0007,
      "step": 16800
    },
    {
      "epoch": 0.53792,
      "grad_norm": 0.005572624038904905,
      "learning_rate": 1.641408e-05,
      "loss": 0.0006,
      "step": 16810
    },
    {
      "epoch": 0.53824,
      "grad_norm": 0.013714288361370564,
      "learning_rate": 1.6411946666666667e-05,
      "loss": 0.0491,
      "step": 16820
    },
    {
      "epoch": 0.53856,
      "grad_norm": 0.01428413949906826,
      "learning_rate": 1.6409813333333335e-05,
      "loss": 0.0093,
      "step": 16830
    },
    {
      "epoch": 0.53888,
      "grad_norm": 0.030580712482333183,
      "learning_rate": 1.640768e-05,
      "loss": 0.0009,
      "step": 16840
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.00859631597995758,
      "learning_rate": 1.6405546666666666e-05,
      "loss": 0.001,
      "step": 16850
    },
    {
      "epoch": 0.53952,
      "grad_norm": 0.015007009729743004,
      "learning_rate": 1.6403413333333335e-05,
      "loss": 0.0006,
      "step": 16860
    },
    {
      "epoch": 0.53984,
      "grad_norm": 0.014844509772956371,
      "learning_rate": 1.640128e-05,
      "loss": 0.0181,
      "step": 16870
    },
    {
      "epoch": 0.54016,
      "grad_norm": 0.031823620200157166,
      "learning_rate": 1.639914666666667e-05,
      "loss": 0.0007,
      "step": 16880
    },
    {
      "epoch": 0.54048,
      "grad_norm": 0.007065914571285248,
      "learning_rate": 1.6397013333333334e-05,
      "loss": 0.006,
      "step": 16890
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.023421481251716614,
      "learning_rate": 1.6394880000000003e-05,
      "loss": 0.0062,
      "step": 16900
    },
    {
      "epoch": 0.54112,
      "grad_norm": 0.03895605728030205,
      "learning_rate": 1.639274666666667e-05,
      "loss": 0.0012,
      "step": 16910
    },
    {
      "epoch": 0.54144,
      "grad_norm": 0.01346171461045742,
      "learning_rate": 1.6390613333333334e-05,
      "loss": 0.0019,
      "step": 16920
    },
    {
      "epoch": 0.54176,
      "grad_norm": 0.013321447186172009,
      "learning_rate": 1.6388480000000003e-05,
      "loss": 0.0006,
      "step": 16930
    },
    {
      "epoch": 0.54208,
      "grad_norm": 0.0035541332326829433,
      "learning_rate": 1.6386346666666668e-05,
      "loss": 0.0005,
      "step": 16940
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.009942485950887203,
      "learning_rate": 1.6384213333333333e-05,
      "loss": 0.0163,
      "step": 16950
    },
    {
      "epoch": 0.54272,
      "grad_norm": 0.009142715483903885,
      "learning_rate": 1.6382080000000002e-05,
      "loss": 0.0006,
      "step": 16960
    },
    {
      "epoch": 0.54304,
      "grad_norm": 0.010740731842815876,
      "learning_rate": 1.6379946666666668e-05,
      "loss": 0.0005,
      "step": 16970
    },
    {
      "epoch": 0.54336,
      "grad_norm": 0.005160598084330559,
      "learning_rate": 1.6377813333333333e-05,
      "loss": 0.0004,
      "step": 16980
    },
    {
      "epoch": 0.54368,
      "grad_norm": 0.010314601473510265,
      "learning_rate": 1.6375680000000002e-05,
      "loss": 0.0124,
      "step": 16990
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.007102685514837503,
      "learning_rate": 1.6373546666666667e-05,
      "loss": 0.0004,
      "step": 17000
    },
    {
      "epoch": 0.54432,
      "grad_norm": 0.006388181354850531,
      "learning_rate": 1.6371413333333336e-05,
      "loss": 0.015,
      "step": 17010
    },
    {
      "epoch": 0.54464,
      "grad_norm": 0.005647857673466206,
      "learning_rate": 1.636928e-05,
      "loss": 0.0004,
      "step": 17020
    },
    {
      "epoch": 0.54496,
      "grad_norm": 0.007888346910476685,
      "learning_rate": 1.636714666666667e-05,
      "loss": 0.0003,
      "step": 17030
    },
    {
      "epoch": 0.54528,
      "grad_norm": 0.015814799815416336,
      "learning_rate": 1.6365013333333335e-05,
      "loss": 0.0005,
      "step": 17040
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.008077528327703476,
      "learning_rate": 1.636288e-05,
      "loss": 0.0003,
      "step": 17050
    },
    {
      "epoch": 0.54592,
      "grad_norm": 0.006486616563051939,
      "learning_rate": 1.6360746666666666e-05,
      "loss": 0.0377,
      "step": 17060
    },
    {
      "epoch": 0.54624,
      "grad_norm": 0.5301576256752014,
      "learning_rate": 1.6358613333333335e-05,
      "loss": 0.0019,
      "step": 17070
    },
    {
      "epoch": 0.54656,
      "grad_norm": 0.005705027375370264,
      "learning_rate": 1.635648e-05,
      "loss": 0.0007,
      "step": 17080
    },
    {
      "epoch": 0.54688,
      "grad_norm": 0.008722960017621517,
      "learning_rate": 1.6354346666666666e-05,
      "loss": 0.0532,
      "step": 17090
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.022483717650175095,
      "learning_rate": 1.6352213333333334e-05,
      "loss": 0.0005,
      "step": 17100
    },
    {
      "epoch": 0.54752,
      "grad_norm": 0.011900920420885086,
      "learning_rate": 1.635008e-05,
      "loss": 0.0237,
      "step": 17110
    },
    {
      "epoch": 0.54784,
      "grad_norm": 0.025427130982279778,
      "learning_rate": 1.634794666666667e-05,
      "loss": 0.0003,
      "step": 17120
    },
    {
      "epoch": 0.54816,
      "grad_norm": 0.004987354390323162,
      "learning_rate": 1.6345813333333337e-05,
      "loss": 0.0379,
      "step": 17130
    },
    {
      "epoch": 0.54848,
      "grad_norm": 0.01972009614109993,
      "learning_rate": 1.6343680000000003e-05,
      "loss": 0.0136,
      "step": 17140
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.004994882270693779,
      "learning_rate": 1.6341546666666668e-05,
      "loss": 0.0005,
      "step": 17150
    },
    {
      "epoch": 0.54912,
      "grad_norm": 0.0058973003178834915,
      "learning_rate": 1.6339413333333333e-05,
      "loss": 0.0218,
      "step": 17160
    },
    {
      "epoch": 0.54944,
      "grad_norm": 0.010474300011992455,
      "learning_rate": 1.6337280000000002e-05,
      "loss": 0.0006,
      "step": 17170
    },
    {
      "epoch": 0.54976,
      "grad_norm": 0.01255826186388731,
      "learning_rate": 1.6335146666666668e-05,
      "loss": 0.0004,
      "step": 17180
    },
    {
      "epoch": 0.55008,
      "grad_norm": 0.02796795964241028,
      "learning_rate": 1.6333013333333333e-05,
      "loss": 0.0005,
      "step": 17190
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.00768160680308938,
      "learning_rate": 1.6330880000000002e-05,
      "loss": 0.0006,
      "step": 17200
    },
    {
      "epoch": 0.55072,
      "grad_norm": 0.004203083924949169,
      "learning_rate": 1.6328746666666667e-05,
      "loss": 0.0003,
      "step": 17210
    },
    {
      "epoch": 0.55104,
      "grad_norm": 0.028849322348833084,
      "learning_rate": 1.6326613333333336e-05,
      "loss": 0.0126,
      "step": 17220
    },
    {
      "epoch": 0.55136,
      "grad_norm": 2.379599094390869,
      "learning_rate": 1.632448e-05,
      "loss": 0.0445,
      "step": 17230
    },
    {
      "epoch": 0.55168,
      "grad_norm": 0.006259124260395765,
      "learning_rate": 1.632234666666667e-05,
      "loss": 0.0004,
      "step": 17240
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.0056764427572488785,
      "learning_rate": 1.6320213333333335e-05,
      "loss": 0.0005,
      "step": 17250
    },
    {
      "epoch": 0.55232,
      "grad_norm": 0.007224116008728743,
      "learning_rate": 1.631808e-05,
      "loss": 0.0028,
      "step": 17260
    },
    {
      "epoch": 0.55264,
      "grad_norm": 0.015809983015060425,
      "learning_rate": 1.631594666666667e-05,
      "loss": 0.0005,
      "step": 17270
    },
    {
      "epoch": 0.55296,
      "grad_norm": 0.005285319872200489,
      "learning_rate": 1.6313813333333335e-05,
      "loss": 0.0004,
      "step": 17280
    },
    {
      "epoch": 0.55328,
      "grad_norm": 0.01479177363216877,
      "learning_rate": 1.631168e-05,
      "loss": 0.0004,
      "step": 17290
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.004237360320985317,
      "learning_rate": 1.630954666666667e-05,
      "loss": 0.0024,
      "step": 17300
    },
    {
      "epoch": 0.55392,
      "grad_norm": 0.012408585287630558,
      "learning_rate": 1.6307413333333334e-05,
      "loss": 0.0501,
      "step": 17310
    },
    {
      "epoch": 0.55424,
      "grad_norm": 0.005244848784059286,
      "learning_rate": 1.630528e-05,
      "loss": 0.0004,
      "step": 17320
    },
    {
      "epoch": 0.55456,
      "grad_norm": 0.005836265627294779,
      "learning_rate": 1.630314666666667e-05,
      "loss": 0.0012,
      "step": 17330
    },
    {
      "epoch": 0.55488,
      "grad_norm": 0.006033594720065594,
      "learning_rate": 1.6301013333333334e-05,
      "loss": 0.0008,
      "step": 17340
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.011604146100580692,
      "learning_rate": 1.6298880000000003e-05,
      "loss": 0.0083,
      "step": 17350
    },
    {
      "epoch": 0.55552,
      "grad_norm": 0.006415731739252806,
      "learning_rate": 1.6296746666666668e-05,
      "loss": 0.0006,
      "step": 17360
    },
    {
      "epoch": 0.55584,
      "grad_norm": 0.011144128628075123,
      "learning_rate": 1.6294613333333337e-05,
      "loss": 0.0004,
      "step": 17370
    },
    {
      "epoch": 0.55616,
      "grad_norm": 0.006591759156435728,
      "learning_rate": 1.6292480000000002e-05,
      "loss": 0.0314,
      "step": 17380
    },
    {
      "epoch": 0.55648,
      "grad_norm": 0.006845675874501467,
      "learning_rate": 1.6290346666666668e-05,
      "loss": 0.0003,
      "step": 17390
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.009548668749630451,
      "learning_rate": 1.6288213333333333e-05,
      "loss": 0.0181,
      "step": 17400
    },
    {
      "epoch": 0.55712,
      "grad_norm": 0.0052825030870735645,
      "learning_rate": 1.6286080000000002e-05,
      "loss": 0.0005,
      "step": 17410
    },
    {
      "epoch": 0.55744,
      "grad_norm": 5.266290664672852,
      "learning_rate": 1.6283946666666667e-05,
      "loss": 0.0215,
      "step": 17420
    },
    {
      "epoch": 0.55776,
      "grad_norm": 0.006205935496836901,
      "learning_rate": 1.6281813333333333e-05,
      "loss": 0.0056,
      "step": 17430
    },
    {
      "epoch": 0.55808,
      "grad_norm": 0.006010144483298063,
      "learning_rate": 1.627968e-05,
      "loss": 0.0005,
      "step": 17440
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.039228081703186035,
      "learning_rate": 1.6277546666666667e-05,
      "loss": 0.0391,
      "step": 17450
    },
    {
      "epoch": 0.55872,
      "grad_norm": 0.008574292063713074,
      "learning_rate": 1.6275413333333335e-05,
      "loss": 0.0006,
      "step": 17460
    },
    {
      "epoch": 0.55904,
      "grad_norm": 0.00689248600974679,
      "learning_rate": 1.627328e-05,
      "loss": 0.0005,
      "step": 17470
    },
    {
      "epoch": 0.55936,
      "grad_norm": 0.005737015511840582,
      "learning_rate": 1.627114666666667e-05,
      "loss": 0.0562,
      "step": 17480
    },
    {
      "epoch": 0.55968,
      "grad_norm": 0.00597362732514739,
      "learning_rate": 1.6269013333333335e-05,
      "loss": 0.0367,
      "step": 17490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.005883547477424145,
      "learning_rate": 1.626688e-05,
      "loss": 0.0005,
      "step": 17500
    },
    {
      "epoch": 0.56032,
      "grad_norm": 0.0072342222556471825,
      "learning_rate": 1.626474666666667e-05,
      "loss": 0.0007,
      "step": 17510
    },
    {
      "epoch": 0.56064,
      "grad_norm": 0.006151384208351374,
      "learning_rate": 1.6262613333333334e-05,
      "loss": 0.0004,
      "step": 17520
    },
    {
      "epoch": 0.56096,
      "grad_norm": 3.8675174713134766,
      "learning_rate": 1.626048e-05,
      "loss": 0.0116,
      "step": 17530
    },
    {
      "epoch": 0.56128,
      "grad_norm": 0.00608885008841753,
      "learning_rate": 1.625834666666667e-05,
      "loss": 0.0191,
      "step": 17540
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.008850731886923313,
      "learning_rate": 1.6256213333333334e-05,
      "loss": 0.0004,
      "step": 17550
    },
    {
      "epoch": 0.56192,
      "grad_norm": 0.007957668974995613,
      "learning_rate": 1.625408e-05,
      "loss": 0.0008,
      "step": 17560
    },
    {
      "epoch": 0.56224,
      "grad_norm": 0.014701077714562416,
      "learning_rate": 1.6251946666666668e-05,
      "loss": 0.0005,
      "step": 17570
    },
    {
      "epoch": 0.56256,
      "grad_norm": 0.03772115707397461,
      "learning_rate": 1.6249813333333334e-05,
      "loss": 0.0196,
      "step": 17580
    },
    {
      "epoch": 0.56288,
      "grad_norm": 0.009811838157474995,
      "learning_rate": 1.6247680000000002e-05,
      "loss": 0.0004,
      "step": 17590
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.03140385076403618,
      "learning_rate": 1.6245546666666668e-05,
      "loss": 0.0006,
      "step": 17600
    },
    {
      "epoch": 0.56352,
      "grad_norm": 0.023722777143120766,
      "learning_rate": 1.6243413333333336e-05,
      "loss": 0.0557,
      "step": 17610
    },
    {
      "epoch": 0.56384,
      "grad_norm": 0.020146694034337997,
      "learning_rate": 1.6241280000000002e-05,
      "loss": 0.0541,
      "step": 17620
    },
    {
      "epoch": 0.56416,
      "grad_norm": 0.054431673139333725,
      "learning_rate": 1.6239146666666667e-05,
      "loss": 0.0088,
      "step": 17630
    },
    {
      "epoch": 0.56448,
      "grad_norm": 0.008496927097439766,
      "learning_rate": 1.6237013333333336e-05,
      "loss": 0.0518,
      "step": 17640
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.037471357733011246,
      "learning_rate": 1.623488e-05,
      "loss": 0.0005,
      "step": 17650
    },
    {
      "epoch": 0.56512,
      "grad_norm": 0.009221923537552357,
      "learning_rate": 1.6232746666666667e-05,
      "loss": 0.0007,
      "step": 17660
    },
    {
      "epoch": 0.56544,
      "grad_norm": 0.009234665893018246,
      "learning_rate": 1.6230613333333332e-05,
      "loss": 0.0419,
      "step": 17670
    },
    {
      "epoch": 0.56576,
      "grad_norm": 0.00978864822536707,
      "learning_rate": 1.622848e-05,
      "loss": 0.0013,
      "step": 17680
    },
    {
      "epoch": 0.56608,
      "grad_norm": 0.012449204921722412,
      "learning_rate": 1.622634666666667e-05,
      "loss": 0.0005,
      "step": 17690
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.009232760407030582,
      "learning_rate": 1.6224213333333335e-05,
      "loss": 0.0006,
      "step": 17700
    },
    {
      "epoch": 0.56672,
      "grad_norm": 0.00645520118996501,
      "learning_rate": 1.6222080000000004e-05,
      "loss": 0.0005,
      "step": 17710
    },
    {
      "epoch": 0.56704,
      "grad_norm": 0.01742754690349102,
      "learning_rate": 1.621994666666667e-05,
      "loss": 0.0014,
      "step": 17720
    },
    {
      "epoch": 0.56736,
      "grad_norm": 0.004862730856984854,
      "learning_rate": 1.6217813333333335e-05,
      "loss": 0.0005,
      "step": 17730
    },
    {
      "epoch": 0.56768,
      "grad_norm": 0.005444931332021952,
      "learning_rate": 1.621568e-05,
      "loss": 0.0012,
      "step": 17740
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.01130647026002407,
      "learning_rate": 1.621354666666667e-05,
      "loss": 0.0006,
      "step": 17750
    },
    {
      "epoch": 0.56832,
      "grad_norm": 0.004404942970722914,
      "learning_rate": 1.6211413333333334e-05,
      "loss": 0.0007,
      "step": 17760
    },
    {
      "epoch": 0.56864,
      "grad_norm": 0.008590629324316978,
      "learning_rate": 1.620928e-05,
      "loss": 0.0419,
      "step": 17770
    },
    {
      "epoch": 0.56896,
      "grad_norm": 0.009971345774829388,
      "learning_rate": 1.6207146666666668e-05,
      "loss": 0.002,
      "step": 17780
    },
    {
      "epoch": 0.56928,
      "grad_norm": 0.01890561915934086,
      "learning_rate": 1.6205013333333334e-05,
      "loss": 0.0005,
      "step": 17790
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.005286029074341059,
      "learning_rate": 1.6202880000000002e-05,
      "loss": 0.0005,
      "step": 17800
    },
    {
      "epoch": 0.56992,
      "grad_norm": 0.010238283313810825,
      "learning_rate": 1.6200746666666668e-05,
      "loss": 0.0004,
      "step": 17810
    },
    {
      "epoch": 0.57024,
      "grad_norm": 0.0075180912390351295,
      "learning_rate": 1.6198613333333336e-05,
      "loss": 0.0056,
      "step": 17820
    },
    {
      "epoch": 0.57056,
      "grad_norm": 0.009645598009228706,
      "learning_rate": 1.6196480000000002e-05,
      "loss": 0.0082,
      "step": 17830
    },
    {
      "epoch": 0.57088,
      "grad_norm": 0.014012332074344158,
      "learning_rate": 1.6194346666666667e-05,
      "loss": 0.0005,
      "step": 17840
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.007015069015324116,
      "learning_rate": 1.6192213333333336e-05,
      "loss": 0.0006,
      "step": 17850
    },
    {
      "epoch": 0.57152,
      "grad_norm": 0.006870867218822241,
      "learning_rate": 1.619008e-05,
      "loss": 0.0006,
      "step": 17860
    },
    {
      "epoch": 0.57184,
      "grad_norm": 0.008737211115658283,
      "learning_rate": 1.6187946666666667e-05,
      "loss": 0.0566,
      "step": 17870
    },
    {
      "epoch": 0.57216,
      "grad_norm": 0.005723545327782631,
      "learning_rate": 1.6185813333333336e-05,
      "loss": 0.0008,
      "step": 17880
    },
    {
      "epoch": 0.57248,
      "grad_norm": 0.00872796680778265,
      "learning_rate": 1.618368e-05,
      "loss": 0.0301,
      "step": 17890
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.01784851774573326,
      "learning_rate": 1.6181546666666666e-05,
      "loss": 0.0005,
      "step": 17900
    },
    {
      "epoch": 0.57312,
      "grad_norm": 0.0029273969121277332,
      "learning_rate": 1.6179413333333335e-05,
      "loss": 0.0007,
      "step": 17910
    },
    {
      "epoch": 0.57344,
      "grad_norm": 0.0129149304702878,
      "learning_rate": 1.617728e-05,
      "loss": 0.0032,
      "step": 17920
    },
    {
      "epoch": 0.57376,
      "grad_norm": 0.007931580767035484,
      "learning_rate": 1.617514666666667e-05,
      "loss": 0.0006,
      "step": 17930
    },
    {
      "epoch": 0.57408,
      "grad_norm": 0.007445762865245342,
      "learning_rate": 1.6173013333333335e-05,
      "loss": 0.0309,
      "step": 17940
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.01032265741378069,
      "learning_rate": 1.6170880000000003e-05,
      "loss": 0.0004,
      "step": 17950
    },
    {
      "epoch": 0.57472,
      "grad_norm": 0.007033883593976498,
      "learning_rate": 1.616874666666667e-05,
      "loss": 0.0004,
      "step": 17960
    },
    {
      "epoch": 0.57504,
      "grad_norm": 0.005912238731980324,
      "learning_rate": 1.6166613333333334e-05,
      "loss": 0.0012,
      "step": 17970
    },
    {
      "epoch": 0.57536,
      "grad_norm": 0.027225282043218613,
      "learning_rate": 1.6164480000000003e-05,
      "loss": 0.0008,
      "step": 17980
    },
    {
      "epoch": 0.57568,
      "grad_norm": 0.008889248594641685,
      "learning_rate": 1.6162346666666668e-05,
      "loss": 0.0197,
      "step": 17990
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.02388838678598404,
      "learning_rate": 1.6160213333333334e-05,
      "loss": 0.0004,
      "step": 18000
    },
    {
      "epoch": 0.57632,
      "grad_norm": 0.04145796597003937,
      "learning_rate": 1.615808e-05,
      "loss": 0.0005,
      "step": 18010
    },
    {
      "epoch": 0.57664,
      "grad_norm": 0.006866331212222576,
      "learning_rate": 1.6155946666666668e-05,
      "loss": 0.0003,
      "step": 18020
    },
    {
      "epoch": 0.57696,
      "grad_norm": 0.04962507635354996,
      "learning_rate": 1.6153813333333333e-05,
      "loss": 0.0208,
      "step": 18030
    },
    {
      "epoch": 0.57728,
      "grad_norm": 0.005740475840866566,
      "learning_rate": 1.6151680000000002e-05,
      "loss": 0.0004,
      "step": 18040
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.007069256156682968,
      "learning_rate": 1.6149546666666667e-05,
      "loss": 0.0004,
      "step": 18050
    },
    {
      "epoch": 0.57792,
      "grad_norm": 0.00790198054164648,
      "learning_rate": 1.6147413333333336e-05,
      "loss": 0.0006,
      "step": 18060
    },
    {
      "epoch": 0.57824,
      "grad_norm": 0.005528813693672419,
      "learning_rate": 1.614528e-05,
      "loss": 0.0003,
      "step": 18070
    },
    {
      "epoch": 0.57856,
      "grad_norm": 0.006962733343243599,
      "learning_rate": 1.6143146666666667e-05,
      "loss": 0.0004,
      "step": 18080
    },
    {
      "epoch": 0.57888,
      "grad_norm": 0.00890094693750143,
      "learning_rate": 1.6141013333333336e-05,
      "loss": 0.0006,
      "step": 18090
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.006172149442136288,
      "learning_rate": 1.613888e-05,
      "loss": 0.0003,
      "step": 18100
    },
    {
      "epoch": 0.57952,
      "grad_norm": 0.004863081034272909,
      "learning_rate": 1.6136746666666666e-05,
      "loss": 0.0003,
      "step": 18110
    },
    {
      "epoch": 0.57984,
      "grad_norm": 0.10115894675254822,
      "learning_rate": 1.6134613333333335e-05,
      "loss": 0.0529,
      "step": 18120
    },
    {
      "epoch": 0.58016,
      "grad_norm": 0.010898931883275509,
      "learning_rate": 1.613248e-05,
      "loss": 0.0006,
      "step": 18130
    },
    {
      "epoch": 0.58048,
      "grad_norm": 0.008997765369713306,
      "learning_rate": 1.6130346666666666e-05,
      "loss": 0.0004,
      "step": 18140
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.010005610063672066,
      "learning_rate": 1.6128213333333335e-05,
      "loss": 0.0006,
      "step": 18150
    },
    {
      "epoch": 0.58112,
      "grad_norm": 0.005536671727895737,
      "learning_rate": 1.6126080000000003e-05,
      "loss": 0.0004,
      "step": 18160
    },
    {
      "epoch": 0.58144,
      "grad_norm": 0.6570156216621399,
      "learning_rate": 1.612394666666667e-05,
      "loss": 0.0016,
      "step": 18170
    },
    {
      "epoch": 0.58176,
      "grad_norm": 0.009800763800740242,
      "learning_rate": 1.6121813333333334e-05,
      "loss": 0.0003,
      "step": 18180
    },
    {
      "epoch": 0.58208,
      "grad_norm": 0.010150873102247715,
      "learning_rate": 1.6119680000000003e-05,
      "loss": 0.0035,
      "step": 18190
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.005635949317365885,
      "learning_rate": 1.6117546666666668e-05,
      "loss": 0.0003,
      "step": 18200
    },
    {
      "epoch": 0.58272,
      "grad_norm": 0.009979951195418835,
      "learning_rate": 1.6115413333333334e-05,
      "loss": 0.0507,
      "step": 18210
    },
    {
      "epoch": 0.58304,
      "grad_norm": 0.008815892040729523,
      "learning_rate": 1.6113280000000002e-05,
      "loss": 0.0335,
      "step": 18220
    },
    {
      "epoch": 0.58336,
      "grad_norm": 0.012523481622338295,
      "learning_rate": 1.6111146666666668e-05,
      "loss": 0.0005,
      "step": 18230
    },
    {
      "epoch": 0.58368,
      "grad_norm": 0.006829879246652126,
      "learning_rate": 1.6109013333333333e-05,
      "loss": 0.0003,
      "step": 18240
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.007962164469063282,
      "learning_rate": 1.6106880000000002e-05,
      "loss": 0.004,
      "step": 18250
    },
    {
      "epoch": 0.58432,
      "grad_norm": 0.029414596036076546,
      "learning_rate": 1.6104746666666667e-05,
      "loss": 0.0005,
      "step": 18260
    },
    {
      "epoch": 0.58464,
      "grad_norm": 0.009702688083052635,
      "learning_rate": 1.6102613333333336e-05,
      "loss": 0.0006,
      "step": 18270
    },
    {
      "epoch": 0.58496,
      "grad_norm": 1.7056615352630615,
      "learning_rate": 1.610048e-05,
      "loss": 0.0546,
      "step": 18280
    },
    {
      "epoch": 0.58528,
      "grad_norm": 0.00911217462271452,
      "learning_rate": 1.609834666666667e-05,
      "loss": 0.0379,
      "step": 18290
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.03677118197083473,
      "learning_rate": 1.6096213333333336e-05,
      "loss": 0.0008,
      "step": 18300
    },
    {
      "epoch": 0.58592,
      "grad_norm": 0.005281697027385235,
      "learning_rate": 1.609408e-05,
      "loss": 0.0008,
      "step": 18310
    },
    {
      "epoch": 0.58624,
      "grad_norm": 0.01769012212753296,
      "learning_rate": 1.609194666666667e-05,
      "loss": 0.0005,
      "step": 18320
    },
    {
      "epoch": 0.58656,
      "grad_norm": 1.7061008214950562,
      "learning_rate": 1.6089813333333335e-05,
      "loss": 0.0211,
      "step": 18330
    },
    {
      "epoch": 0.58688,
      "grad_norm": 0.00692414678633213,
      "learning_rate": 1.608768e-05,
      "loss": 0.0005,
      "step": 18340
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.0866098627448082,
      "learning_rate": 1.6085546666666666e-05,
      "loss": 0.0006,
      "step": 18350
    },
    {
      "epoch": 0.58752,
      "grad_norm": 0.005139687564224005,
      "learning_rate": 1.6083413333333335e-05,
      "loss": 0.0004,
      "step": 18360
    },
    {
      "epoch": 0.58784,
      "grad_norm": 0.005848530679941177,
      "learning_rate": 1.608128e-05,
      "loss": 0.0248,
      "step": 18370
    },
    {
      "epoch": 0.58816,
      "grad_norm": 0.030651504173874855,
      "learning_rate": 1.607914666666667e-05,
      "loss": 0.0005,
      "step": 18380
    },
    {
      "epoch": 0.58848,
      "grad_norm": 0.01591617986559868,
      "learning_rate": 1.6077013333333334e-05,
      "loss": 0.0005,
      "step": 18390
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.008082276210188866,
      "learning_rate": 1.6074880000000003e-05,
      "loss": 0.001,
      "step": 18400
    },
    {
      "epoch": 0.58912,
      "grad_norm": 0.007589387241750956,
      "learning_rate": 1.6072746666666668e-05,
      "loss": 0.0005,
      "step": 18410
    },
    {
      "epoch": 0.58944,
      "grad_norm": 0.0065714409574866295,
      "learning_rate": 1.6070613333333334e-05,
      "loss": 0.0059,
      "step": 18420
    },
    {
      "epoch": 0.58976,
      "grad_norm": 0.009183471091091633,
      "learning_rate": 1.6068480000000002e-05,
      "loss": 0.0021,
      "step": 18430
    },
    {
      "epoch": 0.59008,
      "grad_norm": 0.0185050368309021,
      "learning_rate": 1.6066346666666668e-05,
      "loss": 0.0005,
      "step": 18440
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.011019529774785042,
      "learning_rate": 1.6064213333333333e-05,
      "loss": 0.0008,
      "step": 18450
    },
    {
      "epoch": 0.59072,
      "grad_norm": 0.005762944463640451,
      "learning_rate": 1.6062080000000002e-05,
      "loss": 0.0004,
      "step": 18460
    },
    {
      "epoch": 0.59104,
      "grad_norm": 0.006066249217838049,
      "learning_rate": 1.6059946666666667e-05,
      "loss": 0.064,
      "step": 18470
    },
    {
      "epoch": 0.59136,
      "grad_norm": 0.007433176040649414,
      "learning_rate": 1.6057813333333333e-05,
      "loss": 0.0006,
      "step": 18480
    },
    {
      "epoch": 0.59168,
      "grad_norm": 0.0064173173159360886,
      "learning_rate": 1.605568e-05,
      "loss": 0.0068,
      "step": 18490
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.04674149304628372,
      "learning_rate": 1.6053546666666667e-05,
      "loss": 0.0098,
      "step": 18500
    },
    {
      "epoch": 0.59232,
      "grad_norm": 0.004816717002540827,
      "learning_rate": 1.6051413333333336e-05,
      "loss": 0.0009,
      "step": 18510
    },
    {
      "epoch": 0.59264,
      "grad_norm": 0.0041008261032402515,
      "learning_rate": 1.604928e-05,
      "loss": 0.0003,
      "step": 18520
    },
    {
      "epoch": 0.59296,
      "grad_norm": 0.0032351939007639885,
      "learning_rate": 1.604714666666667e-05,
      "loss": 0.0031,
      "step": 18530
    },
    {
      "epoch": 0.59328,
      "grad_norm": 0.008281230926513672,
      "learning_rate": 1.6045013333333335e-05,
      "loss": 0.0066,
      "step": 18540
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.024779457598924637,
      "learning_rate": 1.604288e-05,
      "loss": 0.0252,
      "step": 18550
    },
    {
      "epoch": 0.59392,
      "grad_norm": 0.004267206881195307,
      "learning_rate": 1.604074666666667e-05,
      "loss": 0.0024,
      "step": 18560
    },
    {
      "epoch": 0.59424,
      "grad_norm": 0.006742196623235941,
      "learning_rate": 1.6038613333333335e-05,
      "loss": 0.0004,
      "step": 18570
    },
    {
      "epoch": 0.59456,
      "grad_norm": 0.007538653910160065,
      "learning_rate": 1.603648e-05,
      "loss": 0.0338,
      "step": 18580
    },
    {
      "epoch": 0.59488,
      "grad_norm": 0.0060672578401863575,
      "learning_rate": 1.6034346666666665e-05,
      "loss": 0.0406,
      "step": 18590
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.010197840631008148,
      "learning_rate": 1.6032213333333334e-05,
      "loss": 0.0009,
      "step": 18600
    },
    {
      "epoch": 0.59552,
      "grad_norm": 0.019144820049405098,
      "learning_rate": 1.603008e-05,
      "loss": 0.0009,
      "step": 18610
    },
    {
      "epoch": 0.59584,
      "grad_norm": 0.009175346232950687,
      "learning_rate": 1.6027946666666668e-05,
      "loss": 0.0005,
      "step": 18620
    },
    {
      "epoch": 0.59616,
      "grad_norm": 0.013149399310350418,
      "learning_rate": 1.6025813333333337e-05,
      "loss": 0.001,
      "step": 18630
    },
    {
      "epoch": 0.59648,
      "grad_norm": 0.06719144433736801,
      "learning_rate": 1.6023680000000002e-05,
      "loss": 0.0008,
      "step": 18640
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.015859995037317276,
      "learning_rate": 1.6021546666666668e-05,
      "loss": 0.0009,
      "step": 18650
    },
    {
      "epoch": 0.59712,
      "grad_norm": 0.007520809303969145,
      "learning_rate": 1.6019413333333337e-05,
      "loss": 0.0003,
      "step": 18660
    },
    {
      "epoch": 0.59744,
      "grad_norm": 0.06773251295089722,
      "learning_rate": 1.6017280000000002e-05,
      "loss": 0.0011,
      "step": 18670
    },
    {
      "epoch": 0.59776,
      "grad_norm": 0.009148182347416878,
      "learning_rate": 1.6015146666666667e-05,
      "loss": 0.0015,
      "step": 18680
    },
    {
      "epoch": 0.59808,
      "grad_norm": 0.01253350730985403,
      "learning_rate": 1.6013013333333333e-05,
      "loss": 0.0544,
      "step": 18690
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.008781357668340206,
      "learning_rate": 1.601088e-05,
      "loss": 0.0004,
      "step": 18700
    },
    {
      "epoch": 0.59872,
      "grad_norm": 0.015915175899863243,
      "learning_rate": 1.6008746666666667e-05,
      "loss": 0.064,
      "step": 18710
    },
    {
      "epoch": 0.59904,
      "grad_norm": 0.008044206537306309,
      "learning_rate": 1.6006613333333336e-05,
      "loss": 0.0004,
      "step": 18720
    },
    {
      "epoch": 0.59936,
      "grad_norm": 0.03624079376459122,
      "learning_rate": 1.600448e-05,
      "loss": 0.0012,
      "step": 18730
    },
    {
      "epoch": 0.59968,
      "grad_norm": 0.008894356898963451,
      "learning_rate": 1.600234666666667e-05,
      "loss": 0.0534,
      "step": 18740
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.02296494133770466,
      "learning_rate": 1.6000213333333335e-05,
      "loss": 0.0007,
      "step": 18750
    },
    {
      "epoch": 0.60032,
      "grad_norm": 0.04124687239527702,
      "learning_rate": 1.599808e-05,
      "loss": 0.0005,
      "step": 18760
    },
    {
      "epoch": 0.60064,
      "grad_norm": 0.015925439074635506,
      "learning_rate": 1.599594666666667e-05,
      "loss": 0.0004,
      "step": 18770
    },
    {
      "epoch": 0.60096,
      "grad_norm": 0.014562596566975117,
      "learning_rate": 1.5993813333333335e-05,
      "loss": 0.0205,
      "step": 18780
    },
    {
      "epoch": 0.60128,
      "grad_norm": 0.016241008415818214,
      "learning_rate": 1.599168e-05,
      "loss": 0.0004,
      "step": 18790
    },
    {
      "epoch": 0.6016,
      "grad_norm": 4.755019664764404,
      "learning_rate": 1.598954666666667e-05,
      "loss": 0.0169,
      "step": 18800
    },
    {
      "epoch": 0.60192,
      "grad_norm": 0.05117999017238617,
      "learning_rate": 1.5987413333333334e-05,
      "loss": 0.0004,
      "step": 18810
    },
    {
      "epoch": 0.60224,
      "grad_norm": 0.05985076352953911,
      "learning_rate": 1.598528e-05,
      "loss": 0.0005,
      "step": 18820
    },
    {
      "epoch": 0.60256,
      "grad_norm": 5.8601813316345215,
      "learning_rate": 1.5983146666666668e-05,
      "loss": 0.016,
      "step": 18830
    },
    {
      "epoch": 0.60288,
      "grad_norm": 0.008502987213432789,
      "learning_rate": 1.5981013333333334e-05,
      "loss": 0.0004,
      "step": 18840
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.007010730914771557,
      "learning_rate": 1.5978880000000002e-05,
      "loss": 0.0238,
      "step": 18850
    },
    {
      "epoch": 0.60352,
      "grad_norm": 0.008032907731831074,
      "learning_rate": 1.5976746666666668e-05,
      "loss": 0.0003,
      "step": 18860
    },
    {
      "epoch": 0.60384,
      "grad_norm": 2.1164960861206055,
      "learning_rate": 1.5974613333333337e-05,
      "loss": 0.0056,
      "step": 18870
    },
    {
      "epoch": 0.60416,
      "grad_norm": 0.006241224706172943,
      "learning_rate": 1.5972480000000002e-05,
      "loss": 0.0004,
      "step": 18880
    },
    {
      "epoch": 0.60448,
      "grad_norm": 0.0038178334943950176,
      "learning_rate": 1.5970346666666667e-05,
      "loss": 0.0004,
      "step": 18890
    },
    {
      "epoch": 0.6048,
      "grad_norm": 3.5617809295654297,
      "learning_rate": 1.5968213333333336e-05,
      "loss": 0.0292,
      "step": 18900
    },
    {
      "epoch": 0.60512,
      "grad_norm": 0.01138173695653677,
      "learning_rate": 1.596608e-05,
      "loss": 0.0592,
      "step": 18910
    },
    {
      "epoch": 0.60544,
      "grad_norm": 0.004556574858725071,
      "learning_rate": 1.5963946666666667e-05,
      "loss": 0.0009,
      "step": 18920
    },
    {
      "epoch": 0.60576,
      "grad_norm": 0.05339987203478813,
      "learning_rate": 1.5961813333333332e-05,
      "loss": 0.0004,
      "step": 18930
    },
    {
      "epoch": 0.60608,
      "grad_norm": 0.01852697692811489,
      "learning_rate": 1.595968e-05,
      "loss": 0.0042,
      "step": 18940
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.006583413574844599,
      "learning_rate": 1.5957546666666666e-05,
      "loss": 0.057,
      "step": 18950
    },
    {
      "epoch": 0.60672,
      "grad_norm": 0.004904472269117832,
      "learning_rate": 1.5955413333333335e-05,
      "loss": 0.0004,
      "step": 18960
    },
    {
      "epoch": 0.60704,
      "grad_norm": 0.03507038205862045,
      "learning_rate": 1.595328e-05,
      "loss": 0.0226,
      "step": 18970
    },
    {
      "epoch": 0.60736,
      "grad_norm": 0.006267218850553036,
      "learning_rate": 1.595114666666667e-05,
      "loss": 0.0004,
      "step": 18980
    },
    {
      "epoch": 0.60768,
      "grad_norm": 0.0065050870180130005,
      "learning_rate": 1.5949013333333335e-05,
      "loss": 0.0012,
      "step": 18990
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.005078020039945841,
      "learning_rate": 1.5946880000000003e-05,
      "loss": 0.0004,
      "step": 19000
    },
    {
      "epoch": 0.60832,
      "grad_norm": 0.04511337727308273,
      "learning_rate": 1.594474666666667e-05,
      "loss": 0.0006,
      "step": 19010
    },
    {
      "epoch": 0.60864,
      "grad_norm": 0.007576251868158579,
      "learning_rate": 1.5942613333333334e-05,
      "loss": 0.0003,
      "step": 19020
    },
    {
      "epoch": 0.60896,
      "grad_norm": 2.0283756256103516,
      "learning_rate": 1.594048e-05,
      "loss": 0.0025,
      "step": 19030
    },
    {
      "epoch": 0.60928,
      "grad_norm": 0.01865973509848118,
      "learning_rate": 1.593834666666667e-05,
      "loss": 0.0004,
      "step": 19040
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.01624344475567341,
      "learning_rate": 1.5936213333333334e-05,
      "loss": 0.0003,
      "step": 19050
    },
    {
      "epoch": 0.60992,
      "grad_norm": 0.02659733034670353,
      "learning_rate": 1.593408e-05,
      "loss": 0.0008,
      "step": 19060
    },
    {
      "epoch": 0.61024,
      "grad_norm": 0.0044623808935284615,
      "learning_rate": 1.5931946666666668e-05,
      "loss": 0.0004,
      "step": 19070
    },
    {
      "epoch": 0.61056,
      "grad_norm": 0.004182884003967047,
      "learning_rate": 1.5929813333333337e-05,
      "loss": 0.0008,
      "step": 19080
    },
    {
      "epoch": 0.61088,
      "grad_norm": 0.013055682182312012,
      "learning_rate": 1.5927680000000002e-05,
      "loss": 0.0003,
      "step": 19090
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.007648404687643051,
      "learning_rate": 1.5925546666666667e-05,
      "loss": 0.0367,
      "step": 19100
    },
    {
      "epoch": 0.61152,
      "grad_norm": 0.0038278226274996996,
      "learning_rate": 1.5923413333333336e-05,
      "loss": 0.0005,
      "step": 19110
    },
    {
      "epoch": 0.61184,
      "grad_norm": 0.00462587084621191,
      "learning_rate": 1.592128e-05,
      "loss": 0.0534,
      "step": 19120
    },
    {
      "epoch": 0.61216,
      "grad_norm": 0.004139276687055826,
      "learning_rate": 1.5919146666666667e-05,
      "loss": 0.0121,
      "step": 19130
    },
    {
      "epoch": 0.61248,
      "grad_norm": 0.009951833635568619,
      "learning_rate": 1.5917013333333336e-05,
      "loss": 0.0319,
      "step": 19140
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.005257409065961838,
      "learning_rate": 1.591488e-05,
      "loss": 0.0003,
      "step": 19150
    },
    {
      "epoch": 0.61312,
      "grad_norm": 0.011839745566248894,
      "learning_rate": 1.5912746666666666e-05,
      "loss": 0.0003,
      "step": 19160
    },
    {
      "epoch": 0.61344,
      "grad_norm": 0.21200869977474213,
      "learning_rate": 1.5910613333333335e-05,
      "loss": 0.0009,
      "step": 19170
    },
    {
      "epoch": 0.61376,
      "grad_norm": 0.010425180196762085,
      "learning_rate": 1.590848e-05,
      "loss": 0.0701,
      "step": 19180
    },
    {
      "epoch": 0.61408,
      "grad_norm": 0.008880143985152245,
      "learning_rate": 1.590634666666667e-05,
      "loss": 0.0007,
      "step": 19190
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.006093055475503206,
      "learning_rate": 1.5904213333333335e-05,
      "loss": 0.0212,
      "step": 19200
    },
    {
      "epoch": 0.61472,
      "grad_norm": 0.00740625848993659,
      "learning_rate": 1.5902080000000003e-05,
      "loss": 0.001,
      "step": 19210
    },
    {
      "epoch": 0.61504,
      "grad_norm": 0.01615491509437561,
      "learning_rate": 1.589994666666667e-05,
      "loss": 0.0004,
      "step": 19220
    },
    {
      "epoch": 0.61536,
      "grad_norm": 0.00905455183237791,
      "learning_rate": 1.5897813333333334e-05,
      "loss": 0.0109,
      "step": 19230
    },
    {
      "epoch": 0.61568,
      "grad_norm": 0.01681612804532051,
      "learning_rate": 1.5895680000000003e-05,
      "loss": 0.0023,
      "step": 19240
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.009654941968619823,
      "learning_rate": 1.589354666666667e-05,
      "loss": 0.0129,
      "step": 19250
    },
    {
      "epoch": 0.61632,
      "grad_norm": 0.010534659959375858,
      "learning_rate": 1.5891413333333334e-05,
      "loss": 0.0003,
      "step": 19260
    },
    {
      "epoch": 0.61664,
      "grad_norm": 0.007346743252128363,
      "learning_rate": 1.588928e-05,
      "loss": 0.0003,
      "step": 19270
    },
    {
      "epoch": 0.61696,
      "grad_norm": 0.005553992465138435,
      "learning_rate": 1.5887146666666668e-05,
      "loss": 0.0004,
      "step": 19280
    },
    {
      "epoch": 0.61728,
      "grad_norm": 0.13406074047088623,
      "learning_rate": 1.5885013333333333e-05,
      "loss": 0.0006,
      "step": 19290
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.0081886425614357,
      "learning_rate": 1.5882880000000002e-05,
      "loss": 0.0005,
      "step": 19300
    },
    {
      "epoch": 0.61792,
      "grad_norm": 0.004646464716643095,
      "learning_rate": 1.5880746666666667e-05,
      "loss": 0.0019,
      "step": 19310
    },
    {
      "epoch": 0.61824,
      "grad_norm": 0.004390597343444824,
      "learning_rate": 1.5878613333333336e-05,
      "loss": 0.0005,
      "step": 19320
    },
    {
      "epoch": 0.61856,
      "grad_norm": 0.005479812156409025,
      "learning_rate": 1.587648e-05,
      "loss": 0.0414,
      "step": 19330
    },
    {
      "epoch": 0.61888,
      "grad_norm": 0.015538979321718216,
      "learning_rate": 1.587434666666667e-05,
      "loss": 0.0006,
      "step": 19340
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.005007006227970123,
      "learning_rate": 1.5872213333333336e-05,
      "loss": 0.0004,
      "step": 19350
    },
    {
      "epoch": 0.61952,
      "grad_norm": 0.00524883484467864,
      "learning_rate": 1.587008e-05,
      "loss": 0.0161,
      "step": 19360
    },
    {
      "epoch": 0.61984,
      "grad_norm": 0.37106627225875854,
      "learning_rate": 1.5867946666666666e-05,
      "loss": 0.0023,
      "step": 19370
    },
    {
      "epoch": 0.62016,
      "grad_norm": 0.013633234426379204,
      "learning_rate": 1.5865813333333335e-05,
      "loss": 0.0354,
      "step": 19380
    },
    {
      "epoch": 0.62048,
      "grad_norm": 0.004976000636816025,
      "learning_rate": 1.586368e-05,
      "loss": 0.0005,
      "step": 19390
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.012562704272568226,
      "learning_rate": 1.5861546666666666e-05,
      "loss": 0.0006,
      "step": 19400
    },
    {
      "epoch": 0.62112,
      "grad_norm": 0.008487639017403126,
      "learning_rate": 1.5859413333333335e-05,
      "loss": 0.0005,
      "step": 19410
    },
    {
      "epoch": 0.62144,
      "grad_norm": 0.019269756972789764,
      "learning_rate": 1.585728e-05,
      "loss": 0.0006,
      "step": 19420
    },
    {
      "epoch": 0.62176,
      "grad_norm": 0.010155814699828625,
      "learning_rate": 1.585514666666667e-05,
      "loss": 0.0005,
      "step": 19430
    },
    {
      "epoch": 0.62208,
      "grad_norm": 0.010310519486665726,
      "learning_rate": 1.5853013333333334e-05,
      "loss": 0.0006,
      "step": 19440
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.014579486101865768,
      "learning_rate": 1.5850880000000003e-05,
      "loss": 0.0004,
      "step": 19450
    },
    {
      "epoch": 0.62272,
      "grad_norm": 0.0062414477579295635,
      "learning_rate": 1.584874666666667e-05,
      "loss": 0.0004,
      "step": 19460
    },
    {
      "epoch": 0.62304,
      "grad_norm": 0.0060941847041249275,
      "learning_rate": 1.5846613333333334e-05,
      "loss": 0.0201,
      "step": 19470
    },
    {
      "epoch": 0.62336,
      "grad_norm": 0.007862451486289501,
      "learning_rate": 1.5844480000000002e-05,
      "loss": 0.0752,
      "step": 19480
    },
    {
      "epoch": 0.62368,
      "grad_norm": 0.006270629819482565,
      "learning_rate": 1.5842346666666668e-05,
      "loss": 0.0389,
      "step": 19490
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.003938135225325823,
      "learning_rate": 1.5840213333333333e-05,
      "loss": 0.0209,
      "step": 19500
    },
    {
      "epoch": 0.62432,
      "grad_norm": 0.007729065604507923,
      "learning_rate": 1.5838080000000002e-05,
      "loss": 0.0008,
      "step": 19510
    },
    {
      "epoch": 0.62464,
      "grad_norm": 0.25307705998420715,
      "learning_rate": 1.5835946666666667e-05,
      "loss": 0.0008,
      "step": 19520
    },
    {
      "epoch": 0.62496,
      "grad_norm": 0.011925549246370792,
      "learning_rate": 1.5833813333333333e-05,
      "loss": 0.0004,
      "step": 19530
    },
    {
      "epoch": 0.62528,
      "grad_norm": 0.10505592823028564,
      "learning_rate": 1.583168e-05,
      "loss": 0.0037,
      "step": 19540
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.0049987006932497025,
      "learning_rate": 1.582954666666667e-05,
      "loss": 0.0004,
      "step": 19550
    },
    {
      "epoch": 0.62592,
      "grad_norm": 0.009198617190122604,
      "learning_rate": 1.5827413333333336e-05,
      "loss": 0.0359,
      "step": 19560
    },
    {
      "epoch": 0.62624,
      "grad_norm": 0.01116827130317688,
      "learning_rate": 1.582528e-05,
      "loss": 0.0594,
      "step": 19570
    },
    {
      "epoch": 0.62656,
      "grad_norm": 0.009103797376155853,
      "learning_rate": 1.582314666666667e-05,
      "loss": 0.051,
      "step": 19580
    },
    {
      "epoch": 0.62688,
      "grad_norm": 1.852043628692627,
      "learning_rate": 1.5821013333333335e-05,
      "loss": 0.0032,
      "step": 19590
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.4995514154434204,
      "learning_rate": 1.581888e-05,
      "loss": 0.0036,
      "step": 19600
    },
    {
      "epoch": 0.62752,
      "grad_norm": 0.008361341431736946,
      "learning_rate": 1.5816746666666666e-05,
      "loss": 0.0008,
      "step": 19610
    },
    {
      "epoch": 0.62784,
      "grad_norm": 0.007644224911928177,
      "learning_rate": 1.5814613333333335e-05,
      "loss": 0.0006,
      "step": 19620
    },
    {
      "epoch": 0.62816,
      "grad_norm": 0.014050119556486607,
      "learning_rate": 1.581248e-05,
      "loss": 0.0007,
      "step": 19630
    },
    {
      "epoch": 0.62848,
      "grad_norm": 0.008553278632462025,
      "learning_rate": 1.581034666666667e-05,
      "loss": 0.0166,
      "step": 19640
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.03307892382144928,
      "learning_rate": 1.5808213333333334e-05,
      "loss": 0.0005,
      "step": 19650
    },
    {
      "epoch": 0.62912,
      "grad_norm": 0.01733812317252159,
      "learning_rate": 1.5806080000000003e-05,
      "loss": 0.0011,
      "step": 19660
    },
    {
      "epoch": 0.62944,
      "grad_norm": 0.009120369330048561,
      "learning_rate": 1.580394666666667e-05,
      "loss": 0.0003,
      "step": 19670
    },
    {
      "epoch": 0.62976,
      "grad_norm": 0.007438657805323601,
      "learning_rate": 1.5801813333333337e-05,
      "loss": 0.0007,
      "step": 19680
    },
    {
      "epoch": 0.63008,
      "grad_norm": 0.07433135062456131,
      "learning_rate": 1.5799680000000003e-05,
      "loss": 0.0006,
      "step": 19690
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.014307145029306412,
      "learning_rate": 1.5797546666666668e-05,
      "loss": 0.0041,
      "step": 19700
    },
    {
      "epoch": 0.63072,
      "grad_norm": 0.005450793541967869,
      "learning_rate": 1.5795413333333333e-05,
      "loss": 0.0004,
      "step": 19710
    },
    {
      "epoch": 0.63104,
      "grad_norm": 0.05302194133400917,
      "learning_rate": 1.5793280000000002e-05,
      "loss": 0.0065,
      "step": 19720
    },
    {
      "epoch": 0.63136,
      "grad_norm": 0.011728024110198021,
      "learning_rate": 1.5791146666666667e-05,
      "loss": 0.0004,
      "step": 19730
    },
    {
      "epoch": 0.63168,
      "grad_norm": 0.003929913975298405,
      "learning_rate": 1.5789013333333333e-05,
      "loss": 0.0003,
      "step": 19740
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.0077598015777766705,
      "learning_rate": 1.578688e-05,
      "loss": 0.0437,
      "step": 19750
    },
    {
      "epoch": 0.63232,
      "grad_norm": 2.761335849761963,
      "learning_rate": 1.5784746666666667e-05,
      "loss": 0.0598,
      "step": 19760
    },
    {
      "epoch": 0.63264,
      "grad_norm": 0.009572653099894524,
      "learning_rate": 1.5782613333333336e-05,
      "loss": 0.0363,
      "step": 19770
    },
    {
      "epoch": 0.63296,
      "grad_norm": 0.33602622151374817,
      "learning_rate": 1.578048e-05,
      "loss": 0.0011,
      "step": 19780
    },
    {
      "epoch": 0.63328,
      "grad_norm": 3.8836822509765625,
      "learning_rate": 1.577834666666667e-05,
      "loss": 0.0228,
      "step": 19790
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.009802867658436298,
      "learning_rate": 1.5776213333333335e-05,
      "loss": 0.001,
      "step": 19800
    },
    {
      "epoch": 0.63392,
      "grad_norm": 0.014940383844077587,
      "learning_rate": 1.577408e-05,
      "loss": 0.0347,
      "step": 19810
    },
    {
      "epoch": 0.63424,
      "grad_norm": 0.015333499759435654,
      "learning_rate": 1.577194666666667e-05,
      "loss": 0.0321,
      "step": 19820
    },
    {
      "epoch": 0.63456,
      "grad_norm": 0.009325142949819565,
      "learning_rate": 1.5769813333333335e-05,
      "loss": 0.0598,
      "step": 19830
    },
    {
      "epoch": 0.63488,
      "grad_norm": 0.0063213868997991085,
      "learning_rate": 1.576768e-05,
      "loss": 0.0007,
      "step": 19840
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.019086947664618492,
      "learning_rate": 1.576554666666667e-05,
      "loss": 0.0505,
      "step": 19850
    },
    {
      "epoch": 0.63552,
      "grad_norm": 0.011027953587472439,
      "learning_rate": 1.5763413333333334e-05,
      "loss": 0.0015,
      "step": 19860
    },
    {
      "epoch": 0.63584,
      "grad_norm": 0.006147363223135471,
      "learning_rate": 1.576128e-05,
      "loss": 0.0006,
      "step": 19870
    },
    {
      "epoch": 0.63616,
      "grad_norm": 0.015838857740163803,
      "learning_rate": 1.575914666666667e-05,
      "loss": 0.0007,
      "step": 19880
    },
    {
      "epoch": 0.63648,
      "grad_norm": 0.010699336417019367,
      "learning_rate": 1.5757013333333334e-05,
      "loss": 0.0007,
      "step": 19890
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.0711754783987999,
      "learning_rate": 1.5754880000000003e-05,
      "loss": 0.0238,
      "step": 19900
    },
    {
      "epoch": 0.63712,
      "grad_norm": 0.02141333557665348,
      "learning_rate": 1.5752746666666668e-05,
      "loss": 0.0018,
      "step": 19910
    },
    {
      "epoch": 0.63744,
      "grad_norm": 0.02005387470126152,
      "learning_rate": 1.5750613333333337e-05,
      "loss": 0.0005,
      "step": 19920
    },
    {
      "epoch": 0.63776,
      "grad_norm": 0.015684014186263084,
      "learning_rate": 1.5748480000000002e-05,
      "loss": 0.0007,
      "step": 19930
    },
    {
      "epoch": 0.63808,
      "grad_norm": 0.013132380321621895,
      "learning_rate": 1.5746346666666667e-05,
      "loss": 0.0018,
      "step": 19940
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.030742764472961426,
      "learning_rate": 1.5744213333333333e-05,
      "loss": 0.0005,
      "step": 19950
    },
    {
      "epoch": 0.63872,
      "grad_norm": 0.0082556689158082,
      "learning_rate": 1.574208e-05,
      "loss": 0.002,
      "step": 19960
    },
    {
      "epoch": 0.63904,
      "grad_norm": 0.00921507179737091,
      "learning_rate": 1.5739946666666667e-05,
      "loss": 0.0008,
      "step": 19970
    },
    {
      "epoch": 0.63936,
      "grad_norm": 0.006067337468266487,
      "learning_rate": 1.5737813333333332e-05,
      "loss": 0.0005,
      "step": 19980
    },
    {
      "epoch": 0.63968,
      "grad_norm": 0.007960360497236252,
      "learning_rate": 1.573568e-05,
      "loss": 0.0011,
      "step": 19990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.010110598057508469,
      "learning_rate": 1.5733546666666666e-05,
      "loss": 0.0004,
      "step": 20000
    },
    {
      "epoch": 0.64032,
      "grad_norm": 0.06811469793319702,
      "learning_rate": 1.5731413333333335e-05,
      "loss": 0.044,
      "step": 20010
    },
    {
      "epoch": 0.64064,
      "grad_norm": 4.270623207092285,
      "learning_rate": 1.5729280000000004e-05,
      "loss": 0.0489,
      "step": 20020
    },
    {
      "epoch": 0.64096,
      "grad_norm": 0.04232225939631462,
      "learning_rate": 1.572714666666667e-05,
      "loss": 0.0048,
      "step": 20030
    },
    {
      "epoch": 0.64128,
      "grad_norm": 0.012299972586333752,
      "learning_rate": 1.5725013333333335e-05,
      "loss": 0.001,
      "step": 20040
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.011480435729026794,
      "learning_rate": 1.572288e-05,
      "loss": 0.0006,
      "step": 20050
    },
    {
      "epoch": 0.64192,
      "grad_norm": 0.041912950575351715,
      "learning_rate": 1.572074666666667e-05,
      "loss": 0.0004,
      "step": 20060
    },
    {
      "epoch": 0.64224,
      "grad_norm": 0.012646188959479332,
      "learning_rate": 1.5718613333333334e-05,
      "loss": 0.0004,
      "step": 20070
    },
    {
      "epoch": 0.64256,
      "grad_norm": 0.004684716463088989,
      "learning_rate": 1.571648e-05,
      "loss": 0.0004,
      "step": 20080
    },
    {
      "epoch": 0.64288,
      "grad_norm": 0.013922472484409809,
      "learning_rate": 1.571434666666667e-05,
      "loss": 0.0008,
      "step": 20090
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.005573829635977745,
      "learning_rate": 1.5712213333333334e-05,
      "loss": 0.0004,
      "step": 20100
    },
    {
      "epoch": 0.64352,
      "grad_norm": 0.016661319881677628,
      "learning_rate": 1.5710080000000003e-05,
      "loss": 0.0008,
      "step": 20110
    },
    {
      "epoch": 0.64384,
      "grad_norm": 0.012339821085333824,
      "learning_rate": 1.5707946666666668e-05,
      "loss": 0.051,
      "step": 20120
    },
    {
      "epoch": 0.64416,
      "grad_norm": 0.006996355950832367,
      "learning_rate": 1.5705813333333337e-05,
      "loss": 0.0312,
      "step": 20130
    },
    {
      "epoch": 0.64448,
      "grad_norm": 0.005869621876627207,
      "learning_rate": 1.5703680000000002e-05,
      "loss": 0.0004,
      "step": 20140
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.11236998438835144,
      "learning_rate": 1.5701546666666667e-05,
      "loss": 0.0111,
      "step": 20150
    },
    {
      "epoch": 0.64512,
      "grad_norm": 0.018409475684165955,
      "learning_rate": 1.5699413333333336e-05,
      "loss": 0.0133,
      "step": 20160
    },
    {
      "epoch": 0.64544,
      "grad_norm": 0.006976770702749491,
      "learning_rate": 1.569728e-05,
      "loss": 0.0265,
      "step": 20170
    },
    {
      "epoch": 0.64576,
      "grad_norm": 0.008388001471757889,
      "learning_rate": 1.5695146666666667e-05,
      "loss": 0.0004,
      "step": 20180
    },
    {
      "epoch": 0.64608,
      "grad_norm": 0.00841684639453888,
      "learning_rate": 1.5693013333333336e-05,
      "loss": 0.0008,
      "step": 20190
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.008569392375648022,
      "learning_rate": 1.569088e-05,
      "loss": 0.0209,
      "step": 20200
    },
    {
      "epoch": 0.64672,
      "grad_norm": 0.007989926263689995,
      "learning_rate": 1.5688746666666667e-05,
      "loss": 0.0005,
      "step": 20210
    },
    {
      "epoch": 0.64704,
      "grad_norm": 0.017318541184067726,
      "learning_rate": 1.5686613333333335e-05,
      "loss": 0.043,
      "step": 20220
    },
    {
      "epoch": 0.64736,
      "grad_norm": 0.01279289461672306,
      "learning_rate": 1.568448e-05,
      "loss": 0.0101,
      "step": 20230
    },
    {
      "epoch": 0.64768,
      "grad_norm": 0.0074513377621769905,
      "learning_rate": 1.568234666666667e-05,
      "loss": 0.0005,
      "step": 20240
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.010840976610779762,
      "learning_rate": 1.5680213333333335e-05,
      "loss": 0.0006,
      "step": 20250
    },
    {
      "epoch": 0.64832,
      "grad_norm": 0.007881375961005688,
      "learning_rate": 1.5678080000000004e-05,
      "loss": 0.0216,
      "step": 20260
    },
    {
      "epoch": 0.64864,
      "grad_norm": 0.024973025545477867,
      "learning_rate": 1.567594666666667e-05,
      "loss": 0.0024,
      "step": 20270
    },
    {
      "epoch": 0.64896,
      "grad_norm": 0.017369717359542847,
      "learning_rate": 1.5673813333333334e-05,
      "loss": 0.0008,
      "step": 20280
    },
    {
      "epoch": 0.64928,
      "grad_norm": 0.006580080837011337,
      "learning_rate": 1.567168e-05,
      "loss": 0.0005,
      "step": 20290
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.008799764327704906,
      "learning_rate": 1.566954666666667e-05,
      "loss": 0.0004,
      "step": 20300
    },
    {
      "epoch": 0.64992,
      "grad_norm": 0.011193593963980675,
      "learning_rate": 1.5667413333333334e-05,
      "loss": 0.0616,
      "step": 20310
    },
    {
      "epoch": 0.65024,
      "grad_norm": 0.007295617833733559,
      "learning_rate": 1.566528e-05,
      "loss": 0.0005,
      "step": 20320
    },
    {
      "epoch": 0.65056,
      "grad_norm": 0.08301297575235367,
      "learning_rate": 1.5663146666666668e-05,
      "loss": 0.0005,
      "step": 20330
    },
    {
      "epoch": 0.65088,
      "grad_norm": 0.008680988103151321,
      "learning_rate": 1.5661013333333333e-05,
      "loss": 0.0307,
      "step": 20340
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.019287409260869026,
      "learning_rate": 1.5658880000000002e-05,
      "loss": 0.0005,
      "step": 20350
    },
    {
      "epoch": 0.65152,
      "grad_norm": 0.008633282035589218,
      "learning_rate": 1.5656746666666667e-05,
      "loss": 0.0005,
      "step": 20360
    },
    {
      "epoch": 0.65184,
      "grad_norm": 0.01595495641231537,
      "learning_rate": 1.5654613333333336e-05,
      "loss": 0.0005,
      "step": 20370
    },
    {
      "epoch": 0.65216,
      "grad_norm": 2.3450233936309814,
      "learning_rate": 1.565248e-05,
      "loss": 0.0042,
      "step": 20380
    },
    {
      "epoch": 0.65248,
      "grad_norm": 0.007129249628633261,
      "learning_rate": 1.5650346666666667e-05,
      "loss": 0.0029,
      "step": 20390
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.005892185959964991,
      "learning_rate": 1.5648213333333336e-05,
      "loss": 0.0116,
      "step": 20400
    },
    {
      "epoch": 0.65312,
      "grad_norm": 0.004277904983609915,
      "learning_rate": 1.564608e-05,
      "loss": 0.0004,
      "step": 20410
    },
    {
      "epoch": 0.65344,
      "grad_norm": 0.013838757760822773,
      "learning_rate": 1.5643946666666667e-05,
      "loss": 0.0088,
      "step": 20420
    },
    {
      "epoch": 0.65376,
      "grad_norm": 0.014322788454592228,
      "learning_rate": 1.5641813333333335e-05,
      "loss": 0.0243,
      "step": 20430
    },
    {
      "epoch": 0.65408,
      "grad_norm": 0.006057289894670248,
      "learning_rate": 1.563968e-05,
      "loss": 0.0004,
      "step": 20440
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.051521606743335724,
      "learning_rate": 1.5637546666666666e-05,
      "loss": 0.0004,
      "step": 20450
    },
    {
      "epoch": 0.65472,
      "grad_norm": 0.005283738486468792,
      "learning_rate": 1.5635413333333335e-05,
      "loss": 0.0011,
      "step": 20460
    },
    {
      "epoch": 0.65504,
      "grad_norm": 0.005187489092350006,
      "learning_rate": 1.563328e-05,
      "loss": 0.0011,
      "step": 20470
    },
    {
      "epoch": 0.65536,
      "grad_norm": 0.005429734010249376,
      "learning_rate": 1.563114666666667e-05,
      "loss": 0.0004,
      "step": 20480
    },
    {
      "epoch": 0.65568,
      "grad_norm": 0.0077198646031320095,
      "learning_rate": 1.5629013333333334e-05,
      "loss": 0.0004,
      "step": 20490
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.012644177302718163,
      "learning_rate": 1.5626880000000003e-05,
      "loss": 0.028,
      "step": 20500
    },
    {
      "epoch": 0.65632,
      "grad_norm": 0.008689756505191326,
      "learning_rate": 1.562474666666667e-05,
      "loss": 0.0006,
      "step": 20510
    },
    {
      "epoch": 0.65664,
      "grad_norm": 0.007687509525567293,
      "learning_rate": 1.5622613333333334e-05,
      "loss": 0.0004,
      "step": 20520
    },
    {
      "epoch": 0.65696,
      "grad_norm": 0.003131293458864093,
      "learning_rate": 1.5620480000000003e-05,
      "loss": 0.0002,
      "step": 20530
    },
    {
      "epoch": 0.65728,
      "grad_norm": 3.4591946601867676,
      "learning_rate": 1.5618346666666668e-05,
      "loss": 0.0079,
      "step": 20540
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.003213671036064625,
      "learning_rate": 1.5616213333333333e-05,
      "loss": 0.0086,
      "step": 20550
    },
    {
      "epoch": 0.65792,
      "grad_norm": 0.021975336596369743,
      "learning_rate": 1.561408e-05,
      "loss": 0.0004,
      "step": 20560
    },
    {
      "epoch": 0.65824,
      "grad_norm": 0.0125304339453578,
      "learning_rate": 1.5611946666666668e-05,
      "loss": 0.0018,
      "step": 20570
    },
    {
      "epoch": 0.65856,
      "grad_norm": 0.06037638336420059,
      "learning_rate": 1.5609813333333336e-05,
      "loss": 0.0004,
      "step": 20580
    },
    {
      "epoch": 0.65888,
      "grad_norm": 0.002913521137088537,
      "learning_rate": 1.560768e-05,
      "loss": 0.0382,
      "step": 20590
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.004125342704355717,
      "learning_rate": 1.560554666666667e-05,
      "loss": 0.0003,
      "step": 20600
    },
    {
      "epoch": 0.65952,
      "grad_norm": 0.006463692057877779,
      "learning_rate": 1.5603413333333336e-05,
      "loss": 0.0003,
      "step": 20610
    },
    {
      "epoch": 0.65984,
      "grad_norm": 0.0055312118493020535,
      "learning_rate": 1.560128e-05,
      "loss": 0.0003,
      "step": 20620
    },
    {
      "epoch": 0.66016,
      "grad_norm": 0.006551711354404688,
      "learning_rate": 1.5599146666666667e-05,
      "loss": 0.0006,
      "step": 20630
    },
    {
      "epoch": 0.66048,
      "grad_norm": 0.0030491380020976067,
      "learning_rate": 1.5597013333333335e-05,
      "loss": 0.0005,
      "step": 20640
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.004051359836012125,
      "learning_rate": 1.559488e-05,
      "loss": 0.0003,
      "step": 20650
    },
    {
      "epoch": 0.66112,
      "grad_norm": 0.0044152927584946156,
      "learning_rate": 1.5592746666666666e-05,
      "loss": 0.0004,
      "step": 20660
    },
    {
      "epoch": 0.66144,
      "grad_norm": 0.0050193509086966515,
      "learning_rate": 1.5590613333333335e-05,
      "loss": 0.0002,
      "step": 20670
    },
    {
      "epoch": 0.66176,
      "grad_norm": 0.05972805991768837,
      "learning_rate": 1.558848e-05,
      "loss": 0.0003,
      "step": 20680
    },
    {
      "epoch": 0.66208,
      "grad_norm": 0.012664572335779667,
      "learning_rate": 1.558634666666667e-05,
      "loss": 0.0003,
      "step": 20690
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.0032565188594162464,
      "learning_rate": 1.5584213333333334e-05,
      "loss": 0.0003,
      "step": 20700
    },
    {
      "epoch": 0.66272,
      "grad_norm": 0.00545291043817997,
      "learning_rate": 1.5582080000000003e-05,
      "loss": 0.0441,
      "step": 20710
    },
    {
      "epoch": 0.66304,
      "grad_norm": 5.6937432289123535,
      "learning_rate": 1.557994666666667e-05,
      "loss": 0.0131,
      "step": 20720
    },
    {
      "epoch": 0.66336,
      "grad_norm": 0.006865277886390686,
      "learning_rate": 1.5577813333333334e-05,
      "loss": 0.0349,
      "step": 20730
    },
    {
      "epoch": 0.66368,
      "grad_norm": 0.007679390721023083,
      "learning_rate": 1.5575680000000003e-05,
      "loss": 0.0097,
      "step": 20740
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.0057571143843233585,
      "learning_rate": 1.5573546666666668e-05,
      "loss": 0.0004,
      "step": 20750
    },
    {
      "epoch": 0.66432,
      "grad_norm": 0.005705915857106447,
      "learning_rate": 1.5571413333333333e-05,
      "loss": 0.0002,
      "step": 20760
    },
    {
      "epoch": 0.66464,
      "grad_norm": 0.002830925164744258,
      "learning_rate": 1.5569280000000002e-05,
      "loss": 0.0003,
      "step": 20770
    },
    {
      "epoch": 0.66496,
      "grad_norm": 0.004297350067645311,
      "learning_rate": 1.5567146666666668e-05,
      "loss": 0.0508,
      "step": 20780
    },
    {
      "epoch": 0.66528,
      "grad_norm": 0.005163885187357664,
      "learning_rate": 1.5565013333333333e-05,
      "loss": 0.0004,
      "step": 20790
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.02080851048231125,
      "learning_rate": 1.556288e-05,
      "loss": 0.0595,
      "step": 20800
    },
    {
      "epoch": 0.66592,
      "grad_norm": 0.015245436690747738,
      "learning_rate": 1.5560746666666667e-05,
      "loss": 0.036,
      "step": 20810
    },
    {
      "epoch": 0.66624,
      "grad_norm": 2.168010950088501,
      "learning_rate": 1.5558613333333336e-05,
      "loss": 0.0441,
      "step": 20820
    },
    {
      "epoch": 0.66656,
      "grad_norm": 0.0040189106948673725,
      "learning_rate": 1.555648e-05,
      "loss": 0.0003,
      "step": 20830
    },
    {
      "epoch": 0.66688,
      "grad_norm": 0.013687538914382458,
      "learning_rate": 1.555434666666667e-05,
      "loss": 0.0008,
      "step": 20840
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.006644971668720245,
      "learning_rate": 1.5552213333333335e-05,
      "loss": 0.0006,
      "step": 20850
    },
    {
      "epoch": 0.66752,
      "grad_norm": 0.007424729876220226,
      "learning_rate": 1.555008e-05,
      "loss": 0.0003,
      "step": 20860
    },
    {
      "epoch": 0.66784,
      "grad_norm": 0.00887062307447195,
      "learning_rate": 1.554794666666667e-05,
      "loss": 0.0004,
      "step": 20870
    },
    {
      "epoch": 0.66816,
      "grad_norm": 0.011150720529258251,
      "learning_rate": 1.5545813333333335e-05,
      "loss": 0.0282,
      "step": 20880
    },
    {
      "epoch": 0.66848,
      "grad_norm": 0.008178566582500935,
      "learning_rate": 1.554368e-05,
      "loss": 0.0202,
      "step": 20890
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.004835185129195452,
      "learning_rate": 1.5541546666666666e-05,
      "loss": 0.0003,
      "step": 20900
    },
    {
      "epoch": 0.66912,
      "grad_norm": 0.0043670074082911015,
      "learning_rate": 1.5539413333333334e-05,
      "loss": 0.05,
      "step": 20910
    },
    {
      "epoch": 0.66944,
      "grad_norm": 0.1375492960214615,
      "learning_rate": 1.553728e-05,
      "loss": 0.0008,
      "step": 20920
    },
    {
      "epoch": 0.66976,
      "grad_norm": 0.3062613606452942,
      "learning_rate": 1.553514666666667e-05,
      "loss": 0.0007,
      "step": 20930
    },
    {
      "epoch": 0.67008,
      "grad_norm": 3.5840554237365723,
      "learning_rate": 1.5533013333333334e-05,
      "loss": 0.0224,
      "step": 20940
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.009800998494029045,
      "learning_rate": 1.5530880000000003e-05,
      "loss": 0.0015,
      "step": 20950
    },
    {
      "epoch": 0.67072,
      "grad_norm": 0.004924944136291742,
      "learning_rate": 1.5528746666666668e-05,
      "loss": 0.0018,
      "step": 20960
    },
    {
      "epoch": 0.67104,
      "grad_norm": 0.006139600183814764,
      "learning_rate": 1.5526613333333333e-05,
      "loss": 0.0887,
      "step": 20970
    },
    {
      "epoch": 0.67136,
      "grad_norm": 0.012386550195515156,
      "learning_rate": 1.5524480000000002e-05,
      "loss": 0.0003,
      "step": 20980
    },
    {
      "epoch": 0.67168,
      "grad_norm": 3.578536033630371,
      "learning_rate": 1.5522346666666668e-05,
      "loss": 0.0513,
      "step": 20990
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.009800258092582226,
      "learning_rate": 1.5520213333333333e-05,
      "loss": 0.0005,
      "step": 21000
    },
    {
      "epoch": 0.67232,
      "grad_norm": 0.0054434859193861485,
      "learning_rate": 1.5518080000000002e-05,
      "loss": 0.0006,
      "step": 21010
    },
    {
      "epoch": 0.67264,
      "grad_norm": 0.0101222088560462,
      "learning_rate": 1.5515946666666667e-05,
      "loss": 0.053,
      "step": 21020
    },
    {
      "epoch": 0.67296,
      "grad_norm": 0.015529985539615154,
      "learning_rate": 1.5513813333333332e-05,
      "loss": 0.0005,
      "step": 21030
    },
    {
      "epoch": 0.67328,
      "grad_norm": 0.021708572283387184,
      "learning_rate": 1.551168e-05,
      "loss": 0.0072,
      "step": 21040
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.331199049949646,
      "learning_rate": 1.550954666666667e-05,
      "loss": 0.0008,
      "step": 21050
    },
    {
      "epoch": 0.67392,
      "grad_norm": 0.042445577681064606,
      "learning_rate": 1.5507413333333335e-05,
      "loss": 0.0005,
      "step": 21060
    },
    {
      "epoch": 0.67424,
      "grad_norm": 0.02456316538155079,
      "learning_rate": 1.550528e-05,
      "loss": 0.0005,
      "step": 21070
    },
    {
      "epoch": 0.67456,
      "grad_norm": 2.7213919162750244,
      "learning_rate": 1.550314666666667e-05,
      "loss": 0.0082,
      "step": 21080
    },
    {
      "epoch": 0.67488,
      "grad_norm": 0.008098562248051167,
      "learning_rate": 1.5501013333333335e-05,
      "loss": 0.0007,
      "step": 21090
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.0036718319170176983,
      "learning_rate": 1.549888e-05,
      "loss": 0.0009,
      "step": 21100
    },
    {
      "epoch": 0.67552,
      "grad_norm": 0.013729169964790344,
      "learning_rate": 1.549674666666667e-05,
      "loss": 0.0004,
      "step": 21110
    },
    {
      "epoch": 0.67584,
      "grad_norm": 0.009409824386239052,
      "learning_rate": 1.5494613333333334e-05,
      "loss": 0.0475,
      "step": 21120
    },
    {
      "epoch": 0.67616,
      "grad_norm": 0.008000656962394714,
      "learning_rate": 1.549248e-05,
      "loss": 0.0194,
      "step": 21130
    },
    {
      "epoch": 0.67648,
      "grad_norm": 0.009829680435359478,
      "learning_rate": 1.549034666666667e-05,
      "loss": 0.0007,
      "step": 21140
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.01873682625591755,
      "learning_rate": 1.5488213333333334e-05,
      "loss": 0.0008,
      "step": 21150
    },
    {
      "epoch": 0.67712,
      "grad_norm": 0.49409160017967224,
      "learning_rate": 1.5486080000000003e-05,
      "loss": 0.0023,
      "step": 21160
    },
    {
      "epoch": 0.67744,
      "grad_norm": 0.00905068963766098,
      "learning_rate": 1.5483946666666668e-05,
      "loss": 0.0003,
      "step": 21170
    },
    {
      "epoch": 0.67776,
      "grad_norm": 0.011067508719861507,
      "learning_rate": 1.5481813333333337e-05,
      "loss": 0.0526,
      "step": 21180
    },
    {
      "epoch": 0.67808,
      "grad_norm": 0.021182380616664886,
      "learning_rate": 1.5479680000000002e-05,
      "loss": 0.0006,
      "step": 21190
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.007522019557654858,
      "learning_rate": 1.5477546666666668e-05,
      "loss": 0.0004,
      "step": 21200
    },
    {
      "epoch": 0.67872,
      "grad_norm": 0.010972610674798489,
      "learning_rate": 1.5475413333333336e-05,
      "loss": 0.001,
      "step": 21210
    },
    {
      "epoch": 0.67904,
      "grad_norm": 0.008845651522278786,
      "learning_rate": 1.5473280000000002e-05,
      "loss": 0.0166,
      "step": 21220
    },
    {
      "epoch": 0.67936,
      "grad_norm": 0.00926103163510561,
      "learning_rate": 1.5471146666666667e-05,
      "loss": 0.0006,
      "step": 21230
    },
    {
      "epoch": 0.67968,
      "grad_norm": 0.007870730012655258,
      "learning_rate": 1.5469013333333332e-05,
      "loss": 0.0122,
      "step": 21240
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.09711959213018417,
      "learning_rate": 1.546688e-05,
      "loss": 0.0008,
      "step": 21250
    },
    {
      "epoch": 0.68032,
      "grad_norm": 0.0041306233033537865,
      "learning_rate": 1.5464746666666667e-05,
      "loss": 0.0006,
      "step": 21260
    },
    {
      "epoch": 0.68064,
      "grad_norm": 0.010438266210258007,
      "learning_rate": 1.5462613333333335e-05,
      "loss": 0.0006,
      "step": 21270
    },
    {
      "epoch": 0.68096,
      "grad_norm": 0.028859848156571388,
      "learning_rate": 1.546048e-05,
      "loss": 0.0021,
      "step": 21280
    },
    {
      "epoch": 0.68128,
      "grad_norm": 0.006347006652504206,
      "learning_rate": 1.545834666666667e-05,
      "loss": 0.0126,
      "step": 21290
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.010051319375634193,
      "learning_rate": 1.5456213333333335e-05,
      "loss": 0.0006,
      "step": 21300
    },
    {
      "epoch": 0.68192,
      "grad_norm": 0.006506905425339937,
      "learning_rate": 1.545408e-05,
      "loss": 0.0004,
      "step": 21310
    },
    {
      "epoch": 0.68224,
      "grad_norm": 0.026237932965159416,
      "learning_rate": 1.545194666666667e-05,
      "loss": 0.0006,
      "step": 21320
    },
    {
      "epoch": 0.68256,
      "grad_norm": 0.0070555866695940495,
      "learning_rate": 1.5449813333333334e-05,
      "loss": 0.0006,
      "step": 21330
    },
    {
      "epoch": 0.68288,
      "grad_norm": 0.006081649102270603,
      "learning_rate": 1.544768e-05,
      "loss": 0.0155,
      "step": 21340
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.01973593793809414,
      "learning_rate": 1.544554666666667e-05,
      "loss": 0.0288,
      "step": 21350
    },
    {
      "epoch": 0.68352,
      "grad_norm": 0.008912389166653156,
      "learning_rate": 1.5443413333333334e-05,
      "loss": 0.0005,
      "step": 21360
    },
    {
      "epoch": 0.68384,
      "grad_norm": 0.00987975113093853,
      "learning_rate": 1.544128e-05,
      "loss": 0.0434,
      "step": 21370
    },
    {
      "epoch": 0.68416,
      "grad_norm": 0.009771648794412613,
      "learning_rate": 1.5439146666666668e-05,
      "loss": 0.0123,
      "step": 21380
    },
    {
      "epoch": 0.68448,
      "grad_norm": 0.012530233711004257,
      "learning_rate": 1.5437013333333333e-05,
      "loss": 0.0005,
      "step": 21390
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.006118759047240019,
      "learning_rate": 1.5434880000000002e-05,
      "loss": 0.0006,
      "step": 21400
    },
    {
      "epoch": 0.68512,
      "grad_norm": 0.005458774045109749,
      "learning_rate": 1.5432746666666668e-05,
      "loss": 0.0005,
      "step": 21410
    },
    {
      "epoch": 0.68544,
      "grad_norm": 0.04517574608325958,
      "learning_rate": 1.5430613333333336e-05,
      "loss": 0.0006,
      "step": 21420
    },
    {
      "epoch": 0.68576,
      "grad_norm": 0.01566150411963463,
      "learning_rate": 1.5428480000000002e-05,
      "loss": 0.0014,
      "step": 21430
    },
    {
      "epoch": 0.68608,
      "grad_norm": 0.0098186070099473,
      "learning_rate": 1.5426346666666667e-05,
      "loss": 0.0008,
      "step": 21440
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.018254779279232025,
      "learning_rate": 1.5424213333333336e-05,
      "loss": 0.0005,
      "step": 21450
    },
    {
      "epoch": 0.68672,
      "grad_norm": 0.007894601672887802,
      "learning_rate": 1.542208e-05,
      "loss": 0.0275,
      "step": 21460
    },
    {
      "epoch": 0.68704,
      "grad_norm": 0.012173017486929893,
      "learning_rate": 1.5419946666666667e-05,
      "loss": 0.0004,
      "step": 21470
    },
    {
      "epoch": 0.68736,
      "grad_norm": 0.007009484339505434,
      "learning_rate": 1.5417813333333332e-05,
      "loss": 0.0004,
      "step": 21480
    },
    {
      "epoch": 0.68768,
      "grad_norm": 0.006333181168884039,
      "learning_rate": 1.541568e-05,
      "loss": 0.0005,
      "step": 21490
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.004429429303854704,
      "learning_rate": 1.5413546666666666e-05,
      "loss": 0.0024,
      "step": 21500
    },
    {
      "epoch": 0.68832,
      "grad_norm": 0.00857448112219572,
      "learning_rate": 1.5411413333333335e-05,
      "loss": 0.0004,
      "step": 21510
    },
    {
      "epoch": 0.68864,
      "grad_norm": 0.008151826448738575,
      "learning_rate": 1.5409280000000004e-05,
      "loss": 0.0156,
      "step": 21520
    },
    {
      "epoch": 0.68896,
      "grad_norm": 0.01624424383044243,
      "learning_rate": 1.540714666666667e-05,
      "loss": 0.0003,
      "step": 21530
    },
    {
      "epoch": 0.68928,
      "grad_norm": 0.19490522146224976,
      "learning_rate": 1.5405013333333334e-05,
      "loss": 0.0012,
      "step": 21540
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.006738563999533653,
      "learning_rate": 1.5402880000000003e-05,
      "loss": 0.0003,
      "step": 21550
    },
    {
      "epoch": 0.68992,
      "grad_norm": 0.005426195450127125,
      "learning_rate": 1.540074666666667e-05,
      "loss": 0.0004,
      "step": 21560
    },
    {
      "epoch": 0.69024,
      "grad_norm": 0.007209343835711479,
      "learning_rate": 1.5398613333333334e-05,
      "loss": 0.0003,
      "step": 21570
    },
    {
      "epoch": 0.69056,
      "grad_norm": 0.0034456041175872087,
      "learning_rate": 1.539648e-05,
      "loss": 0.0004,
      "step": 21580
    },
    {
      "epoch": 0.69088,
      "grad_norm": 0.003666930366307497,
      "learning_rate": 1.5394346666666668e-05,
      "loss": 0.0004,
      "step": 21590
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.003013131208717823,
      "learning_rate": 1.5392213333333333e-05,
      "loss": 0.0005,
      "step": 21600
    },
    {
      "epoch": 0.69152,
      "grad_norm": 0.002831794787198305,
      "learning_rate": 1.5390080000000002e-05,
      "loss": 0.0052,
      "step": 21610
    },
    {
      "epoch": 0.69184,
      "grad_norm": 0.10814683884382248,
      "learning_rate": 1.5387946666666668e-05,
      "loss": 0.0014,
      "step": 21620
    },
    {
      "epoch": 0.69216,
      "grad_norm": 0.00659974617883563,
      "learning_rate": 1.5385813333333336e-05,
      "loss": 0.0003,
      "step": 21630
    },
    {
      "epoch": 0.69248,
      "grad_norm": 0.010778446681797504,
      "learning_rate": 1.5383680000000002e-05,
      "loss": 0.0002,
      "step": 21640
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.007673012558370829,
      "learning_rate": 1.5381546666666667e-05,
      "loss": 0.0003,
      "step": 21650
    },
    {
      "epoch": 0.69312,
      "grad_norm": 0.0044539812952280045,
      "learning_rate": 1.5379413333333336e-05,
      "loss": 0.0003,
      "step": 21660
    },
    {
      "epoch": 0.69344,
      "grad_norm": 0.016773993149399757,
      "learning_rate": 1.537728e-05,
      "loss": 0.0003,
      "step": 21670
    },
    {
      "epoch": 0.69376,
      "grad_norm": 0.030144331976771355,
      "learning_rate": 1.5375146666666667e-05,
      "loss": 0.0516,
      "step": 21680
    },
    {
      "epoch": 0.69408,
      "grad_norm": 0.004023753572255373,
      "learning_rate": 1.5373013333333335e-05,
      "loss": 0.0004,
      "step": 21690
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.010781231336295605,
      "learning_rate": 1.537088e-05,
      "loss": 0.0006,
      "step": 21700
    },
    {
      "epoch": 0.69472,
      "grad_norm": 0.004580394364893436,
      "learning_rate": 1.5368746666666666e-05,
      "loss": 0.0094,
      "step": 21710
    },
    {
      "epoch": 0.69504,
      "grad_norm": 0.008622200228273869,
      "learning_rate": 1.5366613333333335e-05,
      "loss": 0.0003,
      "step": 21720
    },
    {
      "epoch": 0.69536,
      "grad_norm": 0.017331238836050034,
      "learning_rate": 1.536448e-05,
      "loss": 0.0003,
      "step": 21730
    },
    {
      "epoch": 0.69568,
      "grad_norm": 0.007636541500687599,
      "learning_rate": 1.536234666666667e-05,
      "loss": 0.0492,
      "step": 21740
    },
    {
      "epoch": 0.696,
      "grad_norm": 1.4693793058395386,
      "learning_rate": 1.5360213333333334e-05,
      "loss": 0.0575,
      "step": 21750
    },
    {
      "epoch": 0.69632,
      "grad_norm": 0.006587831769138575,
      "learning_rate": 1.5358080000000003e-05,
      "loss": 0.0005,
      "step": 21760
    },
    {
      "epoch": 0.69664,
      "grad_norm": 0.0048348973505198956,
      "learning_rate": 1.535594666666667e-05,
      "loss": 0.0013,
      "step": 21770
    },
    {
      "epoch": 0.69696,
      "grad_norm": 0.006998030003160238,
      "learning_rate": 1.5353813333333334e-05,
      "loss": 0.0004,
      "step": 21780
    },
    {
      "epoch": 0.69728,
      "grad_norm": 0.007231167983263731,
      "learning_rate": 1.5351680000000003e-05,
      "loss": 0.0005,
      "step": 21790
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.010910465382039547,
      "learning_rate": 1.5349546666666668e-05,
      "loss": 0.0007,
      "step": 21800
    },
    {
      "epoch": 0.69792,
      "grad_norm": 0.003529696259647608,
      "learning_rate": 1.5347413333333333e-05,
      "loss": 0.0005,
      "step": 21810
    },
    {
      "epoch": 0.69824,
      "grad_norm": 0.006036913488060236,
      "learning_rate": 1.534528e-05,
      "loss": 0.0004,
      "step": 21820
    },
    {
      "epoch": 0.69856,
      "grad_norm": 0.006524249445647001,
      "learning_rate": 1.5343146666666668e-05,
      "loss": 0.0345,
      "step": 21830
    },
    {
      "epoch": 0.69888,
      "grad_norm": 0.011572657153010368,
      "learning_rate": 1.5341013333333333e-05,
      "loss": 0.0243,
      "step": 21840
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.022552287206053734,
      "learning_rate": 1.5338880000000002e-05,
      "loss": 0.0007,
      "step": 21850
    },
    {
      "epoch": 0.69952,
      "grad_norm": 0.004433976020663977,
      "learning_rate": 1.5336746666666667e-05,
      "loss": 0.0004,
      "step": 21860
    },
    {
      "epoch": 0.69984,
      "grad_norm": 0.006528322119265795,
      "learning_rate": 1.5334613333333336e-05,
      "loss": 0.0006,
      "step": 21870
    },
    {
      "epoch": 0.70016,
      "grad_norm": 0.019775787368416786,
      "learning_rate": 1.533248e-05,
      "loss": 0.0136,
      "step": 21880
    },
    {
      "epoch": 0.70048,
      "grad_norm": 0.0070884013548493385,
      "learning_rate": 1.533034666666667e-05,
      "loss": 0.0296,
      "step": 21890
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.013824074529111385,
      "learning_rate": 1.5328213333333335e-05,
      "loss": 0.0006,
      "step": 21900
    },
    {
      "epoch": 0.70112,
      "grad_norm": 0.011644667945802212,
      "learning_rate": 1.532608e-05,
      "loss": 0.0004,
      "step": 21910
    },
    {
      "epoch": 0.70144,
      "grad_norm": 0.024439554661512375,
      "learning_rate": 1.5323946666666666e-05,
      "loss": 0.0005,
      "step": 21920
    },
    {
      "epoch": 0.70176,
      "grad_norm": 0.01042922493070364,
      "learning_rate": 1.5321813333333335e-05,
      "loss": 0.0048,
      "step": 21930
    },
    {
      "epoch": 0.70208,
      "grad_norm": 0.05077186971902847,
      "learning_rate": 1.531968e-05,
      "loss": 0.0005,
      "step": 21940
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.013688960112631321,
      "learning_rate": 1.5317546666666666e-05,
      "loss": 0.0004,
      "step": 21950
    },
    {
      "epoch": 0.70272,
      "grad_norm": 0.012048640288412571,
      "learning_rate": 1.5315413333333334e-05,
      "loss": 0.0004,
      "step": 21960
    },
    {
      "epoch": 0.70304,
      "grad_norm": 0.03335944190621376,
      "learning_rate": 1.531328e-05,
      "loss": 0.0013,
      "step": 21970
    },
    {
      "epoch": 0.70336,
      "grad_norm": 0.005964995361864567,
      "learning_rate": 1.531114666666667e-05,
      "loss": 0.0563,
      "step": 21980
    },
    {
      "epoch": 0.70368,
      "grad_norm": 0.004129074513912201,
      "learning_rate": 1.5309013333333334e-05,
      "loss": 0.0006,
      "step": 21990
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.00436205742880702,
      "learning_rate": 1.5306880000000003e-05,
      "loss": 0.0003,
      "step": 22000
    },
    {
      "epoch": 0.70432,
      "grad_norm": 0.005986555013805628,
      "learning_rate": 1.5304746666666668e-05,
      "loss": 0.0008,
      "step": 22010
    },
    {
      "epoch": 0.70464,
      "grad_norm": 0.011821758933365345,
      "learning_rate": 1.5302613333333334e-05,
      "loss": 0.0003,
      "step": 22020
    },
    {
      "epoch": 0.70496,
      "grad_norm": 0.009343201294541359,
      "learning_rate": 1.5300480000000002e-05,
      "loss": 0.0004,
      "step": 22030
    },
    {
      "epoch": 0.70528,
      "grad_norm": 0.0069579011760652065,
      "learning_rate": 1.5298346666666668e-05,
      "loss": 0.0003,
      "step": 22040
    },
    {
      "epoch": 0.7056,
      "grad_norm": 5.5267863273620605,
      "learning_rate": 1.5296213333333333e-05,
      "loss": 0.0306,
      "step": 22050
    },
    {
      "epoch": 0.70592,
      "grad_norm": 2.1060831546783447,
      "learning_rate": 1.5294080000000002e-05,
      "loss": 0.096,
      "step": 22060
    },
    {
      "epoch": 0.70624,
      "grad_norm": 0.010816970840096474,
      "learning_rate": 1.5291946666666667e-05,
      "loss": 0.0008,
      "step": 22070
    },
    {
      "epoch": 0.70656,
      "grad_norm": 0.012094548903405666,
      "learning_rate": 1.5289813333333336e-05,
      "loss": 0.0004,
      "step": 22080
    },
    {
      "epoch": 0.70688,
      "grad_norm": 0.024237824603915215,
      "learning_rate": 1.528768e-05,
      "loss": 0.0512,
      "step": 22090
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.008354656398296356,
      "learning_rate": 1.528554666666667e-05,
      "loss": 0.0005,
      "step": 22100
    },
    {
      "epoch": 0.70752,
      "grad_norm": 0.006554591003805399,
      "learning_rate": 1.5283413333333335e-05,
      "loss": 0.0006,
      "step": 22110
    },
    {
      "epoch": 0.70784,
      "grad_norm": 0.04193057492375374,
      "learning_rate": 1.528128e-05,
      "loss": 0.0011,
      "step": 22120
    },
    {
      "epoch": 0.70816,
      "grad_norm": 0.24556571245193481,
      "learning_rate": 1.527914666666667e-05,
      "loss": 0.0016,
      "step": 22130
    },
    {
      "epoch": 0.70848,
      "grad_norm": 0.009079400449991226,
      "learning_rate": 1.5277013333333335e-05,
      "loss": 0.0035,
      "step": 22140
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.007460626773536205,
      "learning_rate": 1.527488e-05,
      "loss": 0.0011,
      "step": 22150
    },
    {
      "epoch": 0.70912,
      "grad_norm": 0.782328188419342,
      "learning_rate": 1.5272746666666666e-05,
      "loss": 0.0456,
      "step": 22160
    },
    {
      "epoch": 0.70944,
      "grad_norm": 0.048050131648778915,
      "learning_rate": 1.5270613333333335e-05,
      "loss": 0.0006,
      "step": 22170
    },
    {
      "epoch": 0.70976,
      "grad_norm": 0.056624967604875565,
      "learning_rate": 1.526848e-05,
      "loss": 0.0391,
      "step": 22180
    },
    {
      "epoch": 0.71008,
      "grad_norm": 0.0059740906581282616,
      "learning_rate": 1.526634666666667e-05,
      "loss": 0.0006,
      "step": 22190
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.007412228267639875,
      "learning_rate": 1.5264213333333334e-05,
      "loss": 0.0007,
      "step": 22200
    },
    {
      "epoch": 0.71072,
      "grad_norm": 0.014186860993504524,
      "learning_rate": 1.5262080000000003e-05,
      "loss": 0.0004,
      "step": 22210
    },
    {
      "epoch": 0.71104,
      "grad_norm": 0.02589516155421734,
      "learning_rate": 1.5259946666666668e-05,
      "loss": 0.0191,
      "step": 22220
    },
    {
      "epoch": 0.71136,
      "grad_norm": 0.008214869536459446,
      "learning_rate": 1.5257813333333335e-05,
      "loss": 0.0122,
      "step": 22230
    },
    {
      "epoch": 0.71168,
      "grad_norm": 2.3997268676757812,
      "learning_rate": 1.5255680000000002e-05,
      "loss": 0.0155,
      "step": 22240
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.009842869825661182,
      "learning_rate": 1.5253546666666668e-05,
      "loss": 0.0269,
      "step": 22250
    },
    {
      "epoch": 0.71232,
      "grad_norm": 0.006342894863337278,
      "learning_rate": 1.5251413333333333e-05,
      "loss": 0.0005,
      "step": 22260
    },
    {
      "epoch": 0.71264,
      "grad_norm": 0.03613680228590965,
      "learning_rate": 1.5249280000000002e-05,
      "loss": 0.0152,
      "step": 22270
    },
    {
      "epoch": 0.71296,
      "grad_norm": 0.017246421426534653,
      "learning_rate": 1.5247146666666667e-05,
      "loss": 0.0005,
      "step": 22280
    },
    {
      "epoch": 0.71328,
      "grad_norm": 0.011845963075757027,
      "learning_rate": 1.5245013333333334e-05,
      "loss": 0.0023,
      "step": 22290
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.023907151073217392,
      "learning_rate": 1.5242880000000001e-05,
      "loss": 0.0647,
      "step": 22300
    },
    {
      "epoch": 0.71392,
      "grad_norm": 0.010410359129309654,
      "learning_rate": 1.5240746666666668e-05,
      "loss": 0.0006,
      "step": 22310
    },
    {
      "epoch": 0.71424,
      "grad_norm": 0.010403149761259556,
      "learning_rate": 1.5238613333333334e-05,
      "loss": 0.001,
      "step": 22320
    },
    {
      "epoch": 0.71456,
      "grad_norm": 0.049724821001291275,
      "learning_rate": 1.5236480000000001e-05,
      "loss": 0.0599,
      "step": 22330
    },
    {
      "epoch": 0.71488,
      "grad_norm": 0.019842078909277916,
      "learning_rate": 1.5234346666666668e-05,
      "loss": 0.0014,
      "step": 22340
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.013577164150774479,
      "learning_rate": 1.5232213333333335e-05,
      "loss": 0.0009,
      "step": 22350
    },
    {
      "epoch": 0.71552,
      "grad_norm": 0.01683777943253517,
      "learning_rate": 1.523008e-05,
      "loss": 0.0005,
      "step": 22360
    },
    {
      "epoch": 0.71584,
      "grad_norm": 0.033906422555446625,
      "learning_rate": 1.5227946666666669e-05,
      "loss": 0.0033,
      "step": 22370
    },
    {
      "epoch": 0.71616,
      "grad_norm": 0.01634283736348152,
      "learning_rate": 1.5225813333333335e-05,
      "loss": 0.0015,
      "step": 22380
    },
    {
      "epoch": 0.71648,
      "grad_norm": 0.014520674012601376,
      "learning_rate": 1.522368e-05,
      "loss": 0.0005,
      "step": 22390
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.02429884858429432,
      "learning_rate": 1.5221546666666669e-05,
      "loss": 0.0405,
      "step": 22400
    },
    {
      "epoch": 0.71712,
      "grad_norm": 0.02816755138337612,
      "learning_rate": 1.5219413333333336e-05,
      "loss": 0.0009,
      "step": 22410
    },
    {
      "epoch": 0.71744,
      "grad_norm": 0.015832507982850075,
      "learning_rate": 1.5217280000000001e-05,
      "loss": 0.0009,
      "step": 22420
    },
    {
      "epoch": 0.71776,
      "grad_norm": 0.013985172845423222,
      "learning_rate": 1.5215146666666666e-05,
      "loss": 0.0006,
      "step": 22430
    },
    {
      "epoch": 0.71808,
      "grad_norm": 0.01867293007671833,
      "learning_rate": 1.5213013333333335e-05,
      "loss": 0.0464,
      "step": 22440
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.10479901731014252,
      "learning_rate": 1.521088e-05,
      "loss": 0.0011,
      "step": 22450
    },
    {
      "epoch": 0.71872,
      "grad_norm": 0.013441408984363079,
      "learning_rate": 1.5208746666666668e-05,
      "loss": 0.0015,
      "step": 22460
    },
    {
      "epoch": 0.71904,
      "grad_norm": 0.010478242300450802,
      "learning_rate": 1.5206613333333335e-05,
      "loss": 0.0005,
      "step": 22470
    },
    {
      "epoch": 0.71936,
      "grad_norm": 0.0085397157818079,
      "learning_rate": 1.5204480000000002e-05,
      "loss": 0.0372,
      "step": 22480
    },
    {
      "epoch": 0.71968,
      "grad_norm": 0.013129991479218006,
      "learning_rate": 1.5202346666666667e-05,
      "loss": 0.0168,
      "step": 22490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.004436487797647715,
      "learning_rate": 1.5200213333333334e-05,
      "loss": 0.0006,
      "step": 22500
    },
    {
      "epoch": 0.72032,
      "grad_norm": 0.00939975492656231,
      "learning_rate": 1.5198080000000001e-05,
      "loss": 0.0005,
      "step": 22510
    },
    {
      "epoch": 0.72064,
      "grad_norm": 0.028457915410399437,
      "learning_rate": 1.5195946666666668e-05,
      "loss": 0.0008,
      "step": 22520
    },
    {
      "epoch": 0.72096,
      "grad_norm": 0.009404951706528664,
      "learning_rate": 1.5193813333333334e-05,
      "loss": 0.0005,
      "step": 22530
    },
    {
      "epoch": 0.72128,
      "grad_norm": 0.019881414249539375,
      "learning_rate": 1.5191680000000003e-05,
      "loss": 0.0017,
      "step": 22540
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.018687311559915543,
      "learning_rate": 1.5189546666666668e-05,
      "loss": 0.0019,
      "step": 22550
    },
    {
      "epoch": 0.72192,
      "grad_norm": 0.007578164339065552,
      "learning_rate": 1.5187413333333333e-05,
      "loss": 0.0005,
      "step": 22560
    },
    {
      "epoch": 0.72224,
      "grad_norm": 0.6194393038749695,
      "learning_rate": 1.5185280000000002e-05,
      "loss": 0.0015,
      "step": 22570
    },
    {
      "epoch": 0.72256,
      "grad_norm": 0.007080398965626955,
      "learning_rate": 1.5183146666666667e-05,
      "loss": 0.0005,
      "step": 22580
    },
    {
      "epoch": 0.72288,
      "grad_norm": 0.02569607086479664,
      "learning_rate": 1.5181013333333335e-05,
      "loss": 0.0007,
      "step": 22590
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.007290617097169161,
      "learning_rate": 1.517888e-05,
      "loss": 0.0004,
      "step": 22600
    },
    {
      "epoch": 0.72352,
      "grad_norm": 0.004427412059158087,
      "learning_rate": 1.5176746666666669e-05,
      "loss": 0.0006,
      "step": 22610
    },
    {
      "epoch": 0.72384,
      "grad_norm": 0.009686636738479137,
      "learning_rate": 1.5174613333333334e-05,
      "loss": 0.0004,
      "step": 22620
    },
    {
      "epoch": 0.72416,
      "grad_norm": 0.012271246872842312,
      "learning_rate": 1.5172480000000001e-05,
      "loss": 0.0007,
      "step": 22630
    },
    {
      "epoch": 0.72448,
      "grad_norm": 0.023130645975470543,
      "learning_rate": 1.5170346666666668e-05,
      "loss": 0.0007,
      "step": 22640
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.009114867076277733,
      "learning_rate": 1.5168213333333335e-05,
      "loss": 0.0004,
      "step": 22650
    },
    {
      "epoch": 0.72512,
      "grad_norm": 0.010632535442709923,
      "learning_rate": 1.516608e-05,
      "loss": 0.0008,
      "step": 22660
    },
    {
      "epoch": 0.72544,
      "grad_norm": 0.01877271756529808,
      "learning_rate": 1.5163946666666666e-05,
      "loss": 0.0403,
      "step": 22670
    },
    {
      "epoch": 0.72576,
      "grad_norm": 0.005475280340760946,
      "learning_rate": 1.5161813333333335e-05,
      "loss": 0.0004,
      "step": 22680
    },
    {
      "epoch": 0.72608,
      "grad_norm": 0.00950050912797451,
      "learning_rate": 1.5159680000000002e-05,
      "loss": 0.0018,
      "step": 22690
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.0292983241379261,
      "learning_rate": 1.5157546666666667e-05,
      "loss": 0.0006,
      "step": 22700
    },
    {
      "epoch": 0.72672,
      "grad_norm": 0.012648846954107285,
      "learning_rate": 1.5155413333333336e-05,
      "loss": 0.0423,
      "step": 22710
    },
    {
      "epoch": 0.72704,
      "grad_norm": 0.01437341794371605,
      "learning_rate": 1.5153280000000001e-05,
      "loss": 0.0006,
      "step": 22720
    },
    {
      "epoch": 0.72736,
      "grad_norm": 0.01653245836496353,
      "learning_rate": 1.5151146666666667e-05,
      "loss": 0.0004,
      "step": 22730
    },
    {
      "epoch": 0.72768,
      "grad_norm": 0.003243278479203582,
      "learning_rate": 1.5149013333333336e-05,
      "loss": 0.0031,
      "step": 22740
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.0072404127568006516,
      "learning_rate": 1.5146880000000001e-05,
      "loss": 0.0003,
      "step": 22750
    },
    {
      "epoch": 0.72832,
      "grad_norm": 0.0073670148849487305,
      "learning_rate": 1.5144746666666668e-05,
      "loss": 0.0351,
      "step": 22760
    },
    {
      "epoch": 0.72864,
      "grad_norm": 0.013204121962189674,
      "learning_rate": 1.5142613333333333e-05,
      "loss": 0.0033,
      "step": 22770
    },
    {
      "epoch": 0.72896,
      "grad_norm": 0.009584427811205387,
      "learning_rate": 1.5140480000000002e-05,
      "loss": 0.0004,
      "step": 22780
    },
    {
      "epoch": 0.72928,
      "grad_norm": 0.30192145705223083,
      "learning_rate": 1.5138346666666668e-05,
      "loss": 0.0013,
      "step": 22790
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.0069018020294606686,
      "learning_rate": 1.5136213333333335e-05,
      "loss": 0.0004,
      "step": 22800
    },
    {
      "epoch": 0.72992,
      "grad_norm": 0.006255629938095808,
      "learning_rate": 1.5134080000000002e-05,
      "loss": 0.0006,
      "step": 22810
    },
    {
      "epoch": 0.73024,
      "grad_norm": 0.04568549990653992,
      "learning_rate": 1.5131946666666669e-05,
      "loss": 0.0003,
      "step": 22820
    },
    {
      "epoch": 0.73056,
      "grad_norm": 0.005593704525381327,
      "learning_rate": 1.5129813333333334e-05,
      "loss": 0.0003,
      "step": 22830
    },
    {
      "epoch": 0.73088,
      "grad_norm": 0.028371671214699745,
      "learning_rate": 1.512768e-05,
      "loss": 0.0005,
      "step": 22840
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.005173375364392996,
      "learning_rate": 1.5125546666666668e-05,
      "loss": 0.0186,
      "step": 22850
    },
    {
      "epoch": 0.73152,
      "grad_norm": 0.005813329480588436,
      "learning_rate": 1.5123413333333334e-05,
      "loss": 0.0003,
      "step": 22860
    },
    {
      "epoch": 0.73184,
      "grad_norm": 0.0076172794215381145,
      "learning_rate": 1.512128e-05,
      "loss": 0.0452,
      "step": 22870
    },
    {
      "epoch": 0.73216,
      "grad_norm": 0.0047310213558375835,
      "learning_rate": 1.511914666666667e-05,
      "loss": 0.0491,
      "step": 22880
    },
    {
      "epoch": 0.73248,
      "grad_norm": 0.005610882304608822,
      "learning_rate": 1.5117013333333335e-05,
      "loss": 0.009,
      "step": 22890
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.015378935262560844,
      "learning_rate": 1.511488e-05,
      "loss": 0.0004,
      "step": 22900
    },
    {
      "epoch": 0.73312,
      "grad_norm": 0.007735346909612417,
      "learning_rate": 1.5112746666666669e-05,
      "loss": 0.0003,
      "step": 22910
    },
    {
      "epoch": 0.73344,
      "grad_norm": 0.009845701046288013,
      "learning_rate": 1.5110613333333334e-05,
      "loss": 0.0004,
      "step": 22920
    },
    {
      "epoch": 0.73376,
      "grad_norm": 0.0038437179755419493,
      "learning_rate": 1.5108480000000001e-05,
      "loss": 0.007,
      "step": 22930
    },
    {
      "epoch": 0.73408,
      "grad_norm": 0.006373433396220207,
      "learning_rate": 1.5106346666666667e-05,
      "loss": 0.0003,
      "step": 22940
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.004883418790996075,
      "learning_rate": 1.5104213333333336e-05,
      "loss": 0.0495,
      "step": 22950
    },
    {
      "epoch": 0.73472,
      "grad_norm": 0.007243501488119364,
      "learning_rate": 1.5102080000000001e-05,
      "loss": 0.0004,
      "step": 22960
    },
    {
      "epoch": 0.73504,
      "grad_norm": 0.005760573782026768,
      "learning_rate": 1.5099946666666668e-05,
      "loss": 0.0016,
      "step": 22970
    },
    {
      "epoch": 0.73536,
      "grad_norm": 3.9926090240478516,
      "learning_rate": 1.5097813333333335e-05,
      "loss": 0.0171,
      "step": 22980
    },
    {
      "epoch": 0.73568,
      "grad_norm": 0.09783735871315002,
      "learning_rate": 1.5095680000000002e-05,
      "loss": 0.0005,
      "step": 22990
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.01324577908962965,
      "learning_rate": 1.5093546666666668e-05,
      "loss": 0.0005,
      "step": 23000
    },
    {
      "epoch": 0.73632,
      "grad_norm": 0.007267736364156008,
      "learning_rate": 1.5091413333333333e-05,
      "loss": 0.0003,
      "step": 23010
    },
    {
      "epoch": 0.73664,
      "grad_norm": 0.0682920515537262,
      "learning_rate": 1.5089280000000002e-05,
      "loss": 0.0004,
      "step": 23020
    },
    {
      "epoch": 0.73696,
      "grad_norm": 0.09346083551645279,
      "learning_rate": 1.5087146666666667e-05,
      "loss": 0.0048,
      "step": 23030
    },
    {
      "epoch": 0.73728,
      "grad_norm": 0.007002854719758034,
      "learning_rate": 1.5085013333333334e-05,
      "loss": 0.0151,
      "step": 23040
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.007703091949224472,
      "learning_rate": 1.5082880000000001e-05,
      "loss": 0.001,
      "step": 23050
    },
    {
      "epoch": 0.73792,
      "grad_norm": 0.006637695711106062,
      "learning_rate": 1.5080746666666668e-05,
      "loss": 0.0005,
      "step": 23060
    },
    {
      "epoch": 0.73824,
      "grad_norm": 0.6789957880973816,
      "learning_rate": 1.5078613333333334e-05,
      "loss": 0.0018,
      "step": 23070
    },
    {
      "epoch": 0.73856,
      "grad_norm": 0.009173383004963398,
      "learning_rate": 1.5076480000000002e-05,
      "loss": 0.0029,
      "step": 23080
    },
    {
      "epoch": 0.73888,
      "grad_norm": 0.00703813461586833,
      "learning_rate": 1.5074346666666668e-05,
      "loss": 0.0002,
      "step": 23090
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.0034327146131545305,
      "learning_rate": 1.5072213333333335e-05,
      "loss": 0.0002,
      "step": 23100
    },
    {
      "epoch": 0.73952,
      "grad_norm": 0.00586831197142601,
      "learning_rate": 1.507008e-05,
      "loss": 0.0004,
      "step": 23110
    },
    {
      "epoch": 0.73984,
      "grad_norm": 0.00484106270596385,
      "learning_rate": 1.5067946666666669e-05,
      "loss": 0.0003,
      "step": 23120
    },
    {
      "epoch": 0.74016,
      "grad_norm": 0.004982771817594767,
      "learning_rate": 1.5065813333333334e-05,
      "loss": 0.0003,
      "step": 23130
    },
    {
      "epoch": 0.74048,
      "grad_norm": 0.012533724308013916,
      "learning_rate": 1.506368e-05,
      "loss": 0.0169,
      "step": 23140
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.006441531237214804,
      "learning_rate": 1.5061546666666669e-05,
      "loss": 0.0018,
      "step": 23150
    },
    {
      "epoch": 0.74112,
      "grad_norm": 0.0036224822979420424,
      "learning_rate": 1.5059413333333336e-05,
      "loss": 0.0005,
      "step": 23160
    },
    {
      "epoch": 0.74144,
      "grad_norm": 0.0026752923149615526,
      "learning_rate": 1.5057280000000001e-05,
      "loss": 0.0385,
      "step": 23170
    },
    {
      "epoch": 0.74176,
      "grad_norm": 0.0052496688440442085,
      "learning_rate": 1.5055146666666666e-05,
      "loss": 0.0003,
      "step": 23180
    },
    {
      "epoch": 0.74208,
      "grad_norm": 0.009184247814118862,
      "learning_rate": 1.5053013333333335e-05,
      "loss": 0.0003,
      "step": 23190
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.10342131555080414,
      "learning_rate": 1.505088e-05,
      "loss": 0.0005,
      "step": 23200
    },
    {
      "epoch": 0.74272,
      "grad_norm": 0.003967660013586283,
      "learning_rate": 1.5048746666666668e-05,
      "loss": 0.0628,
      "step": 23210
    },
    {
      "epoch": 0.74304,
      "grad_norm": 0.006904344540089369,
      "learning_rate": 1.5046613333333335e-05,
      "loss": 0.0003,
      "step": 23220
    },
    {
      "epoch": 0.74336,
      "grad_norm": 0.005350644234567881,
      "learning_rate": 1.5044480000000002e-05,
      "loss": 0.0003,
      "step": 23230
    },
    {
      "epoch": 0.74368,
      "grad_norm": 0.004379752092063427,
      "learning_rate": 1.5042346666666667e-05,
      "loss": 0.0003,
      "step": 23240
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.008250623010098934,
      "learning_rate": 1.5040213333333336e-05,
      "loss": 0.0006,
      "step": 23250
    },
    {
      "epoch": 0.74432,
      "grad_norm": 0.00631836848333478,
      "learning_rate": 1.5038080000000001e-05,
      "loss": 0.0002,
      "step": 23260
    },
    {
      "epoch": 0.74464,
      "grad_norm": 0.003880083095282316,
      "learning_rate": 1.5035946666666668e-05,
      "loss": 0.0017,
      "step": 23270
    },
    {
      "epoch": 0.74496,
      "grad_norm": 0.00667070085182786,
      "learning_rate": 1.5033813333333334e-05,
      "loss": 0.0004,
      "step": 23280
    },
    {
      "epoch": 0.74528,
      "grad_norm": 1.0741041898727417,
      "learning_rate": 1.5031680000000002e-05,
      "loss": 0.0047,
      "step": 23290
    },
    {
      "epoch": 0.7456,
      "grad_norm": 4.4368696212768555,
      "learning_rate": 1.5029546666666668e-05,
      "loss": 0.0359,
      "step": 23300
    },
    {
      "epoch": 0.74592,
      "grad_norm": 0.002415020950138569,
      "learning_rate": 1.5027413333333333e-05,
      "loss": 0.0004,
      "step": 23310
    },
    {
      "epoch": 0.74624,
      "grad_norm": 0.00637067761272192,
      "learning_rate": 1.5025280000000002e-05,
      "loss": 0.0426,
      "step": 23320
    },
    {
      "epoch": 0.74656,
      "grad_norm": 0.006889516953378916,
      "learning_rate": 1.5023146666666667e-05,
      "loss": 0.0018,
      "step": 23330
    },
    {
      "epoch": 0.74688,
      "grad_norm": 0.014166587963700294,
      "learning_rate": 1.5021013333333334e-05,
      "loss": 0.0648,
      "step": 23340
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.00639719795435667,
      "learning_rate": 1.501888e-05,
      "loss": 0.0005,
      "step": 23350
    },
    {
      "epoch": 0.74752,
      "grad_norm": 0.005699200555682182,
      "learning_rate": 1.5016746666666669e-05,
      "loss": 0.0006,
      "step": 23360
    },
    {
      "epoch": 0.74784,
      "grad_norm": 0.007197046186774969,
      "learning_rate": 1.5014613333333334e-05,
      "loss": 0.0038,
      "step": 23370
    },
    {
      "epoch": 0.74816,
      "grad_norm": 3.014951705932617,
      "learning_rate": 1.5012480000000001e-05,
      "loss": 0.0858,
      "step": 23380
    },
    {
      "epoch": 0.74848,
      "grad_norm": 0.007746328599750996,
      "learning_rate": 1.5010346666666668e-05,
      "loss": 0.0003,
      "step": 23390
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.037790533155202866,
      "learning_rate": 1.5008213333333335e-05,
      "loss": 0.028,
      "step": 23400
    },
    {
      "epoch": 0.74912,
      "grad_norm": 0.010192999616265297,
      "learning_rate": 1.500608e-05,
      "loss": 0.0003,
      "step": 23410
    },
    {
      "epoch": 0.74944,
      "grad_norm": 0.011020230129361153,
      "learning_rate": 1.500394666666667e-05,
      "loss": 0.0004,
      "step": 23420
    },
    {
      "epoch": 0.74976,
      "grad_norm": 0.03290150687098503,
      "learning_rate": 1.5001813333333335e-05,
      "loss": 0.0438,
      "step": 23430
    },
    {
      "epoch": 0.75008,
      "grad_norm": 0.004965605214238167,
      "learning_rate": 1.4999680000000002e-05,
      "loss": 0.0005,
      "step": 23440
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.009152967482805252,
      "learning_rate": 1.4997546666666667e-05,
      "loss": 0.0003,
      "step": 23450
    },
    {
      "epoch": 0.75072,
      "grad_norm": 0.004643394146114588,
      "learning_rate": 1.4995413333333336e-05,
      "loss": 0.0005,
      "step": 23460
    },
    {
      "epoch": 0.75104,
      "grad_norm": 0.02696854993700981,
      "learning_rate": 1.4993280000000001e-05,
      "loss": 0.0601,
      "step": 23470
    },
    {
      "epoch": 0.75136,
      "grad_norm": 0.009444605559110641,
      "learning_rate": 1.4991146666666667e-05,
      "loss": 0.0003,
      "step": 23480
    },
    {
      "epoch": 0.75168,
      "grad_norm": 0.023920467123389244,
      "learning_rate": 1.4989013333333335e-05,
      "loss": 0.0005,
      "step": 23490
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.07411074638366699,
      "learning_rate": 1.498688e-05,
      "loss": 0.0031,
      "step": 23500
    },
    {
      "epoch": 0.75232,
      "grad_norm": 0.004177604336291552,
      "learning_rate": 1.4984746666666668e-05,
      "loss": 0.041,
      "step": 23510
    },
    {
      "epoch": 0.75264,
      "grad_norm": 0.2526058256626129,
      "learning_rate": 1.4982613333333333e-05,
      "loss": 0.005,
      "step": 23520
    },
    {
      "epoch": 0.75296,
      "grad_norm": 0.010484659112989902,
      "learning_rate": 1.4980480000000002e-05,
      "loss": 0.0005,
      "step": 23530
    },
    {
      "epoch": 0.75328,
      "grad_norm": 0.015260225161910057,
      "learning_rate": 1.4978346666666667e-05,
      "loss": 0.0004,
      "step": 23540
    },
    {
      "epoch": 0.7536,
      "grad_norm": 4.55979061126709,
      "learning_rate": 1.4976213333333334e-05,
      "loss": 0.032,
      "step": 23550
    },
    {
      "epoch": 0.75392,
      "grad_norm": 0.0062665631994605064,
      "learning_rate": 1.4974080000000001e-05,
      "loss": 0.0035,
      "step": 23560
    },
    {
      "epoch": 0.75424,
      "grad_norm": 0.005202160216867924,
      "learning_rate": 1.4971946666666669e-05,
      "loss": 0.0004,
      "step": 23570
    },
    {
      "epoch": 0.75456,
      "grad_norm": 0.004860322922468185,
      "learning_rate": 1.4969813333333334e-05,
      "loss": 0.0003,
      "step": 23580
    },
    {
      "epoch": 0.75488,
      "grad_norm": 0.012165160849690437,
      "learning_rate": 1.4967680000000003e-05,
      "loss": 0.0156,
      "step": 23590
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.023915760219097137,
      "learning_rate": 1.4965546666666668e-05,
      "loss": 0.0004,
      "step": 23600
    },
    {
      "epoch": 0.75552,
      "grad_norm": 0.009325740858912468,
      "learning_rate": 1.4963413333333333e-05,
      "loss": 0.0005,
      "step": 23610
    },
    {
      "epoch": 0.75584,
      "grad_norm": 0.00831026304513216,
      "learning_rate": 1.496128e-05,
      "loss": 0.0003,
      "step": 23620
    },
    {
      "epoch": 0.75616,
      "grad_norm": 0.004881519358605146,
      "learning_rate": 1.495914666666667e-05,
      "loss": 0.0006,
      "step": 23630
    },
    {
      "epoch": 0.75648,
      "grad_norm": 0.04037395492196083,
      "learning_rate": 1.4957013333333335e-05,
      "loss": 0.0009,
      "step": 23640
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.007643245626240969,
      "learning_rate": 1.495488e-05,
      "loss": 0.0115,
      "step": 23650
    },
    {
      "epoch": 0.75712,
      "grad_norm": 0.003289185930043459,
      "learning_rate": 1.4952746666666669e-05,
      "loss": 0.0425,
      "step": 23660
    },
    {
      "epoch": 0.75744,
      "grad_norm": 0.0038341409526765347,
      "learning_rate": 1.4950613333333334e-05,
      "loss": 0.0016,
      "step": 23670
    },
    {
      "epoch": 0.75776,
      "grad_norm": 0.006979398429393768,
      "learning_rate": 1.4948480000000001e-05,
      "loss": 0.0343,
      "step": 23680
    },
    {
      "epoch": 0.75808,
      "grad_norm": 0.010167248547077179,
      "learning_rate": 1.4946346666666667e-05,
      "loss": 0.0047,
      "step": 23690
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.004174449946731329,
      "learning_rate": 1.4944213333333335e-05,
      "loss": 0.0004,
      "step": 23700
    },
    {
      "epoch": 0.75872,
      "grad_norm": 0.008985108695924282,
      "learning_rate": 1.494208e-05,
      "loss": 0.0003,
      "step": 23710
    },
    {
      "epoch": 0.75904,
      "grad_norm": 0.00942122284322977,
      "learning_rate": 1.4939946666666668e-05,
      "loss": 0.02,
      "step": 23720
    },
    {
      "epoch": 0.75936,
      "grad_norm": 0.00518441665917635,
      "learning_rate": 1.4937813333333335e-05,
      "loss": 0.0004,
      "step": 23730
    },
    {
      "epoch": 0.75968,
      "grad_norm": 0.006699107587337494,
      "learning_rate": 1.4935680000000002e-05,
      "loss": 0.0387,
      "step": 23740
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.004199457820504904,
      "learning_rate": 1.4933546666666667e-05,
      "loss": 0.0009,
      "step": 23750
    },
    {
      "epoch": 0.76032,
      "grad_norm": 0.004646286368370056,
      "learning_rate": 1.4931413333333336e-05,
      "loss": 0.0005,
      "step": 23760
    },
    {
      "epoch": 0.76064,
      "grad_norm": 0.007301639299839735,
      "learning_rate": 1.4929280000000002e-05,
      "loss": 0.0064,
      "step": 23770
    },
    {
      "epoch": 0.76096,
      "grad_norm": 0.005741803906857967,
      "learning_rate": 1.4927146666666667e-05,
      "loss": 0.0226,
      "step": 23780
    },
    {
      "epoch": 0.76128,
      "grad_norm": 0.008940059691667557,
      "learning_rate": 1.4925013333333334e-05,
      "loss": 0.0007,
      "step": 23790
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.003274280810728669,
      "learning_rate": 1.4922880000000001e-05,
      "loss": 0.0005,
      "step": 23800
    },
    {
      "epoch": 0.76192,
      "grad_norm": 0.009159989655017853,
      "learning_rate": 1.4920746666666668e-05,
      "loss": 0.0494,
      "step": 23810
    },
    {
      "epoch": 0.76224,
      "grad_norm": 0.008701899088919163,
      "learning_rate": 1.4918613333333333e-05,
      "loss": 0.0004,
      "step": 23820
    },
    {
      "epoch": 0.76256,
      "grad_norm": 0.0047410642728209496,
      "learning_rate": 1.4916480000000002e-05,
      "loss": 0.0014,
      "step": 23830
    },
    {
      "epoch": 0.76288,
      "grad_norm": 0.09505487978458405,
      "learning_rate": 1.4914346666666668e-05,
      "loss": 0.0005,
      "step": 23840
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.008582892827689648,
      "learning_rate": 1.4912213333333335e-05,
      "loss": 0.0137,
      "step": 23850
    },
    {
      "epoch": 0.76352,
      "grad_norm": 0.6020164489746094,
      "learning_rate": 1.491008e-05,
      "loss": 0.0018,
      "step": 23860
    },
    {
      "epoch": 0.76384,
      "grad_norm": 0.012750295922160149,
      "learning_rate": 1.4907946666666669e-05,
      "loss": 0.0006,
      "step": 23870
    },
    {
      "epoch": 0.76416,
      "grad_norm": 0.02704533003270626,
      "learning_rate": 1.4905813333333334e-05,
      "loss": 0.0063,
      "step": 23880
    },
    {
      "epoch": 0.76448,
      "grad_norm": 0.013968131504952908,
      "learning_rate": 1.490368e-05,
      "loss": 0.0004,
      "step": 23890
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.0053936452604830265,
      "learning_rate": 1.4901546666666668e-05,
      "loss": 0.0005,
      "step": 23900
    },
    {
      "epoch": 0.76512,
      "grad_norm": 0.04476012662053108,
      "learning_rate": 1.4899413333333335e-05,
      "loss": 0.0323,
      "step": 23910
    },
    {
      "epoch": 0.76544,
      "grad_norm": 0.004835784435272217,
      "learning_rate": 1.489728e-05,
      "loss": 0.0147,
      "step": 23920
    },
    {
      "epoch": 0.76576,
      "grad_norm": 3.9597980976104736,
      "learning_rate": 1.489514666666667e-05,
      "loss": 0.0126,
      "step": 23930
    },
    {
      "epoch": 0.76608,
      "grad_norm": 0.013097506947815418,
      "learning_rate": 1.4893013333333335e-05,
      "loss": 0.0008,
      "step": 23940
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.008219663053750992,
      "learning_rate": 1.489088e-05,
      "loss": 0.0005,
      "step": 23950
    },
    {
      "epoch": 0.76672,
      "grad_norm": 0.00522593455389142,
      "learning_rate": 1.4888746666666667e-05,
      "loss": 0.0081,
      "step": 23960
    },
    {
      "epoch": 0.76704,
      "grad_norm": 0.006004386581480503,
      "learning_rate": 1.4886613333333334e-05,
      "loss": 0.0003,
      "step": 23970
    },
    {
      "epoch": 0.76736,
      "grad_norm": 0.018987400457262993,
      "learning_rate": 1.4884480000000002e-05,
      "loss": 0.0003,
      "step": 23980
    },
    {
      "epoch": 0.76768,
      "grad_norm": 0.005949296988546848,
      "learning_rate": 1.4882346666666667e-05,
      "loss": 0.0004,
      "step": 23990
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.0062486412934958935,
      "learning_rate": 1.4880213333333336e-05,
      "loss": 0.0005,
      "step": 24000
    },
    {
      "epoch": 0.76832,
      "grad_norm": 0.01909574866294861,
      "learning_rate": 1.4878080000000001e-05,
      "loss": 0.0006,
      "step": 24010
    },
    {
      "epoch": 0.76864,
      "grad_norm": 0.5331904888153076,
      "learning_rate": 1.4875946666666668e-05,
      "loss": 0.0196,
      "step": 24020
    },
    {
      "epoch": 0.76896,
      "grad_norm": 0.014809620566666126,
      "learning_rate": 1.4873813333333335e-05,
      "loss": 0.001,
      "step": 24030
    },
    {
      "epoch": 0.76928,
      "grad_norm": 5.939159393310547,
      "learning_rate": 1.4871680000000002e-05,
      "loss": 0.0565,
      "step": 24040
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.0059358468279242516,
      "learning_rate": 1.4869546666666668e-05,
      "loss": 0.0003,
      "step": 24050
    },
    {
      "epoch": 0.76992,
      "grad_norm": 0.634605348110199,
      "learning_rate": 1.4867413333333333e-05,
      "loss": 0.0022,
      "step": 24060
    },
    {
      "epoch": 0.77024,
      "grad_norm": 2.964205265045166,
      "learning_rate": 1.4865280000000002e-05,
      "loss": 0.0166,
      "step": 24070
    },
    {
      "epoch": 0.77056,
      "grad_norm": 0.02122538350522518,
      "learning_rate": 1.4863146666666667e-05,
      "loss": 0.0004,
      "step": 24080
    },
    {
      "epoch": 0.77088,
      "grad_norm": 0.007771217729896307,
      "learning_rate": 1.4861013333333334e-05,
      "loss": 0.0003,
      "step": 24090
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.023899871855974197,
      "learning_rate": 1.4858880000000003e-05,
      "loss": 0.0193,
      "step": 24100
    },
    {
      "epoch": 0.77152,
      "grad_norm": 0.004560369998216629,
      "learning_rate": 1.4856746666666668e-05,
      "loss": 0.0008,
      "step": 24110
    },
    {
      "epoch": 0.77184,
      "grad_norm": 0.0031840279698371887,
      "learning_rate": 1.4854613333333334e-05,
      "loss": 0.0004,
      "step": 24120
    },
    {
      "epoch": 0.77216,
      "grad_norm": 0.006171771325170994,
      "learning_rate": 1.485248e-05,
      "loss": 0.0007,
      "step": 24130
    },
    {
      "epoch": 0.77248,
      "grad_norm": 0.006345116999000311,
      "learning_rate": 1.4850346666666668e-05,
      "loss": 0.0356,
      "step": 24140
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.007408568169921637,
      "learning_rate": 1.4848213333333335e-05,
      "loss": 0.0507,
      "step": 24150
    },
    {
      "epoch": 0.77312,
      "grad_norm": 0.005199773702770472,
      "learning_rate": 1.484608e-05,
      "loss": 0.0036,
      "step": 24160
    },
    {
      "epoch": 0.77344,
      "grad_norm": 0.004709620960056782,
      "learning_rate": 1.4843946666666669e-05,
      "loss": 0.0065,
      "step": 24170
    },
    {
      "epoch": 0.77376,
      "grad_norm": 0.007863802835345268,
      "learning_rate": 1.4841813333333334e-05,
      "loss": 0.0065,
      "step": 24180
    },
    {
      "epoch": 0.77408,
      "grad_norm": 0.18673405051231384,
      "learning_rate": 1.4839680000000002e-05,
      "loss": 0.001,
      "step": 24190
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.019352762028574944,
      "learning_rate": 1.4837546666666669e-05,
      "loss": 0.0005,
      "step": 24200
    },
    {
      "epoch": 0.77472,
      "grad_norm": 0.013744713738560677,
      "learning_rate": 1.4835413333333336e-05,
      "loss": 0.0037,
      "step": 24210
    },
    {
      "epoch": 0.77504,
      "grad_norm": 0.00599877443164587,
      "learning_rate": 1.4833280000000001e-05,
      "loss": 0.0004,
      "step": 24220
    },
    {
      "epoch": 0.77536,
      "grad_norm": 4.386270523071289,
      "learning_rate": 1.4831146666666666e-05,
      "loss": 0.0647,
      "step": 24230
    },
    {
      "epoch": 0.77568,
      "grad_norm": 0.012936056591570377,
      "learning_rate": 1.4829013333333335e-05,
      "loss": 0.0006,
      "step": 24240
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.010195277631282806,
      "learning_rate": 1.482688e-05,
      "loss": 0.0005,
      "step": 24250
    },
    {
      "epoch": 0.77632,
      "grad_norm": 0.004740435630083084,
      "learning_rate": 1.4824746666666668e-05,
      "loss": 0.0014,
      "step": 24260
    },
    {
      "epoch": 0.77664,
      "grad_norm": 0.005099376663565636,
      "learning_rate": 1.4822613333333335e-05,
      "loss": 0.0003,
      "step": 24270
    },
    {
      "epoch": 0.77696,
      "grad_norm": 0.008966382592916489,
      "learning_rate": 1.4820480000000002e-05,
      "loss": 0.0004,
      "step": 24280
    },
    {
      "epoch": 0.77728,
      "grad_norm": 0.009380615316331387,
      "learning_rate": 1.4818346666666667e-05,
      "loss": 0.0005,
      "step": 24290
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.008663943968713284,
      "learning_rate": 1.4816213333333334e-05,
      "loss": 0.001,
      "step": 24300
    },
    {
      "epoch": 0.77792,
      "grad_norm": 0.010607586242258549,
      "learning_rate": 1.4814080000000001e-05,
      "loss": 0.0071,
      "step": 24310
    },
    {
      "epoch": 0.77824,
      "grad_norm": 0.008727896958589554,
      "learning_rate": 1.4811946666666668e-05,
      "loss": 0.0004,
      "step": 24320
    },
    {
      "epoch": 0.77856,
      "grad_norm": 0.010067751631140709,
      "learning_rate": 1.4809813333333334e-05,
      "loss": 0.002,
      "step": 24330
    },
    {
      "epoch": 0.77888,
      "grad_norm": 0.0028758568223565817,
      "learning_rate": 1.4807680000000003e-05,
      "loss": 0.053,
      "step": 24340
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.00879490002989769,
      "learning_rate": 1.4805546666666668e-05,
      "loss": 0.0006,
      "step": 24350
    },
    {
      "epoch": 0.77952,
      "grad_norm": 0.01737263984978199,
      "learning_rate": 1.4803413333333333e-05,
      "loss": 0.0009,
      "step": 24360
    },
    {
      "epoch": 0.77984,
      "grad_norm": 0.0048322370275855064,
      "learning_rate": 1.4801280000000002e-05,
      "loss": 0.0007,
      "step": 24370
    },
    {
      "epoch": 0.78016,
      "grad_norm": 0.0076964301988482475,
      "learning_rate": 1.4799146666666669e-05,
      "loss": 0.0004,
      "step": 24380
    },
    {
      "epoch": 0.78048,
      "grad_norm": 0.006167770829051733,
      "learning_rate": 1.4797013333333335e-05,
      "loss": 0.0003,
      "step": 24390
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.005661969073116779,
      "learning_rate": 1.479488e-05,
      "loss": 0.0003,
      "step": 24400
    },
    {
      "epoch": 0.78112,
      "grad_norm": 0.009448175318539143,
      "learning_rate": 1.4792746666666669e-05,
      "loss": 0.0003,
      "step": 24410
    },
    {
      "epoch": 0.78144,
      "grad_norm": 0.00419126870110631,
      "learning_rate": 1.4790613333333334e-05,
      "loss": 0.0554,
      "step": 24420
    },
    {
      "epoch": 0.78176,
      "grad_norm": 0.004876499529927969,
      "learning_rate": 1.4788480000000001e-05,
      "loss": 0.0289,
      "step": 24430
    },
    {
      "epoch": 0.78208,
      "grad_norm": 0.006429504137486219,
      "learning_rate": 1.4786346666666668e-05,
      "loss": 0.0005,
      "step": 24440
    },
    {
      "epoch": 0.7824,
      "grad_norm": 3.0106165409088135,
      "learning_rate": 1.4784213333333335e-05,
      "loss": 0.0068,
      "step": 24450
    },
    {
      "epoch": 0.78272,
      "grad_norm": 0.006538152229040861,
      "learning_rate": 1.478208e-05,
      "loss": 0.0003,
      "step": 24460
    },
    {
      "epoch": 0.78304,
      "grad_norm": 0.07742297649383545,
      "learning_rate": 1.4779946666666668e-05,
      "loss": 0.0821,
      "step": 24470
    },
    {
      "epoch": 0.78336,
      "grad_norm": 0.0058842315338552,
      "learning_rate": 1.4777813333333335e-05,
      "loss": 0.0493,
      "step": 24480
    },
    {
      "epoch": 0.78368,
      "grad_norm": 0.009605452418327332,
      "learning_rate": 1.4775680000000002e-05,
      "loss": 0.0008,
      "step": 24490
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.007693585939705372,
      "learning_rate": 1.4773546666666667e-05,
      "loss": 0.001,
      "step": 24500
    },
    {
      "epoch": 0.78432,
      "grad_norm": 0.007693121675401926,
      "learning_rate": 1.4771413333333336e-05,
      "loss": 0.0222,
      "step": 24510
    },
    {
      "epoch": 0.78464,
      "grad_norm": 0.008282054215669632,
      "learning_rate": 1.4769280000000001e-05,
      "loss": 0.0188,
      "step": 24520
    },
    {
      "epoch": 0.78496,
      "grad_norm": 0.012059282511472702,
      "learning_rate": 1.4767146666666667e-05,
      "loss": 0.0023,
      "step": 24530
    },
    {
      "epoch": 0.78528,
      "grad_norm": 0.0035200200509279966,
      "learning_rate": 1.4765013333333335e-05,
      "loss": 0.0015,
      "step": 24540
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.006989176385104656,
      "learning_rate": 1.4762880000000001e-05,
      "loss": 0.0005,
      "step": 24550
    },
    {
      "epoch": 0.78592,
      "grad_norm": 0.0037787286564707756,
      "learning_rate": 1.4760746666666668e-05,
      "loss": 0.0004,
      "step": 24560
    },
    {
      "epoch": 0.78624,
      "grad_norm": 0.005097902379930019,
      "learning_rate": 1.4758613333333333e-05,
      "loss": 0.0003,
      "step": 24570
    },
    {
      "epoch": 0.78656,
      "grad_norm": 0.00934410747140646,
      "learning_rate": 1.4756480000000002e-05,
      "loss": 0.0586,
      "step": 24580
    },
    {
      "epoch": 0.78688,
      "grad_norm": 0.008486852049827576,
      "learning_rate": 1.4754346666666667e-05,
      "loss": 0.0005,
      "step": 24590
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.004353917669504881,
      "learning_rate": 1.4752213333333335e-05,
      "loss": 0.0003,
      "step": 24600
    },
    {
      "epoch": 0.78752,
      "grad_norm": 2.1338844299316406,
      "learning_rate": 1.4750080000000002e-05,
      "loss": 0.0273,
      "step": 24610
    },
    {
      "epoch": 0.78784,
      "grad_norm": 0.005299747921526432,
      "learning_rate": 1.4747946666666669e-05,
      "loss": 0.0004,
      "step": 24620
    },
    {
      "epoch": 0.78816,
      "grad_norm": 0.04905460774898529,
      "learning_rate": 1.4745813333333334e-05,
      "loss": 0.0005,
      "step": 24630
    },
    {
      "epoch": 0.78848,
      "grad_norm": 0.010879075154662132,
      "learning_rate": 1.474368e-05,
      "loss": 0.0018,
      "step": 24640
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.501166582107544,
      "learning_rate": 1.4741546666666668e-05,
      "loss": 0.0078,
      "step": 24650
    },
    {
      "epoch": 0.78912,
      "grad_norm": 0.010215125046670437,
      "learning_rate": 1.4739413333333335e-05,
      "loss": 0.0003,
      "step": 24660
    },
    {
      "epoch": 0.78944,
      "grad_norm": 1.5102276802062988,
      "learning_rate": 1.473728e-05,
      "loss": 0.0023,
      "step": 24670
    },
    {
      "epoch": 0.78976,
      "grad_norm": 0.009823017753660679,
      "learning_rate": 1.473514666666667e-05,
      "loss": 0.0003,
      "step": 24680
    },
    {
      "epoch": 0.79008,
      "grad_norm": 0.011121310293674469,
      "learning_rate": 1.4733013333333335e-05,
      "loss": 0.0017,
      "step": 24690
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.003653100924566388,
      "learning_rate": 1.473088e-05,
      "loss": 0.0004,
      "step": 24700
    },
    {
      "epoch": 0.79072,
      "grad_norm": 0.004647460300475359,
      "learning_rate": 1.4728746666666669e-05,
      "loss": 0.0006,
      "step": 24710
    },
    {
      "epoch": 0.79104,
      "grad_norm": 0.11042995750904083,
      "learning_rate": 1.4726613333333334e-05,
      "loss": 0.0012,
      "step": 24720
    },
    {
      "epoch": 0.79136,
      "grad_norm": 0.006288428790867329,
      "learning_rate": 1.4724480000000001e-05,
      "loss": 0.0006,
      "step": 24730
    },
    {
      "epoch": 0.79168,
      "grad_norm": 0.0032671578228473663,
      "learning_rate": 1.4722346666666667e-05,
      "loss": 0.043,
      "step": 24740
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.005729529075324535,
      "learning_rate": 1.4720213333333336e-05,
      "loss": 0.0011,
      "step": 24750
    },
    {
      "epoch": 0.79232,
      "grad_norm": 0.05666675046086311,
      "learning_rate": 1.4718080000000001e-05,
      "loss": 0.0003,
      "step": 24760
    },
    {
      "epoch": 0.79264,
      "grad_norm": 0.00917765498161316,
      "learning_rate": 1.4715946666666668e-05,
      "loss": 0.0003,
      "step": 24770
    },
    {
      "epoch": 0.79296,
      "grad_norm": 0.004954391159117222,
      "learning_rate": 1.4713813333333335e-05,
      "loss": 0.0008,
      "step": 24780
    },
    {
      "epoch": 0.79328,
      "grad_norm": 0.004422618076205254,
      "learning_rate": 1.4711680000000002e-05,
      "loss": 0.0003,
      "step": 24790
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.0036478405818343163,
      "learning_rate": 1.4709546666666667e-05,
      "loss": 0.0173,
      "step": 24800
    },
    {
      "epoch": 0.79392,
      "grad_norm": 0.00298679294064641,
      "learning_rate": 1.4707413333333333e-05,
      "loss": 0.048,
      "step": 24810
    },
    {
      "epoch": 0.79424,
      "grad_norm": 0.0050445375964045525,
      "learning_rate": 1.4705280000000002e-05,
      "loss": 0.0006,
      "step": 24820
    },
    {
      "epoch": 0.79456,
      "grad_norm": 0.005561176221817732,
      "learning_rate": 1.4703146666666667e-05,
      "loss": 0.0006,
      "step": 24830
    },
    {
      "epoch": 0.79488,
      "grad_norm": 0.009679258801043034,
      "learning_rate": 1.4701013333333334e-05,
      "loss": 0.0003,
      "step": 24840
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.009515278041362762,
      "learning_rate": 1.4698880000000003e-05,
      "loss": 0.0003,
      "step": 24850
    },
    {
      "epoch": 0.79552,
      "grad_norm": 3.478323459625244,
      "learning_rate": 1.4696746666666668e-05,
      "loss": 0.0997,
      "step": 24860
    },
    {
      "epoch": 0.79584,
      "grad_norm": 0.006779974326491356,
      "learning_rate": 1.4694613333333334e-05,
      "loss": 0.0003,
      "step": 24870
    },
    {
      "epoch": 0.79616,
      "grad_norm": 0.016493402421474457,
      "learning_rate": 1.4692480000000002e-05,
      "loss": 0.0006,
      "step": 24880
    },
    {
      "epoch": 0.79648,
      "grad_norm": 0.12005753815174103,
      "learning_rate": 1.4690346666666668e-05,
      "loss": 0.0007,
      "step": 24890
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.1291808933019638,
      "learning_rate": 1.4688213333333335e-05,
      "loss": 0.0253,
      "step": 24900
    },
    {
      "epoch": 0.79712,
      "grad_norm": 0.054951827973127365,
      "learning_rate": 1.468608e-05,
      "loss": 0.0005,
      "step": 24910
    },
    {
      "epoch": 0.79744,
      "grad_norm": 0.006220545154064894,
      "learning_rate": 1.4683946666666669e-05,
      "loss": 0.0081,
      "step": 24920
    },
    {
      "epoch": 0.79776,
      "grad_norm": 0.004927660804241896,
      "learning_rate": 1.4681813333333334e-05,
      "loss": 0.0004,
      "step": 24930
    },
    {
      "epoch": 0.79808,
      "grad_norm": 0.007145551033318043,
      "learning_rate": 1.4679680000000001e-05,
      "loss": 0.0004,
      "step": 24940
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.029866283759474754,
      "learning_rate": 1.4677546666666668e-05,
      "loss": 0.0723,
      "step": 24950
    },
    {
      "epoch": 0.79872,
      "grad_norm": 0.024887340143322945,
      "learning_rate": 1.4675413333333336e-05,
      "loss": 0.0074,
      "step": 24960
    },
    {
      "epoch": 0.79904,
      "grad_norm": 0.0065990472212433815,
      "learning_rate": 1.4673280000000001e-05,
      "loss": 0.001,
      "step": 24970
    },
    {
      "epoch": 0.79936,
      "grad_norm": 0.0420149490237236,
      "learning_rate": 1.4671146666666666e-05,
      "loss": 0.0004,
      "step": 24980
    },
    {
      "epoch": 0.79968,
      "grad_norm": 0.007408187724649906,
      "learning_rate": 1.4669013333333335e-05,
      "loss": 0.0114,
      "step": 24990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.03605630248785019,
      "learning_rate": 1.466688e-05,
      "loss": 0.0502,
      "step": 25000
    },
    {
      "epoch": 0.80032,
      "grad_norm": 0.06421563774347305,
      "learning_rate": 1.4664746666666667e-05,
      "loss": 0.0005,
      "step": 25010
    },
    {
      "epoch": 0.80064,
      "grad_norm": 0.015334759838879108,
      "learning_rate": 1.4662613333333335e-05,
      "loss": 0.0008,
      "step": 25020
    },
    {
      "epoch": 0.80096,
      "grad_norm": 0.017632808536291122,
      "learning_rate": 1.4660480000000002e-05,
      "loss": 0.0005,
      "step": 25030
    },
    {
      "epoch": 0.80128,
      "grad_norm": 0.00897501315921545,
      "learning_rate": 1.4658346666666667e-05,
      "loss": 0.0023,
      "step": 25040
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.005619424395263195,
      "learning_rate": 1.4656213333333336e-05,
      "loss": 0.0016,
      "step": 25050
    },
    {
      "epoch": 0.80192,
      "grad_norm": 0.006411267910152674,
      "learning_rate": 1.4654080000000001e-05,
      "loss": 0.0003,
      "step": 25060
    },
    {
      "epoch": 0.80224,
      "grad_norm": 0.015295987948775291,
      "learning_rate": 1.4651946666666668e-05,
      "loss": 0.0389,
      "step": 25070
    },
    {
      "epoch": 0.80256,
      "grad_norm": 0.005986078176647425,
      "learning_rate": 1.4649813333333334e-05,
      "loss": 0.0007,
      "step": 25080
    },
    {
      "epoch": 0.80288,
      "grad_norm": 0.0055546835064888,
      "learning_rate": 1.4647680000000002e-05,
      "loss": 0.0013,
      "step": 25090
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.01869920641183853,
      "learning_rate": 1.4645546666666668e-05,
      "loss": 0.0005,
      "step": 25100
    },
    {
      "epoch": 0.80352,
      "grad_norm": 0.008129525929689407,
      "learning_rate": 1.4643413333333333e-05,
      "loss": 0.0006,
      "step": 25110
    },
    {
      "epoch": 0.80384,
      "grad_norm": 0.027183745056390762,
      "learning_rate": 1.4641280000000002e-05,
      "loss": 0.0009,
      "step": 25120
    },
    {
      "epoch": 0.80416,
      "grad_norm": 0.025239046663045883,
      "learning_rate": 1.4639146666666669e-05,
      "loss": 0.0007,
      "step": 25130
    },
    {
      "epoch": 0.80448,
      "grad_norm": 0.0075747850351035595,
      "learning_rate": 1.4637013333333334e-05,
      "loss": 0.0023,
      "step": 25140
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.049729399383068085,
      "learning_rate": 1.463488e-05,
      "loss": 0.001,
      "step": 25150
    },
    {
      "epoch": 0.80512,
      "grad_norm": 0.006097342818975449,
      "learning_rate": 1.4632746666666668e-05,
      "loss": 0.0003,
      "step": 25160
    },
    {
      "epoch": 0.80544,
      "grad_norm": 0.006330110132694244,
      "learning_rate": 1.4630613333333334e-05,
      "loss": 0.0003,
      "step": 25170
    },
    {
      "epoch": 0.80576,
      "grad_norm": 0.009173114784061909,
      "learning_rate": 1.4628480000000001e-05,
      "loss": 0.0007,
      "step": 25180
    },
    {
      "epoch": 0.80608,
      "grad_norm": 0.4734523296356201,
      "learning_rate": 1.4626346666666668e-05,
      "loss": 0.004,
      "step": 25190
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.0037188956048339605,
      "learning_rate": 1.4624213333333335e-05,
      "loss": 0.0014,
      "step": 25200
    },
    {
      "epoch": 0.80672,
      "grad_norm": 0.009408453479409218,
      "learning_rate": 1.462208e-05,
      "loss": 0.0005,
      "step": 25210
    },
    {
      "epoch": 0.80704,
      "grad_norm": 0.004418613389134407,
      "learning_rate": 1.461994666666667e-05,
      "loss": 0.0007,
      "step": 25220
    },
    {
      "epoch": 0.80736,
      "grad_norm": 0.011512026190757751,
      "learning_rate": 1.4617813333333335e-05,
      "loss": 0.0003,
      "step": 25230
    },
    {
      "epoch": 0.80768,
      "grad_norm": 0.004265964031219482,
      "learning_rate": 1.4615680000000002e-05,
      "loss": 0.0006,
      "step": 25240
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.006810079328715801,
      "learning_rate": 1.4613546666666667e-05,
      "loss": 0.0434,
      "step": 25250
    },
    {
      "epoch": 0.80832,
      "grad_norm": 0.03148351609706879,
      "learning_rate": 1.4611413333333336e-05,
      "loss": 0.0004,
      "step": 25260
    },
    {
      "epoch": 0.80864,
      "grad_norm": 0.02752183936536312,
      "learning_rate": 1.4609280000000001e-05,
      "loss": 0.0004,
      "step": 25270
    },
    {
      "epoch": 0.80896,
      "grad_norm": 0.013992833904922009,
      "learning_rate": 1.4607146666666667e-05,
      "loss": 0.0006,
      "step": 25280
    },
    {
      "epoch": 0.80928,
      "grad_norm": 0.013630026020109653,
      "learning_rate": 1.4605013333333335e-05,
      "loss": 0.0502,
      "step": 25290
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.0026245638728141785,
      "learning_rate": 1.460288e-05,
      "loss": 0.0008,
      "step": 25300
    },
    {
      "epoch": 0.80992,
      "grad_norm": 0.005087028257548809,
      "learning_rate": 1.4600746666666668e-05,
      "loss": 0.0011,
      "step": 25310
    },
    {
      "epoch": 0.81024,
      "grad_norm": 0.02399999462068081,
      "learning_rate": 1.4598613333333333e-05,
      "loss": 0.0071,
      "step": 25320
    },
    {
      "epoch": 0.81056,
      "grad_norm": 0.013541300781071186,
      "learning_rate": 1.4596480000000002e-05,
      "loss": 0.0144,
      "step": 25330
    },
    {
      "epoch": 0.81088,
      "grad_norm": 0.008786199614405632,
      "learning_rate": 1.4594346666666667e-05,
      "loss": 0.0006,
      "step": 25340
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.010051984339952469,
      "learning_rate": 1.4592213333333334e-05,
      "loss": 0.0003,
      "step": 25350
    },
    {
      "epoch": 0.81152,
      "grad_norm": 0.022933954373002052,
      "learning_rate": 1.4590080000000001e-05,
      "loss": 0.0003,
      "step": 25360
    },
    {
      "epoch": 0.81184,
      "grad_norm": 0.006869570817798376,
      "learning_rate": 1.4587946666666669e-05,
      "loss": 0.0003,
      "step": 25370
    },
    {
      "epoch": 0.81216,
      "grad_norm": 0.0057258689776062965,
      "learning_rate": 1.4585813333333334e-05,
      "loss": 0.0004,
      "step": 25380
    },
    {
      "epoch": 0.81248,
      "grad_norm": 0.0039141299203038216,
      "learning_rate": 1.4583680000000003e-05,
      "loss": 0.0003,
      "step": 25390
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.009493183344602585,
      "learning_rate": 1.4581546666666668e-05,
      "loss": 0.0003,
      "step": 25400
    },
    {
      "epoch": 0.81312,
      "grad_norm": 0.0045957863330841064,
      "learning_rate": 1.4579413333333335e-05,
      "loss": 0.0044,
      "step": 25410
    },
    {
      "epoch": 0.81344,
      "grad_norm": 0.004920536652207375,
      "learning_rate": 1.457728e-05,
      "loss": 0.0008,
      "step": 25420
    },
    {
      "epoch": 0.81376,
      "grad_norm": 0.0038151934277266264,
      "learning_rate": 1.457514666666667e-05,
      "loss": 0.0014,
      "step": 25430
    },
    {
      "epoch": 0.81408,
      "grad_norm": 0.02722458355128765,
      "learning_rate": 1.4573013333333335e-05,
      "loss": 0.0164,
      "step": 25440
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.010852011851966381,
      "learning_rate": 1.457088e-05,
      "loss": 0.0555,
      "step": 25450
    },
    {
      "epoch": 0.81472,
      "grad_norm": 0.031733740121126175,
      "learning_rate": 1.4568746666666669e-05,
      "loss": 0.0004,
      "step": 25460
    },
    {
      "epoch": 0.81504,
      "grad_norm": 0.011012518778443336,
      "learning_rate": 1.4566613333333334e-05,
      "loss": 0.0003,
      "step": 25470
    },
    {
      "epoch": 0.81536,
      "grad_norm": 0.003921290393918753,
      "learning_rate": 1.4564480000000001e-05,
      "loss": 0.0236,
      "step": 25480
    },
    {
      "epoch": 0.81568,
      "grad_norm": 0.005780232138931751,
      "learning_rate": 1.4562346666666667e-05,
      "loss": 0.0047,
      "step": 25490
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.005725076887756586,
      "learning_rate": 1.4560213333333335e-05,
      "loss": 0.0009,
      "step": 25500
    },
    {
      "epoch": 0.81632,
      "grad_norm": 0.004742488265037537,
      "learning_rate": 1.455808e-05,
      "loss": 0.0003,
      "step": 25510
    },
    {
      "epoch": 0.81664,
      "grad_norm": 0.006547836586833,
      "learning_rate": 1.4555946666666668e-05,
      "loss": 0.0008,
      "step": 25520
    },
    {
      "epoch": 0.81696,
      "grad_norm": 0.005747745744884014,
      "learning_rate": 1.4553813333333335e-05,
      "loss": 0.0049,
      "step": 25530
    },
    {
      "epoch": 0.81728,
      "grad_norm": 0.005773276090621948,
      "learning_rate": 1.4551680000000002e-05,
      "loss": 0.0019,
      "step": 25540
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.006339799612760544,
      "learning_rate": 1.4549546666666667e-05,
      "loss": 0.0002,
      "step": 25550
    },
    {
      "epoch": 0.81792,
      "grad_norm": 0.07890033721923828,
      "learning_rate": 1.4547413333333336e-05,
      "loss": 0.0004,
      "step": 25560
    },
    {
      "epoch": 0.81824,
      "grad_norm": 0.006798157002776861,
      "learning_rate": 1.4545280000000001e-05,
      "loss": 0.0006,
      "step": 25570
    },
    {
      "epoch": 0.81856,
      "grad_norm": 0.004928379785269499,
      "learning_rate": 1.4543146666666667e-05,
      "loss": 0.0111,
      "step": 25580
    },
    {
      "epoch": 0.81888,
      "grad_norm": 0.005765174515545368,
      "learning_rate": 1.4541013333333334e-05,
      "loss": 0.0002,
      "step": 25590
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.0035405613016337156,
      "learning_rate": 1.4538880000000003e-05,
      "loss": 0.0014,
      "step": 25600
    },
    {
      "epoch": 0.81952,
      "grad_norm": 0.004792670253664255,
      "learning_rate": 1.4536746666666668e-05,
      "loss": 0.0006,
      "step": 25610
    },
    {
      "epoch": 0.81984,
      "grad_norm": 0.007313303183764219,
      "learning_rate": 1.4534613333333333e-05,
      "loss": 0.0022,
      "step": 25620
    },
    {
      "epoch": 0.82016,
      "grad_norm": 0.018071044236421585,
      "learning_rate": 1.4532480000000002e-05,
      "loss": 0.0005,
      "step": 25630
    },
    {
      "epoch": 0.82048,
      "grad_norm": 0.008362500928342342,
      "learning_rate": 1.4530346666666668e-05,
      "loss": 0.0121,
      "step": 25640
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.008248494938015938,
      "learning_rate": 1.4528213333333335e-05,
      "loss": 0.0005,
      "step": 25650
    },
    {
      "epoch": 0.82112,
      "grad_norm": 0.005400148686021566,
      "learning_rate": 1.452608e-05,
      "loss": 0.0004,
      "step": 25660
    },
    {
      "epoch": 0.82144,
      "grad_norm": 0.010120953433215618,
      "learning_rate": 1.4523946666666669e-05,
      "loss": 0.0003,
      "step": 25670
    },
    {
      "epoch": 0.82176,
      "grad_norm": 0.004067123401910067,
      "learning_rate": 1.4521813333333334e-05,
      "loss": 0.0023,
      "step": 25680
    },
    {
      "epoch": 0.82208,
      "grad_norm": 0.004118124954402447,
      "learning_rate": 1.4519680000000001e-05,
      "loss": 0.0004,
      "step": 25690
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.005998941138386726,
      "learning_rate": 1.4517546666666668e-05,
      "loss": 0.0002,
      "step": 25700
    },
    {
      "epoch": 0.82272,
      "grad_norm": 0.006172399502247572,
      "learning_rate": 1.4515413333333335e-05,
      "loss": 0.0003,
      "step": 25710
    },
    {
      "epoch": 0.82304,
      "grad_norm": 0.005907461047172546,
      "learning_rate": 1.451328e-05,
      "loss": 0.0004,
      "step": 25720
    },
    {
      "epoch": 0.82336,
      "grad_norm": 0.005744142457842827,
      "learning_rate": 1.451114666666667e-05,
      "loss": 0.0003,
      "step": 25730
    },
    {
      "epoch": 0.82368,
      "grad_norm": 0.02141692489385605,
      "learning_rate": 1.4509013333333335e-05,
      "loss": 0.0017,
      "step": 25740
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.008129341527819633,
      "learning_rate": 1.450688e-05,
      "loss": 0.0008,
      "step": 25750
    },
    {
      "epoch": 0.82432,
      "grad_norm": 0.005716528277844191,
      "learning_rate": 1.4504746666666667e-05,
      "loss": 0.0196,
      "step": 25760
    },
    {
      "epoch": 0.82464,
      "grad_norm": 0.0024610720574855804,
      "learning_rate": 1.4502613333333334e-05,
      "loss": 0.0002,
      "step": 25770
    },
    {
      "epoch": 0.82496,
      "grad_norm": 0.0032547907903790474,
      "learning_rate": 1.4500480000000001e-05,
      "loss": 0.0004,
      "step": 25780
    },
    {
      "epoch": 0.82528,
      "grad_norm": 0.007219549734145403,
      "learning_rate": 1.4498346666666667e-05,
      "loss": 0.0096,
      "step": 25790
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.008594166487455368,
      "learning_rate": 1.4496213333333336e-05,
      "loss": 0.0002,
      "step": 25800
    },
    {
      "epoch": 0.82592,
      "grad_norm": 0.00869186781346798,
      "learning_rate": 1.4494080000000001e-05,
      "loss": 0.0003,
      "step": 25810
    },
    {
      "epoch": 0.82624,
      "grad_norm": 0.0020702597685158253,
      "learning_rate": 1.4491946666666668e-05,
      "loss": 0.0003,
      "step": 25820
    },
    {
      "epoch": 0.82656,
      "grad_norm": 0.0041802190244197845,
      "learning_rate": 1.4489813333333333e-05,
      "loss": 0.0007,
      "step": 25830
    },
    {
      "epoch": 0.82688,
      "grad_norm": 0.006381683051586151,
      "learning_rate": 1.4487680000000002e-05,
      "loss": 0.0485,
      "step": 25840
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.015137429349124432,
      "learning_rate": 1.4485546666666668e-05,
      "loss": 0.0002,
      "step": 25850
    },
    {
      "epoch": 0.82752,
      "grad_norm": 0.0062796431593596935,
      "learning_rate": 1.4483413333333333e-05,
      "loss": 0.0414,
      "step": 25860
    },
    {
      "epoch": 0.82784,
      "grad_norm": 0.003586430102586746,
      "learning_rate": 1.4481280000000002e-05,
      "loss": 0.0004,
      "step": 25870
    },
    {
      "epoch": 0.82816,
      "grad_norm": 0.005315160378813744,
      "learning_rate": 1.4479146666666669e-05,
      "loss": 0.0004,
      "step": 25880
    },
    {
      "epoch": 0.82848,
      "grad_norm": 0.005471743177622557,
      "learning_rate": 1.4477013333333334e-05,
      "loss": 0.0003,
      "step": 25890
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.006437036674469709,
      "learning_rate": 1.4474880000000003e-05,
      "loss": 0.0004,
      "step": 25900
    },
    {
      "epoch": 0.82912,
      "grad_norm": 0.002280821092426777,
      "learning_rate": 1.4472746666666668e-05,
      "loss": 0.0002,
      "step": 25910
    },
    {
      "epoch": 0.82944,
      "grad_norm": 0.011353148147463799,
      "learning_rate": 1.4470613333333334e-05,
      "loss": 0.0002,
      "step": 25920
    },
    {
      "epoch": 0.82976,
      "grad_norm": 0.003975437488406897,
      "learning_rate": 1.446848e-05,
      "loss": 0.0019,
      "step": 25930
    },
    {
      "epoch": 0.83008,
      "grad_norm": 0.003018802497535944,
      "learning_rate": 1.4466346666666668e-05,
      "loss": 0.0003,
      "step": 25940
    },
    {
      "epoch": 0.8304,
      "grad_norm": 4.779410362243652,
      "learning_rate": 1.4464213333333335e-05,
      "loss": 0.0394,
      "step": 25950
    },
    {
      "epoch": 0.83072,
      "grad_norm": 0.00826229527592659,
      "learning_rate": 1.446208e-05,
      "loss": 0.0036,
      "step": 25960
    },
    {
      "epoch": 0.83104,
      "grad_norm": 0.46702221035957336,
      "learning_rate": 1.4459946666666669e-05,
      "loss": 0.0006,
      "step": 25970
    },
    {
      "epoch": 0.83136,
      "grad_norm": 0.007564760744571686,
      "learning_rate": 1.4457813333333334e-05,
      "loss": 0.0585,
      "step": 25980
    },
    {
      "epoch": 0.83168,
      "grad_norm": 0.035931166261434555,
      "learning_rate": 1.4455680000000001e-05,
      "loss": 0.048,
      "step": 25990
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.016650715842843056,
      "learning_rate": 1.4453546666666667e-05,
      "loss": 0.0004,
      "step": 26000
    },
    {
      "epoch": 0.83232,
      "grad_norm": 0.0073569645173847675,
      "learning_rate": 1.4451413333333336e-05,
      "loss": 0.0002,
      "step": 26010
    },
    {
      "epoch": 0.83264,
      "grad_norm": 0.0037973274011164904,
      "learning_rate": 1.4449280000000001e-05,
      "loss": 0.0002,
      "step": 26020
    },
    {
      "epoch": 0.83296,
      "grad_norm": 0.006790217477828264,
      "learning_rate": 1.4447146666666666e-05,
      "loss": 0.0002,
      "step": 26030
    },
    {
      "epoch": 0.83328,
      "grad_norm": 0.02136463299393654,
      "learning_rate": 1.4445013333333335e-05,
      "loss": 0.0003,
      "step": 26040
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.008638191968202591,
      "learning_rate": 1.444288e-05,
      "loss": 0.0123,
      "step": 26050
    },
    {
      "epoch": 0.83392,
      "grad_norm": 0.0062284632585942745,
      "learning_rate": 1.4440746666666668e-05,
      "loss": 0.0003,
      "step": 26060
    },
    {
      "epoch": 0.83424,
      "grad_norm": 0.005453909747302532,
      "learning_rate": 1.4438613333333336e-05,
      "loss": 0.0492,
      "step": 26070
    },
    {
      "epoch": 0.83456,
      "grad_norm": 0.016310464590787888,
      "learning_rate": 1.4436480000000002e-05,
      "loss": 0.0452,
      "step": 26080
    },
    {
      "epoch": 0.83488,
      "grad_norm": 0.0051767281256616116,
      "learning_rate": 1.4434346666666667e-05,
      "loss": 0.0005,
      "step": 26090
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.009967445395886898,
      "learning_rate": 1.4432213333333334e-05,
      "loss": 0.0007,
      "step": 26100
    },
    {
      "epoch": 0.83552,
      "grad_norm": 0.004316668026149273,
      "learning_rate": 1.4430080000000001e-05,
      "loss": 0.0003,
      "step": 26110
    },
    {
      "epoch": 0.83584,
      "grad_norm": 0.008090076968073845,
      "learning_rate": 1.4427946666666668e-05,
      "loss": 0.0004,
      "step": 26120
    },
    {
      "epoch": 0.83616,
      "grad_norm": 0.002577562350779772,
      "learning_rate": 1.4425813333333334e-05,
      "loss": 0.0004,
      "step": 26130
    },
    {
      "epoch": 0.83648,
      "grad_norm": 0.00861936155706644,
      "learning_rate": 1.4423680000000002e-05,
      "loss": 0.0005,
      "step": 26140
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.007303949445486069,
      "learning_rate": 1.4421546666666668e-05,
      "loss": 0.0005,
      "step": 26150
    },
    {
      "epoch": 0.83712,
      "grad_norm": 0.011355356313288212,
      "learning_rate": 1.4419413333333335e-05,
      "loss": 0.0059,
      "step": 26160
    },
    {
      "epoch": 0.83744,
      "grad_norm": 0.026623228564858437,
      "learning_rate": 1.441728e-05,
      "loss": 0.0007,
      "step": 26170
    },
    {
      "epoch": 0.83776,
      "grad_norm": 0.008532010018825531,
      "learning_rate": 1.4415146666666669e-05,
      "loss": 0.0003,
      "step": 26180
    },
    {
      "epoch": 0.83808,
      "grad_norm": 0.06609824299812317,
      "learning_rate": 1.4413013333333334e-05,
      "loss": 0.0006,
      "step": 26190
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.00279603386297822,
      "learning_rate": 1.441088e-05,
      "loss": 0.0083,
      "step": 26200
    },
    {
      "epoch": 0.83872,
      "grad_norm": 0.012002186849713326,
      "learning_rate": 1.4408746666666669e-05,
      "loss": 0.0226,
      "step": 26210
    },
    {
      "epoch": 0.83904,
      "grad_norm": 0.00921151228249073,
      "learning_rate": 1.4406613333333334e-05,
      "loss": 0.0004,
      "step": 26220
    },
    {
      "epoch": 0.83936,
      "grad_norm": 0.004548782017081976,
      "learning_rate": 1.4404480000000001e-05,
      "loss": 0.0003,
      "step": 26230
    },
    {
      "epoch": 0.83968,
      "grad_norm": 0.006442139856517315,
      "learning_rate": 1.4402346666666668e-05,
      "loss": 0.0003,
      "step": 26240
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.010003851726651192,
      "learning_rate": 1.4400213333333335e-05,
      "loss": 0.1072,
      "step": 26250
    },
    {
      "epoch": 0.84032,
      "grad_norm": 0.004311335738748312,
      "learning_rate": 1.439808e-05,
      "loss": 0.0009,
      "step": 26260
    },
    {
      "epoch": 0.84064,
      "grad_norm": 0.032050032168626785,
      "learning_rate": 1.4395946666666668e-05,
      "loss": 0.0004,
      "step": 26270
    },
    {
      "epoch": 0.84096,
      "grad_norm": 1.1145424842834473,
      "learning_rate": 1.4393813333333335e-05,
      "loss": 0.0784,
      "step": 26280
    },
    {
      "epoch": 0.84128,
      "grad_norm": 0.00440179230645299,
      "learning_rate": 1.4391680000000002e-05,
      "loss": 0.0004,
      "step": 26290
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.004892744589596987,
      "learning_rate": 1.4389546666666667e-05,
      "loss": 0.0006,
      "step": 26300
    },
    {
      "epoch": 0.84192,
      "grad_norm": 0.005836165975779295,
      "learning_rate": 1.4387413333333336e-05,
      "loss": 0.015,
      "step": 26310
    },
    {
      "epoch": 0.84224,
      "grad_norm": 0.009588669054210186,
      "learning_rate": 1.4385280000000001e-05,
      "loss": 0.0005,
      "step": 26320
    },
    {
      "epoch": 0.84256,
      "grad_norm": 0.004641362931579351,
      "learning_rate": 1.4383146666666667e-05,
      "loss": 0.0378,
      "step": 26330
    },
    {
      "epoch": 0.84288,
      "grad_norm": 0.007304590195417404,
      "learning_rate": 1.4381013333333334e-05,
      "loss": 0.033,
      "step": 26340
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.07802720367908478,
      "learning_rate": 1.4378880000000003e-05,
      "loss": 0.0059,
      "step": 26350
    },
    {
      "epoch": 0.84352,
      "grad_norm": 0.03668588399887085,
      "learning_rate": 1.4376746666666668e-05,
      "loss": 0.0006,
      "step": 26360
    },
    {
      "epoch": 0.84384,
      "grad_norm": 0.004987508989870548,
      "learning_rate": 1.4374613333333333e-05,
      "loss": 0.0025,
      "step": 26370
    },
    {
      "epoch": 0.84416,
      "grad_norm": 0.009002145379781723,
      "learning_rate": 1.4372480000000002e-05,
      "loss": 0.0005,
      "step": 26380
    },
    {
      "epoch": 0.84448,
      "grad_norm": 0.0048685744404792786,
      "learning_rate": 1.4370346666666667e-05,
      "loss": 0.0032,
      "step": 26390
    },
    {
      "epoch": 0.8448,
      "grad_norm": 1.2278324365615845,
      "learning_rate": 1.4368213333333334e-05,
      "loss": 0.0549,
      "step": 26400
    },
    {
      "epoch": 0.84512,
      "grad_norm": 0.008479374460875988,
      "learning_rate": 1.4366080000000002e-05,
      "loss": 0.0007,
      "step": 26410
    },
    {
      "epoch": 0.84544,
      "grad_norm": 0.011417877860367298,
      "learning_rate": 1.4363946666666669e-05,
      "loss": 0.0005,
      "step": 26420
    },
    {
      "epoch": 0.84576,
      "grad_norm": 0.02471216209232807,
      "learning_rate": 1.4361813333333334e-05,
      "loss": 0.0464,
      "step": 26430
    },
    {
      "epoch": 0.84608,
      "grad_norm": 0.01005722489207983,
      "learning_rate": 1.4359680000000001e-05,
      "loss": 0.0005,
      "step": 26440
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.0063303387723863125,
      "learning_rate": 1.4357546666666668e-05,
      "loss": 0.0022,
      "step": 26450
    },
    {
      "epoch": 0.84672,
      "grad_norm": 0.01500319130718708,
      "learning_rate": 1.4355413333333335e-05,
      "loss": 0.0169,
      "step": 26460
    },
    {
      "epoch": 0.84704,
      "grad_norm": 0.00992299523204565,
      "learning_rate": 1.435328e-05,
      "loss": 0.0005,
      "step": 26470
    },
    {
      "epoch": 0.84736,
      "grad_norm": 0.009267129935324192,
      "learning_rate": 1.435114666666667e-05,
      "loss": 0.0386,
      "step": 26480
    },
    {
      "epoch": 0.84768,
      "grad_norm": 0.012478050775825977,
      "learning_rate": 1.4349013333333335e-05,
      "loss": 0.0013,
      "step": 26490
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.012255570851266384,
      "learning_rate": 1.434688e-05,
      "loss": 0.048,
      "step": 26500
    },
    {
      "epoch": 0.84832,
      "grad_norm": 0.012308941222727299,
      "learning_rate": 1.4344746666666667e-05,
      "loss": 0.0019,
      "step": 26510
    },
    {
      "epoch": 0.84864,
      "grad_norm": 0.015338672325015068,
      "learning_rate": 1.4342613333333334e-05,
      "loss": 0.0007,
      "step": 26520
    },
    {
      "epoch": 0.84896,
      "grad_norm": 0.005837411619722843,
      "learning_rate": 1.4340480000000001e-05,
      "loss": 0.0006,
      "step": 26530
    },
    {
      "epoch": 0.84928,
      "grad_norm": 0.01008571870625019,
      "learning_rate": 1.4338346666666667e-05,
      "loss": 0.0015,
      "step": 26540
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.0065283928997814655,
      "learning_rate": 1.4336213333333335e-05,
      "loss": 0.0004,
      "step": 26550
    },
    {
      "epoch": 0.84992,
      "grad_norm": 0.00471399025991559,
      "learning_rate": 1.433408e-05,
      "loss": 0.001,
      "step": 26560
    },
    {
      "epoch": 0.85024,
      "grad_norm": 0.011596865952014923,
      "learning_rate": 1.4331946666666668e-05,
      "loss": 0.0004,
      "step": 26570
    },
    {
      "epoch": 0.85056,
      "grad_norm": 0.0039472742937505245,
      "learning_rate": 1.4329813333333335e-05,
      "loss": 0.0004,
      "step": 26580
    },
    {
      "epoch": 0.85088,
      "grad_norm": 0.005235331133008003,
      "learning_rate": 1.4327680000000002e-05,
      "loss": 0.0013,
      "step": 26590
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.01768575608730316,
      "learning_rate": 1.4325546666666667e-05,
      "loss": 0.0004,
      "step": 26600
    },
    {
      "epoch": 0.85152,
      "grad_norm": 0.023694107308983803,
      "learning_rate": 1.4323413333333333e-05,
      "loss": 0.0003,
      "step": 26610
    },
    {
      "epoch": 0.85184,
      "grad_norm": 0.007684883661568165,
      "learning_rate": 1.4321280000000002e-05,
      "loss": 0.001,
      "step": 26620
    },
    {
      "epoch": 0.85216,
      "grad_norm": 0.007806586567312479,
      "learning_rate": 1.4319146666666669e-05,
      "loss": 0.0009,
      "step": 26630
    },
    {
      "epoch": 0.85248,
      "grad_norm": 0.006505653727799654,
      "learning_rate": 1.4317013333333334e-05,
      "loss": 0.0015,
      "step": 26640
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.025084037333726883,
      "learning_rate": 1.4314880000000003e-05,
      "loss": 0.0039,
      "step": 26650
    },
    {
      "epoch": 0.85312,
      "grad_norm": 0.004527030047029257,
      "learning_rate": 1.4312746666666668e-05,
      "loss": 0.0003,
      "step": 26660
    },
    {
      "epoch": 0.85344,
      "grad_norm": 0.0026782050263136625,
      "learning_rate": 1.4310613333333334e-05,
      "loss": 0.0371,
      "step": 26670
    },
    {
      "epoch": 0.85376,
      "grad_norm": 0.004616548307240009,
      "learning_rate": 1.430848e-05,
      "loss": 0.0007,
      "step": 26680
    },
    {
      "epoch": 0.85408,
      "grad_norm": 0.007794671226292849,
      "learning_rate": 1.4306346666666668e-05,
      "loss": 0.0004,
      "step": 26690
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.03900264576077461,
      "learning_rate": 1.4304213333333335e-05,
      "loss": 0.0116,
      "step": 26700
    },
    {
      "epoch": 0.85472,
      "grad_norm": 0.007276811636984348,
      "learning_rate": 1.430208e-05,
      "loss": 0.0003,
      "step": 26710
    },
    {
      "epoch": 0.85504,
      "grad_norm": 0.010058199986815453,
      "learning_rate": 1.4299946666666669e-05,
      "loss": 0.0003,
      "step": 26720
    },
    {
      "epoch": 0.85536,
      "grad_norm": 0.007115711458027363,
      "learning_rate": 1.4297813333333334e-05,
      "loss": 0.0003,
      "step": 26730
    },
    {
      "epoch": 0.85568,
      "grad_norm": 0.0043760621920228004,
      "learning_rate": 1.4295680000000001e-05,
      "loss": 0.0003,
      "step": 26740
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.007664802484214306,
      "learning_rate": 1.4293546666666668e-05,
      "loss": 0.0003,
      "step": 26750
    },
    {
      "epoch": 0.85632,
      "grad_norm": 0.005465338006615639,
      "learning_rate": 1.4291413333333335e-05,
      "loss": 0.0005,
      "step": 26760
    },
    {
      "epoch": 0.85664,
      "grad_norm": 0.010828939266502857,
      "learning_rate": 1.4289280000000001e-05,
      "loss": 0.0006,
      "step": 26770
    },
    {
      "epoch": 0.85696,
      "grad_norm": 0.009435316547751427,
      "learning_rate": 1.4287146666666666e-05,
      "loss": 0.0004,
      "step": 26780
    },
    {
      "epoch": 0.85728,
      "grad_norm": 0.006899289786815643,
      "learning_rate": 1.4285013333333335e-05,
      "loss": 0.0004,
      "step": 26790
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.005916395224630833,
      "learning_rate": 1.428288e-05,
      "loss": 0.0006,
      "step": 26800
    },
    {
      "epoch": 0.85792,
      "grad_norm": 0.010656091384589672,
      "learning_rate": 1.4280746666666667e-05,
      "loss": 0.0223,
      "step": 26810
    },
    {
      "epoch": 0.85824,
      "grad_norm": 0.02354258857667446,
      "learning_rate": 1.4278613333333336e-05,
      "loss": 0.0004,
      "step": 26820
    },
    {
      "epoch": 0.85856,
      "grad_norm": 0.005986628122627735,
      "learning_rate": 1.4276480000000002e-05,
      "loss": 0.0003,
      "step": 26830
    },
    {
      "epoch": 0.85888,
      "grad_norm": 0.003524961182847619,
      "learning_rate": 1.4274346666666667e-05,
      "loss": 0.0003,
      "step": 26840
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.07035484910011292,
      "learning_rate": 1.4272213333333334e-05,
      "loss": 0.0305,
      "step": 26850
    },
    {
      "epoch": 0.85952,
      "grad_norm": 0.005269485525786877,
      "learning_rate": 1.4270080000000001e-05,
      "loss": 0.0003,
      "step": 26860
    },
    {
      "epoch": 0.85984,
      "grad_norm": 1.57256019115448,
      "learning_rate": 1.4267946666666668e-05,
      "loss": 0.0014,
      "step": 26870
    },
    {
      "epoch": 0.86016,
      "grad_norm": 0.0060257622972130775,
      "learning_rate": 1.4265813333333334e-05,
      "loss": 0.0117,
      "step": 26880
    },
    {
      "epoch": 0.86048,
      "grad_norm": 0.008695624768733978,
      "learning_rate": 1.4263680000000002e-05,
      "loss": 0.0002,
      "step": 26890
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.003794765332713723,
      "learning_rate": 1.4261546666666668e-05,
      "loss": 0.0518,
      "step": 26900
    },
    {
      "epoch": 0.86112,
      "grad_norm": 0.004213003907352686,
      "learning_rate": 1.4259413333333335e-05,
      "loss": 0.0004,
      "step": 26910
    },
    {
      "epoch": 0.86144,
      "grad_norm": 0.00492773437872529,
      "learning_rate": 1.4257280000000002e-05,
      "loss": 0.0708,
      "step": 26920
    },
    {
      "epoch": 0.86176,
      "grad_norm": 1.0278958082199097,
      "learning_rate": 1.4255146666666669e-05,
      "loss": 0.0021,
      "step": 26930
    },
    {
      "epoch": 0.86208,
      "grad_norm": 0.0038147151935845613,
      "learning_rate": 1.4253013333333334e-05,
      "loss": 0.0003,
      "step": 26940
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.004826087038964033,
      "learning_rate": 1.425088e-05,
      "loss": 0.0474,
      "step": 26950
    },
    {
      "epoch": 0.86272,
      "grad_norm": 0.007019148673862219,
      "learning_rate": 1.4248746666666668e-05,
      "loss": 0.0298,
      "step": 26960
    },
    {
      "epoch": 0.86304,
      "grad_norm": 0.010556313209235668,
      "learning_rate": 1.4246613333333334e-05,
      "loss": 0.0006,
      "step": 26970
    },
    {
      "epoch": 0.86336,
      "grad_norm": 0.0029617720283567905,
      "learning_rate": 1.4244480000000001e-05,
      "loss": 0.0282,
      "step": 26980
    },
    {
      "epoch": 0.86368,
      "grad_norm": 0.0040692416951060295,
      "learning_rate": 1.4242346666666668e-05,
      "loss": 0.0003,
      "step": 26990
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.005965874530375004,
      "learning_rate": 1.4240213333333335e-05,
      "loss": 0.0003,
      "step": 27000
    },
    {
      "epoch": 0.86432,
      "grad_norm": 0.00623686658218503,
      "learning_rate": 1.423808e-05,
      "loss": 0.0003,
      "step": 27010
    },
    {
      "epoch": 0.86464,
      "grad_norm": 0.006557415705174208,
      "learning_rate": 1.4235946666666667e-05,
      "loss": 0.041,
      "step": 27020
    },
    {
      "epoch": 0.86496,
      "grad_norm": 0.009575297124683857,
      "learning_rate": 1.4233813333333335e-05,
      "loss": 0.0485,
      "step": 27030
    },
    {
      "epoch": 0.86528,
      "grad_norm": 0.008493509143590927,
      "learning_rate": 1.4231680000000002e-05,
      "loss": 0.0005,
      "step": 27040
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.005161297973245382,
      "learning_rate": 1.4229546666666667e-05,
      "loss": 0.0068,
      "step": 27050
    },
    {
      "epoch": 0.86592,
      "grad_norm": 0.008798319846391678,
      "learning_rate": 1.4227413333333336e-05,
      "loss": 0.0004,
      "step": 27060
    },
    {
      "epoch": 0.86624,
      "grad_norm": 0.008608561009168625,
      "learning_rate": 1.4225280000000001e-05,
      "loss": 0.037,
      "step": 27070
    },
    {
      "epoch": 0.86656,
      "grad_norm": 0.01856336183845997,
      "learning_rate": 1.4223146666666667e-05,
      "loss": 0.0005,
      "step": 27080
    },
    {
      "epoch": 0.86688,
      "grad_norm": 0.010458028875291348,
      "learning_rate": 1.4221013333333335e-05,
      "loss": 0.0004,
      "step": 27090
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.014560035429894924,
      "learning_rate": 1.4218880000000002e-05,
      "loss": 0.0004,
      "step": 27100
    },
    {
      "epoch": 0.86752,
      "grad_norm": 0.011431321501731873,
      "learning_rate": 1.4216746666666668e-05,
      "loss": 0.0252,
      "step": 27110
    },
    {
      "epoch": 0.86784,
      "grad_norm": 0.024635523557662964,
      "learning_rate": 1.4214613333333333e-05,
      "loss": 0.0211,
      "step": 27120
    },
    {
      "epoch": 0.86816,
      "grad_norm": 0.010855741798877716,
      "learning_rate": 1.4212480000000002e-05,
      "loss": 0.0006,
      "step": 27130
    },
    {
      "epoch": 0.86848,
      "grad_norm": 0.015444779768586159,
      "learning_rate": 1.4210346666666667e-05,
      "loss": 0.0005,
      "step": 27140
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.2646641433238983,
      "learning_rate": 1.4208213333333334e-05,
      "loss": 0.0008,
      "step": 27150
    },
    {
      "epoch": 0.86912,
      "grad_norm": 0.004115414340049028,
      "learning_rate": 1.4206080000000001e-05,
      "loss": 0.0008,
      "step": 27160
    },
    {
      "epoch": 0.86944,
      "grad_norm": 0.005955894012004137,
      "learning_rate": 1.4203946666666668e-05,
      "loss": 0.0007,
      "step": 27170
    },
    {
      "epoch": 0.86976,
      "grad_norm": 0.005325105041265488,
      "learning_rate": 1.4201813333333334e-05,
      "loss": 0.0005,
      "step": 27180
    },
    {
      "epoch": 0.87008,
      "grad_norm": 0.004704815335571766,
      "learning_rate": 1.4199680000000001e-05,
      "loss": 0.0038,
      "step": 27190
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.0051002344116568565,
      "learning_rate": 1.4197546666666668e-05,
      "loss": 0.0013,
      "step": 27200
    },
    {
      "epoch": 0.87072,
      "grad_norm": 0.015587975271046162,
      "learning_rate": 1.4195413333333335e-05,
      "loss": 0.0004,
      "step": 27210
    },
    {
      "epoch": 0.87104,
      "grad_norm": 0.006279230583459139,
      "learning_rate": 1.419328e-05,
      "loss": 0.0004,
      "step": 27220
    },
    {
      "epoch": 0.87136,
      "grad_norm": 0.004992365837097168,
      "learning_rate": 1.419114666666667e-05,
      "loss": 0.0002,
      "step": 27230
    },
    {
      "epoch": 0.87168,
      "grad_norm": 0.008408622816205025,
      "learning_rate": 1.4189013333333335e-05,
      "loss": 0.0101,
      "step": 27240
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.01532751228660345,
      "learning_rate": 1.418688e-05,
      "loss": 0.0004,
      "step": 27250
    },
    {
      "epoch": 0.87232,
      "grad_norm": 0.03640410304069519,
      "learning_rate": 1.4184746666666669e-05,
      "loss": 0.1208,
      "step": 27260
    },
    {
      "epoch": 0.87264,
      "grad_norm": 0.007073512300848961,
      "learning_rate": 1.4182613333333334e-05,
      "loss": 0.0005,
      "step": 27270
    },
    {
      "epoch": 0.87296,
      "grad_norm": 1.4579427242279053,
      "learning_rate": 1.4180480000000001e-05,
      "loss": 0.0025,
      "step": 27280
    },
    {
      "epoch": 0.87328,
      "grad_norm": 0.006649194285273552,
      "learning_rate": 1.4178346666666667e-05,
      "loss": 0.0003,
      "step": 27290
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.01790522038936615,
      "learning_rate": 1.4176213333333335e-05,
      "loss": 0.0006,
      "step": 27300
    },
    {
      "epoch": 0.87392,
      "grad_norm": 0.005222114734351635,
      "learning_rate": 1.417408e-05,
      "loss": 0.0003,
      "step": 27310
    },
    {
      "epoch": 0.87424,
      "grad_norm": 0.007768855430185795,
      "learning_rate": 1.4171946666666668e-05,
      "loss": 0.0005,
      "step": 27320
    },
    {
      "epoch": 0.87456,
      "grad_norm": 0.00889931246638298,
      "learning_rate": 1.4169813333333335e-05,
      "loss": 0.0304,
      "step": 27330
    },
    {
      "epoch": 0.87488,
      "grad_norm": 2.1462929248809814,
      "learning_rate": 1.4167680000000002e-05,
      "loss": 0.0021,
      "step": 27340
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.005838701501488686,
      "learning_rate": 1.4165546666666667e-05,
      "loss": 0.0019,
      "step": 27350
    },
    {
      "epoch": 0.87552,
      "grad_norm": 0.007502399384975433,
      "learning_rate": 1.4163413333333333e-05,
      "loss": 0.0004,
      "step": 27360
    },
    {
      "epoch": 0.87584,
      "grad_norm": 0.011504202149808407,
      "learning_rate": 1.4161280000000001e-05,
      "loss": 0.0006,
      "step": 27370
    },
    {
      "epoch": 0.87616,
      "grad_norm": 0.008039179258048534,
      "learning_rate": 1.4159146666666668e-05,
      "loss": 0.0476,
      "step": 27380
    },
    {
      "epoch": 0.87648,
      "grad_norm": 0.007724364288151264,
      "learning_rate": 1.4157013333333334e-05,
      "loss": 0.0003,
      "step": 27390
    },
    {
      "epoch": 0.8768,
      "grad_norm": 1.118118405342102,
      "learning_rate": 1.4154880000000003e-05,
      "loss": 0.0029,
      "step": 27400
    },
    {
      "epoch": 0.87712,
      "grad_norm": 0.026336751878261566,
      "learning_rate": 1.4152746666666668e-05,
      "loss": 0.0005,
      "step": 27410
    },
    {
      "epoch": 0.87744,
      "grad_norm": 0.00802742037922144,
      "learning_rate": 1.4150613333333333e-05,
      "loss": 0.0007,
      "step": 27420
    },
    {
      "epoch": 0.87776,
      "grad_norm": 0.10517211258411407,
      "learning_rate": 1.4148480000000002e-05,
      "loss": 0.0005,
      "step": 27430
    },
    {
      "epoch": 0.87808,
      "grad_norm": 0.010041569359600544,
      "learning_rate": 1.4146346666666668e-05,
      "loss": 0.0004,
      "step": 27440
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.007159233558923006,
      "learning_rate": 1.4144213333333335e-05,
      "loss": 0.0004,
      "step": 27450
    },
    {
      "epoch": 0.87872,
      "grad_norm": 0.005268790293484926,
      "learning_rate": 1.414208e-05,
      "loss": 0.0195,
      "step": 27460
    },
    {
      "epoch": 0.87904,
      "grad_norm": 0.013985463418066502,
      "learning_rate": 1.4139946666666669e-05,
      "loss": 0.0027,
      "step": 27470
    },
    {
      "epoch": 0.87936,
      "grad_norm": 0.019575845450162888,
      "learning_rate": 1.4137813333333334e-05,
      "loss": 0.0139,
      "step": 27480
    },
    {
      "epoch": 0.87968,
      "grad_norm": 0.26173490285873413,
      "learning_rate": 1.4135680000000001e-05,
      "loss": 0.0007,
      "step": 27490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.010771551169455051,
      "learning_rate": 1.4133546666666668e-05,
      "loss": 0.0003,
      "step": 27500
    },
    {
      "epoch": 0.88032,
      "grad_norm": 4.69296407699585,
      "learning_rate": 1.4131413333333335e-05,
      "loss": 0.04,
      "step": 27510
    },
    {
      "epoch": 0.88064,
      "grad_norm": 0.006831743288785219,
      "learning_rate": 1.412928e-05,
      "loss": 0.0007,
      "step": 27520
    },
    {
      "epoch": 0.88096,
      "grad_norm": 0.00499434769153595,
      "learning_rate": 1.4127146666666666e-05,
      "loss": 0.0156,
      "step": 27530
    },
    {
      "epoch": 0.88128,
      "grad_norm": 0.004668909125030041,
      "learning_rate": 1.4125013333333335e-05,
      "loss": 0.0003,
      "step": 27540
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.006714758463203907,
      "learning_rate": 1.412288e-05,
      "loss": 0.0014,
      "step": 27550
    },
    {
      "epoch": 0.88192,
      "grad_norm": 0.0028422228060662746,
      "learning_rate": 1.4120746666666667e-05,
      "loss": 0.0009,
      "step": 27560
    },
    {
      "epoch": 0.88224,
      "grad_norm": 0.007602905388921499,
      "learning_rate": 1.4118613333333336e-05,
      "loss": 0.0004,
      "step": 27570
    },
    {
      "epoch": 0.88256,
      "grad_norm": 0.01328997127711773,
      "learning_rate": 1.4116480000000001e-05,
      "loss": 0.013,
      "step": 27580
    },
    {
      "epoch": 0.88288,
      "grad_norm": 0.01642770692706108,
      "learning_rate": 1.4114346666666667e-05,
      "loss": 0.0005,
      "step": 27590
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.011123445816338062,
      "learning_rate": 1.4112213333333336e-05,
      "loss": 0.0009,
      "step": 27600
    },
    {
      "epoch": 0.88352,
      "grad_norm": 0.011695516295731068,
      "learning_rate": 1.4110080000000001e-05,
      "loss": 0.0004,
      "step": 27610
    },
    {
      "epoch": 0.88384,
      "grad_norm": 0.005921092350035906,
      "learning_rate": 1.4107946666666668e-05,
      "loss": 0.0003,
      "step": 27620
    },
    {
      "epoch": 0.88416,
      "grad_norm": 0.028019418939948082,
      "learning_rate": 1.4105813333333333e-05,
      "loss": 0.0464,
      "step": 27630
    },
    {
      "epoch": 0.88448,
      "grad_norm": 0.026383664458990097,
      "learning_rate": 1.4103680000000002e-05,
      "loss": 0.0036,
      "step": 27640
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.00402471236884594,
      "learning_rate": 1.4101546666666668e-05,
      "loss": 0.103,
      "step": 27650
    },
    {
      "epoch": 0.88512,
      "grad_norm": 0.003551408415660262,
      "learning_rate": 1.4099413333333335e-05,
      "loss": 0.0005,
      "step": 27660
    },
    {
      "epoch": 0.88544,
      "grad_norm": 0.011236217804253101,
      "learning_rate": 1.4097280000000002e-05,
      "loss": 0.0005,
      "step": 27670
    },
    {
      "epoch": 0.88576,
      "grad_norm": 0.011168470606207848,
      "learning_rate": 1.4095146666666669e-05,
      "loss": 0.0005,
      "step": 27680
    },
    {
      "epoch": 0.88608,
      "grad_norm": 0.5862883925437927,
      "learning_rate": 1.4093013333333334e-05,
      "loss": 0.0016,
      "step": 27690
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.012088727205991745,
      "learning_rate": 1.409088e-05,
      "loss": 0.0006,
      "step": 27700
    },
    {
      "epoch": 0.88672,
      "grad_norm": 0.010870126076042652,
      "learning_rate": 1.4088746666666668e-05,
      "loss": 0.0437,
      "step": 27710
    },
    {
      "epoch": 0.88704,
      "grad_norm": 0.005222619511187077,
      "learning_rate": 1.4086613333333334e-05,
      "loss": 0.0015,
      "step": 27720
    },
    {
      "epoch": 0.88736,
      "grad_norm": 0.010785538703203201,
      "learning_rate": 1.408448e-05,
      "loss": 0.0051,
      "step": 27730
    },
    {
      "epoch": 0.88768,
      "grad_norm": 0.006432848051190376,
      "learning_rate": 1.4082346666666668e-05,
      "loss": 0.0015,
      "step": 27740
    },
    {
      "epoch": 0.888,
      "grad_norm": 1.643254280090332,
      "learning_rate": 1.4080213333333335e-05,
      "loss": 0.0484,
      "step": 27750
    },
    {
      "epoch": 0.88832,
      "grad_norm": 1.5486853122711182,
      "learning_rate": 1.407808e-05,
      "loss": 0.0022,
      "step": 27760
    },
    {
      "epoch": 0.88864,
      "grad_norm": 0.011749612167477608,
      "learning_rate": 1.4075946666666669e-05,
      "loss": 0.0005,
      "step": 27770
    },
    {
      "epoch": 0.88896,
      "grad_norm": 0.019832268357276917,
      "learning_rate": 1.4073813333333334e-05,
      "loss": 0.0497,
      "step": 27780
    },
    {
      "epoch": 0.88928,
      "grad_norm": 0.004792878869920969,
      "learning_rate": 1.4071680000000001e-05,
      "loss": 0.0004,
      "step": 27790
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.005993411410599947,
      "learning_rate": 1.4069546666666667e-05,
      "loss": 0.0008,
      "step": 27800
    },
    {
      "epoch": 0.88992,
      "grad_norm": 0.011587687768042088,
      "learning_rate": 1.4067413333333336e-05,
      "loss": 0.0007,
      "step": 27810
    },
    {
      "epoch": 0.89024,
      "grad_norm": 0.008426412008702755,
      "learning_rate": 1.4065280000000001e-05,
      "loss": 0.0005,
      "step": 27820
    },
    {
      "epoch": 0.89056,
      "grad_norm": 0.007648908533155918,
      "learning_rate": 1.4063146666666666e-05,
      "loss": 0.0003,
      "step": 27830
    },
    {
      "epoch": 0.89088,
      "grad_norm": 0.03591971471905708,
      "learning_rate": 1.4061013333333335e-05,
      "loss": 0.0004,
      "step": 27840
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.005057110451161861,
      "learning_rate": 1.4058880000000002e-05,
      "loss": 0.0004,
      "step": 27850
    },
    {
      "epoch": 0.89152,
      "grad_norm": 0.004036544356495142,
      "learning_rate": 1.4056746666666668e-05,
      "loss": 0.0645,
      "step": 27860
    },
    {
      "epoch": 0.89184,
      "grad_norm": 0.022705968469381332,
      "learning_rate": 1.4054613333333333e-05,
      "loss": 0.0004,
      "step": 27870
    },
    {
      "epoch": 0.89216,
      "grad_norm": 0.25899311900138855,
      "learning_rate": 1.4052480000000002e-05,
      "loss": 0.0247,
      "step": 27880
    },
    {
      "epoch": 0.89248,
      "grad_norm": 0.006615207996219397,
      "learning_rate": 1.4050346666666667e-05,
      "loss": 0.0004,
      "step": 27890
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.005822991952300072,
      "learning_rate": 1.4048213333333334e-05,
      "loss": 0.0004,
      "step": 27900
    },
    {
      "epoch": 0.89312,
      "grad_norm": 0.01282818615436554,
      "learning_rate": 1.4046080000000001e-05,
      "loss": 0.0008,
      "step": 27910
    },
    {
      "epoch": 0.89344,
      "grad_norm": 0.005768813192844391,
      "learning_rate": 1.4043946666666668e-05,
      "loss": 0.0005,
      "step": 27920
    },
    {
      "epoch": 0.89376,
      "grad_norm": 0.00660957396030426,
      "learning_rate": 1.4041813333333334e-05,
      "loss": 0.0339,
      "step": 27930
    },
    {
      "epoch": 0.89408,
      "grad_norm": 0.007775739300996065,
      "learning_rate": 1.4039680000000002e-05,
      "loss": 0.0005,
      "step": 27940
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.007486088667064905,
      "learning_rate": 1.4037546666666668e-05,
      "loss": 0.0005,
      "step": 27950
    },
    {
      "epoch": 0.89472,
      "grad_norm": 0.010848593898117542,
      "learning_rate": 1.4035413333333335e-05,
      "loss": 0.0005,
      "step": 27960
    },
    {
      "epoch": 0.89504,
      "grad_norm": 0.01103323232382536,
      "learning_rate": 1.403328e-05,
      "loss": 0.0005,
      "step": 27970
    },
    {
      "epoch": 0.89536,
      "grad_norm": 0.00951631460338831,
      "learning_rate": 1.4031146666666669e-05,
      "loss": 0.0006,
      "step": 27980
    },
    {
      "epoch": 0.89568,
      "grad_norm": 0.009702356532216072,
      "learning_rate": 1.4029013333333334e-05,
      "loss": 0.0004,
      "step": 27990
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.01122212316840887,
      "learning_rate": 1.402688e-05,
      "loss": 0.0192,
      "step": 28000
    },
    {
      "epoch": 0.89632,
      "grad_norm": 0.007986367680132389,
      "learning_rate": 1.4024746666666669e-05,
      "loss": 0.0404,
      "step": 28010
    },
    {
      "epoch": 0.89664,
      "grad_norm": 0.037363406270742416,
      "learning_rate": 1.4022613333333334e-05,
      "loss": 0.0005,
      "step": 28020
    },
    {
      "epoch": 0.89696,
      "grad_norm": 0.005991462152451277,
      "learning_rate": 1.4020480000000001e-05,
      "loss": 0.0149,
      "step": 28030
    },
    {
      "epoch": 0.89728,
      "grad_norm": 2.565129041671753,
      "learning_rate": 1.4018346666666666e-05,
      "loss": 0.0277,
      "step": 28040
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.00580891827121377,
      "learning_rate": 1.4016213333333335e-05,
      "loss": 0.0008,
      "step": 28050
    },
    {
      "epoch": 0.89792,
      "grad_norm": 0.01077525969594717,
      "learning_rate": 1.401408e-05,
      "loss": 0.0075,
      "step": 28060
    },
    {
      "epoch": 0.89824,
      "grad_norm": 0.012044047005474567,
      "learning_rate": 1.4011946666666668e-05,
      "loss": 0.0006,
      "step": 28070
    },
    {
      "epoch": 0.89856,
      "grad_norm": 1.6771697998046875,
      "learning_rate": 1.4009813333333335e-05,
      "loss": 0.0024,
      "step": 28080
    },
    {
      "epoch": 0.89888,
      "grad_norm": 0.08713353425264359,
      "learning_rate": 1.4007680000000002e-05,
      "loss": 0.0012,
      "step": 28090
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.00625432888045907,
      "learning_rate": 1.4005546666666667e-05,
      "loss": 0.0007,
      "step": 28100
    },
    {
      "epoch": 0.89952,
      "grad_norm": 4.134991645812988,
      "learning_rate": 1.4003413333333336e-05,
      "loss": 0.0073,
      "step": 28110
    },
    {
      "epoch": 0.89984,
      "grad_norm": 0.007813146337866783,
      "learning_rate": 1.4001280000000001e-05,
      "loss": 0.009,
      "step": 28120
    },
    {
      "epoch": 0.90016,
      "grad_norm": 0.03085905872285366,
      "learning_rate": 1.3999146666666668e-05,
      "loss": 0.0016,
      "step": 28130
    },
    {
      "epoch": 0.90048,
      "grad_norm": 0.004985417705029249,
      "learning_rate": 1.3997013333333334e-05,
      "loss": 0.0018,
      "step": 28140
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.01135605201125145,
      "learning_rate": 1.3994880000000002e-05,
      "loss": 0.0038,
      "step": 28150
    },
    {
      "epoch": 0.90112,
      "grad_norm": 0.029248395934700966,
      "learning_rate": 1.3992746666666668e-05,
      "loss": 0.0004,
      "step": 28160
    },
    {
      "epoch": 0.90144,
      "grad_norm": 0.0075722914189100266,
      "learning_rate": 1.3990613333333333e-05,
      "loss": 0.0003,
      "step": 28170
    },
    {
      "epoch": 0.90176,
      "grad_norm": 0.010415400378406048,
      "learning_rate": 1.3988480000000002e-05,
      "loss": 0.0032,
      "step": 28180
    },
    {
      "epoch": 0.90208,
      "grad_norm": 0.004946366883814335,
      "learning_rate": 1.3986346666666667e-05,
      "loss": 0.0005,
      "step": 28190
    },
    {
      "epoch": 0.9024,
      "grad_norm": 7.992459297180176,
      "learning_rate": 1.3984213333333334e-05,
      "loss": 0.0304,
      "step": 28200
    },
    {
      "epoch": 0.90272,
      "grad_norm": 0.0037607632111757994,
      "learning_rate": 1.398208e-05,
      "loss": 0.0062,
      "step": 28210
    },
    {
      "epoch": 0.90304,
      "grad_norm": 0.011458108201622963,
      "learning_rate": 1.3979946666666669e-05,
      "loss": 0.0006,
      "step": 28220
    },
    {
      "epoch": 0.90336,
      "grad_norm": 0.006711646448820829,
      "learning_rate": 1.3977813333333334e-05,
      "loss": 0.0267,
      "step": 28230
    },
    {
      "epoch": 0.90368,
      "grad_norm": 0.14845919609069824,
      "learning_rate": 1.3975680000000001e-05,
      "loss": 0.0636,
      "step": 28240
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.010766081511974335,
      "learning_rate": 1.3973546666666668e-05,
      "loss": 0.0004,
      "step": 28250
    },
    {
      "epoch": 0.90432,
      "grad_norm": 0.012538845650851727,
      "learning_rate": 1.3971413333333335e-05,
      "loss": 0.0013,
      "step": 28260
    },
    {
      "epoch": 0.90464,
      "grad_norm": 0.012636448256671429,
      "learning_rate": 1.396928e-05,
      "loss": 0.0031,
      "step": 28270
    },
    {
      "epoch": 0.90496,
      "grad_norm": 0.008310999721288681,
      "learning_rate": 1.396714666666667e-05,
      "loss": 0.0007,
      "step": 28280
    },
    {
      "epoch": 0.90528,
      "grad_norm": 0.004494406282901764,
      "learning_rate": 1.3965013333333335e-05,
      "loss": 0.0004,
      "step": 28290
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.020528841763734818,
      "learning_rate": 1.396288e-05,
      "loss": 0.0367,
      "step": 28300
    },
    {
      "epoch": 0.90592,
      "grad_norm": 0.004221379291266203,
      "learning_rate": 1.3960746666666667e-05,
      "loss": 0.004,
      "step": 28310
    },
    {
      "epoch": 0.90624,
      "grad_norm": 0.007340771611779928,
      "learning_rate": 1.3958613333333336e-05,
      "loss": 0.0005,
      "step": 28320
    },
    {
      "epoch": 0.90656,
      "grad_norm": 0.003911011852324009,
      "learning_rate": 1.3956480000000001e-05,
      "loss": 0.0004,
      "step": 28330
    },
    {
      "epoch": 0.90688,
      "grad_norm": 0.011646008118987083,
      "learning_rate": 1.3954346666666667e-05,
      "loss": 0.0073,
      "step": 28340
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.004226880148053169,
      "learning_rate": 1.3952213333333335e-05,
      "loss": 0.0003,
      "step": 28350
    },
    {
      "epoch": 0.90752,
      "grad_norm": 0.00967468786984682,
      "learning_rate": 1.395008e-05,
      "loss": 0.0346,
      "step": 28360
    },
    {
      "epoch": 0.90784,
      "grad_norm": 0.014687611721456051,
      "learning_rate": 1.3947946666666668e-05,
      "loss": 0.022,
      "step": 28370
    },
    {
      "epoch": 0.90816,
      "grad_norm": 0.12226223200559616,
      "learning_rate": 1.3945813333333333e-05,
      "loss": 0.053,
      "step": 28380
    },
    {
      "epoch": 0.90848,
      "grad_norm": 0.01942499913275242,
      "learning_rate": 1.3943680000000002e-05,
      "loss": 0.0004,
      "step": 28390
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.003995481878519058,
      "learning_rate": 1.3941546666666667e-05,
      "loss": 0.0007,
      "step": 28400
    },
    {
      "epoch": 0.90912,
      "grad_norm": 0.010470950976014137,
      "learning_rate": 1.3939413333333334e-05,
      "loss": 0.0069,
      "step": 28410
    },
    {
      "epoch": 0.90944,
      "grad_norm": 0.01611749827861786,
      "learning_rate": 1.3937280000000002e-05,
      "loss": 0.0005,
      "step": 28420
    },
    {
      "epoch": 0.90976,
      "grad_norm": 0.19365891814231873,
      "learning_rate": 1.3935146666666669e-05,
      "loss": 0.0008,
      "step": 28430
    },
    {
      "epoch": 0.91008,
      "grad_norm": 0.009384325705468655,
      "learning_rate": 1.3933013333333334e-05,
      "loss": 0.0006,
      "step": 28440
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.03969305381178856,
      "learning_rate": 1.3930880000000003e-05,
      "loss": 0.0014,
      "step": 28450
    },
    {
      "epoch": 0.91072,
      "grad_norm": 0.012486713007092476,
      "learning_rate": 1.3928746666666668e-05,
      "loss": 0.0004,
      "step": 28460
    },
    {
      "epoch": 0.91104,
      "grad_norm": 0.043481867760419846,
      "learning_rate": 1.3926613333333333e-05,
      "loss": 0.0066,
      "step": 28470
    },
    {
      "epoch": 0.91136,
      "grad_norm": 0.005913292523473501,
      "learning_rate": 1.392448e-05,
      "loss": 0.0004,
      "step": 28480
    },
    {
      "epoch": 0.91168,
      "grad_norm": 0.004332242999225855,
      "learning_rate": 1.3922346666666668e-05,
      "loss": 0.0004,
      "step": 28490
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.022452857345342636,
      "learning_rate": 1.3920213333333335e-05,
      "loss": 0.0004,
      "step": 28500
    },
    {
      "epoch": 0.91232,
      "grad_norm": 0.02644280344247818,
      "learning_rate": 1.391808e-05,
      "loss": 0.0004,
      "step": 28510
    },
    {
      "epoch": 0.91264,
      "grad_norm": 1.958300232887268,
      "learning_rate": 1.3915946666666669e-05,
      "loss": 0.024,
      "step": 28520
    },
    {
      "epoch": 0.91296,
      "grad_norm": 0.0018318819347769022,
      "learning_rate": 1.3913813333333334e-05,
      "loss": 0.0014,
      "step": 28530
    },
    {
      "epoch": 0.91328,
      "grad_norm": 0.008497785776853561,
      "learning_rate": 1.3911680000000001e-05,
      "loss": 0.0008,
      "step": 28540
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.004949324764311314,
      "learning_rate": 1.3909546666666667e-05,
      "loss": 0.0003,
      "step": 28550
    },
    {
      "epoch": 0.91392,
      "grad_norm": 0.01976737193763256,
      "learning_rate": 1.3907413333333335e-05,
      "loss": 0.0006,
      "step": 28560
    },
    {
      "epoch": 0.91424,
      "grad_norm": 0.004141046199947596,
      "learning_rate": 1.390528e-05,
      "loss": 0.0007,
      "step": 28570
    },
    {
      "epoch": 0.91456,
      "grad_norm": 0.006440572906285524,
      "learning_rate": 1.3903146666666666e-05,
      "loss": 0.0002,
      "step": 28580
    },
    {
      "epoch": 0.91488,
      "grad_norm": 0.004029099829494953,
      "learning_rate": 1.3901013333333335e-05,
      "loss": 0.0016,
      "step": 28590
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.006926964037120342,
      "learning_rate": 1.3898880000000002e-05,
      "loss": 0.0004,
      "step": 28600
    },
    {
      "epoch": 0.91552,
      "grad_norm": 0.02403980679810047,
      "learning_rate": 1.3896746666666667e-05,
      "loss": 0.0043,
      "step": 28610
    },
    {
      "epoch": 0.91584,
      "grad_norm": 0.005053895525634289,
      "learning_rate": 1.3894613333333336e-05,
      "loss": 0.0003,
      "step": 28620
    },
    {
      "epoch": 0.91616,
      "grad_norm": 0.004897178616374731,
      "learning_rate": 1.3892480000000002e-05,
      "loss": 0.0002,
      "step": 28630
    },
    {
      "epoch": 0.91648,
      "grad_norm": 0.004926925525069237,
      "learning_rate": 1.3890346666666667e-05,
      "loss": 0.0003,
      "step": 28640
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.007716260384768248,
      "learning_rate": 1.3888213333333334e-05,
      "loss": 0.0003,
      "step": 28650
    },
    {
      "epoch": 0.91712,
      "grad_norm": 0.29790666699409485,
      "learning_rate": 1.3886080000000001e-05,
      "loss": 0.0009,
      "step": 28660
    },
    {
      "epoch": 0.91744,
      "grad_norm": 1.7222446203231812,
      "learning_rate": 1.3883946666666668e-05,
      "loss": 0.0528,
      "step": 28670
    },
    {
      "epoch": 0.91776,
      "grad_norm": 2.2239115238189697,
      "learning_rate": 1.3881813333333334e-05,
      "loss": 0.0031,
      "step": 28680
    },
    {
      "epoch": 0.91808,
      "grad_norm": 0.0044382731430232525,
      "learning_rate": 1.3879680000000002e-05,
      "loss": 0.0003,
      "step": 28690
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.004359063692390919,
      "learning_rate": 1.3877546666666668e-05,
      "loss": 0.0003,
      "step": 28700
    },
    {
      "epoch": 0.91872,
      "grad_norm": 0.005449594464153051,
      "learning_rate": 1.3875413333333335e-05,
      "loss": 0.0003,
      "step": 28710
    },
    {
      "epoch": 0.91904,
      "grad_norm": 0.013203023932874203,
      "learning_rate": 1.387328e-05,
      "loss": 0.0003,
      "step": 28720
    },
    {
      "epoch": 0.91936,
      "grad_norm": 0.004228352103382349,
      "learning_rate": 1.3871146666666669e-05,
      "loss": 0.001,
      "step": 28730
    },
    {
      "epoch": 0.91968,
      "grad_norm": 0.004077441990375519,
      "learning_rate": 1.3869013333333334e-05,
      "loss": 0.0004,
      "step": 28740
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.002898833714425564,
      "learning_rate": 1.386688e-05,
      "loss": 0.0005,
      "step": 28750
    },
    {
      "epoch": 0.92032,
      "grad_norm": 0.00976494513452053,
      "learning_rate": 1.3864746666666668e-05,
      "loss": 0.0002,
      "step": 28760
    },
    {
      "epoch": 0.92064,
      "grad_norm": 0.005985110066831112,
      "learning_rate": 1.3862613333333334e-05,
      "loss": 0.0002,
      "step": 28770
    },
    {
      "epoch": 0.92096,
      "grad_norm": 0.03799191117286682,
      "learning_rate": 1.386048e-05,
      "loss": 0.0005,
      "step": 28780
    },
    {
      "epoch": 0.92128,
      "grad_norm": 0.010016809217631817,
      "learning_rate": 1.385834666666667e-05,
      "loss": 0.0276,
      "step": 28790
    },
    {
      "epoch": 0.9216,
      "grad_norm": 1.344788670539856,
      "learning_rate": 1.3856213333333335e-05,
      "loss": 0.0016,
      "step": 28800
    },
    {
      "epoch": 0.92192,
      "grad_norm": 0.009371316060423851,
      "learning_rate": 1.385408e-05,
      "loss": 0.0002,
      "step": 28810
    },
    {
      "epoch": 0.92224,
      "grad_norm": 0.007052295841276646,
      "learning_rate": 1.3851946666666667e-05,
      "loss": 0.0002,
      "step": 28820
    },
    {
      "epoch": 0.92256,
      "grad_norm": 0.005396521184593439,
      "learning_rate": 1.3849813333333334e-05,
      "loss": 0.0342,
      "step": 28830
    },
    {
      "epoch": 0.92288,
      "grad_norm": 0.0028189122676849365,
      "learning_rate": 1.3847680000000002e-05,
      "loss": 0.0002,
      "step": 28840
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.0091567886993289,
      "learning_rate": 1.3845546666666667e-05,
      "loss": 0.0297,
      "step": 28850
    },
    {
      "epoch": 0.92352,
      "grad_norm": 0.007198769599199295,
      "learning_rate": 1.3843413333333336e-05,
      "loss": 0.0004,
      "step": 28860
    },
    {
      "epoch": 0.92384,
      "grad_norm": 0.0055541591718792915,
      "learning_rate": 1.3841280000000001e-05,
      "loss": 0.0004,
      "step": 28870
    },
    {
      "epoch": 0.92416,
      "grad_norm": 0.0063947695307433605,
      "learning_rate": 1.3839146666666668e-05,
      "loss": 0.0018,
      "step": 28880
    },
    {
      "epoch": 0.92448,
      "grad_norm": 0.005637927912175655,
      "learning_rate": 1.3837013333333334e-05,
      "loss": 0.0003,
      "step": 28890
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.0020720278844237328,
      "learning_rate": 1.3834880000000002e-05,
      "loss": 0.0005,
      "step": 28900
    },
    {
      "epoch": 0.92512,
      "grad_norm": 0.006136929616332054,
      "learning_rate": 1.3832746666666668e-05,
      "loss": 0.0003,
      "step": 28910
    },
    {
      "epoch": 0.92544,
      "grad_norm": 0.00547671178355813,
      "learning_rate": 1.3830613333333333e-05,
      "loss": 0.0006,
      "step": 28920
    },
    {
      "epoch": 0.92576,
      "grad_norm": 0.007665058597922325,
      "learning_rate": 1.3828480000000002e-05,
      "loss": 0.0003,
      "step": 28930
    },
    {
      "epoch": 0.92608,
      "grad_norm": 0.00796232558786869,
      "learning_rate": 1.3826346666666667e-05,
      "loss": 0.0131,
      "step": 28940
    },
    {
      "epoch": 0.9264,
      "grad_norm": 1.7699819803237915,
      "learning_rate": 1.3824213333333334e-05,
      "loss": 0.0421,
      "step": 28950
    },
    {
      "epoch": 0.92672,
      "grad_norm": 0.005679958499968052,
      "learning_rate": 1.3822080000000001e-05,
      "loss": 0.0004,
      "step": 28960
    },
    {
      "epoch": 0.92704,
      "grad_norm": 0.005059768911451101,
      "learning_rate": 1.3819946666666668e-05,
      "loss": 0.0636,
      "step": 28970
    },
    {
      "epoch": 0.92736,
      "grad_norm": 0.008978203870356083,
      "learning_rate": 1.3817813333333334e-05,
      "loss": 0.0005,
      "step": 28980
    },
    {
      "epoch": 0.92768,
      "grad_norm": 0.011851000599563122,
      "learning_rate": 1.3815680000000001e-05,
      "loss": 0.091,
      "step": 28990
    },
    {
      "epoch": 0.928,
      "grad_norm": 3.100641965866089,
      "learning_rate": 1.3813546666666668e-05,
      "loss": 0.0061,
      "step": 29000
    },
    {
      "epoch": 0.92832,
      "grad_norm": 0.017191782593727112,
      "learning_rate": 1.3811413333333335e-05,
      "loss": 0.0004,
      "step": 29010
    },
    {
      "epoch": 0.92864,
      "grad_norm": 0.004810163285583258,
      "learning_rate": 1.380928e-05,
      "loss": 0.0504,
      "step": 29020
    },
    {
      "epoch": 0.92896,
      "grad_norm": 0.010509880259633064,
      "learning_rate": 1.3807146666666669e-05,
      "loss": 0.0003,
      "step": 29030
    },
    {
      "epoch": 0.92928,
      "grad_norm": 0.004608028568327427,
      "learning_rate": 1.3805013333333335e-05,
      "loss": 0.0008,
      "step": 29040
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.013537413440644741,
      "learning_rate": 1.380288e-05,
      "loss": 0.0006,
      "step": 29050
    },
    {
      "epoch": 0.92992,
      "grad_norm": 0.006334169302135706,
      "learning_rate": 1.3800746666666667e-05,
      "loss": 0.0007,
      "step": 29060
    },
    {
      "epoch": 0.93024,
      "grad_norm": 0.004823296330869198,
      "learning_rate": 1.3798613333333336e-05,
      "loss": 0.0004,
      "step": 29070
    },
    {
      "epoch": 0.93056,
      "grad_norm": 0.004639934282749891,
      "learning_rate": 1.3796480000000001e-05,
      "loss": 0.0119,
      "step": 29080
    },
    {
      "epoch": 0.93088,
      "grad_norm": 1.8537176847457886,
      "learning_rate": 1.3794346666666666e-05,
      "loss": 0.0432,
      "step": 29090
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.01467435248196125,
      "learning_rate": 1.3792213333333335e-05,
      "loss": 0.0024,
      "step": 29100
    },
    {
      "epoch": 0.93152,
      "grad_norm": 0.007724469061940908,
      "learning_rate": 1.379008e-05,
      "loss": 0.0006,
      "step": 29110
    },
    {
      "epoch": 0.93184,
      "grad_norm": 0.018318742513656616,
      "learning_rate": 1.3787946666666668e-05,
      "loss": 0.0004,
      "step": 29120
    },
    {
      "epoch": 0.93216,
      "grad_norm": 2.148668050765991,
      "learning_rate": 1.3785813333333335e-05,
      "loss": 0.0093,
      "step": 29130
    },
    {
      "epoch": 0.93248,
      "grad_norm": 5.449027061462402,
      "learning_rate": 1.3783680000000002e-05,
      "loss": 0.0171,
      "step": 29140
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.0066634174436330795,
      "learning_rate": 1.3781546666666667e-05,
      "loss": 0.0004,
      "step": 29150
    },
    {
      "epoch": 0.93312,
      "grad_norm": 0.004372658673673868,
      "learning_rate": 1.3779413333333334e-05,
      "loss": 0.0003,
      "step": 29160
    },
    {
      "epoch": 0.93344,
      "grad_norm": 0.006036639213562012,
      "learning_rate": 1.3777280000000001e-05,
      "loss": 0.046,
      "step": 29170
    },
    {
      "epoch": 0.93376,
      "grad_norm": 0.007008710410445929,
      "learning_rate": 1.3775146666666668e-05,
      "loss": 0.0005,
      "step": 29180
    },
    {
      "epoch": 0.93408,
      "grad_norm": 0.004052163101732731,
      "learning_rate": 1.3773013333333334e-05,
      "loss": 0.0006,
      "step": 29190
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.006626300048083067,
      "learning_rate": 1.3770880000000003e-05,
      "loss": 0.0005,
      "step": 29200
    },
    {
      "epoch": 0.93472,
      "grad_norm": 0.00489778770133853,
      "learning_rate": 1.3768746666666668e-05,
      "loss": 0.0014,
      "step": 29210
    },
    {
      "epoch": 0.93504,
      "grad_norm": 0.006209782790392637,
      "learning_rate": 1.3766613333333333e-05,
      "loss": 0.0009,
      "step": 29220
    },
    {
      "epoch": 0.93536,
      "grad_norm": 0.034057144075632095,
      "learning_rate": 1.376448e-05,
      "loss": 0.0004,
      "step": 29230
    },
    {
      "epoch": 0.93568,
      "grad_norm": 0.03401898220181465,
      "learning_rate": 1.3762346666666667e-05,
      "loss": 0.0542,
      "step": 29240
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.006560001987963915,
      "learning_rate": 1.3760213333333335e-05,
      "loss": 0.0151,
      "step": 29250
    },
    {
      "epoch": 0.93632,
      "grad_norm": 0.010706830769777298,
      "learning_rate": 1.375808e-05,
      "loss": 0.0009,
      "step": 29260
    },
    {
      "epoch": 0.93664,
      "grad_norm": 0.020083339884877205,
      "learning_rate": 1.3755946666666669e-05,
      "loss": 0.0005,
      "step": 29270
    },
    {
      "epoch": 0.93696,
      "grad_norm": 0.025284431874752045,
      "learning_rate": 1.3753813333333334e-05,
      "loss": 0.0024,
      "step": 29280
    },
    {
      "epoch": 0.93728,
      "grad_norm": 0.004704832099378109,
      "learning_rate": 1.3751680000000001e-05,
      "loss": 0.0003,
      "step": 29290
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.011754175648093224,
      "learning_rate": 1.3749546666666668e-05,
      "loss": 0.0004,
      "step": 29300
    },
    {
      "epoch": 0.93792,
      "grad_norm": 0.00939231738448143,
      "learning_rate": 1.3747413333333335e-05,
      "loss": 0.055,
      "step": 29310
    },
    {
      "epoch": 0.93824,
      "grad_norm": 0.01456616260111332,
      "learning_rate": 1.374528e-05,
      "loss": 0.0005,
      "step": 29320
    },
    {
      "epoch": 0.93856,
      "grad_norm": 0.04387693107128143,
      "learning_rate": 1.3743146666666666e-05,
      "loss": 0.0005,
      "step": 29330
    },
    {
      "epoch": 0.93888,
      "grad_norm": 0.010708598420023918,
      "learning_rate": 1.3741013333333335e-05,
      "loss": 0.014,
      "step": 29340
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.005857545882463455,
      "learning_rate": 1.3738880000000002e-05,
      "loss": 0.0412,
      "step": 29350
    },
    {
      "epoch": 0.93952,
      "grad_norm": 0.01202984619885683,
      "learning_rate": 1.3736746666666667e-05,
      "loss": 0.0006,
      "step": 29360
    },
    {
      "epoch": 0.93984,
      "grad_norm": 0.014261065982282162,
      "learning_rate": 1.3734613333333336e-05,
      "loss": 0.0015,
      "step": 29370
    },
    {
      "epoch": 0.94016,
      "grad_norm": 0.008500459603965282,
      "learning_rate": 1.3732480000000001e-05,
      "loss": 0.0008,
      "step": 29380
    },
    {
      "epoch": 0.94048,
      "grad_norm": 0.010587341152131557,
      "learning_rate": 1.3730346666666667e-05,
      "loss": 0.0005,
      "step": 29390
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.014082754030823708,
      "learning_rate": 1.3728213333333334e-05,
      "loss": 0.0014,
      "step": 29400
    },
    {
      "epoch": 0.94112,
      "grad_norm": 0.022628916427493095,
      "learning_rate": 1.3726080000000001e-05,
      "loss": 0.0076,
      "step": 29410
    },
    {
      "epoch": 0.94144,
      "grad_norm": 0.007437942549586296,
      "learning_rate": 1.3723946666666668e-05,
      "loss": 0.0004,
      "step": 29420
    },
    {
      "epoch": 0.94176,
      "grad_norm": 0.007512358482927084,
      "learning_rate": 1.3721813333333333e-05,
      "loss": 0.0007,
      "step": 29430
    },
    {
      "epoch": 0.94208,
      "grad_norm": 0.01105406042188406,
      "learning_rate": 1.3719680000000002e-05,
      "loss": 0.0071,
      "step": 29440
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.0038224407471716404,
      "learning_rate": 1.3717546666666667e-05,
      "loss": 0.0347,
      "step": 29450
    },
    {
      "epoch": 0.94272,
      "grad_norm": 0.22827838361263275,
      "learning_rate": 1.3715413333333335e-05,
      "loss": 0.0006,
      "step": 29460
    },
    {
      "epoch": 0.94304,
      "grad_norm": 0.008050646632909775,
      "learning_rate": 1.3713280000000002e-05,
      "loss": 0.0004,
      "step": 29470
    },
    {
      "epoch": 0.94336,
      "grad_norm": 0.020008966326713562,
      "learning_rate": 1.3711146666666669e-05,
      "loss": 0.0005,
      "step": 29480
    },
    {
      "epoch": 0.94368,
      "grad_norm": 0.0075273229740560055,
      "learning_rate": 1.3709013333333334e-05,
      "loss": 0.0154,
      "step": 29490
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.02519235759973526,
      "learning_rate": 1.370688e-05,
      "loss": 0.0006,
      "step": 29500
    },
    {
      "epoch": 0.94432,
      "grad_norm": 0.004528129007667303,
      "learning_rate": 1.3704746666666668e-05,
      "loss": 0.0479,
      "step": 29510
    },
    {
      "epoch": 0.94464,
      "grad_norm": 0.05582689866423607,
      "learning_rate": 1.3702613333333334e-05,
      "loss": 0.0007,
      "step": 29520
    },
    {
      "epoch": 0.94496,
      "grad_norm": 0.01176716573536396,
      "learning_rate": 1.370048e-05,
      "loss": 0.0004,
      "step": 29530
    },
    {
      "epoch": 0.94528,
      "grad_norm": 0.012965734116733074,
      "learning_rate": 1.369834666666667e-05,
      "loss": 0.0149,
      "step": 29540
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.003657394554466009,
      "learning_rate": 1.3696213333333335e-05,
      "loss": 0.0008,
      "step": 29550
    },
    {
      "epoch": 0.94592,
      "grad_norm": 0.015110238455235958,
      "learning_rate": 1.369408e-05,
      "loss": 0.0004,
      "step": 29560
    },
    {
      "epoch": 0.94624,
      "grad_norm": 0.0033495784737169743,
      "learning_rate": 1.3691946666666667e-05,
      "loss": 0.0147,
      "step": 29570
    },
    {
      "epoch": 0.94656,
      "grad_norm": 0.015009858645498753,
      "learning_rate": 1.3689813333333334e-05,
      "loss": 0.0006,
      "step": 29580
    },
    {
      "epoch": 0.94688,
      "grad_norm": 0.013448046520352364,
      "learning_rate": 1.3687680000000001e-05,
      "loss": 0.0057,
      "step": 29590
    },
    {
      "epoch": 0.9472,
      "grad_norm": 4.93174409866333,
      "learning_rate": 1.3685546666666667e-05,
      "loss": 0.0172,
      "step": 29600
    },
    {
      "epoch": 0.94752,
      "grad_norm": 0.009250367991626263,
      "learning_rate": 1.3683413333333336e-05,
      "loss": 0.0004,
      "step": 29610
    },
    {
      "epoch": 0.94784,
      "grad_norm": 0.004123765043914318,
      "learning_rate": 1.3681280000000001e-05,
      "loss": 0.0146,
      "step": 29620
    },
    {
      "epoch": 0.94816,
      "grad_norm": 3.2883572578430176,
      "learning_rate": 1.3679146666666668e-05,
      "loss": 0.0169,
      "step": 29630
    },
    {
      "epoch": 0.94848,
      "grad_norm": 0.01490712072700262,
      "learning_rate": 1.3677013333333335e-05,
      "loss": 0.0003,
      "step": 29640
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.01777558960020542,
      "learning_rate": 1.3674880000000002e-05,
      "loss": 0.0004,
      "step": 29650
    },
    {
      "epoch": 0.94912,
      "grad_norm": 0.01601010002195835,
      "learning_rate": 1.3672746666666668e-05,
      "loss": 0.0007,
      "step": 29660
    },
    {
      "epoch": 0.94944,
      "grad_norm": 0.010495693422853947,
      "learning_rate": 1.3670613333333333e-05,
      "loss": 0.0011,
      "step": 29670
    },
    {
      "epoch": 0.94976,
      "grad_norm": 0.015196211636066437,
      "learning_rate": 1.3668480000000002e-05,
      "loss": 0.0013,
      "step": 29680
    },
    {
      "epoch": 0.95008,
      "grad_norm": 0.006130633410066366,
      "learning_rate": 1.3666346666666667e-05,
      "loss": 0.0073,
      "step": 29690
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.012418385595083237,
      "learning_rate": 1.3664213333333334e-05,
      "loss": 0.001,
      "step": 29700
    },
    {
      "epoch": 0.95072,
      "grad_norm": 0.03446812182664871,
      "learning_rate": 1.3662080000000001e-05,
      "loss": 0.0003,
      "step": 29710
    },
    {
      "epoch": 0.95104,
      "grad_norm": 0.008167588151991367,
      "learning_rate": 1.3659946666666668e-05,
      "loss": 0.0319,
      "step": 29720
    },
    {
      "epoch": 0.95136,
      "grad_norm": 0.008916542865335941,
      "learning_rate": 1.3657813333333334e-05,
      "loss": 0.0004,
      "step": 29730
    },
    {
      "epoch": 0.95168,
      "grad_norm": 0.0038876717444509268,
      "learning_rate": 1.365568e-05,
      "loss": 0.0069,
      "step": 29740
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.02517245151102543,
      "learning_rate": 1.3653546666666668e-05,
      "loss": 0.0341,
      "step": 29750
    },
    {
      "epoch": 0.95232,
      "grad_norm": 0.0178033709526062,
      "learning_rate": 1.3651413333333335e-05,
      "loss": 0.0005,
      "step": 29760
    },
    {
      "epoch": 0.95264,
      "grad_norm": 0.008579776622354984,
      "learning_rate": 1.364928e-05,
      "loss": 0.0126,
      "step": 29770
    },
    {
      "epoch": 0.95296,
      "grad_norm": 0.05026239901781082,
      "learning_rate": 1.3647146666666669e-05,
      "loss": 0.002,
      "step": 29780
    },
    {
      "epoch": 0.95328,
      "grad_norm": 0.008678465150296688,
      "learning_rate": 1.3645013333333334e-05,
      "loss": 0.0003,
      "step": 29790
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.006592827383428812,
      "learning_rate": 1.364288e-05,
      "loss": 0.0006,
      "step": 29800
    },
    {
      "epoch": 0.95392,
      "grad_norm": 0.03632678836584091,
      "learning_rate": 1.3640746666666668e-05,
      "loss": 0.0456,
      "step": 29810
    },
    {
      "epoch": 0.95424,
      "grad_norm": 0.005846081301569939,
      "learning_rate": 1.3638613333333336e-05,
      "loss": 0.0004,
      "step": 29820
    },
    {
      "epoch": 0.95456,
      "grad_norm": 0.0075959935784339905,
      "learning_rate": 1.3636480000000001e-05,
      "loss": 0.0003,
      "step": 29830
    },
    {
      "epoch": 0.95488,
      "grad_norm": 0.011370779946446419,
      "learning_rate": 1.3634346666666666e-05,
      "loss": 0.0004,
      "step": 29840
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.004914984107017517,
      "learning_rate": 1.3632213333333335e-05,
      "loss": 0.0003,
      "step": 29850
    },
    {
      "epoch": 0.95552,
      "grad_norm": 0.006098222453147173,
      "learning_rate": 1.363008e-05,
      "loss": 0.0182,
      "step": 29860
    },
    {
      "epoch": 0.95584,
      "grad_norm": 0.010970561765134335,
      "learning_rate": 1.3627946666666668e-05,
      "loss": 0.0182,
      "step": 29870
    },
    {
      "epoch": 0.95616,
      "grad_norm": 0.018774626776576042,
      "learning_rate": 1.3625813333333335e-05,
      "loss": 0.0005,
      "step": 29880
    },
    {
      "epoch": 0.95648,
      "grad_norm": 0.016207700595259666,
      "learning_rate": 1.3623680000000002e-05,
      "loss": 0.0006,
      "step": 29890
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.021375881507992744,
      "learning_rate": 1.3621546666666667e-05,
      "loss": 0.0054,
      "step": 29900
    },
    {
      "epoch": 0.95712,
      "grad_norm": 0.0055805048905313015,
      "learning_rate": 1.3619413333333334e-05,
      "loss": 0.0104,
      "step": 29910
    },
    {
      "epoch": 0.95744,
      "grad_norm": 0.010873104445636272,
      "learning_rate": 1.3617280000000001e-05,
      "loss": 0.0046,
      "step": 29920
    },
    {
      "epoch": 0.95776,
      "grad_norm": 0.029541971161961555,
      "learning_rate": 1.3615146666666668e-05,
      "loss": 0.0348,
      "step": 29930
    },
    {
      "epoch": 0.95808,
      "grad_norm": 0.01447138749063015,
      "learning_rate": 1.3613013333333334e-05,
      "loss": 0.0008,
      "step": 29940
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.010578843764960766,
      "learning_rate": 1.3610880000000002e-05,
      "loss": 0.0007,
      "step": 29950
    },
    {
      "epoch": 0.95872,
      "grad_norm": 0.008507356978952885,
      "learning_rate": 1.3608746666666668e-05,
      "loss": 0.0005,
      "step": 29960
    },
    {
      "epoch": 0.95904,
      "grad_norm": 0.006094710435718298,
      "learning_rate": 1.3606613333333333e-05,
      "loss": 0.0309,
      "step": 29970
    },
    {
      "epoch": 0.95936,
      "grad_norm": 0.005058336071670055,
      "learning_rate": 1.3604480000000002e-05,
      "loss": 0.0553,
      "step": 29980
    },
    {
      "epoch": 0.95968,
      "grad_norm": 0.009183024987578392,
      "learning_rate": 1.3602346666666667e-05,
      "loss": 0.0004,
      "step": 29990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.01758759655058384,
      "learning_rate": 1.3600213333333334e-05,
      "loss": 0.0044,
      "step": 30000
    },
    {
      "epoch": 0.96032,
      "grad_norm": 0.042652737349271774,
      "learning_rate": 1.359808e-05,
      "loss": 0.0004,
      "step": 30010
    },
    {
      "epoch": 0.96064,
      "grad_norm": 0.013143557123839855,
      "learning_rate": 1.3595946666666669e-05,
      "loss": 0.0005,
      "step": 30020
    },
    {
      "epoch": 0.96096,
      "grad_norm": 0.0067819831892848015,
      "learning_rate": 1.3593813333333334e-05,
      "loss": 0.0004,
      "step": 30030
    },
    {
      "epoch": 0.96128,
      "grad_norm": 0.005144335795193911,
      "learning_rate": 1.3591680000000001e-05,
      "loss": 0.0178,
      "step": 30040
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.0044499593786895275,
      "learning_rate": 1.3589546666666668e-05,
      "loss": 0.001,
      "step": 30050
    },
    {
      "epoch": 0.96192,
      "grad_norm": 0.01485105324536562,
      "learning_rate": 1.3587413333333335e-05,
      "loss": 0.0013,
      "step": 30060
    },
    {
      "epoch": 0.96224,
      "grad_norm": 0.006315840873867273,
      "learning_rate": 1.358528e-05,
      "loss": 0.0009,
      "step": 30070
    },
    {
      "epoch": 0.96256,
      "grad_norm": 0.0059559582732617855,
      "learning_rate": 1.3583146666666666e-05,
      "loss": 0.0061,
      "step": 30080
    },
    {
      "epoch": 0.96288,
      "grad_norm": 0.04391009360551834,
      "learning_rate": 1.3581013333333335e-05,
      "loss": 0.0056,
      "step": 30090
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.007481875829398632,
      "learning_rate": 1.3578880000000002e-05,
      "loss": 0.0004,
      "step": 30100
    },
    {
      "epoch": 0.96352,
      "grad_norm": 0.013783843256533146,
      "learning_rate": 1.3576746666666667e-05,
      "loss": 0.0003,
      "step": 30110
    },
    {
      "epoch": 0.96384,
      "grad_norm": 0.008086556568741798,
      "learning_rate": 1.3574613333333336e-05,
      "loss": 0.0002,
      "step": 30120
    },
    {
      "epoch": 0.96416,
      "grad_norm": 0.005174408201128244,
      "learning_rate": 1.3572480000000001e-05,
      "loss": 0.0019,
      "step": 30130
    },
    {
      "epoch": 0.96448,
      "grad_norm": 0.00789160467684269,
      "learning_rate": 1.3570346666666667e-05,
      "loss": 0.0006,
      "step": 30140
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.0046337079256772995,
      "learning_rate": 1.3568213333333335e-05,
      "loss": 0.0004,
      "step": 30150
    },
    {
      "epoch": 0.96512,
      "grad_norm": 0.010076765902340412,
      "learning_rate": 1.356608e-05,
      "loss": 0.0003,
      "step": 30160
    },
    {
      "epoch": 0.96544,
      "grad_norm": 0.03957086801528931,
      "learning_rate": 1.3563946666666668e-05,
      "loss": 0.0004,
      "step": 30170
    },
    {
      "epoch": 0.96576,
      "grad_norm": 0.005494988523423672,
      "learning_rate": 1.3561813333333333e-05,
      "loss": 0.0003,
      "step": 30180
    },
    {
      "epoch": 0.96608,
      "grad_norm": 0.040826763957738876,
      "learning_rate": 1.3559680000000002e-05,
      "loss": 0.0003,
      "step": 30190
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.0033272842410951853,
      "learning_rate": 1.3557546666666667e-05,
      "loss": 0.0002,
      "step": 30200
    },
    {
      "epoch": 0.96672,
      "grad_norm": 0.004439757205545902,
      "learning_rate": 1.3555413333333334e-05,
      "loss": 0.0004,
      "step": 30210
    },
    {
      "epoch": 0.96704,
      "grad_norm": 0.005777436308562756,
      "learning_rate": 1.3553280000000001e-05,
      "loss": 0.0003,
      "step": 30220
    },
    {
      "epoch": 0.96736,
      "grad_norm": 0.028919367119669914,
      "learning_rate": 1.3551146666666669e-05,
      "loss": 0.0004,
      "step": 30230
    },
    {
      "epoch": 0.96768,
      "grad_norm": 0.004680532030761242,
      "learning_rate": 1.3549013333333334e-05,
      "loss": 0.0002,
      "step": 30240
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.004352845251560211,
      "learning_rate": 1.354688e-05,
      "loss": 0.0002,
      "step": 30250
    },
    {
      "epoch": 0.96832,
      "grad_norm": 0.009200477972626686,
      "learning_rate": 1.3544746666666668e-05,
      "loss": 0.0003,
      "step": 30260
    },
    {
      "epoch": 0.96864,
      "grad_norm": 0.005810340400785208,
      "learning_rate": 1.3542613333333333e-05,
      "loss": 0.0448,
      "step": 30270
    },
    {
      "epoch": 0.96896,
      "grad_norm": 2.3281476497650146,
      "learning_rate": 1.354048e-05,
      "loss": 0.0106,
      "step": 30280
    },
    {
      "epoch": 0.96928,
      "grad_norm": 0.013371186330914497,
      "learning_rate": 1.353834666666667e-05,
      "loss": 0.0005,
      "step": 30290
    },
    {
      "epoch": 0.9696,
      "grad_norm": 4.078163146972656,
      "learning_rate": 1.3536213333333335e-05,
      "loss": 0.0222,
      "step": 30300
    },
    {
      "epoch": 0.96992,
      "grad_norm": 0.011690545827150345,
      "learning_rate": 1.353408e-05,
      "loss": 0.0243,
      "step": 30310
    },
    {
      "epoch": 0.97024,
      "grad_norm": 0.16064724326133728,
      "learning_rate": 1.3531946666666669e-05,
      "loss": 0.0007,
      "step": 30320
    },
    {
      "epoch": 0.97056,
      "grad_norm": 0.013017695397138596,
      "learning_rate": 1.3529813333333334e-05,
      "loss": 0.0004,
      "step": 30330
    },
    {
      "epoch": 0.97088,
      "grad_norm": 0.002499892609193921,
      "learning_rate": 1.3527680000000001e-05,
      "loss": 0.0002,
      "step": 30340
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.18264853954315186,
      "learning_rate": 1.3525546666666667e-05,
      "loss": 0.0006,
      "step": 30350
    },
    {
      "epoch": 0.97152,
      "grad_norm": 0.017546525225043297,
      "learning_rate": 1.3523413333333335e-05,
      "loss": 0.0026,
      "step": 30360
    },
    {
      "epoch": 0.97184,
      "grad_norm": 0.006077297031879425,
      "learning_rate": 1.352128e-05,
      "loss": 0.0004,
      "step": 30370
    },
    {
      "epoch": 0.97216,
      "grad_norm": 0.0064515620470047,
      "learning_rate": 1.3519146666666668e-05,
      "loss": 0.0003,
      "step": 30380
    },
    {
      "epoch": 0.97248,
      "grad_norm": 0.004672213923186064,
      "learning_rate": 1.3517013333333335e-05,
      "loss": 0.0075,
      "step": 30390
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.003857241477817297,
      "learning_rate": 1.3514880000000002e-05,
      "loss": 0.0003,
      "step": 30400
    },
    {
      "epoch": 0.97312,
      "grad_norm": 0.010934511199593544,
      "learning_rate": 1.3512746666666667e-05,
      "loss": 0.0004,
      "step": 30410
    },
    {
      "epoch": 0.97344,
      "grad_norm": 0.0046039484441280365,
      "learning_rate": 1.3510613333333333e-05,
      "loss": 0.0306,
      "step": 30420
    },
    {
      "epoch": 0.97376,
      "grad_norm": 0.012501231394708157,
      "learning_rate": 1.3508480000000001e-05,
      "loss": 0.0234,
      "step": 30430
    },
    {
      "epoch": 0.97408,
      "grad_norm": 0.002916343742981553,
      "learning_rate": 1.3506346666666667e-05,
      "loss": 0.0003,
      "step": 30440
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.004750300198793411,
      "learning_rate": 1.3504213333333334e-05,
      "loss": 0.0004,
      "step": 30450
    },
    {
      "epoch": 0.97472,
      "grad_norm": 0.26874831318855286,
      "learning_rate": 1.3502080000000001e-05,
      "loss": 0.0006,
      "step": 30460
    },
    {
      "epoch": 0.97504,
      "grad_norm": 0.008295970968902111,
      "learning_rate": 1.3499946666666668e-05,
      "loss": 0.0004,
      "step": 30470
    },
    {
      "epoch": 0.97536,
      "grad_norm": 0.004350916016846895,
      "learning_rate": 1.3497813333333333e-05,
      "loss": 0.0004,
      "step": 30480
    },
    {
      "epoch": 0.97568,
      "grad_norm": 0.02162339724600315,
      "learning_rate": 1.3495680000000002e-05,
      "loss": 0.0011,
      "step": 30490
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.002463951474055648,
      "learning_rate": 1.3493546666666668e-05,
      "loss": 0.0208,
      "step": 30500
    },
    {
      "epoch": 0.97632,
      "grad_norm": 0.006205265410244465,
      "learning_rate": 1.3491413333333335e-05,
      "loss": 0.0249,
      "step": 30510
    },
    {
      "epoch": 0.97664,
      "grad_norm": 0.0032865875400602818,
      "learning_rate": 1.348928e-05,
      "loss": 0.043,
      "step": 30520
    },
    {
      "epoch": 0.97696,
      "grad_norm": 0.0057627297937870026,
      "learning_rate": 1.3487146666666669e-05,
      "loss": 0.0006,
      "step": 30530
    },
    {
      "epoch": 0.97728,
      "grad_norm": 0.031544849276542664,
      "learning_rate": 1.3485013333333334e-05,
      "loss": 0.0004,
      "step": 30540
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.006325156427919865,
      "learning_rate": 1.3482880000000001e-05,
      "loss": 0.0008,
      "step": 30550
    },
    {
      "epoch": 0.97792,
      "grad_norm": 0.010210554115474224,
      "learning_rate": 1.3480746666666668e-05,
      "loss": 0.0181,
      "step": 30560
    },
    {
      "epoch": 0.97824,
      "grad_norm": 0.010540721006691456,
      "learning_rate": 1.3478613333333335e-05,
      "loss": 0.0002,
      "step": 30570
    },
    {
      "epoch": 0.97856,
      "grad_norm": 0.00407407945021987,
      "learning_rate": 1.347648e-05,
      "loss": 0.0004,
      "step": 30580
    },
    {
      "epoch": 0.97888,
      "grad_norm": 0.0032243365421891212,
      "learning_rate": 1.3474346666666666e-05,
      "loss": 0.0014,
      "step": 30590
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.004214578773826361,
      "learning_rate": 1.3472213333333335e-05,
      "loss": 0.0003,
      "step": 30600
    },
    {
      "epoch": 0.97952,
      "grad_norm": 0.047490209341049194,
      "learning_rate": 1.347008e-05,
      "loss": 0.0004,
      "step": 30610
    },
    {
      "epoch": 0.97984,
      "grad_norm": 0.00532775791361928,
      "learning_rate": 1.3467946666666667e-05,
      "loss": 0.0004,
      "step": 30620
    },
    {
      "epoch": 0.98016,
      "grad_norm": 0.022767623886466026,
      "learning_rate": 1.3465813333333334e-05,
      "loss": 0.0289,
      "step": 30630
    },
    {
      "epoch": 0.98048,
      "grad_norm": 0.0037302931305021048,
      "learning_rate": 1.3463680000000002e-05,
      "loss": 0.0006,
      "step": 30640
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.0045378184877336025,
      "learning_rate": 1.3461546666666667e-05,
      "loss": 0.0002,
      "step": 30650
    },
    {
      "epoch": 0.98112,
      "grad_norm": 4.182582378387451,
      "learning_rate": 1.3459413333333336e-05,
      "loss": 0.0223,
      "step": 30660
    },
    {
      "epoch": 0.98144,
      "grad_norm": 0.010394949465990067,
      "learning_rate": 1.3457280000000001e-05,
      "loss": 0.0004,
      "step": 30670
    },
    {
      "epoch": 0.98176,
      "grad_norm": 0.007573406677693129,
      "learning_rate": 1.3455146666666668e-05,
      "loss": 0.0003,
      "step": 30680
    },
    {
      "epoch": 0.98208,
      "grad_norm": 0.004684919025748968,
      "learning_rate": 1.3453013333333333e-05,
      "loss": 0.0003,
      "step": 30690
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.013263133354485035,
      "learning_rate": 1.3450880000000002e-05,
      "loss": 0.0003,
      "step": 30700
    },
    {
      "epoch": 0.98272,
      "grad_norm": 0.016772298142313957,
      "learning_rate": 1.3448746666666668e-05,
      "loss": 0.0006,
      "step": 30710
    },
    {
      "epoch": 0.98304,
      "grad_norm": 0.00543719669803977,
      "learning_rate": 1.3446613333333333e-05,
      "loss": 0.0004,
      "step": 30720
    },
    {
      "epoch": 0.98336,
      "grad_norm": 0.004657902754843235,
      "learning_rate": 1.3444480000000002e-05,
      "loss": 0.0099,
      "step": 30730
    },
    {
      "epoch": 0.98368,
      "grad_norm": 0.04548138007521629,
      "learning_rate": 1.3442346666666669e-05,
      "loss": 0.0493,
      "step": 30740
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.0037120890337973833,
      "learning_rate": 1.3440213333333334e-05,
      "loss": 0.0003,
      "step": 30750
    },
    {
      "epoch": 0.98432,
      "grad_norm": 0.6255123019218445,
      "learning_rate": 1.343808e-05,
      "loss": 0.0277,
      "step": 30760
    },
    {
      "epoch": 0.98464,
      "grad_norm": 0.002476571360602975,
      "learning_rate": 1.3435946666666668e-05,
      "loss": 0.0002,
      "step": 30770
    },
    {
      "epoch": 0.98496,
      "grad_norm": 5.859961986541748,
      "learning_rate": 1.3433813333333334e-05,
      "loss": 0.0165,
      "step": 30780
    },
    {
      "epoch": 0.98528,
      "grad_norm": 0.001814978546462953,
      "learning_rate": 1.343168e-05,
      "loss": 0.0003,
      "step": 30790
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.006367641035467386,
      "learning_rate": 1.3429546666666668e-05,
      "loss": 0.0004,
      "step": 30800
    },
    {
      "epoch": 0.98592,
      "grad_norm": 0.006263844668865204,
      "learning_rate": 1.3427413333333335e-05,
      "loss": 0.0002,
      "step": 30810
    },
    {
      "epoch": 0.98624,
      "grad_norm": 0.006888781674206257,
      "learning_rate": 1.342528e-05,
      "loss": 0.0011,
      "step": 30820
    },
    {
      "epoch": 0.98656,
      "grad_norm": 0.00932338833808899,
      "learning_rate": 1.3423146666666669e-05,
      "loss": 0.0372,
      "step": 30830
    },
    {
      "epoch": 0.98688,
      "grad_norm": 0.018069829791784286,
      "learning_rate": 1.3421013333333334e-05,
      "loss": 0.0002,
      "step": 30840
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.002065733540803194,
      "learning_rate": 1.3418880000000002e-05,
      "loss": 0.0003,
      "step": 30850
    },
    {
      "epoch": 0.98752,
      "grad_norm": 0.004195574205368757,
      "learning_rate": 1.3416746666666667e-05,
      "loss": 0.0003,
      "step": 30860
    },
    {
      "epoch": 0.98784,
      "grad_norm": 0.0063221994787454605,
      "learning_rate": 1.3414613333333336e-05,
      "loss": 0.0235,
      "step": 30870
    },
    {
      "epoch": 0.98816,
      "grad_norm": 0.0032494261395186186,
      "learning_rate": 1.3412480000000001e-05,
      "loss": 0.0003,
      "step": 30880
    },
    {
      "epoch": 0.98848,
      "grad_norm": 1.6591322422027588,
      "learning_rate": 1.3410346666666666e-05,
      "loss": 0.0492,
      "step": 30890
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.004173213616013527,
      "learning_rate": 1.3408213333333335e-05,
      "loss": 0.0054,
      "step": 30900
    },
    {
      "epoch": 0.98912,
      "grad_norm": 0.0022519880440086126,
      "learning_rate": 1.340608e-05,
      "loss": 0.0003,
      "step": 30910
    },
    {
      "epoch": 0.98944,
      "grad_norm": 0.021512826904654503,
      "learning_rate": 1.3403946666666668e-05,
      "loss": 0.0003,
      "step": 30920
    },
    {
      "epoch": 0.98976,
      "grad_norm": 0.016248155385255814,
      "learning_rate": 1.3401813333333333e-05,
      "loss": 0.0003,
      "step": 30930
    },
    {
      "epoch": 0.99008,
      "grad_norm": 0.013474446721374989,
      "learning_rate": 1.3399680000000002e-05,
      "loss": 0.0015,
      "step": 30940
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.017018424347043037,
      "learning_rate": 1.3397546666666667e-05,
      "loss": 0.0006,
      "step": 30950
    },
    {
      "epoch": 0.99072,
      "grad_norm": 0.003222835250198841,
      "learning_rate": 1.3395413333333334e-05,
      "loss": 0.0002,
      "step": 30960
    },
    {
      "epoch": 0.99104,
      "grad_norm": 0.007028778549283743,
      "learning_rate": 1.3393280000000001e-05,
      "loss": 0.0004,
      "step": 30970
    },
    {
      "epoch": 0.99136,
      "grad_norm": 0.006082909647375345,
      "learning_rate": 1.3391146666666668e-05,
      "loss": 0.0042,
      "step": 30980
    },
    {
      "epoch": 0.99168,
      "grad_norm": 0.005128119140863419,
      "learning_rate": 1.3389013333333334e-05,
      "loss": 0.0002,
      "step": 30990
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.0035313009284436703,
      "learning_rate": 1.3386880000000003e-05,
      "loss": 0.0015,
      "step": 31000
    },
    {
      "epoch": 0.99232,
      "grad_norm": 0.003609102452173829,
      "learning_rate": 1.3384746666666668e-05,
      "loss": 0.0002,
      "step": 31010
    },
    {
      "epoch": 0.99264,
      "grad_norm": 0.004652590025216341,
      "learning_rate": 1.3382613333333335e-05,
      "loss": 0.0003,
      "step": 31020
    },
    {
      "epoch": 0.99296,
      "grad_norm": 0.0024922362063080072,
      "learning_rate": 1.338048e-05,
      "loss": 0.0002,
      "step": 31030
    },
    {
      "epoch": 0.99328,
      "grad_norm": 0.006822152994573116,
      "learning_rate": 1.3378346666666669e-05,
      "loss": 0.0548,
      "step": 31040
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.002561077941209078,
      "learning_rate": 1.3376213333333334e-05,
      "loss": 0.0002,
      "step": 31050
    },
    {
      "epoch": 0.99392,
      "grad_norm": 0.332916259765625,
      "learning_rate": 1.337408e-05,
      "loss": 0.0007,
      "step": 31060
    },
    {
      "epoch": 0.99424,
      "grad_norm": 0.003498790320008993,
      "learning_rate": 1.3371946666666669e-05,
      "loss": 0.0003,
      "step": 31070
    },
    {
      "epoch": 0.99456,
      "grad_norm": 0.002392089692875743,
      "learning_rate": 1.3369813333333334e-05,
      "loss": 0.0013,
      "step": 31080
    },
    {
      "epoch": 0.99488,
      "grad_norm": 0.0022662023548036814,
      "learning_rate": 1.3367680000000001e-05,
      "loss": 0.0003,
      "step": 31090
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.00666343467310071,
      "learning_rate": 1.3365546666666666e-05,
      "loss": 0.0003,
      "step": 31100
    },
    {
      "epoch": 0.99552,
      "grad_norm": 0.002542938804253936,
      "learning_rate": 1.3363413333333335e-05,
      "loss": 0.0003,
      "step": 31110
    },
    {
      "epoch": 0.99584,
      "grad_norm": 0.004817556124180555,
      "learning_rate": 1.336128e-05,
      "loss": 0.0541,
      "step": 31120
    },
    {
      "epoch": 0.99616,
      "grad_norm": 0.0038416956085711718,
      "learning_rate": 1.3359146666666668e-05,
      "loss": 0.0004,
      "step": 31130
    },
    {
      "epoch": 0.99648,
      "grad_norm": 0.0048261648043990135,
      "learning_rate": 1.3357013333333335e-05,
      "loss": 0.0002,
      "step": 31140
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.015800008550286293,
      "learning_rate": 1.3354880000000002e-05,
      "loss": 0.1146,
      "step": 31150
    },
    {
      "epoch": 0.99712,
      "grad_norm": 0.03679948300123215,
      "learning_rate": 1.3352746666666667e-05,
      "loss": 0.0029,
      "step": 31160
    },
    {
      "epoch": 0.99744,
      "grad_norm": 0.00795119907706976,
      "learning_rate": 1.3350613333333336e-05,
      "loss": 0.0005,
      "step": 31170
    },
    {
      "epoch": 0.99776,
      "grad_norm": 0.007090227212756872,
      "learning_rate": 1.3348480000000001e-05,
      "loss": 0.0187,
      "step": 31180
    },
    {
      "epoch": 0.99808,
      "grad_norm": 0.004151808097958565,
      "learning_rate": 1.3346346666666667e-05,
      "loss": 0.0002,
      "step": 31190
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.022678550332784653,
      "learning_rate": 1.3344213333333334e-05,
      "loss": 0.0032,
      "step": 31200
    },
    {
      "epoch": 0.99872,
      "grad_norm": 0.004876875784248114,
      "learning_rate": 1.3342080000000003e-05,
      "loss": 0.0007,
      "step": 31210
    },
    {
      "epoch": 0.99904,
      "grad_norm": 0.0019875706639140844,
      "learning_rate": 1.3339946666666668e-05,
      "loss": 0.0002,
      "step": 31220
    },
    {
      "epoch": 0.99936,
      "grad_norm": 3.984354257583618,
      "learning_rate": 1.3337813333333333e-05,
      "loss": 0.013,
      "step": 31230
    },
    {
      "epoch": 0.99968,
      "grad_norm": 0.025502502918243408,
      "learning_rate": 1.3335680000000002e-05,
      "loss": 0.0002,
      "step": 31240
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.004356625955551863,
      "learning_rate": 1.3333546666666667e-05,
      "loss": 0.0005,
      "step": 31250
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.0019,
      "eval_f1": 0.001899191538345146,
      "eval_loss": 10.045947074890137,
      "eval_precision": 0.0018855795903119062,
      "eval_recall": 0.001913739185431177,
      "eval_runtime": 38.464,
      "eval_samples_per_second": 259.984,
      "eval_steps_per_second": 16.249,
      "step": 31250
    },
    {
      "epoch": 1.00032,
      "grad_norm": 0.001957364846020937,
      "learning_rate": 1.3331413333333335e-05,
      "loss": 0.0004,
      "step": 31260
    },
    {
      "epoch": 1.00064,
      "grad_norm": 6.232793807983398,
      "learning_rate": 1.332928e-05,
      "loss": 0.0097,
      "step": 31270
    },
    {
      "epoch": 1.00096,
      "grad_norm": 0.004760242532938719,
      "learning_rate": 1.3327146666666669e-05,
      "loss": 0.0257,
      "step": 31280
    },
    {
      "epoch": 1.00128,
      "grad_norm": 0.002757931826636195,
      "learning_rate": 1.3325013333333334e-05,
      "loss": 0.0002,
      "step": 31290
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.0038093659095466137,
      "learning_rate": 1.3322880000000001e-05,
      "loss": 0.0146,
      "step": 31300
    },
    {
      "epoch": 1.00192,
      "grad_norm": 0.0031089414842426777,
      "learning_rate": 1.3320746666666668e-05,
      "loss": 0.0003,
      "step": 31310
    },
    {
      "epoch": 1.00224,
      "grad_norm": 0.004573545418679714,
      "learning_rate": 1.3318613333333335e-05,
      "loss": 0.0008,
      "step": 31320
    },
    {
      "epoch": 1.00256,
      "grad_norm": 0.00636116461828351,
      "learning_rate": 1.331648e-05,
      "loss": 0.0004,
      "step": 31330
    },
    {
      "epoch": 1.00288,
      "grad_norm": 0.0025102237705141306,
      "learning_rate": 1.331434666666667e-05,
      "loss": 0.0003,
      "step": 31340
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.0035833120346069336,
      "learning_rate": 1.3312213333333335e-05,
      "loss": 0.0323,
      "step": 31350
    },
    {
      "epoch": 1.00352,
      "grad_norm": 0.005500476341694593,
      "learning_rate": 1.331008e-05,
      "loss": 0.0003,
      "step": 31360
    },
    {
      "epoch": 1.00384,
      "grad_norm": 0.10578763484954834,
      "learning_rate": 1.3307946666666667e-05,
      "loss": 0.0005,
      "step": 31370
    },
    {
      "epoch": 1.00416,
      "grad_norm": 4.532318115234375,
      "learning_rate": 1.3305813333333334e-05,
      "loss": 0.0291,
      "step": 31380
    },
    {
      "epoch": 1.00448,
      "grad_norm": 0.007555091287940741,
      "learning_rate": 1.3303680000000001e-05,
      "loss": 0.0013,
      "step": 31390
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.0029384924564510584,
      "learning_rate": 1.3301546666666667e-05,
      "loss": 0.0003,
      "step": 31400
    },
    {
      "epoch": 1.00512,
      "grad_norm": 0.008005967363715172,
      "learning_rate": 1.3299413333333335e-05,
      "loss": 0.0003,
      "step": 31410
    },
    {
      "epoch": 1.0054400000000001,
      "grad_norm": 0.005921294447034597,
      "learning_rate": 1.3297280000000001e-05,
      "loss": 0.002,
      "step": 31420
    },
    {
      "epoch": 1.00576,
      "grad_norm": 0.00623476505279541,
      "learning_rate": 1.3295146666666668e-05,
      "loss": 0.0003,
      "step": 31430
    },
    {
      "epoch": 1.00608,
      "grad_norm": 0.03107338584959507,
      "learning_rate": 1.3293013333333333e-05,
      "loss": 0.0003,
      "step": 31440
    },
    {
      "epoch": 1.0064,
      "grad_norm": 2.4158120155334473,
      "learning_rate": 1.3290880000000002e-05,
      "loss": 0.0039,
      "step": 31450
    },
    {
      "epoch": 1.00672,
      "grad_norm": 0.0037732168566435575,
      "learning_rate": 1.3288746666666667e-05,
      "loss": 0.0005,
      "step": 31460
    },
    {
      "epoch": 1.00704,
      "grad_norm": 0.003882031189277768,
      "learning_rate": 1.3286613333333333e-05,
      "loss": 0.0011,
      "step": 31470
    },
    {
      "epoch": 1.00736,
      "grad_norm": 0.006257303524762392,
      "learning_rate": 1.3284480000000002e-05,
      "loss": 0.0003,
      "step": 31480
    },
    {
      "epoch": 1.00768,
      "grad_norm": 0.00408264622092247,
      "learning_rate": 1.3282346666666669e-05,
      "loss": 0.009,
      "step": 31490
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.006303081288933754,
      "learning_rate": 1.3280213333333334e-05,
      "loss": 0.0004,
      "step": 31500
    },
    {
      "epoch": 1.00832,
      "grad_norm": 0.0023977740202099085,
      "learning_rate": 1.3278080000000003e-05,
      "loss": 0.0004,
      "step": 31510
    },
    {
      "epoch": 1.00864,
      "grad_norm": 0.01343304943293333,
      "learning_rate": 1.3275946666666668e-05,
      "loss": 0.0002,
      "step": 31520
    },
    {
      "epoch": 1.00896,
      "grad_norm": 0.005439314059913158,
      "learning_rate": 1.3273813333333334e-05,
      "loss": 0.0002,
      "step": 31530
    },
    {
      "epoch": 1.00928,
      "grad_norm": 0.005058903247117996,
      "learning_rate": 1.327168e-05,
      "loss": 0.0003,
      "step": 31540
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.005354538559913635,
      "learning_rate": 1.3269546666666668e-05,
      "loss": 0.0003,
      "step": 31550
    },
    {
      "epoch": 1.00992,
      "grad_norm": 0.008821937255561352,
      "learning_rate": 1.3267413333333335e-05,
      "loss": 0.0003,
      "step": 31560
    },
    {
      "epoch": 1.01024,
      "grad_norm": 0.006081031169742346,
      "learning_rate": 1.326528e-05,
      "loss": 0.0004,
      "step": 31570
    },
    {
      "epoch": 1.01056,
      "grad_norm": 0.004325611982494593,
      "learning_rate": 1.3263146666666669e-05,
      "loss": 0.0002,
      "step": 31580
    },
    {
      "epoch": 1.01088,
      "grad_norm": 0.0031502952333539724,
      "learning_rate": 1.3261013333333334e-05,
      "loss": 0.0002,
      "step": 31590
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.00871081743389368,
      "learning_rate": 1.3258880000000001e-05,
      "loss": 0.0006,
      "step": 31600
    },
    {
      "epoch": 1.01152,
      "grad_norm": 0.015720991417765617,
      "learning_rate": 1.3256746666666667e-05,
      "loss": 0.0006,
      "step": 31610
    },
    {
      "epoch": 1.01184,
      "grad_norm": 0.0042230114340782166,
      "learning_rate": 1.3254613333333336e-05,
      "loss": 0.0002,
      "step": 31620
    },
    {
      "epoch": 1.01216,
      "grad_norm": 0.00429821340367198,
      "learning_rate": 1.3252480000000001e-05,
      "loss": 0.0028,
      "step": 31630
    },
    {
      "epoch": 1.01248,
      "grad_norm": 0.002835505409166217,
      "learning_rate": 1.3250346666666666e-05,
      "loss": 0.0232,
      "step": 31640
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.0048531582579016685,
      "learning_rate": 1.3248213333333335e-05,
      "loss": 0.0003,
      "step": 31650
    },
    {
      "epoch": 1.01312,
      "grad_norm": 0.0035194994416087866,
      "learning_rate": 1.324608e-05,
      "loss": 0.0002,
      "step": 31660
    },
    {
      "epoch": 1.01344,
      "grad_norm": 0.008476624265313148,
      "learning_rate": 1.3243946666666667e-05,
      "loss": 0.0002,
      "step": 31670
    },
    {
      "epoch": 1.01376,
      "grad_norm": 0.005438228137791157,
      "learning_rate": 1.3241813333333336e-05,
      "loss": 0.0002,
      "step": 31680
    },
    {
      "epoch": 1.01408,
      "grad_norm": 0.004842726979404688,
      "learning_rate": 1.3239680000000002e-05,
      "loss": 0.057,
      "step": 31690
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.006474942900240421,
      "learning_rate": 1.3237546666666667e-05,
      "loss": 0.0006,
      "step": 31700
    },
    {
      "epoch": 1.01472,
      "grad_norm": 0.0026282554026693106,
      "learning_rate": 1.3235413333333334e-05,
      "loss": 0.0003,
      "step": 31710
    },
    {
      "epoch": 1.01504,
      "grad_norm": 0.0058732652105391026,
      "learning_rate": 1.3233280000000001e-05,
      "loss": 0.0021,
      "step": 31720
    },
    {
      "epoch": 1.01536,
      "grad_norm": 0.019588740542531013,
      "learning_rate": 1.3231146666666668e-05,
      "loss": 0.0158,
      "step": 31730
    },
    {
      "epoch": 1.01568,
      "grad_norm": 0.004755096975713968,
      "learning_rate": 1.3229013333333334e-05,
      "loss": 0.0573,
      "step": 31740
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.002817138098180294,
      "learning_rate": 1.3226880000000002e-05,
      "loss": 0.0013,
      "step": 31750
    },
    {
      "epoch": 1.01632,
      "grad_norm": 0.0383538194000721,
      "learning_rate": 1.3224746666666668e-05,
      "loss": 0.0003,
      "step": 31760
    },
    {
      "epoch": 1.01664,
      "grad_norm": 0.010860239155590534,
      "learning_rate": 1.3222613333333335e-05,
      "loss": 0.0463,
      "step": 31770
    },
    {
      "epoch": 1.01696,
      "grad_norm": 0.004927330184727907,
      "learning_rate": 1.322048e-05,
      "loss": 0.0002,
      "step": 31780
    },
    {
      "epoch": 1.01728,
      "grad_norm": 0.0072294981218874454,
      "learning_rate": 1.3218346666666669e-05,
      "loss": 0.0005,
      "step": 31790
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.004509873688220978,
      "learning_rate": 1.3216213333333334e-05,
      "loss": 0.0023,
      "step": 31800
    },
    {
      "epoch": 1.01792,
      "grad_norm": 0.0022027073428034782,
      "learning_rate": 1.321408e-05,
      "loss": 0.0002,
      "step": 31810
    },
    {
      "epoch": 1.01824,
      "grad_norm": 0.005520976614207029,
      "learning_rate": 1.3211946666666668e-05,
      "loss": 0.0004,
      "step": 31820
    },
    {
      "epoch": 1.01856,
      "grad_norm": 0.01818873919546604,
      "learning_rate": 1.3209813333333334e-05,
      "loss": 0.0524,
      "step": 31830
    },
    {
      "epoch": 1.01888,
      "grad_norm": 0.0037338752299547195,
      "learning_rate": 1.3207680000000001e-05,
      "loss": 0.0002,
      "step": 31840
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.38233691453933716,
      "learning_rate": 1.3205546666666668e-05,
      "loss": 0.0007,
      "step": 31850
    },
    {
      "epoch": 1.01952,
      "grad_norm": 0.06254830211400986,
      "learning_rate": 1.3203413333333335e-05,
      "loss": 0.0574,
      "step": 31860
    },
    {
      "epoch": 1.01984,
      "grad_norm": 0.004534498788416386,
      "learning_rate": 1.320128e-05,
      "loss": 0.0008,
      "step": 31870
    },
    {
      "epoch": 1.02016,
      "grad_norm": 0.0028025407809764147,
      "learning_rate": 1.3199146666666668e-05,
      "loss": 0.0634,
      "step": 31880
    },
    {
      "epoch": 1.02048,
      "grad_norm": 0.004576594568789005,
      "learning_rate": 1.3197013333333335e-05,
      "loss": 0.0015,
      "step": 31890
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.00350804696790874,
      "learning_rate": 1.3194880000000002e-05,
      "loss": 0.0003,
      "step": 31900
    },
    {
      "epoch": 1.02112,
      "grad_norm": 1.5961040258407593,
      "learning_rate": 1.3192746666666667e-05,
      "loss": 0.0049,
      "step": 31910
    },
    {
      "epoch": 1.02144,
      "grad_norm": 0.003490010742098093,
      "learning_rate": 1.3190613333333336e-05,
      "loss": 0.0003,
      "step": 31920
    },
    {
      "epoch": 1.02176,
      "grad_norm": 3.822000503540039,
      "learning_rate": 1.3188480000000001e-05,
      "loss": 0.0096,
      "step": 31930
    },
    {
      "epoch": 1.02208,
      "grad_norm": 0.003049563616514206,
      "learning_rate": 1.3186346666666667e-05,
      "loss": 0.0003,
      "step": 31940
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.026783525943756104,
      "learning_rate": 1.3184213333333334e-05,
      "loss": 0.0003,
      "step": 31950
    },
    {
      "epoch": 1.02272,
      "grad_norm": 0.008236275054514408,
      "learning_rate": 1.3182080000000002e-05,
      "loss": 0.0102,
      "step": 31960
    },
    {
      "epoch": 1.02304,
      "grad_norm": 0.008983273059129715,
      "learning_rate": 1.3179946666666668e-05,
      "loss": 0.0003,
      "step": 31970
    },
    {
      "epoch": 1.02336,
      "grad_norm": 0.032270025461912155,
      "learning_rate": 1.3177813333333333e-05,
      "loss": 0.0004,
      "step": 31980
    },
    {
      "epoch": 1.02368,
      "grad_norm": 0.002971755340695381,
      "learning_rate": 1.3175680000000002e-05,
      "loss": 0.0005,
      "step": 31990
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.00419626384973526,
      "learning_rate": 1.3173546666666667e-05,
      "loss": 0.0553,
      "step": 32000
    },
    {
      "epoch": 1.02432,
      "grad_norm": 0.011429944075644016,
      "learning_rate": 1.3171413333333334e-05,
      "loss": 0.0002,
      "step": 32010
    },
    {
      "epoch": 1.02464,
      "grad_norm": 0.010680677369236946,
      "learning_rate": 1.3169280000000001e-05,
      "loss": 0.0005,
      "step": 32020
    },
    {
      "epoch": 1.02496,
      "grad_norm": 0.003299437928944826,
      "learning_rate": 1.3167146666666668e-05,
      "loss": 0.0003,
      "step": 32030
    },
    {
      "epoch": 1.02528,
      "grad_norm": 0.009249790571630001,
      "learning_rate": 1.3165013333333334e-05,
      "loss": 0.0003,
      "step": 32040
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.008967328816652298,
      "learning_rate": 1.3162880000000001e-05,
      "loss": 0.0003,
      "step": 32050
    },
    {
      "epoch": 1.02592,
      "grad_norm": 0.009099545888602734,
      "learning_rate": 1.3160746666666668e-05,
      "loss": 0.0003,
      "step": 32060
    },
    {
      "epoch": 1.02624,
      "grad_norm": 0.003217518562451005,
      "learning_rate": 1.3158613333333335e-05,
      "loss": 0.0355,
      "step": 32070
    },
    {
      "epoch": 1.02656,
      "grad_norm": 0.0157826766371727,
      "learning_rate": 1.315648e-05,
      "loss": 0.0007,
      "step": 32080
    },
    {
      "epoch": 1.02688,
      "grad_norm": 0.007537885569036007,
      "learning_rate": 1.315434666666667e-05,
      "loss": 0.0004,
      "step": 32090
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.004639004822820425,
      "learning_rate": 1.3152213333333335e-05,
      "loss": 0.0013,
      "step": 32100
    },
    {
      "epoch": 1.02752,
      "grad_norm": 1.0497788190841675,
      "learning_rate": 1.315008e-05,
      "loss": 0.0015,
      "step": 32110
    },
    {
      "epoch": 1.02784,
      "grad_norm": 0.005372483748942614,
      "learning_rate": 1.3147946666666667e-05,
      "loss": 0.0886,
      "step": 32120
    },
    {
      "epoch": 1.02816,
      "grad_norm": 0.0038258356507867575,
      "learning_rate": 1.3145813333333334e-05,
      "loss": 0.0002,
      "step": 32130
    },
    {
      "epoch": 1.02848,
      "grad_norm": 0.0019501346396282315,
      "learning_rate": 1.3143680000000001e-05,
      "loss": 0.0022,
      "step": 32140
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.004565075971186161,
      "learning_rate": 1.3141546666666667e-05,
      "loss": 0.0004,
      "step": 32150
    },
    {
      "epoch": 1.02912,
      "grad_norm": 2.6872198581695557,
      "learning_rate": 1.3139413333333335e-05,
      "loss": 0.0578,
      "step": 32160
    },
    {
      "epoch": 1.02944,
      "grad_norm": 0.7100468873977661,
      "learning_rate": 1.313728e-05,
      "loss": 0.0009,
      "step": 32170
    },
    {
      "epoch": 1.02976,
      "grad_norm": 0.019328458234667778,
      "learning_rate": 1.3135146666666668e-05,
      "loss": 0.0007,
      "step": 32180
    },
    {
      "epoch": 1.03008,
      "grad_norm": 0.005100918933749199,
      "learning_rate": 1.3133013333333335e-05,
      "loss": 0.0002,
      "step": 32190
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.009657558053731918,
      "learning_rate": 1.3130880000000002e-05,
      "loss": 0.0477,
      "step": 32200
    },
    {
      "epoch": 1.03072,
      "grad_norm": 0.008071117103099823,
      "learning_rate": 1.3128746666666667e-05,
      "loss": 0.0005,
      "step": 32210
    },
    {
      "epoch": 1.03104,
      "grad_norm": 1.2008334398269653,
      "learning_rate": 1.3126613333333333e-05,
      "loss": 0.0496,
      "step": 32220
    },
    {
      "epoch": 1.03136,
      "grad_norm": 0.004219021648168564,
      "learning_rate": 1.3124480000000001e-05,
      "loss": 0.0003,
      "step": 32230
    },
    {
      "epoch": 1.03168,
      "grad_norm": 0.00679056765511632,
      "learning_rate": 1.3122346666666669e-05,
      "loss": 0.0004,
      "step": 32240
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.003373101120814681,
      "learning_rate": 1.3120213333333334e-05,
      "loss": 0.0003,
      "step": 32250
    },
    {
      "epoch": 1.03232,
      "grad_norm": 0.0032821448985487223,
      "learning_rate": 1.3118080000000003e-05,
      "loss": 0.0005,
      "step": 32260
    },
    {
      "epoch": 1.03264,
      "grad_norm": 0.014315953478217125,
      "learning_rate": 1.3115946666666668e-05,
      "loss": 0.0003,
      "step": 32270
    },
    {
      "epoch": 1.03296,
      "grad_norm": 0.04727908968925476,
      "learning_rate": 1.3113813333333333e-05,
      "loss": 0.0004,
      "step": 32280
    },
    {
      "epoch": 1.03328,
      "grad_norm": 0.013050935231149197,
      "learning_rate": 1.311168e-05,
      "loss": 0.0007,
      "step": 32290
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.007199141196906567,
      "learning_rate": 1.3109546666666668e-05,
      "loss": 0.0008,
      "step": 32300
    },
    {
      "epoch": 1.03392,
      "grad_norm": 0.009679826907813549,
      "learning_rate": 1.3107413333333335e-05,
      "loss": 0.0003,
      "step": 32310
    },
    {
      "epoch": 1.03424,
      "grad_norm": 0.0043675582855939865,
      "learning_rate": 1.310528e-05,
      "loss": 0.0005,
      "step": 32320
    },
    {
      "epoch": 1.03456,
      "grad_norm": 0.004633172880858183,
      "learning_rate": 1.3103146666666669e-05,
      "loss": 0.061,
      "step": 32330
    },
    {
      "epoch": 1.03488,
      "grad_norm": 0.011282214894890785,
      "learning_rate": 1.3101013333333334e-05,
      "loss": 0.0004,
      "step": 32340
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.006064902525395155,
      "learning_rate": 1.3098880000000001e-05,
      "loss": 0.018,
      "step": 32350
    },
    {
      "epoch": 1.03552,
      "grad_norm": 0.004668394103646278,
      "learning_rate": 1.3096746666666668e-05,
      "loss": 0.0003,
      "step": 32360
    },
    {
      "epoch": 1.03584,
      "grad_norm": 0.010101418942213058,
      "learning_rate": 1.3094613333333335e-05,
      "loss": 0.0003,
      "step": 32370
    },
    {
      "epoch": 1.03616,
      "grad_norm": 0.005008429288864136,
      "learning_rate": 1.309248e-05,
      "loss": 0.024,
      "step": 32380
    },
    {
      "epoch": 1.03648,
      "grad_norm": 0.03763237223029137,
      "learning_rate": 1.3090346666666666e-05,
      "loss": 0.014,
      "step": 32390
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.014279946684837341,
      "learning_rate": 1.3088213333333335e-05,
      "loss": 0.0005,
      "step": 32400
    },
    {
      "epoch": 1.03712,
      "grad_norm": 0.011325854808092117,
      "learning_rate": 1.308608e-05,
      "loss": 0.0012,
      "step": 32410
    },
    {
      "epoch": 1.03744,
      "grad_norm": 0.006057568825781345,
      "learning_rate": 1.3083946666666667e-05,
      "loss": 0.0007,
      "step": 32420
    },
    {
      "epoch": 1.03776,
      "grad_norm": 0.00734564708545804,
      "learning_rate": 1.3081813333333336e-05,
      "loss": 0.0003,
      "step": 32430
    },
    {
      "epoch": 1.03808,
      "grad_norm": 0.03216129541397095,
      "learning_rate": 1.3079680000000001e-05,
      "loss": 0.0004,
      "step": 32440
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.0031981978099793196,
      "learning_rate": 1.3077546666666667e-05,
      "loss": 0.0003,
      "step": 32450
    },
    {
      "epoch": 1.03872,
      "grad_norm": 0.003137726802378893,
      "learning_rate": 1.3075413333333336e-05,
      "loss": 0.0002,
      "step": 32460
    },
    {
      "epoch": 1.03904,
      "grad_norm": 0.008507565595209599,
      "learning_rate": 1.3073280000000001e-05,
      "loss": 0.0004,
      "step": 32470
    },
    {
      "epoch": 1.03936,
      "grad_norm": 0.008618605323135853,
      "learning_rate": 1.3071146666666668e-05,
      "loss": 0.0395,
      "step": 32480
    },
    {
      "epoch": 1.03968,
      "grad_norm": 0.0039198738522827625,
      "learning_rate": 1.3069013333333333e-05,
      "loss": 0.0127,
      "step": 32490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.0046364008449018,
      "learning_rate": 1.3066880000000002e-05,
      "loss": 0.0002,
      "step": 32500
    },
    {
      "epoch": 1.04032,
      "grad_norm": 0.009714817628264427,
      "learning_rate": 1.3064746666666668e-05,
      "loss": 0.0003,
      "step": 32510
    },
    {
      "epoch": 1.04064,
      "grad_norm": 0.021209707483649254,
      "learning_rate": 1.3062613333333335e-05,
      "loss": 0.0314,
      "step": 32520
    },
    {
      "epoch": 1.04096,
      "grad_norm": 0.00454286765307188,
      "learning_rate": 1.3060480000000002e-05,
      "loss": 0.0003,
      "step": 32530
    },
    {
      "epoch": 1.04128,
      "grad_norm": 0.006028314121067524,
      "learning_rate": 1.3058346666666669e-05,
      "loss": 0.0359,
      "step": 32540
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.007058285176753998,
      "learning_rate": 1.3056213333333334e-05,
      "loss": 0.0045,
      "step": 32550
    },
    {
      "epoch": 1.04192,
      "grad_norm": 0.006093539763242006,
      "learning_rate": 1.305408e-05,
      "loss": 0.0003,
      "step": 32560
    },
    {
      "epoch": 1.04224,
      "grad_norm": 0.028513291850686073,
      "learning_rate": 1.3051946666666668e-05,
      "loss": 0.001,
      "step": 32570
    },
    {
      "epoch": 1.04256,
      "grad_norm": 0.005361837800592184,
      "learning_rate": 1.3049813333333334e-05,
      "loss": 0.0532,
      "step": 32580
    },
    {
      "epoch": 1.04288,
      "grad_norm": 0.008494201116263866,
      "learning_rate": 1.304768e-05,
      "loss": 0.0003,
      "step": 32590
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.00497157359495759,
      "learning_rate": 1.3045546666666668e-05,
      "loss": 0.0005,
      "step": 32600
    },
    {
      "epoch": 1.04352,
      "grad_norm": 0.06478794664144516,
      "learning_rate": 1.3043413333333335e-05,
      "loss": 0.0005,
      "step": 32610
    },
    {
      "epoch": 1.04384,
      "grad_norm": 0.009083658456802368,
      "learning_rate": 1.304128e-05,
      "loss": 0.0003,
      "step": 32620
    },
    {
      "epoch": 1.04416,
      "grad_norm": 0.00300766434520483,
      "learning_rate": 1.3039146666666669e-05,
      "loss": 0.0003,
      "step": 32630
    },
    {
      "epoch": 1.04448,
      "grad_norm": 0.0048815784975886345,
      "learning_rate": 1.3037013333333334e-05,
      "loss": 0.0036,
      "step": 32640
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.04598650336265564,
      "learning_rate": 1.3034880000000001e-05,
      "loss": 0.0034,
      "step": 32650
    },
    {
      "epoch": 1.04512,
      "grad_norm": 0.0050194100476801395,
      "learning_rate": 1.3032746666666667e-05,
      "loss": 0.0002,
      "step": 32660
    },
    {
      "epoch": 1.04544,
      "grad_norm": 0.0037061995826661587,
      "learning_rate": 1.3030613333333336e-05,
      "loss": 0.0003,
      "step": 32670
    },
    {
      "epoch": 1.04576,
      "grad_norm": 0.011169526726007462,
      "learning_rate": 1.3028480000000001e-05,
      "loss": 0.0406,
      "step": 32680
    },
    {
      "epoch": 1.04608,
      "grad_norm": 0.003614557208493352,
      "learning_rate": 1.3026346666666666e-05,
      "loss": 0.0003,
      "step": 32690
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.007459508720785379,
      "learning_rate": 1.3024213333333335e-05,
      "loss": 0.0578,
      "step": 32700
    },
    {
      "epoch": 1.04672,
      "grad_norm": 0.005680205300450325,
      "learning_rate": 1.3022080000000002e-05,
      "loss": 0.0004,
      "step": 32710
    },
    {
      "epoch": 1.04704,
      "grad_norm": 0.004128503147512674,
      "learning_rate": 1.3019946666666668e-05,
      "loss": 0.0003,
      "step": 32720
    },
    {
      "epoch": 1.04736,
      "grad_norm": 0.06964707374572754,
      "learning_rate": 1.3017813333333333e-05,
      "loss": 0.0003,
      "step": 32730
    },
    {
      "epoch": 1.04768,
      "grad_norm": 0.0046025230549275875,
      "learning_rate": 1.3015680000000002e-05,
      "loss": 0.0005,
      "step": 32740
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.013979691080749035,
      "learning_rate": 1.3013546666666667e-05,
      "loss": 0.0535,
      "step": 32750
    },
    {
      "epoch": 1.04832,
      "grad_norm": 0.04873286560177803,
      "learning_rate": 1.3011413333333334e-05,
      "loss": 0.0011,
      "step": 32760
    },
    {
      "epoch": 1.04864,
      "grad_norm": 0.005707185715436935,
      "learning_rate": 1.3009280000000001e-05,
      "loss": 0.0011,
      "step": 32770
    },
    {
      "epoch": 1.04896,
      "grad_norm": 0.015618524514138699,
      "learning_rate": 1.3007146666666668e-05,
      "loss": 0.0289,
      "step": 32780
    },
    {
      "epoch": 1.04928,
      "grad_norm": 0.00795876793563366,
      "learning_rate": 1.3005013333333334e-05,
      "loss": 0.0006,
      "step": 32790
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.05270593985915184,
      "learning_rate": 1.3002880000000002e-05,
      "loss": 0.0003,
      "step": 32800
    },
    {
      "epoch": 1.04992,
      "grad_norm": 0.007945760153234005,
      "learning_rate": 1.3000746666666668e-05,
      "loss": 0.0004,
      "step": 32810
    },
    {
      "epoch": 1.05024,
      "grad_norm": 0.005646258592605591,
      "learning_rate": 1.2998613333333335e-05,
      "loss": 0.0003,
      "step": 32820
    },
    {
      "epoch": 1.05056,
      "grad_norm": 0.007499741856008768,
      "learning_rate": 1.299648e-05,
      "loss": 0.0054,
      "step": 32830
    },
    {
      "epoch": 1.05088,
      "grad_norm": 0.003114580176770687,
      "learning_rate": 1.2994346666666669e-05,
      "loss": 0.0322,
      "step": 32840
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.004439149051904678,
      "learning_rate": 1.2992213333333334e-05,
      "loss": 0.0047,
      "step": 32850
    },
    {
      "epoch": 1.05152,
      "grad_norm": 0.005853945389389992,
      "learning_rate": 1.299008e-05,
      "loss": 0.04,
      "step": 32860
    },
    {
      "epoch": 1.0518399999999999,
      "grad_norm": 0.004475035239011049,
      "learning_rate": 1.2987946666666669e-05,
      "loss": 0.0003,
      "step": 32870
    },
    {
      "epoch": 1.05216,
      "grad_norm": 0.003913396038115025,
      "learning_rate": 1.2985813333333334e-05,
      "loss": 0.0002,
      "step": 32880
    },
    {
      "epoch": 1.05248,
      "grad_norm": 0.006414920557290316,
      "learning_rate": 1.2983680000000001e-05,
      "loss": 0.0574,
      "step": 32890
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.0058035957626998425,
      "learning_rate": 1.2981546666666666e-05,
      "loss": 0.0045,
      "step": 32900
    },
    {
      "epoch": 1.05312,
      "grad_norm": 0.02455718256533146,
      "learning_rate": 1.2979413333333335e-05,
      "loss": 0.0167,
      "step": 32910
    },
    {
      "epoch": 1.05344,
      "grad_norm": 0.006268704775720835,
      "learning_rate": 1.297728e-05,
      "loss": 0.0005,
      "step": 32920
    },
    {
      "epoch": 1.05376,
      "grad_norm": 0.005026328843086958,
      "learning_rate": 1.2975146666666668e-05,
      "loss": 0.0003,
      "step": 32930
    },
    {
      "epoch": 1.05408,
      "grad_norm": 0.020429391413927078,
      "learning_rate": 1.2973013333333335e-05,
      "loss": 0.0004,
      "step": 32940
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.0406414270401001,
      "learning_rate": 1.2970880000000002e-05,
      "loss": 0.027,
      "step": 32950
    },
    {
      "epoch": 1.05472,
      "grad_norm": 0.006439145188778639,
      "learning_rate": 1.2968746666666667e-05,
      "loss": 0.0007,
      "step": 32960
    },
    {
      "epoch": 1.05504,
      "grad_norm": 0.00351371755823493,
      "learning_rate": 1.2966613333333336e-05,
      "loss": 0.0005,
      "step": 32970
    },
    {
      "epoch": 1.05536,
      "grad_norm": 0.6600530743598938,
      "learning_rate": 1.2964480000000001e-05,
      "loss": 0.0009,
      "step": 32980
    },
    {
      "epoch": 1.05568,
      "grad_norm": 0.009395168162882328,
      "learning_rate": 1.2962346666666668e-05,
      "loss": 0.0226,
      "step": 32990
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.004306083079427481,
      "learning_rate": 1.2960213333333334e-05,
      "loss": 0.0005,
      "step": 33000
    },
    {
      "epoch": 1.05632,
      "grad_norm": 0.004494816064834595,
      "learning_rate": 1.2958080000000002e-05,
      "loss": 0.0004,
      "step": 33010
    },
    {
      "epoch": 1.05664,
      "grad_norm": 0.00479443185031414,
      "learning_rate": 1.2955946666666668e-05,
      "loss": 0.0005,
      "step": 33020
    },
    {
      "epoch": 1.05696,
      "grad_norm": 0.022545013576745987,
      "learning_rate": 1.2953813333333333e-05,
      "loss": 0.0003,
      "step": 33030
    },
    {
      "epoch": 1.05728,
      "grad_norm": 0.002721762517467141,
      "learning_rate": 1.2951680000000002e-05,
      "loss": 0.0104,
      "step": 33040
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.009453465230762959,
      "learning_rate": 1.2949546666666667e-05,
      "loss": 0.0027,
      "step": 33050
    },
    {
      "epoch": 1.05792,
      "grad_norm": 0.01225005928426981,
      "learning_rate": 1.2947413333333334e-05,
      "loss": 0.0003,
      "step": 33060
    },
    {
      "epoch": 1.05824,
      "grad_norm": 0.004323483910411596,
      "learning_rate": 1.294528e-05,
      "loss": 0.003,
      "step": 33070
    },
    {
      "epoch": 1.05856,
      "grad_norm": 1.8345834016799927,
      "learning_rate": 1.2943146666666669e-05,
      "loss": 0.0037,
      "step": 33080
    },
    {
      "epoch": 1.05888,
      "grad_norm": 0.005516692064702511,
      "learning_rate": 1.2941013333333334e-05,
      "loss": 0.0031,
      "step": 33090
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.0048569743521511555,
      "learning_rate": 1.2938880000000001e-05,
      "loss": 0.0047,
      "step": 33100
    },
    {
      "epoch": 1.05952,
      "grad_norm": 0.010787637904286385,
      "learning_rate": 1.2936746666666668e-05,
      "loss": 0.0003,
      "step": 33110
    },
    {
      "epoch": 1.05984,
      "grad_norm": 0.004630634561181068,
      "learning_rate": 1.2934613333333335e-05,
      "loss": 0.0005,
      "step": 33120
    },
    {
      "epoch": 1.06016,
      "grad_norm": 0.0038603227585554123,
      "learning_rate": 1.293248e-05,
      "loss": 0.0002,
      "step": 33130
    },
    {
      "epoch": 1.06048,
      "grad_norm": 0.004271398764103651,
      "learning_rate": 1.293034666666667e-05,
      "loss": 0.0006,
      "step": 33140
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.01380961760878563,
      "learning_rate": 1.2928213333333335e-05,
      "loss": 0.0005,
      "step": 33150
    },
    {
      "epoch": 1.06112,
      "grad_norm": 0.005364987067878246,
      "learning_rate": 1.292608e-05,
      "loss": 0.0004,
      "step": 33160
    },
    {
      "epoch": 1.06144,
      "grad_norm": 0.00361230387352407,
      "learning_rate": 1.2923946666666667e-05,
      "loss": 0.0154,
      "step": 33170
    },
    {
      "epoch": 1.06176,
      "grad_norm": 0.004514949396252632,
      "learning_rate": 1.2921813333333336e-05,
      "loss": 0.0002,
      "step": 33180
    },
    {
      "epoch": 1.06208,
      "grad_norm": 0.004888376221060753,
      "learning_rate": 1.2919680000000001e-05,
      "loss": 0.0003,
      "step": 33190
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.00540545117110014,
      "learning_rate": 1.2917546666666667e-05,
      "loss": 0.024,
      "step": 33200
    },
    {
      "epoch": 1.06272,
      "grad_norm": 0.0035550345201045275,
      "learning_rate": 1.2915413333333335e-05,
      "loss": 0.0002,
      "step": 33210
    },
    {
      "epoch": 1.06304,
      "grad_norm": 0.00525203300639987,
      "learning_rate": 1.291328e-05,
      "loss": 0.0002,
      "step": 33220
    },
    {
      "epoch": 1.06336,
      "grad_norm": 0.004004130605608225,
      "learning_rate": 1.2911146666666668e-05,
      "loss": 0.0005,
      "step": 33230
    },
    {
      "epoch": 1.06368,
      "grad_norm": 0.0030824546702206135,
      "learning_rate": 1.2909013333333333e-05,
      "loss": 0.0002,
      "step": 33240
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.003131040371954441,
      "learning_rate": 1.2906880000000002e-05,
      "loss": 0.0003,
      "step": 33250
    },
    {
      "epoch": 1.06432,
      "grad_norm": 0.018016451969742775,
      "learning_rate": 1.2904746666666667e-05,
      "loss": 0.0002,
      "step": 33260
    },
    {
      "epoch": 1.06464,
      "grad_norm": 0.006543951109051704,
      "learning_rate": 1.2902613333333334e-05,
      "loss": 0.0428,
      "step": 33270
    },
    {
      "epoch": 1.06496,
      "grad_norm": 0.006403331179171801,
      "learning_rate": 1.2900480000000002e-05,
      "loss": 0.0534,
      "step": 33280
    },
    {
      "epoch": 1.06528,
      "grad_norm": 0.013098509050905704,
      "learning_rate": 1.2898346666666669e-05,
      "loss": 0.0008,
      "step": 33290
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.007280335761606693,
      "learning_rate": 1.2896213333333334e-05,
      "loss": 0.0003,
      "step": 33300
    },
    {
      "epoch": 1.06592,
      "grad_norm": 0.005045118276029825,
      "learning_rate": 1.2894080000000003e-05,
      "loss": 0.0006,
      "step": 33310
    },
    {
      "epoch": 1.06624,
      "grad_norm": 0.0035858198534697294,
      "learning_rate": 1.2891946666666668e-05,
      "loss": 0.0003,
      "step": 33320
    },
    {
      "epoch": 1.06656,
      "grad_norm": 0.009962121024727821,
      "learning_rate": 1.2889813333333334e-05,
      "loss": 0.0003,
      "step": 33330
    },
    {
      "epoch": 1.06688,
      "grad_norm": 0.012087078765034676,
      "learning_rate": 1.288768e-05,
      "loss": 0.006,
      "step": 33340
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.06127895042300224,
      "learning_rate": 1.2885546666666668e-05,
      "loss": 0.0483,
      "step": 33350
    },
    {
      "epoch": 1.06752,
      "grad_norm": 0.0044734482653439045,
      "learning_rate": 1.2883413333333335e-05,
      "loss": 0.0003,
      "step": 33360
    },
    {
      "epoch": 1.06784,
      "grad_norm": 0.007119487505406141,
      "learning_rate": 1.288128e-05,
      "loss": 0.0056,
      "step": 33370
    },
    {
      "epoch": 1.06816,
      "grad_norm": 0.005202834028750658,
      "learning_rate": 1.2879146666666669e-05,
      "loss": 0.0003,
      "step": 33380
    },
    {
      "epoch": 1.06848,
      "grad_norm": 0.005421256646513939,
      "learning_rate": 1.2877013333333334e-05,
      "loss": 0.0023,
      "step": 33390
    },
    {
      "epoch": 1.0688,
      "grad_norm": 1.6180843114852905,
      "learning_rate": 1.2874880000000001e-05,
      "loss": 0.0066,
      "step": 33400
    },
    {
      "epoch": 1.06912,
      "grad_norm": 0.0071054077707231045,
      "learning_rate": 1.2872746666666667e-05,
      "loss": 0.024,
      "step": 33410
    },
    {
      "epoch": 1.06944,
      "grad_norm": 0.0024862054269760847,
      "learning_rate": 1.2870613333333335e-05,
      "loss": 0.0264,
      "step": 33420
    },
    {
      "epoch": 1.06976,
      "grad_norm": 0.04316450655460358,
      "learning_rate": 1.286848e-05,
      "loss": 0.0004,
      "step": 33430
    },
    {
      "epoch": 1.07008,
      "grad_norm": 0.1847979724407196,
      "learning_rate": 1.2866346666666666e-05,
      "loss": 0.0013,
      "step": 33440
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.004385653883218765,
      "learning_rate": 1.2864213333333335e-05,
      "loss": 0.0002,
      "step": 33450
    },
    {
      "epoch": 1.0707200000000001,
      "grad_norm": 0.002864215290173888,
      "learning_rate": 1.2862080000000002e-05,
      "loss": 0.0002,
      "step": 33460
    },
    {
      "epoch": 1.07104,
      "grad_norm": 0.007411134894937277,
      "learning_rate": 1.2859946666666667e-05,
      "loss": 0.0005,
      "step": 33470
    },
    {
      "epoch": 1.07136,
      "grad_norm": 0.0035282005555927753,
      "learning_rate": 1.2857813333333336e-05,
      "loss": 0.0004,
      "step": 33480
    },
    {
      "epoch": 1.07168,
      "grad_norm": 0.00933822337538004,
      "learning_rate": 1.2855680000000002e-05,
      "loss": 0.0265,
      "step": 33490
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.0031225678976625204,
      "learning_rate": 1.2853546666666667e-05,
      "loss": 0.0627,
      "step": 33500
    },
    {
      "epoch": 1.07232,
      "grad_norm": 0.003826715052127838,
      "learning_rate": 1.2851413333333334e-05,
      "loss": 0.0011,
      "step": 33510
    },
    {
      "epoch": 1.07264,
      "grad_norm": 0.007010897621512413,
      "learning_rate": 1.2849280000000001e-05,
      "loss": 0.0003,
      "step": 33520
    },
    {
      "epoch": 1.07296,
      "grad_norm": 0.005606178659945726,
      "learning_rate": 1.2847146666666668e-05,
      "loss": 0.0009,
      "step": 33530
    },
    {
      "epoch": 1.07328,
      "grad_norm": 0.0031511455308645964,
      "learning_rate": 1.2845013333333334e-05,
      "loss": 0.0003,
      "step": 33540
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.01878911256790161,
      "learning_rate": 1.2842880000000002e-05,
      "loss": 0.0003,
      "step": 33550
    },
    {
      "epoch": 1.07392,
      "grad_norm": 4.798201084136963,
      "learning_rate": 1.2840746666666668e-05,
      "loss": 0.0458,
      "step": 33560
    },
    {
      "epoch": 1.07424,
      "grad_norm": 0.02403232827782631,
      "learning_rate": 1.2838613333333335e-05,
      "loss": 0.0003,
      "step": 33570
    },
    {
      "epoch": 1.07456,
      "grad_norm": 0.005685076583176851,
      "learning_rate": 1.283648e-05,
      "loss": 0.0017,
      "step": 33580
    },
    {
      "epoch": 1.07488,
      "grad_norm": 0.004899537190794945,
      "learning_rate": 1.2834346666666669e-05,
      "loss": 0.0002,
      "step": 33590
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.04104924574494362,
      "learning_rate": 1.2832213333333334e-05,
      "loss": 0.0003,
      "step": 33600
    },
    {
      "epoch": 1.07552,
      "grad_norm": 0.004505062010139227,
      "learning_rate": 1.283008e-05,
      "loss": 0.0002,
      "step": 33610
    },
    {
      "epoch": 1.07584,
      "grad_norm": 0.02204105257987976,
      "learning_rate": 1.2827946666666668e-05,
      "loss": 0.1058,
      "step": 33620
    },
    {
      "epoch": 1.07616,
      "grad_norm": 0.004754652734845877,
      "learning_rate": 1.2825813333333334e-05,
      "loss": 0.0002,
      "step": 33630
    },
    {
      "epoch": 1.07648,
      "grad_norm": 0.0037020796444267035,
      "learning_rate": 1.2823680000000001e-05,
      "loss": 0.0003,
      "step": 33640
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.014607802033424377,
      "learning_rate": 1.282154666666667e-05,
      "loss": 0.0003,
      "step": 33650
    },
    {
      "epoch": 1.07712,
      "grad_norm": 0.05359195917844772,
      "learning_rate": 1.2819413333333335e-05,
      "loss": 0.0003,
      "step": 33660
    },
    {
      "epoch": 1.07744,
      "grad_norm": 3.336005687713623,
      "learning_rate": 1.281728e-05,
      "loss": 0.0069,
      "step": 33670
    },
    {
      "epoch": 1.07776,
      "grad_norm": 0.00936112366616726,
      "learning_rate": 1.2815146666666667e-05,
      "loss": 0.0004,
      "step": 33680
    },
    {
      "epoch": 1.07808,
      "grad_norm": 0.004766356665641069,
      "learning_rate": 1.2813013333333335e-05,
      "loss": 0.0003,
      "step": 33690
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.023540159687399864,
      "learning_rate": 1.2810880000000002e-05,
      "loss": 0.0003,
      "step": 33700
    },
    {
      "epoch": 1.07872,
      "grad_norm": 0.0032120044343173504,
      "learning_rate": 1.2808746666666667e-05,
      "loss": 0.0004,
      "step": 33710
    },
    {
      "epoch": 1.07904,
      "grad_norm": 0.004280564375221729,
      "learning_rate": 1.2806613333333336e-05,
      "loss": 0.0003,
      "step": 33720
    },
    {
      "epoch": 1.07936,
      "grad_norm": 0.005142717156559229,
      "learning_rate": 1.2804480000000001e-05,
      "loss": 0.0098,
      "step": 33730
    },
    {
      "epoch": 1.07968,
      "grad_norm": 0.006479733623564243,
      "learning_rate": 1.2802346666666668e-05,
      "loss": 0.0009,
      "step": 33740
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.004021656233817339,
      "learning_rate": 1.2800213333333334e-05,
      "loss": 0.0004,
      "step": 33750
    },
    {
      "epoch": 1.08032,
      "grad_norm": 0.003747595241293311,
      "learning_rate": 1.2798080000000002e-05,
      "loss": 0.002,
      "step": 33760
    },
    {
      "epoch": 1.08064,
      "grad_norm": 0.003810552880167961,
      "learning_rate": 1.2795946666666668e-05,
      "loss": 0.037,
      "step": 33770
    },
    {
      "epoch": 1.08096,
      "grad_norm": 0.002740273019298911,
      "learning_rate": 1.2793813333333333e-05,
      "loss": 0.0042,
      "step": 33780
    },
    {
      "epoch": 1.08128,
      "grad_norm": 0.006168387830257416,
      "learning_rate": 1.2791680000000002e-05,
      "loss": 0.0002,
      "step": 33790
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.004335837438702583,
      "learning_rate": 1.2789546666666667e-05,
      "loss": 0.0008,
      "step": 33800
    },
    {
      "epoch": 1.08192,
      "grad_norm": 0.006366532761603594,
      "learning_rate": 1.2787413333333334e-05,
      "loss": 0.0003,
      "step": 33810
    },
    {
      "epoch": 1.08224,
      "grad_norm": 0.005526629276573658,
      "learning_rate": 1.2785280000000001e-05,
      "loss": 0.0003,
      "step": 33820
    },
    {
      "epoch": 1.08256,
      "grad_norm": 0.1167849525809288,
      "learning_rate": 1.2783146666666668e-05,
      "loss": 0.0007,
      "step": 33830
    },
    {
      "epoch": 1.08288,
      "grad_norm": 0.0042437054216861725,
      "learning_rate": 1.2781013333333334e-05,
      "loss": 0.0002,
      "step": 33840
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.16450200974941254,
      "learning_rate": 1.2778880000000001e-05,
      "loss": 0.0004,
      "step": 33850
    },
    {
      "epoch": 1.08352,
      "grad_norm": 0.0064295269548892975,
      "learning_rate": 1.2776746666666668e-05,
      "loss": 0.0003,
      "step": 33860
    },
    {
      "epoch": 1.08384,
      "grad_norm": 0.004615124315023422,
      "learning_rate": 1.2774613333333335e-05,
      "loss": 0.0002,
      "step": 33870
    },
    {
      "epoch": 1.08416,
      "grad_norm": 0.00737694650888443,
      "learning_rate": 1.277248e-05,
      "loss": 0.0003,
      "step": 33880
    },
    {
      "epoch": 1.08448,
      "grad_norm": 0.0032355706207454205,
      "learning_rate": 1.277034666666667e-05,
      "loss": 0.0022,
      "step": 33890
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.003555282484740019,
      "learning_rate": 1.2768213333333335e-05,
      "loss": 0.0003,
      "step": 33900
    },
    {
      "epoch": 1.08512,
      "grad_norm": 0.0030933180823922157,
      "learning_rate": 1.276608e-05,
      "loss": 0.0006,
      "step": 33910
    },
    {
      "epoch": 1.08544,
      "grad_norm": 3.238037586212158,
      "learning_rate": 1.2763946666666667e-05,
      "loss": 0.0043,
      "step": 33920
    },
    {
      "epoch": 1.08576,
      "grad_norm": 0.006369246635586023,
      "learning_rate": 1.2761813333333336e-05,
      "loss": 0.054,
      "step": 33930
    },
    {
      "epoch": 1.08608,
      "grad_norm": 0.0039084251038730145,
      "learning_rate": 1.2759680000000001e-05,
      "loss": 0.0005,
      "step": 33940
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.0044120014645159245,
      "learning_rate": 1.2757546666666667e-05,
      "loss": 0.0003,
      "step": 33950
    },
    {
      "epoch": 1.08672,
      "grad_norm": 0.004255198873579502,
      "learning_rate": 1.2755413333333335e-05,
      "loss": 0.0004,
      "step": 33960
    },
    {
      "epoch": 1.08704,
      "grad_norm": 0.002684860024601221,
      "learning_rate": 1.275328e-05,
      "loss": 0.0003,
      "step": 33970
    },
    {
      "epoch": 1.0873599999999999,
      "grad_norm": 0.003628515638411045,
      "learning_rate": 1.2751146666666668e-05,
      "loss": 0.0003,
      "step": 33980
    },
    {
      "epoch": 1.08768,
      "grad_norm": 0.002490115351974964,
      "learning_rate": 1.2749013333333335e-05,
      "loss": 0.0003,
      "step": 33990
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.002344085369259119,
      "learning_rate": 1.2746880000000002e-05,
      "loss": 0.0002,
      "step": 34000
    },
    {
      "epoch": 1.08832,
      "grad_norm": 0.004917367827147245,
      "learning_rate": 1.2744746666666667e-05,
      "loss": 0.0002,
      "step": 34010
    },
    {
      "epoch": 1.08864,
      "grad_norm": 0.004502931609749794,
      "learning_rate": 1.2742613333333334e-05,
      "loss": 0.0002,
      "step": 34020
    },
    {
      "epoch": 1.08896,
      "grad_norm": 0.0035261402372270823,
      "learning_rate": 1.2740480000000001e-05,
      "loss": 0.0003,
      "step": 34030
    },
    {
      "epoch": 1.08928,
      "grad_norm": 0.004296762403100729,
      "learning_rate": 1.2738346666666668e-05,
      "loss": 0.0007,
      "step": 34040
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.002624862128868699,
      "learning_rate": 1.2736213333333334e-05,
      "loss": 0.0002,
      "step": 34050
    },
    {
      "epoch": 1.08992,
      "grad_norm": 0.006048556882888079,
      "learning_rate": 1.2734080000000003e-05,
      "loss": 0.0028,
      "step": 34060
    },
    {
      "epoch": 1.09024,
      "grad_norm": 0.003643177915364504,
      "learning_rate": 1.2731946666666668e-05,
      "loss": 0.0002,
      "step": 34070
    },
    {
      "epoch": 1.09056,
      "grad_norm": 0.006638620514422655,
      "learning_rate": 1.2729813333333333e-05,
      "loss": 0.0024,
      "step": 34080
    },
    {
      "epoch": 1.09088,
      "grad_norm": 0.003666914999485016,
      "learning_rate": 1.272768e-05,
      "loss": 0.0773,
      "step": 34090
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.0542198084294796,
      "learning_rate": 1.2725546666666667e-05,
      "loss": 0.0005,
      "step": 34100
    },
    {
      "epoch": 1.09152,
      "grad_norm": 0.006153435446321964,
      "learning_rate": 1.2723413333333335e-05,
      "loss": 0.0003,
      "step": 34110
    },
    {
      "epoch": 1.09184,
      "grad_norm": 0.004758855327963829,
      "learning_rate": 1.272128e-05,
      "loss": 0.0002,
      "step": 34120
    },
    {
      "epoch": 1.09216,
      "grad_norm": 2.998671770095825,
      "learning_rate": 1.2719146666666669e-05,
      "loss": 0.0055,
      "step": 34130
    },
    {
      "epoch": 1.0924800000000001,
      "grad_norm": 0.004606586415320635,
      "learning_rate": 1.2717013333333334e-05,
      "loss": 0.0002,
      "step": 34140
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.01632409542798996,
      "learning_rate": 1.2714880000000001e-05,
      "loss": 0.0117,
      "step": 34150
    },
    {
      "epoch": 1.09312,
      "grad_norm": 0.007562062703073025,
      "learning_rate": 1.2712746666666668e-05,
      "loss": 0.0003,
      "step": 34160
    },
    {
      "epoch": 1.09344,
      "grad_norm": 0.003817361081019044,
      "learning_rate": 1.2710613333333335e-05,
      "loss": 0.0006,
      "step": 34170
    },
    {
      "epoch": 1.09376,
      "grad_norm": 0.004794076085090637,
      "learning_rate": 1.270848e-05,
      "loss": 0.0002,
      "step": 34180
    },
    {
      "epoch": 1.09408,
      "grad_norm": 0.003782927757129073,
      "learning_rate": 1.2706346666666666e-05,
      "loss": 0.0003,
      "step": 34190
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.003921052441000938,
      "learning_rate": 1.2704213333333335e-05,
      "loss": 0.0046,
      "step": 34200
    },
    {
      "epoch": 1.09472,
      "grad_norm": 0.0048621054738759995,
      "learning_rate": 1.2702080000000002e-05,
      "loss": 0.0002,
      "step": 34210
    },
    {
      "epoch": 1.09504,
      "grad_norm": 0.005280341953039169,
      "learning_rate": 1.2699946666666667e-05,
      "loss": 0.0423,
      "step": 34220
    },
    {
      "epoch": 1.09536,
      "grad_norm": 0.00509154936298728,
      "learning_rate": 1.2697813333333336e-05,
      "loss": 0.0002,
      "step": 34230
    },
    {
      "epoch": 1.09568,
      "grad_norm": 0.004552135244011879,
      "learning_rate": 1.2695680000000001e-05,
      "loss": 0.0125,
      "step": 34240
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.00568477064371109,
      "learning_rate": 1.2693546666666667e-05,
      "loss": 0.0002,
      "step": 34250
    },
    {
      "epoch": 1.09632,
      "grad_norm": 0.005849740467965603,
      "learning_rate": 1.2691413333333334e-05,
      "loss": 0.0003,
      "step": 34260
    },
    {
      "epoch": 1.09664,
      "grad_norm": 0.002857396611943841,
      "learning_rate": 1.2689280000000001e-05,
      "loss": 0.0006,
      "step": 34270
    },
    {
      "epoch": 1.09696,
      "grad_norm": 0.004415079951286316,
      "learning_rate": 1.2687146666666668e-05,
      "loss": 0.0007,
      "step": 34280
    },
    {
      "epoch": 1.09728,
      "grad_norm": 0.002788668032735586,
      "learning_rate": 1.2685013333333333e-05,
      "loss": 0.0002,
      "step": 34290
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.19354963302612305,
      "learning_rate": 1.2682880000000002e-05,
      "loss": 0.0008,
      "step": 34300
    },
    {
      "epoch": 1.09792,
      "grad_norm": 0.31577256321907043,
      "learning_rate": 1.2680746666666668e-05,
      "loss": 0.0009,
      "step": 34310
    },
    {
      "epoch": 1.09824,
      "grad_norm": 0.009698249399662018,
      "learning_rate": 1.2678613333333335e-05,
      "loss": 0.0004,
      "step": 34320
    },
    {
      "epoch": 1.09856,
      "grad_norm": 0.003270258428528905,
      "learning_rate": 1.2676480000000002e-05,
      "loss": 0.0002,
      "step": 34330
    },
    {
      "epoch": 1.09888,
      "grad_norm": 0.009949161671102047,
      "learning_rate": 1.2674346666666669e-05,
      "loss": 0.0004,
      "step": 34340
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.00593259371817112,
      "learning_rate": 1.2672213333333334e-05,
      "loss": 0.0003,
      "step": 34350
    },
    {
      "epoch": 1.09952,
      "grad_norm": 0.004412502981722355,
      "learning_rate": 1.267008e-05,
      "loss": 0.0002,
      "step": 34360
    },
    {
      "epoch": 1.09984,
      "grad_norm": 0.0019162525422871113,
      "learning_rate": 1.2667946666666668e-05,
      "loss": 0.0002,
      "step": 34370
    },
    {
      "epoch": 1.10016,
      "grad_norm": 0.013306945562362671,
      "learning_rate": 1.2665813333333334e-05,
      "loss": 0.0007,
      "step": 34380
    },
    {
      "epoch": 1.10048,
      "grad_norm": 0.003177960403263569,
      "learning_rate": 1.266368e-05,
      "loss": 0.001,
      "step": 34390
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.0040682475082576275,
      "learning_rate": 1.266154666666667e-05,
      "loss": 0.0002,
      "step": 34400
    },
    {
      "epoch": 1.10112,
      "grad_norm": 1.2692737579345703,
      "learning_rate": 1.2659413333333335e-05,
      "loss": 0.0065,
      "step": 34410
    },
    {
      "epoch": 1.10144,
      "grad_norm": 0.0038344734348356724,
      "learning_rate": 1.265728e-05,
      "loss": 0.0002,
      "step": 34420
    },
    {
      "epoch": 1.10176,
      "grad_norm": 0.004441196098923683,
      "learning_rate": 1.2655146666666667e-05,
      "loss": 0.0385,
      "step": 34430
    },
    {
      "epoch": 1.10208,
      "grad_norm": 0.0027774167247116566,
      "learning_rate": 1.2653013333333334e-05,
      "loss": 0.0002,
      "step": 34440
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.009210041724145412,
      "learning_rate": 1.2650880000000001e-05,
      "loss": 0.0133,
      "step": 34450
    },
    {
      "epoch": 1.10272,
      "grad_norm": 0.005250806920230389,
      "learning_rate": 1.2648746666666667e-05,
      "loss": 0.0006,
      "step": 34460
    },
    {
      "epoch": 1.10304,
      "grad_norm": 0.002667434746399522,
      "learning_rate": 1.2646613333333336e-05,
      "loss": 0.0047,
      "step": 34470
    },
    {
      "epoch": 1.10336,
      "grad_norm": 0.0032064702827483416,
      "learning_rate": 1.2644480000000001e-05,
      "loss": 0.0002,
      "step": 34480
    },
    {
      "epoch": 1.10368,
      "grad_norm": 0.007931333966553211,
      "learning_rate": 1.2642346666666668e-05,
      "loss": 0.0006,
      "step": 34490
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.006213256157934666,
      "learning_rate": 1.2640213333333335e-05,
      "loss": 0.0002,
      "step": 34500
    },
    {
      "epoch": 1.10432,
      "grad_norm": 0.006030415650457144,
      "learning_rate": 1.2638080000000002e-05,
      "loss": 0.0002,
      "step": 34510
    },
    {
      "epoch": 1.10464,
      "grad_norm": 0.002638685517013073,
      "learning_rate": 1.2635946666666668e-05,
      "loss": 0.0457,
      "step": 34520
    },
    {
      "epoch": 1.10496,
      "grad_norm": 0.003472996177151799,
      "learning_rate": 1.2633813333333333e-05,
      "loss": 0.0327,
      "step": 34530
    },
    {
      "epoch": 1.10528,
      "grad_norm": 0.0039586955681443214,
      "learning_rate": 1.2631680000000002e-05,
      "loss": 0.0548,
      "step": 34540
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.00862851645797491,
      "learning_rate": 1.2629546666666667e-05,
      "loss": 0.0229,
      "step": 34550
    },
    {
      "epoch": 1.10592,
      "grad_norm": 0.0024884154554456472,
      "learning_rate": 1.2627413333333334e-05,
      "loss": 0.001,
      "step": 34560
    },
    {
      "epoch": 1.1062400000000001,
      "grad_norm": 0.006187076214700937,
      "learning_rate": 1.2625280000000001e-05,
      "loss": 0.0002,
      "step": 34570
    },
    {
      "epoch": 1.10656,
      "grad_norm": 0.00617585051804781,
      "learning_rate": 1.2623146666666668e-05,
      "loss": 0.0003,
      "step": 34580
    },
    {
      "epoch": 1.10688,
      "grad_norm": 0.003703637281432748,
      "learning_rate": 1.2621013333333334e-05,
      "loss": 0.0002,
      "step": 34590
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.0023069928865879774,
      "learning_rate": 1.261888e-05,
      "loss": 0.0003,
      "step": 34600
    },
    {
      "epoch": 1.10752,
      "grad_norm": 0.01061080303043127,
      "learning_rate": 1.2616746666666668e-05,
      "loss": 0.0005,
      "step": 34610
    },
    {
      "epoch": 1.10784,
      "grad_norm": 0.00410464545711875,
      "learning_rate": 1.2614613333333335e-05,
      "loss": 0.0054,
      "step": 34620
    },
    {
      "epoch": 1.10816,
      "grad_norm": 0.029146118089556694,
      "learning_rate": 1.261248e-05,
      "loss": 0.0014,
      "step": 34630
    },
    {
      "epoch": 1.10848,
      "grad_norm": 0.004391518887132406,
      "learning_rate": 1.2610346666666669e-05,
      "loss": 0.0003,
      "step": 34640
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.008301293477416039,
      "learning_rate": 1.2608213333333334e-05,
      "loss": 0.0002,
      "step": 34650
    },
    {
      "epoch": 1.1091199999999999,
      "grad_norm": 0.007578065153211355,
      "learning_rate": 1.260608e-05,
      "loss": 0.0563,
      "step": 34660
    },
    {
      "epoch": 1.10944,
      "grad_norm": 0.007246633525937796,
      "learning_rate": 1.2603946666666669e-05,
      "loss": 0.0003,
      "step": 34670
    },
    {
      "epoch": 1.10976,
      "grad_norm": 0.023222150281071663,
      "learning_rate": 1.2601813333333336e-05,
      "loss": 0.0003,
      "step": 34680
    },
    {
      "epoch": 1.11008,
      "grad_norm": 0.00455391826108098,
      "learning_rate": 1.2599680000000001e-05,
      "loss": 0.0257,
      "step": 34690
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.006671730894595385,
      "learning_rate": 1.2597546666666666e-05,
      "loss": 0.0007,
      "step": 34700
    },
    {
      "epoch": 1.11072,
      "grad_norm": 0.06270033121109009,
      "learning_rate": 1.2595413333333335e-05,
      "loss": 0.0003,
      "step": 34710
    },
    {
      "epoch": 1.11104,
      "grad_norm": 0.006309220567345619,
      "learning_rate": 1.259328e-05,
      "loss": 0.0016,
      "step": 34720
    },
    {
      "epoch": 1.11136,
      "grad_norm": 0.003674167674034834,
      "learning_rate": 1.2591146666666668e-05,
      "loss": 0.0004,
      "step": 34730
    },
    {
      "epoch": 1.11168,
      "grad_norm": 0.004883000627160072,
      "learning_rate": 1.2589013333333335e-05,
      "loss": 0.0002,
      "step": 34740
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.20175230503082275,
      "learning_rate": 1.2586880000000002e-05,
      "loss": 0.0004,
      "step": 34750
    },
    {
      "epoch": 1.11232,
      "grad_norm": 0.004007786512374878,
      "learning_rate": 1.2584746666666667e-05,
      "loss": 0.0002,
      "step": 34760
    },
    {
      "epoch": 1.11264,
      "grad_norm": 0.02673465758562088,
      "learning_rate": 1.2582613333333334e-05,
      "loss": 0.0668,
      "step": 34770
    },
    {
      "epoch": 1.11296,
      "grad_norm": 0.003628354985266924,
      "learning_rate": 1.2580480000000001e-05,
      "loss": 0.0003,
      "step": 34780
    },
    {
      "epoch": 1.11328,
      "grad_norm": 0.009369192644953728,
      "learning_rate": 1.2578346666666668e-05,
      "loss": 0.0004,
      "step": 34790
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.004839008674025536,
      "learning_rate": 1.2576213333333334e-05,
      "loss": 0.0002,
      "step": 34800
    },
    {
      "epoch": 1.11392,
      "grad_norm": 0.003938467241823673,
      "learning_rate": 1.2574080000000002e-05,
      "loss": 0.0012,
      "step": 34810
    },
    {
      "epoch": 1.11424,
      "grad_norm": 0.006095984019339085,
      "learning_rate": 1.2571946666666668e-05,
      "loss": 0.0004,
      "step": 34820
    },
    {
      "epoch": 1.11456,
      "grad_norm": 0.006724692415446043,
      "learning_rate": 1.2569813333333333e-05,
      "loss": 0.0132,
      "step": 34830
    },
    {
      "epoch": 1.11488,
      "grad_norm": 0.0036911428906023502,
      "learning_rate": 1.2567680000000002e-05,
      "loss": 0.0002,
      "step": 34840
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.004122452810406685,
      "learning_rate": 1.2565546666666667e-05,
      "loss": 0.0173,
      "step": 34850
    },
    {
      "epoch": 1.11552,
      "grad_norm": 0.005052251275628805,
      "learning_rate": 1.2563413333333334e-05,
      "loss": 0.0309,
      "step": 34860
    },
    {
      "epoch": 1.11584,
      "grad_norm": 0.0070733800530433655,
      "learning_rate": 1.256128e-05,
      "loss": 0.0004,
      "step": 34870
    },
    {
      "epoch": 1.11616,
      "grad_norm": 0.015320506878197193,
      "learning_rate": 1.2559146666666669e-05,
      "loss": 0.0003,
      "step": 34880
    },
    {
      "epoch": 1.11648,
      "grad_norm": 0.005961019080132246,
      "learning_rate": 1.2557013333333334e-05,
      "loss": 0.0002,
      "step": 34890
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.004886826034635305,
      "learning_rate": 1.2554880000000001e-05,
      "loss": 0.0004,
      "step": 34900
    },
    {
      "epoch": 1.11712,
      "grad_norm": 0.5727308988571167,
      "learning_rate": 1.2552746666666668e-05,
      "loss": 0.0007,
      "step": 34910
    },
    {
      "epoch": 1.11744,
      "grad_norm": 0.014840877614915371,
      "learning_rate": 1.2550613333333335e-05,
      "loss": 0.0003,
      "step": 34920
    },
    {
      "epoch": 1.11776,
      "grad_norm": 0.005514867138117552,
      "learning_rate": 1.254848e-05,
      "loss": 0.0003,
      "step": 34930
    },
    {
      "epoch": 1.11808,
      "grad_norm": 0.005505543202161789,
      "learning_rate": 1.2546346666666666e-05,
      "loss": 0.0003,
      "step": 34940
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.003926033619791269,
      "learning_rate": 1.2544213333333335e-05,
      "loss": 0.0002,
      "step": 34950
    },
    {
      "epoch": 1.11872,
      "grad_norm": 0.003615336026996374,
      "learning_rate": 1.2542080000000002e-05,
      "loss": 0.0002,
      "step": 34960
    },
    {
      "epoch": 1.11904,
      "grad_norm": 0.002444545039907098,
      "learning_rate": 1.2539946666666667e-05,
      "loss": 0.0005,
      "step": 34970
    },
    {
      "epoch": 1.11936,
      "grad_norm": 0.006660322193056345,
      "learning_rate": 1.2537813333333336e-05,
      "loss": 0.0004,
      "step": 34980
    },
    {
      "epoch": 1.11968,
      "grad_norm": 0.0034113021101802588,
      "learning_rate": 1.2535680000000001e-05,
      "loss": 0.0004,
      "step": 34990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.0546351857483387,
      "learning_rate": 1.2533546666666667e-05,
      "loss": 0.0003,
      "step": 35000
    },
    {
      "epoch": 1.12032,
      "grad_norm": 0.005546269938349724,
      "learning_rate": 1.2531413333333335e-05,
      "loss": 0.0002,
      "step": 35010
    },
    {
      "epoch": 1.12064,
      "grad_norm": 0.004894813988357782,
      "learning_rate": 1.252928e-05,
      "loss": 0.0001,
      "step": 35020
    },
    {
      "epoch": 1.12096,
      "grad_norm": 0.004219592548906803,
      "learning_rate": 1.2527146666666668e-05,
      "loss": 0.0002,
      "step": 35030
    },
    {
      "epoch": 1.12128,
      "grad_norm": 0.011794975027441978,
      "learning_rate": 1.2525013333333333e-05,
      "loss": 0.0394,
      "step": 35040
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.04396980628371239,
      "learning_rate": 1.2522880000000002e-05,
      "loss": 0.0003,
      "step": 35050
    },
    {
      "epoch": 1.12192,
      "grad_norm": 0.004370849579572678,
      "learning_rate": 1.2520746666666667e-05,
      "loss": 0.0002,
      "step": 35060
    },
    {
      "epoch": 1.12224,
      "grad_norm": 0.003203035332262516,
      "learning_rate": 1.2518613333333334e-05,
      "loss": 0.0002,
      "step": 35070
    },
    {
      "epoch": 1.12256,
      "grad_norm": 0.0031475855503231287,
      "learning_rate": 1.2516480000000001e-05,
      "loss": 0.0002,
      "step": 35080
    },
    {
      "epoch": 1.12288,
      "grad_norm": 0.002258219290524721,
      "learning_rate": 1.2514346666666669e-05,
      "loss": 0.0002,
      "step": 35090
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.002941704588010907,
      "learning_rate": 1.2512213333333334e-05,
      "loss": 0.0002,
      "step": 35100
    },
    {
      "epoch": 1.12352,
      "grad_norm": 0.004591985139995813,
      "learning_rate": 1.251008e-05,
      "loss": 0.0004,
      "step": 35110
    },
    {
      "epoch": 1.12384,
      "grad_norm": 0.004132643807679415,
      "learning_rate": 1.2507946666666668e-05,
      "loss": 0.0007,
      "step": 35120
    },
    {
      "epoch": 1.12416,
      "grad_norm": 0.0532805435359478,
      "learning_rate": 1.2505813333333333e-05,
      "loss": 0.0003,
      "step": 35130
    },
    {
      "epoch": 1.12448,
      "grad_norm": 0.0021276662591844797,
      "learning_rate": 1.250368e-05,
      "loss": 0.0002,
      "step": 35140
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.005655842833220959,
      "learning_rate": 1.250154666666667e-05,
      "loss": 0.0002,
      "step": 35150
    },
    {
      "epoch": 1.12512,
      "grad_norm": 0.0027752204332500696,
      "learning_rate": 1.2499413333333335e-05,
      "loss": 0.0075,
      "step": 35160
    },
    {
      "epoch": 1.12544,
      "grad_norm": 0.0027827578596770763,
      "learning_rate": 1.249728e-05,
      "loss": 0.0021,
      "step": 35170
    },
    {
      "epoch": 1.12576,
      "grad_norm": 0.052481770515441895,
      "learning_rate": 1.2495146666666669e-05,
      "loss": 0.0003,
      "step": 35180
    },
    {
      "epoch": 1.12608,
      "grad_norm": 0.0039906916208565235,
      "learning_rate": 1.2493013333333334e-05,
      "loss": 0.0001,
      "step": 35190
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.0015848182374611497,
      "learning_rate": 1.2490880000000001e-05,
      "loss": 0.0006,
      "step": 35200
    },
    {
      "epoch": 1.12672,
      "grad_norm": 0.0030349232256412506,
      "learning_rate": 1.2488746666666667e-05,
      "loss": 0.0346,
      "step": 35210
    },
    {
      "epoch": 1.12704,
      "grad_norm": 0.0021048823837190866,
      "learning_rate": 1.2486613333333335e-05,
      "loss": 0.0004,
      "step": 35220
    },
    {
      "epoch": 1.12736,
      "grad_norm": 0.0027460348792374134,
      "learning_rate": 1.248448e-05,
      "loss": 0.0002,
      "step": 35230
    },
    {
      "epoch": 1.12768,
      "grad_norm": 0.0033568416256457567,
      "learning_rate": 1.2482346666666668e-05,
      "loss": 0.0004,
      "step": 35240
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.004064355045557022,
      "learning_rate": 1.2480213333333335e-05,
      "loss": 0.0001,
      "step": 35250
    },
    {
      "epoch": 1.12832,
      "grad_norm": 0.00258691911585629,
      "learning_rate": 1.2478080000000002e-05,
      "loss": 0.0139,
      "step": 35260
    },
    {
      "epoch": 1.12864,
      "grad_norm": 0.0031263541895896196,
      "learning_rate": 1.2475946666666667e-05,
      "loss": 0.0144,
      "step": 35270
    },
    {
      "epoch": 1.12896,
      "grad_norm": 0.0023901609238237143,
      "learning_rate": 1.2473813333333333e-05,
      "loss": 0.0007,
      "step": 35280
    },
    {
      "epoch": 1.12928,
      "grad_norm": 0.00253277993761003,
      "learning_rate": 1.2471680000000002e-05,
      "loss": 0.0002,
      "step": 35290
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.0026368845719844103,
      "learning_rate": 1.2469546666666667e-05,
      "loss": 0.0187,
      "step": 35300
    },
    {
      "epoch": 1.12992,
      "grad_norm": 0.0038424134254455566,
      "learning_rate": 1.2467413333333334e-05,
      "loss": 0.0002,
      "step": 35310
    },
    {
      "epoch": 1.13024,
      "grad_norm": 0.04493557661771774,
      "learning_rate": 1.2465280000000001e-05,
      "loss": 0.0184,
      "step": 35320
    },
    {
      "epoch": 1.13056,
      "grad_norm": 0.001961559522897005,
      "learning_rate": 1.2463146666666668e-05,
      "loss": 0.0029,
      "step": 35330
    },
    {
      "epoch": 1.1308799999999999,
      "grad_norm": 0.005967818200588226,
      "learning_rate": 1.2461013333333333e-05,
      "loss": 0.0002,
      "step": 35340
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.03116878680884838,
      "learning_rate": 1.2458880000000002e-05,
      "loss": 0.0003,
      "step": 35350
    },
    {
      "epoch": 1.13152,
      "grad_norm": 0.010749508626759052,
      "learning_rate": 1.2456746666666668e-05,
      "loss": 0.0002,
      "step": 35360
    },
    {
      "epoch": 1.13184,
      "grad_norm": 0.010167811065912247,
      "learning_rate": 1.2454613333333335e-05,
      "loss": 0.0002,
      "step": 35370
    },
    {
      "epoch": 1.13216,
      "grad_norm": 0.061748649924993515,
      "learning_rate": 1.245248e-05,
      "loss": 0.0453,
      "step": 35380
    },
    {
      "epoch": 1.13248,
      "grad_norm": 0.003422554349526763,
      "learning_rate": 1.2450346666666669e-05,
      "loss": 0.0002,
      "step": 35390
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.0038812849670648575,
      "learning_rate": 1.2448213333333334e-05,
      "loss": 0.0584,
      "step": 35400
    },
    {
      "epoch": 1.13312,
      "grad_norm": 0.008672153577208519,
      "learning_rate": 1.244608e-05,
      "loss": 0.0052,
      "step": 35410
    },
    {
      "epoch": 1.13344,
      "grad_norm": 0.005796549376100302,
      "learning_rate": 1.2443946666666668e-05,
      "loss": 0.0002,
      "step": 35420
    },
    {
      "epoch": 1.13376,
      "grad_norm": 0.009679440408945084,
      "learning_rate": 1.2441813333333335e-05,
      "loss": 0.0005,
      "step": 35430
    },
    {
      "epoch": 1.13408,
      "grad_norm": 0.01704695262014866,
      "learning_rate": 1.243968e-05,
      "loss": 0.0003,
      "step": 35440
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.010347302071750164,
      "learning_rate": 1.2437546666666666e-05,
      "loss": 0.0003,
      "step": 35450
    },
    {
      "epoch": 1.13472,
      "grad_norm": 3.108124256134033,
      "learning_rate": 1.2435413333333335e-05,
      "loss": 0.0034,
      "step": 35460
    },
    {
      "epoch": 1.13504,
      "grad_norm": 0.007725697010755539,
      "learning_rate": 1.243328e-05,
      "loss": 0.0002,
      "step": 35470
    },
    {
      "epoch": 1.13536,
      "grad_norm": 0.007487324997782707,
      "learning_rate": 1.2431146666666667e-05,
      "loss": 0.0005,
      "step": 35480
    },
    {
      "epoch": 1.13568,
      "grad_norm": 0.023466283455491066,
      "learning_rate": 1.2429013333333334e-05,
      "loss": 0.0192,
      "step": 35490
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.003568344982340932,
      "learning_rate": 1.2426880000000002e-05,
      "loss": 0.0003,
      "step": 35500
    },
    {
      "epoch": 1.13632,
      "grad_norm": 0.02423788793385029,
      "learning_rate": 1.2424746666666667e-05,
      "loss": 0.0002,
      "step": 35510
    },
    {
      "epoch": 1.13664,
      "grad_norm": 0.014088215306401253,
      "learning_rate": 1.2422613333333336e-05,
      "loss": 0.0003,
      "step": 35520
    },
    {
      "epoch": 1.13696,
      "grad_norm": 0.012522357515990734,
      "learning_rate": 1.2420480000000001e-05,
      "loss": 0.0516,
      "step": 35530
    },
    {
      "epoch": 1.13728,
      "grad_norm": 0.002516769804060459,
      "learning_rate": 1.2418346666666668e-05,
      "loss": 0.0002,
      "step": 35540
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.029220838099718094,
      "learning_rate": 1.2416213333333334e-05,
      "loss": 0.0005,
      "step": 35550
    },
    {
      "epoch": 1.13792,
      "grad_norm": 0.0017503376584500074,
      "learning_rate": 1.2414080000000002e-05,
      "loss": 0.0003,
      "step": 35560
    },
    {
      "epoch": 1.13824,
      "grad_norm": 0.46238723397254944,
      "learning_rate": 1.2411946666666668e-05,
      "loss": 0.0023,
      "step": 35570
    },
    {
      "epoch": 1.13856,
      "grad_norm": 0.008148415945470333,
      "learning_rate": 1.2409813333333333e-05,
      "loss": 0.0561,
      "step": 35580
    },
    {
      "epoch": 1.13888,
      "grad_norm": 0.0049239532090723515,
      "learning_rate": 1.2407680000000002e-05,
      "loss": 0.0003,
      "step": 35590
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.004331758711487055,
      "learning_rate": 1.2405546666666667e-05,
      "loss": 0.0292,
      "step": 35600
    },
    {
      "epoch": 1.13952,
      "grad_norm": 0.004761660937219858,
      "learning_rate": 1.2403413333333334e-05,
      "loss": 0.0266,
      "step": 35610
    },
    {
      "epoch": 1.13984,
      "grad_norm": 0.0032997012604027987,
      "learning_rate": 1.240128e-05,
      "loss": 0.029,
      "step": 35620
    },
    {
      "epoch": 1.14016,
      "grad_norm": 0.002486846409738064,
      "learning_rate": 1.2399146666666668e-05,
      "loss": 0.0002,
      "step": 35630
    },
    {
      "epoch": 1.14048,
      "grad_norm": 0.007708157412707806,
      "learning_rate": 1.2397013333333334e-05,
      "loss": 0.0002,
      "step": 35640
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.0025913554709404707,
      "learning_rate": 1.239488e-05,
      "loss": 0.0002,
      "step": 35650
    },
    {
      "epoch": 1.14112,
      "grad_norm": 0.008652656339108944,
      "learning_rate": 1.2392746666666668e-05,
      "loss": 0.0002,
      "step": 35660
    },
    {
      "epoch": 1.14144,
      "grad_norm": 0.0026045856066048145,
      "learning_rate": 1.2390613333333335e-05,
      "loss": 0.0003,
      "step": 35670
    },
    {
      "epoch": 1.14176,
      "grad_norm": 0.009267168119549751,
      "learning_rate": 1.238848e-05,
      "loss": 0.0004,
      "step": 35680
    },
    {
      "epoch": 1.14208,
      "grad_norm": 0.005335052963346243,
      "learning_rate": 1.2386346666666669e-05,
      "loss": 0.001,
      "step": 35690
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.07867275923490524,
      "learning_rate": 1.2384213333333334e-05,
      "loss": 0.0007,
      "step": 35700
    },
    {
      "epoch": 1.14272,
      "grad_norm": 0.005394940264523029,
      "learning_rate": 1.2382080000000002e-05,
      "loss": 0.0002,
      "step": 35710
    },
    {
      "epoch": 1.14304,
      "grad_norm": 0.005204594694077969,
      "learning_rate": 1.2379946666666667e-05,
      "loss": 0.0002,
      "step": 35720
    },
    {
      "epoch": 1.14336,
      "grad_norm": 0.024971557781100273,
      "learning_rate": 1.2377813333333336e-05,
      "loss": 0.0182,
      "step": 35730
    },
    {
      "epoch": 1.14368,
      "grad_norm": 0.0030467191245406866,
      "learning_rate": 1.2375680000000001e-05,
      "loss": 0.0049,
      "step": 35740
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.002826705342158675,
      "learning_rate": 1.2373546666666666e-05,
      "loss": 0.0008,
      "step": 35750
    },
    {
      "epoch": 1.14432,
      "grad_norm": 0.0031805543694645166,
      "learning_rate": 1.2371413333333335e-05,
      "loss": 0.0499,
      "step": 35760
    },
    {
      "epoch": 1.1446399999999999,
      "grad_norm": 0.002453528344631195,
      "learning_rate": 1.236928e-05,
      "loss": 0.0002,
      "step": 35770
    },
    {
      "epoch": 1.14496,
      "grad_norm": 0.0031520333141088486,
      "learning_rate": 1.2367146666666668e-05,
      "loss": 0.0003,
      "step": 35780
    },
    {
      "epoch": 1.14528,
      "grad_norm": 0.015614431351423264,
      "learning_rate": 1.2365013333333333e-05,
      "loss": 0.0016,
      "step": 35790
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.004606843926012516,
      "learning_rate": 1.2362880000000002e-05,
      "loss": 0.0002,
      "step": 35800
    },
    {
      "epoch": 1.14592,
      "grad_norm": 0.003078164765611291,
      "learning_rate": 1.2360746666666667e-05,
      "loss": 0.0002,
      "step": 35810
    },
    {
      "epoch": 1.14624,
      "grad_norm": 0.2880130112171173,
      "learning_rate": 1.2358613333333334e-05,
      "loss": 0.0381,
      "step": 35820
    },
    {
      "epoch": 1.14656,
      "grad_norm": 0.004809707403182983,
      "learning_rate": 1.2356480000000001e-05,
      "loss": 0.0005,
      "step": 35830
    },
    {
      "epoch": 1.14688,
      "grad_norm": 0.0029033671598881483,
      "learning_rate": 1.2354346666666668e-05,
      "loss": 0.0002,
      "step": 35840
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.005160312633961439,
      "learning_rate": 1.2352213333333334e-05,
      "loss": 0.043,
      "step": 35850
    },
    {
      "epoch": 1.14752,
      "grad_norm": 0.0048995777033269405,
      "learning_rate": 1.2350080000000003e-05,
      "loss": 0.0002,
      "step": 35860
    },
    {
      "epoch": 1.14784,
      "grad_norm": 0.012230107560753822,
      "learning_rate": 1.2347946666666668e-05,
      "loss": 0.0004,
      "step": 35870
    },
    {
      "epoch": 1.14816,
      "grad_norm": 0.02472282201051712,
      "learning_rate": 1.2345813333333333e-05,
      "loss": 0.0007,
      "step": 35880
    },
    {
      "epoch": 1.14848,
      "grad_norm": 0.00236959639005363,
      "learning_rate": 1.234368e-05,
      "loss": 0.0002,
      "step": 35890
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.0038739123847335577,
      "learning_rate": 1.2341546666666669e-05,
      "loss": 0.048,
      "step": 35900
    },
    {
      "epoch": 1.14912,
      "grad_norm": 0.004234994761645794,
      "learning_rate": 1.2339413333333335e-05,
      "loss": 0.0004,
      "step": 35910
    },
    {
      "epoch": 1.14944,
      "grad_norm": 0.008364090695977211,
      "learning_rate": 1.233728e-05,
      "loss": 0.0003,
      "step": 35920
    },
    {
      "epoch": 1.1497600000000001,
      "grad_norm": 0.0032499295193701982,
      "learning_rate": 1.2335146666666669e-05,
      "loss": 0.0007,
      "step": 35930
    },
    {
      "epoch": 1.15008,
      "grad_norm": 0.0029405076056718826,
      "learning_rate": 1.2333013333333334e-05,
      "loss": 0.0003,
      "step": 35940
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.0058045643381774426,
      "learning_rate": 1.2330880000000001e-05,
      "loss": 0.0003,
      "step": 35950
    },
    {
      "epoch": 1.15072,
      "grad_norm": 0.004192756488919258,
      "learning_rate": 1.2328746666666666e-05,
      "loss": 0.0305,
      "step": 35960
    },
    {
      "epoch": 1.15104,
      "grad_norm": 0.06990303844213486,
      "learning_rate": 1.2326613333333335e-05,
      "loss": 0.0044,
      "step": 35970
    },
    {
      "epoch": 1.15136,
      "grad_norm": 0.004368681460618973,
      "learning_rate": 1.232448e-05,
      "loss": 0.0002,
      "step": 35980
    },
    {
      "epoch": 1.15168,
      "grad_norm": 0.01201664563268423,
      "learning_rate": 1.2322346666666668e-05,
      "loss": 0.0003,
      "step": 35990
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.008012310601770878,
      "learning_rate": 1.2320213333333335e-05,
      "loss": 0.0591,
      "step": 36000
    },
    {
      "epoch": 1.15232,
      "grad_norm": 1.130062460899353,
      "learning_rate": 1.2318080000000002e-05,
      "loss": 0.0013,
      "step": 36010
    },
    {
      "epoch": 1.1526399999999999,
      "grad_norm": 0.0046897633001208305,
      "learning_rate": 1.2315946666666667e-05,
      "loss": 0.0003,
      "step": 36020
    },
    {
      "epoch": 1.15296,
      "grad_norm": 0.012771041132509708,
      "learning_rate": 1.2313813333333336e-05,
      "loss": 0.0005,
      "step": 36030
    },
    {
      "epoch": 1.15328,
      "grad_norm": 0.005370532628148794,
      "learning_rate": 1.2311680000000001e-05,
      "loss": 0.0132,
      "step": 36040
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.004193960689008236,
      "learning_rate": 1.2309546666666667e-05,
      "loss": 0.0004,
      "step": 36050
    },
    {
      "epoch": 1.15392,
      "grad_norm": 0.41200879216194153,
      "learning_rate": 1.2307413333333334e-05,
      "loss": 0.0278,
      "step": 36060
    },
    {
      "epoch": 1.15424,
      "grad_norm": 0.004614732693880796,
      "learning_rate": 1.2305280000000001e-05,
      "loss": 0.0002,
      "step": 36070
    },
    {
      "epoch": 1.15456,
      "grad_norm": 0.003524928819388151,
      "learning_rate": 1.2303146666666668e-05,
      "loss": 0.001,
      "step": 36080
    },
    {
      "epoch": 1.15488,
      "grad_norm": 0.00833738874644041,
      "learning_rate": 1.2301013333333333e-05,
      "loss": 0.0195,
      "step": 36090
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.057291179895401,
      "learning_rate": 1.2298880000000002e-05,
      "loss": 0.0008,
      "step": 36100
    },
    {
      "epoch": 1.15552,
      "grad_norm": 0.003434692742303014,
      "learning_rate": 1.2296746666666667e-05,
      "loss": 0.0011,
      "step": 36110
    },
    {
      "epoch": 1.15584,
      "grad_norm": 0.004999026656150818,
      "learning_rate": 1.2294613333333335e-05,
      "loss": 0.0011,
      "step": 36120
    },
    {
      "epoch": 1.15616,
      "grad_norm": 0.3308088183403015,
      "learning_rate": 1.229248e-05,
      "loss": 0.0007,
      "step": 36130
    },
    {
      "epoch": 1.15648,
      "grad_norm": 0.0036532203666865826,
      "learning_rate": 1.2290346666666669e-05,
      "loss": 0.0013,
      "step": 36140
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.005398065783083439,
      "learning_rate": 1.2288213333333334e-05,
      "loss": 0.0107,
      "step": 36150
    },
    {
      "epoch": 1.15712,
      "grad_norm": 0.005666681565344334,
      "learning_rate": 1.228608e-05,
      "loss": 0.0006,
      "step": 36160
    },
    {
      "epoch": 1.15744,
      "grad_norm": 0.004519979003816843,
      "learning_rate": 1.2283946666666668e-05,
      "loss": 0.0032,
      "step": 36170
    },
    {
      "epoch": 1.1577600000000001,
      "grad_norm": 0.02273728884756565,
      "learning_rate": 1.2281813333333335e-05,
      "loss": 0.0002,
      "step": 36180
    },
    {
      "epoch": 1.15808,
      "grad_norm": 0.004429388791322708,
      "learning_rate": 1.227968e-05,
      "loss": 0.0003,
      "step": 36190
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.0032006199471652508,
      "learning_rate": 1.227754666666667e-05,
      "loss": 0.0498,
      "step": 36200
    },
    {
      "epoch": 1.15872,
      "grad_norm": 0.004108238499611616,
      "learning_rate": 1.2275413333333335e-05,
      "loss": 0.0003,
      "step": 36210
    },
    {
      "epoch": 1.15904,
      "grad_norm": 0.09596025943756104,
      "learning_rate": 1.227328e-05,
      "loss": 0.0004,
      "step": 36220
    },
    {
      "epoch": 1.15936,
      "grad_norm": 0.0036867884919047356,
      "learning_rate": 1.2271146666666667e-05,
      "loss": 0.0349,
      "step": 36230
    },
    {
      "epoch": 1.15968,
      "grad_norm": 0.0034909932874143124,
      "learning_rate": 1.2269013333333334e-05,
      "loss": 0.0002,
      "step": 36240
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.014266960322856903,
      "learning_rate": 1.2266880000000001e-05,
      "loss": 0.0542,
      "step": 36250
    },
    {
      "epoch": 1.16032,
      "grad_norm": 0.0033241799101233482,
      "learning_rate": 1.2264746666666667e-05,
      "loss": 0.0038,
      "step": 36260
    },
    {
      "epoch": 1.16064,
      "grad_norm": 0.008421900682151318,
      "learning_rate": 1.2262613333333336e-05,
      "loss": 0.0012,
      "step": 36270
    },
    {
      "epoch": 1.16096,
      "grad_norm": 0.00978267565369606,
      "learning_rate": 1.2260480000000001e-05,
      "loss": 0.0002,
      "step": 36280
    },
    {
      "epoch": 1.16128,
      "grad_norm": 0.012397141195833683,
      "learning_rate": 1.2258346666666668e-05,
      "loss": 0.0003,
      "step": 36290
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.012837446294724941,
      "learning_rate": 1.2256213333333333e-05,
      "loss": 0.0397,
      "step": 36300
    },
    {
      "epoch": 1.16192,
      "grad_norm": 0.008738108910620213,
      "learning_rate": 1.2254080000000002e-05,
      "loss": 0.0373,
      "step": 36310
    },
    {
      "epoch": 1.16224,
      "grad_norm": 0.00418217945843935,
      "learning_rate": 1.2251946666666667e-05,
      "loss": 0.0002,
      "step": 36320
    },
    {
      "epoch": 1.16256,
      "grad_norm": 0.008562656119465828,
      "learning_rate": 1.2249813333333333e-05,
      "loss": 0.0025,
      "step": 36330
    },
    {
      "epoch": 1.16288,
      "grad_norm": 0.005144048016518354,
      "learning_rate": 1.2247680000000002e-05,
      "loss": 0.0018,
      "step": 36340
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.005149106960743666,
      "learning_rate": 1.2245546666666667e-05,
      "loss": 0.0003,
      "step": 36350
    },
    {
      "epoch": 1.16352,
      "grad_norm": 5.401615619659424,
      "learning_rate": 1.2243413333333334e-05,
      "loss": 0.0235,
      "step": 36360
    },
    {
      "epoch": 1.16384,
      "grad_norm": 0.004464118741452694,
      "learning_rate": 1.2241280000000003e-05,
      "loss": 0.0002,
      "step": 36370
    },
    {
      "epoch": 1.16416,
      "grad_norm": 0.003695592051371932,
      "learning_rate": 1.2239146666666668e-05,
      "loss": 0.0415,
      "step": 36380
    },
    {
      "epoch": 1.16448,
      "grad_norm": 0.003669640514999628,
      "learning_rate": 1.2237013333333334e-05,
      "loss": 0.0187,
      "step": 36390
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.01827418804168701,
      "learning_rate": 1.223488e-05,
      "loss": 0.0007,
      "step": 36400
    },
    {
      "epoch": 1.16512,
      "grad_norm": 0.03315280005335808,
      "learning_rate": 1.2232746666666668e-05,
      "loss": 0.0004,
      "step": 36410
    },
    {
      "epoch": 1.16544,
      "grad_norm": 0.006654034368693829,
      "learning_rate": 1.2230613333333335e-05,
      "loss": 0.0003,
      "step": 36420
    },
    {
      "epoch": 1.16576,
      "grad_norm": 0.0025229554157704115,
      "learning_rate": 1.222848e-05,
      "loss": 0.0545,
      "step": 36430
    },
    {
      "epoch": 1.16608,
      "grad_norm": 0.009447531774640083,
      "learning_rate": 1.2226346666666669e-05,
      "loss": 0.0589,
      "step": 36440
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.015423222444951534,
      "learning_rate": 1.2224213333333334e-05,
      "loss": 0.0003,
      "step": 36450
    },
    {
      "epoch": 1.16672,
      "grad_norm": 0.003392266808077693,
      "learning_rate": 1.2222080000000001e-05,
      "loss": 0.0004,
      "step": 36460
    },
    {
      "epoch": 1.16704,
      "grad_norm": 2.183070659637451,
      "learning_rate": 1.2219946666666667e-05,
      "loss": 0.0275,
      "step": 36470
    },
    {
      "epoch": 1.16736,
      "grad_norm": 0.008935066871345043,
      "learning_rate": 1.2217813333333336e-05,
      "loss": 0.0006,
      "step": 36480
    },
    {
      "epoch": 1.16768,
      "grad_norm": 0.007804032880812883,
      "learning_rate": 1.2215680000000001e-05,
      "loss": 0.1056,
      "step": 36490
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.019716860726475716,
      "learning_rate": 1.2213546666666666e-05,
      "loss": 0.0083,
      "step": 36500
    },
    {
      "epoch": 1.16832,
      "grad_norm": 0.023068683221936226,
      "learning_rate": 1.2211413333333335e-05,
      "loss": 0.0005,
      "step": 36510
    },
    {
      "epoch": 1.16864,
      "grad_norm": 0.015956174582242966,
      "learning_rate": 1.220928e-05,
      "loss": 0.0005,
      "step": 36520
    },
    {
      "epoch": 1.16896,
      "grad_norm": 0.02566642127931118,
      "learning_rate": 1.2207146666666668e-05,
      "loss": 0.0009,
      "step": 36530
    },
    {
      "epoch": 1.16928,
      "grad_norm": 0.010293557308614254,
      "learning_rate": 1.2205013333333335e-05,
      "loss": 0.0017,
      "step": 36540
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.007297764532268047,
      "learning_rate": 1.2202880000000002e-05,
      "loss": 0.0005,
      "step": 36550
    },
    {
      "epoch": 1.16992,
      "grad_norm": 0.005232079420238733,
      "learning_rate": 1.2200746666666667e-05,
      "loss": 0.0006,
      "step": 36560
    },
    {
      "epoch": 1.17024,
      "grad_norm": 0.006733553484082222,
      "learning_rate": 1.2198613333333334e-05,
      "loss": 0.0023,
      "step": 36570
    },
    {
      "epoch": 1.17056,
      "grad_norm": 0.013005055487155914,
      "learning_rate": 1.2196480000000001e-05,
      "loss": 0.0005,
      "step": 36580
    },
    {
      "epoch": 1.17088,
      "grad_norm": 0.012730988673865795,
      "learning_rate": 1.2194346666666668e-05,
      "loss": 0.0004,
      "step": 36590
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.10542681068181992,
      "learning_rate": 1.2192213333333334e-05,
      "loss": 0.0586,
      "step": 36600
    },
    {
      "epoch": 1.1715200000000001,
      "grad_norm": 0.011251608841121197,
      "learning_rate": 1.2190080000000002e-05,
      "loss": 0.0005,
      "step": 36610
    },
    {
      "epoch": 1.17184,
      "grad_norm": 0.011009990237653255,
      "learning_rate": 1.2187946666666668e-05,
      "loss": 0.0004,
      "step": 36620
    },
    {
      "epoch": 1.17216,
      "grad_norm": 0.01698412001132965,
      "learning_rate": 1.2185813333333333e-05,
      "loss": 0.0004,
      "step": 36630
    },
    {
      "epoch": 1.17248,
      "grad_norm": 0.3734525740146637,
      "learning_rate": 1.218368e-05,
      "loss": 0.0009,
      "step": 36640
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.007538012694567442,
      "learning_rate": 1.2181546666666669e-05,
      "loss": 0.0004,
      "step": 36650
    },
    {
      "epoch": 1.17312,
      "grad_norm": 0.013548449613153934,
      "learning_rate": 1.2179413333333334e-05,
      "loss": 0.0007,
      "step": 36660
    },
    {
      "epoch": 1.17344,
      "grad_norm": 0.009324364364147186,
      "learning_rate": 1.217728e-05,
      "loss": 0.0004,
      "step": 36670
    },
    {
      "epoch": 1.17376,
      "grad_norm": 0.01730383187532425,
      "learning_rate": 1.2175146666666668e-05,
      "loss": 0.0003,
      "step": 36680
    },
    {
      "epoch": 1.17408,
      "grad_norm": 0.0048112692311406136,
      "learning_rate": 1.2173013333333334e-05,
      "loss": 0.0004,
      "step": 36690
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.036202963441610336,
      "learning_rate": 1.2170880000000001e-05,
      "loss": 0.034,
      "step": 36700
    },
    {
      "epoch": 1.17472,
      "grad_norm": 0.004567313939332962,
      "learning_rate": 1.2168746666666668e-05,
      "loss": 0.0005,
      "step": 36710
    },
    {
      "epoch": 1.17504,
      "grad_norm": 0.003964934963732958,
      "learning_rate": 1.2166613333333335e-05,
      "loss": 0.0007,
      "step": 36720
    },
    {
      "epoch": 1.17536,
      "grad_norm": 0.006787613965570927,
      "learning_rate": 1.216448e-05,
      "loss": 0.0003,
      "step": 36730
    },
    {
      "epoch": 1.17568,
      "grad_norm": 0.006356607656925917,
      "learning_rate": 1.2162346666666668e-05,
      "loss": 0.0497,
      "step": 36740
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.05675316974520683,
      "learning_rate": 1.2160213333333335e-05,
      "loss": 0.0008,
      "step": 36750
    },
    {
      "epoch": 1.17632,
      "grad_norm": 0.005509169306606054,
      "learning_rate": 1.2158080000000002e-05,
      "loss": 0.0003,
      "step": 36760
    },
    {
      "epoch": 1.17664,
      "grad_norm": 0.005121610593050718,
      "learning_rate": 1.2155946666666667e-05,
      "loss": 0.0097,
      "step": 36770
    },
    {
      "epoch": 1.17696,
      "grad_norm": 0.015315975062549114,
      "learning_rate": 1.2153813333333336e-05,
      "loss": 0.0004,
      "step": 36780
    },
    {
      "epoch": 1.17728,
      "grad_norm": 0.004473281558603048,
      "learning_rate": 1.2151680000000001e-05,
      "loss": 0.0357,
      "step": 36790
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.005218271631747484,
      "learning_rate": 1.2149546666666667e-05,
      "loss": 0.0004,
      "step": 36800
    },
    {
      "epoch": 1.17792,
      "grad_norm": 0.006528459023684263,
      "learning_rate": 1.2147413333333334e-05,
      "loss": 0.0004,
      "step": 36810
    },
    {
      "epoch": 1.17824,
      "grad_norm": 0.00690790731459856,
      "learning_rate": 1.214528e-05,
      "loss": 0.0003,
      "step": 36820
    },
    {
      "epoch": 1.17856,
      "grad_norm": 0.004954416770488024,
      "learning_rate": 1.2143146666666668e-05,
      "loss": 0.0003,
      "step": 36830
    },
    {
      "epoch": 1.17888,
      "grad_norm": 0.014996618032455444,
      "learning_rate": 1.2141013333333333e-05,
      "loss": 0.0006,
      "step": 36840
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.0072022792883217335,
      "learning_rate": 1.2138880000000002e-05,
      "loss": 0.0006,
      "step": 36850
    },
    {
      "epoch": 1.1795200000000001,
      "grad_norm": 0.003708458738401532,
      "learning_rate": 1.2136746666666667e-05,
      "loss": 0.0004,
      "step": 36860
    },
    {
      "epoch": 1.17984,
      "grad_norm": 0.01353093795478344,
      "learning_rate": 1.2134613333333334e-05,
      "loss": 0.0528,
      "step": 36870
    },
    {
      "epoch": 1.1801599999999999,
      "grad_norm": 0.007104106247425079,
      "learning_rate": 1.2132480000000001e-05,
      "loss": 0.0143,
      "step": 36880
    },
    {
      "epoch": 1.18048,
      "grad_norm": 0.007730705663561821,
      "learning_rate": 1.2130346666666669e-05,
      "loss": 0.0467,
      "step": 36890
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.005703909322619438,
      "learning_rate": 1.2128213333333334e-05,
      "loss": 0.0005,
      "step": 36900
    },
    {
      "epoch": 1.18112,
      "grad_norm": 0.004870764445513487,
      "learning_rate": 1.212608e-05,
      "loss": 0.0004,
      "step": 36910
    },
    {
      "epoch": 1.18144,
      "grad_norm": 0.009920351207256317,
      "learning_rate": 1.2123946666666668e-05,
      "loss": 0.0209,
      "step": 36920
    },
    {
      "epoch": 1.18176,
      "grad_norm": 0.011752886697649956,
      "learning_rate": 1.2121813333333335e-05,
      "loss": 0.0439,
      "step": 36930
    },
    {
      "epoch": 1.18208,
      "grad_norm": 0.008715098723769188,
      "learning_rate": 1.211968e-05,
      "loss": 0.0005,
      "step": 36940
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.013632713817059994,
      "learning_rate": 1.211754666666667e-05,
      "loss": 0.0006,
      "step": 36950
    },
    {
      "epoch": 1.18272,
      "grad_norm": 0.010639687068760395,
      "learning_rate": 1.2115413333333335e-05,
      "loss": 0.0004,
      "step": 36960
    },
    {
      "epoch": 1.18304,
      "grad_norm": 0.010260537266731262,
      "learning_rate": 1.211328e-05,
      "loss": 0.0006,
      "step": 36970
    },
    {
      "epoch": 1.18336,
      "grad_norm": 0.010345292277634144,
      "learning_rate": 1.2111146666666667e-05,
      "loss": 0.0004,
      "step": 36980
    },
    {
      "epoch": 1.18368,
      "grad_norm": 1.7448935508728027,
      "learning_rate": 1.2109013333333334e-05,
      "loss": 0.0464,
      "step": 36990
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.012999854981899261,
      "learning_rate": 1.2106880000000001e-05,
      "loss": 0.0009,
      "step": 37000
    },
    {
      "epoch": 1.18432,
      "grad_norm": 0.0068431575782597065,
      "learning_rate": 1.2104746666666667e-05,
      "loss": 0.0006,
      "step": 37010
    },
    {
      "epoch": 1.18464,
      "grad_norm": 0.006731783971190453,
      "learning_rate": 1.2102613333333335e-05,
      "loss": 0.0006,
      "step": 37020
    },
    {
      "epoch": 1.18496,
      "grad_norm": 0.022680828347802162,
      "learning_rate": 1.210048e-05,
      "loss": 0.0022,
      "step": 37030
    },
    {
      "epoch": 1.1852800000000001,
      "grad_norm": 0.011840502731502056,
      "learning_rate": 1.2098346666666668e-05,
      "loss": 0.0004,
      "step": 37040
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.005784510634839535,
      "learning_rate": 1.2096213333333335e-05,
      "loss": 0.0009,
      "step": 37050
    },
    {
      "epoch": 1.18592,
      "grad_norm": 0.007551453076303005,
      "learning_rate": 1.2094080000000002e-05,
      "loss": 0.0004,
      "step": 37060
    },
    {
      "epoch": 1.18624,
      "grad_norm": 0.003911416046321392,
      "learning_rate": 1.2091946666666667e-05,
      "loss": 0.0028,
      "step": 37070
    },
    {
      "epoch": 1.18656,
      "grad_norm": 0.007913162931799889,
      "learning_rate": 1.2089813333333333e-05,
      "loss": 0.0008,
      "step": 37080
    },
    {
      "epoch": 1.18688,
      "grad_norm": 0.006712102331221104,
      "learning_rate": 1.2087680000000001e-05,
      "loss": 0.0827,
      "step": 37090
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.004906397312879562,
      "learning_rate": 1.2085546666666667e-05,
      "loss": 0.0156,
      "step": 37100
    },
    {
      "epoch": 1.18752,
      "grad_norm": 0.005640601273626089,
      "learning_rate": 1.2083413333333334e-05,
      "loss": 0.0004,
      "step": 37110
    },
    {
      "epoch": 1.18784,
      "grad_norm": 0.009164865128695965,
      "learning_rate": 1.2081280000000003e-05,
      "loss": 0.0229,
      "step": 37120
    },
    {
      "epoch": 1.1881599999999999,
      "grad_norm": 0.002458816859871149,
      "learning_rate": 1.2079146666666668e-05,
      "loss": 0.0003,
      "step": 37130
    },
    {
      "epoch": 1.18848,
      "grad_norm": 0.0061063445173203945,
      "learning_rate": 1.2077013333333333e-05,
      "loss": 0.0469,
      "step": 37140
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.014158911071717739,
      "learning_rate": 1.207488e-05,
      "loss": 0.0008,
      "step": 37150
    },
    {
      "epoch": 1.18912,
      "grad_norm": 0.0056022279895842075,
      "learning_rate": 1.2072746666666668e-05,
      "loss": 0.0147,
      "step": 37160
    },
    {
      "epoch": 1.18944,
      "grad_norm": 0.008687576279044151,
      "learning_rate": 1.2070613333333335e-05,
      "loss": 0.0005,
      "step": 37170
    },
    {
      "epoch": 1.18976,
      "grad_norm": 0.01775113306939602,
      "learning_rate": 1.206848e-05,
      "loss": 0.0005,
      "step": 37180
    },
    {
      "epoch": 1.19008,
      "grad_norm": 0.012098385952413082,
      "learning_rate": 1.2066346666666669e-05,
      "loss": 0.0006,
      "step": 37190
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.1085992157459259,
      "learning_rate": 1.2064213333333334e-05,
      "loss": 0.0016,
      "step": 37200
    },
    {
      "epoch": 1.19072,
      "grad_norm": 0.009709663689136505,
      "learning_rate": 1.2062080000000001e-05,
      "loss": 0.0008,
      "step": 37210
    },
    {
      "epoch": 1.19104,
      "grad_norm": 0.00807665754109621,
      "learning_rate": 1.2059946666666668e-05,
      "loss": 0.0005,
      "step": 37220
    },
    {
      "epoch": 1.19136,
      "grad_norm": 0.005139575805515051,
      "learning_rate": 1.2057813333333335e-05,
      "loss": 0.0004,
      "step": 37230
    },
    {
      "epoch": 1.19168,
      "grad_norm": 0.022957755252718925,
      "learning_rate": 1.205568e-05,
      "loss": 0.0129,
      "step": 37240
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.01086390670388937,
      "learning_rate": 1.2053546666666666e-05,
      "loss": 0.0005,
      "step": 37250
    },
    {
      "epoch": 1.19232,
      "grad_norm": 0.014126840978860855,
      "learning_rate": 1.2051413333333335e-05,
      "loss": 0.0004,
      "step": 37260
    },
    {
      "epoch": 1.19264,
      "grad_norm": 0.01649600826203823,
      "learning_rate": 1.204928e-05,
      "loss": 0.0007,
      "step": 37270
    },
    {
      "epoch": 1.19296,
      "grad_norm": 0.004449670203030109,
      "learning_rate": 1.2047146666666667e-05,
      "loss": 0.0016,
      "step": 37280
    },
    {
      "epoch": 1.1932800000000001,
      "grad_norm": 13.86600112915039,
      "learning_rate": 1.2045013333333334e-05,
      "loss": 0.0183,
      "step": 37290
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.006241416092962027,
      "learning_rate": 1.2042880000000001e-05,
      "loss": 0.0005,
      "step": 37300
    },
    {
      "epoch": 1.19392,
      "grad_norm": 0.017449991777539253,
      "learning_rate": 1.2040746666666667e-05,
      "loss": 0.0004,
      "step": 37310
    },
    {
      "epoch": 1.19424,
      "grad_norm": 0.16382604837417603,
      "learning_rate": 1.2038613333333334e-05,
      "loss": 0.0056,
      "step": 37320
    },
    {
      "epoch": 1.19456,
      "grad_norm": 0.13039439916610718,
      "learning_rate": 1.2036480000000001e-05,
      "loss": 0.0005,
      "step": 37330
    },
    {
      "epoch": 1.19488,
      "grad_norm": 0.06915948539972305,
      "learning_rate": 1.2034346666666668e-05,
      "loss": 0.0005,
      "step": 37340
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.032481737434864044,
      "learning_rate": 1.2032213333333333e-05,
      "loss": 0.0013,
      "step": 37350
    },
    {
      "epoch": 1.19552,
      "grad_norm": 0.007258344441652298,
      "learning_rate": 1.2030080000000002e-05,
      "loss": 0.0003,
      "step": 37360
    },
    {
      "epoch": 1.19584,
      "grad_norm": 0.013379559852182865,
      "learning_rate": 1.2027946666666668e-05,
      "loss": 0.0003,
      "step": 37370
    },
    {
      "epoch": 1.19616,
      "grad_norm": 0.010650780983269215,
      "learning_rate": 1.2025813333333333e-05,
      "loss": 0.0003,
      "step": 37380
    },
    {
      "epoch": 1.19648,
      "grad_norm": 0.007623780053108931,
      "learning_rate": 1.2023680000000002e-05,
      "loss": 0.0002,
      "step": 37390
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.009497435763478279,
      "learning_rate": 1.2021546666666669e-05,
      "loss": 0.0003,
      "step": 37400
    },
    {
      "epoch": 1.19712,
      "grad_norm": 0.005055233836174011,
      "learning_rate": 1.2019413333333334e-05,
      "loss": 0.0003,
      "step": 37410
    },
    {
      "epoch": 1.19744,
      "grad_norm": 0.014135065488517284,
      "learning_rate": 1.201728e-05,
      "loss": 0.0003,
      "step": 37420
    },
    {
      "epoch": 1.19776,
      "grad_norm": 0.0726243406534195,
      "learning_rate": 1.2015146666666668e-05,
      "loss": 0.0004,
      "step": 37430
    },
    {
      "epoch": 1.19808,
      "grad_norm": 0.003915303852409124,
      "learning_rate": 1.2013013333333334e-05,
      "loss": 0.0003,
      "step": 37440
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.009664984419941902,
      "learning_rate": 1.201088e-05,
      "loss": 0.0004,
      "step": 37450
    },
    {
      "epoch": 1.19872,
      "grad_norm": 0.010355216450989246,
      "learning_rate": 1.2008746666666668e-05,
      "loss": 0.0003,
      "step": 37460
    },
    {
      "epoch": 1.19904,
      "grad_norm": 1.0284000635147095,
      "learning_rate": 1.2006613333333335e-05,
      "loss": 0.0106,
      "step": 37470
    },
    {
      "epoch": 1.19936,
      "grad_norm": 0.029872065410017967,
      "learning_rate": 1.200448e-05,
      "loss": 0.0003,
      "step": 37480
    },
    {
      "epoch": 1.19968,
      "grad_norm": 0.0043918052688241005,
      "learning_rate": 1.2002346666666667e-05,
      "loss": 0.0003,
      "step": 37490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.012156975455582142,
      "learning_rate": 1.2000213333333334e-05,
      "loss": 0.0123,
      "step": 37500
    },
    {
      "epoch": 1.20032,
      "grad_norm": 0.18301042914390564,
      "learning_rate": 1.1998080000000002e-05,
      "loss": 0.0075,
      "step": 37510
    },
    {
      "epoch": 1.20064,
      "grad_norm": 0.03524428606033325,
      "learning_rate": 1.1995946666666667e-05,
      "loss": 0.0003,
      "step": 37520
    },
    {
      "epoch": 1.20096,
      "grad_norm": 0.009041744284331799,
      "learning_rate": 1.1993813333333336e-05,
      "loss": 0.0003,
      "step": 37530
    },
    {
      "epoch": 1.20128,
      "grad_norm": 0.006807498633861542,
      "learning_rate": 1.1991680000000001e-05,
      "loss": 0.0002,
      "step": 37540
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.008565554395318031,
      "learning_rate": 1.1989546666666666e-05,
      "loss": 0.0003,
      "step": 37550
    },
    {
      "epoch": 1.2019199999999999,
      "grad_norm": 0.0040163761004805565,
      "learning_rate": 1.1987413333333335e-05,
      "loss": 0.0002,
      "step": 37560
    },
    {
      "epoch": 1.20224,
      "grad_norm": 0.006651694420725107,
      "learning_rate": 1.198528e-05,
      "loss": 0.0002,
      "step": 37570
    },
    {
      "epoch": 1.20256,
      "grad_norm": 0.0048389313742518425,
      "learning_rate": 1.1983146666666668e-05,
      "loss": 0.0002,
      "step": 37580
    },
    {
      "epoch": 1.20288,
      "grad_norm": 0.0027255932800471783,
      "learning_rate": 1.1981013333333333e-05,
      "loss": 0.0003,
      "step": 37590
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.012587285600602627,
      "learning_rate": 1.1978880000000002e-05,
      "loss": 0.0331,
      "step": 37600
    },
    {
      "epoch": 1.20352,
      "grad_norm": 0.008380986750125885,
      "learning_rate": 1.1976746666666667e-05,
      "loss": 0.0608,
      "step": 37610
    },
    {
      "epoch": 1.20384,
      "grad_norm": 0.006535292137414217,
      "learning_rate": 1.1974613333333334e-05,
      "loss": 0.0004,
      "step": 37620
    },
    {
      "epoch": 1.20416,
      "grad_norm": 0.00834510289132595,
      "learning_rate": 1.1972480000000001e-05,
      "loss": 0.0003,
      "step": 37630
    },
    {
      "epoch": 1.20448,
      "grad_norm": 0.005813887342810631,
      "learning_rate": 1.1970346666666668e-05,
      "loss": 0.0009,
      "step": 37640
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.008261322975158691,
      "learning_rate": 1.1968213333333334e-05,
      "loss": 0.0003,
      "step": 37650
    },
    {
      "epoch": 1.20512,
      "grad_norm": 0.43426060676574707,
      "learning_rate": 1.1966079999999999e-05,
      "loss": 0.0012,
      "step": 37660
    },
    {
      "epoch": 1.20544,
      "grad_norm": 0.010604030452668667,
      "learning_rate": 1.1963946666666668e-05,
      "loss": 0.0002,
      "step": 37670
    },
    {
      "epoch": 1.20576,
      "grad_norm": 0.003553977934643626,
      "learning_rate": 1.1961813333333335e-05,
      "loss": 0.0003,
      "step": 37680
    },
    {
      "epoch": 1.20608,
      "grad_norm": 0.00422693183645606,
      "learning_rate": 1.195968e-05,
      "loss": 0.0003,
      "step": 37690
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.014722785912454128,
      "learning_rate": 1.1957546666666669e-05,
      "loss": 0.0003,
      "step": 37700
    },
    {
      "epoch": 1.20672,
      "grad_norm": 0.006111686583608389,
      "learning_rate": 1.1955413333333334e-05,
      "loss": 0.0277,
      "step": 37710
    },
    {
      "epoch": 1.2070400000000001,
      "grad_norm": 0.010557238012552261,
      "learning_rate": 1.195328e-05,
      "loss": 0.0003,
      "step": 37720
    },
    {
      "epoch": 1.20736,
      "grad_norm": 0.034732118248939514,
      "learning_rate": 1.1951146666666669e-05,
      "loss": 0.0003,
      "step": 37730
    },
    {
      "epoch": 1.20768,
      "grad_norm": 0.00596308708190918,
      "learning_rate": 1.1949013333333334e-05,
      "loss": 0.0003,
      "step": 37740
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.030935874208807945,
      "learning_rate": 1.1946880000000001e-05,
      "loss": 0.0004,
      "step": 37750
    },
    {
      "epoch": 1.20832,
      "grad_norm": 0.00775969447568059,
      "learning_rate": 1.1944746666666666e-05,
      "loss": 0.0003,
      "step": 37760
    },
    {
      "epoch": 1.20864,
      "grad_norm": 0.006973784416913986,
      "learning_rate": 1.1942613333333335e-05,
      "loss": 0.0003,
      "step": 37770
    },
    {
      "epoch": 1.20896,
      "grad_norm": 0.005575800780206919,
      "learning_rate": 1.194048e-05,
      "loss": 0.0003,
      "step": 37780
    },
    {
      "epoch": 1.20928,
      "grad_norm": 0.005900469142943621,
      "learning_rate": 1.1938346666666668e-05,
      "loss": 0.0004,
      "step": 37790
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.008692953735589981,
      "learning_rate": 1.1936213333333335e-05,
      "loss": 0.0576,
      "step": 37800
    },
    {
      "epoch": 1.2099199999999999,
      "grad_norm": 0.0071945530362427235,
      "learning_rate": 1.1934080000000002e-05,
      "loss": 0.0002,
      "step": 37810
    },
    {
      "epoch": 1.21024,
      "grad_norm": 0.006894622929394245,
      "learning_rate": 1.1931946666666667e-05,
      "loss": 0.0003,
      "step": 37820
    },
    {
      "epoch": 1.21056,
      "grad_norm": 0.0713573470711708,
      "learning_rate": 1.1929813333333333e-05,
      "loss": 0.0011,
      "step": 37830
    },
    {
      "epoch": 1.21088,
      "grad_norm": 0.008004858158528805,
      "learning_rate": 1.1927680000000001e-05,
      "loss": 0.0005,
      "step": 37840
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.003571326145902276,
      "learning_rate": 1.1925546666666667e-05,
      "loss": 0.0002,
      "step": 37850
    },
    {
      "epoch": 1.21152,
      "grad_norm": 0.0037957774475216866,
      "learning_rate": 1.1923413333333334e-05,
      "loss": 0.0002,
      "step": 37860
    },
    {
      "epoch": 1.21184,
      "grad_norm": 0.007945495657622814,
      "learning_rate": 1.1921280000000003e-05,
      "loss": 0.0343,
      "step": 37870
    },
    {
      "epoch": 1.21216,
      "grad_norm": 0.33735164999961853,
      "learning_rate": 1.1919146666666668e-05,
      "loss": 0.0346,
      "step": 37880
    },
    {
      "epoch": 1.21248,
      "grad_norm": 0.0034497391898185015,
      "learning_rate": 1.1917013333333333e-05,
      "loss": 0.0002,
      "step": 37890
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.006036459933966398,
      "learning_rate": 1.1914880000000002e-05,
      "loss": 0.0463,
      "step": 37900
    },
    {
      "epoch": 1.21312,
      "grad_norm": 0.0026961492840200663,
      "learning_rate": 1.1912746666666667e-05,
      "loss": 0.0003,
      "step": 37910
    },
    {
      "epoch": 1.21344,
      "grad_norm": 0.005977215711027384,
      "learning_rate": 1.1910613333333334e-05,
      "loss": 0.0003,
      "step": 37920
    },
    {
      "epoch": 1.21376,
      "grad_norm": 0.004792663734406233,
      "learning_rate": 1.190848e-05,
      "loss": 0.0003,
      "step": 37930
    },
    {
      "epoch": 1.21408,
      "grad_norm": 0.03548932075500488,
      "learning_rate": 1.1906346666666669e-05,
      "loss": 0.0004,
      "step": 37940
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.006887557916343212,
      "learning_rate": 1.1904213333333334e-05,
      "loss": 0.0019,
      "step": 37950
    },
    {
      "epoch": 1.21472,
      "grad_norm": 4.4965434074401855,
      "learning_rate": 1.1902080000000001e-05,
      "loss": 0.0192,
      "step": 37960
    },
    {
      "epoch": 1.2150400000000001,
      "grad_norm": 0.06843378394842148,
      "learning_rate": 1.1899946666666668e-05,
      "loss": 0.0003,
      "step": 37970
    },
    {
      "epoch": 1.21536,
      "grad_norm": 0.003436824306845665,
      "learning_rate": 1.1897813333333335e-05,
      "loss": 0.0002,
      "step": 37980
    },
    {
      "epoch": 1.21568,
      "grad_norm": 0.003558433847501874,
      "learning_rate": 1.189568e-05,
      "loss": 0.0004,
      "step": 37990
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.004822409246116877,
      "learning_rate": 1.1893546666666666e-05,
      "loss": 0.0064,
      "step": 38000
    },
    {
      "epoch": 1.21632,
      "grad_norm": 0.006167898885905743,
      "learning_rate": 1.1891413333333335e-05,
      "loss": 0.0002,
      "step": 38010
    },
    {
      "epoch": 1.21664,
      "grad_norm": 0.018609847873449326,
      "learning_rate": 1.188928e-05,
      "loss": 0.0003,
      "step": 38020
    },
    {
      "epoch": 1.21696,
      "grad_norm": 0.005043750628829002,
      "learning_rate": 1.1887146666666667e-05,
      "loss": 0.0025,
      "step": 38030
    },
    {
      "epoch": 1.21728,
      "grad_norm": 0.00449629919603467,
      "learning_rate": 1.1885013333333334e-05,
      "loss": 0.0158,
      "step": 38040
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.006875492166727781,
      "learning_rate": 1.1882880000000001e-05,
      "loss": 0.0021,
      "step": 38050
    },
    {
      "epoch": 1.21792,
      "grad_norm": 0.005312825087457895,
      "learning_rate": 1.1880746666666667e-05,
      "loss": 0.0003,
      "step": 38060
    },
    {
      "epoch": 1.21824,
      "grad_norm": 0.0019824649207293987,
      "learning_rate": 1.1878613333333335e-05,
      "loss": 0.0003,
      "step": 38070
    },
    {
      "epoch": 1.21856,
      "grad_norm": 0.010994362644851208,
      "learning_rate": 1.1876480000000001e-05,
      "loss": 0.0358,
      "step": 38080
    },
    {
      "epoch": 1.21888,
      "grad_norm": 0.005338474176824093,
      "learning_rate": 1.1874346666666668e-05,
      "loss": 0.0003,
      "step": 38090
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.005952716805040836,
      "learning_rate": 1.1872213333333333e-05,
      "loss": 0.0002,
      "step": 38100
    },
    {
      "epoch": 1.21952,
      "grad_norm": 0.00635957159101963,
      "learning_rate": 1.1870080000000002e-05,
      "loss": 0.0003,
      "step": 38110
    },
    {
      "epoch": 1.21984,
      "grad_norm": 0.017688695341348648,
      "learning_rate": 1.1867946666666667e-05,
      "loss": 0.0647,
      "step": 38120
    },
    {
      "epoch": 1.22016,
      "grad_norm": 0.0048589687794446945,
      "learning_rate": 1.1865813333333333e-05,
      "loss": 0.0531,
      "step": 38130
    },
    {
      "epoch": 1.22048,
      "grad_norm": 0.009369945153594017,
      "learning_rate": 1.1863680000000002e-05,
      "loss": 0.0521,
      "step": 38140
    },
    {
      "epoch": 1.2208,
      "grad_norm": 2.778212070465088,
      "learning_rate": 1.1861546666666669e-05,
      "loss": 0.0309,
      "step": 38150
    },
    {
      "epoch": 1.22112,
      "grad_norm": 0.003401957219466567,
      "learning_rate": 1.1859413333333334e-05,
      "loss": 0.0003,
      "step": 38160
    },
    {
      "epoch": 1.22144,
      "grad_norm": 0.004915480501949787,
      "learning_rate": 1.185728e-05,
      "loss": 0.0025,
      "step": 38170
    },
    {
      "epoch": 1.22176,
      "grad_norm": 0.07900650054216385,
      "learning_rate": 1.1855146666666668e-05,
      "loss": 0.0004,
      "step": 38180
    },
    {
      "epoch": 1.22208,
      "grad_norm": 0.009469405747950077,
      "learning_rate": 1.1853013333333334e-05,
      "loss": 0.0009,
      "step": 38190
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.031062521040439606,
      "learning_rate": 1.185088e-05,
      "loss": 0.0004,
      "step": 38200
    },
    {
      "epoch": 1.22272,
      "grad_norm": 0.009105674922466278,
      "learning_rate": 1.1848746666666668e-05,
      "loss": 0.0004,
      "step": 38210
    },
    {
      "epoch": 1.22304,
      "grad_norm": 0.003837017808109522,
      "learning_rate": 1.1846613333333335e-05,
      "loss": 0.0003,
      "step": 38220
    },
    {
      "epoch": 1.22336,
      "grad_norm": 0.012385799549520016,
      "learning_rate": 1.184448e-05,
      "loss": 0.0004,
      "step": 38230
    },
    {
      "epoch": 1.2236799999999999,
      "grad_norm": 0.01622743159532547,
      "learning_rate": 1.1842346666666669e-05,
      "loss": 0.0006,
      "step": 38240
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.05176416039466858,
      "learning_rate": 1.1840213333333334e-05,
      "loss": 0.0004,
      "step": 38250
    },
    {
      "epoch": 1.22432,
      "grad_norm": 0.6694216728210449,
      "learning_rate": 1.1838080000000001e-05,
      "loss": 0.0014,
      "step": 38260
    },
    {
      "epoch": 1.22464,
      "grad_norm": 0.006412352900952101,
      "learning_rate": 1.1835946666666667e-05,
      "loss": 0.0003,
      "step": 38270
    },
    {
      "epoch": 1.22496,
      "grad_norm": 0.0062467255629599094,
      "learning_rate": 1.1833813333333335e-05,
      "loss": 0.0024,
      "step": 38280
    },
    {
      "epoch": 1.22528,
      "grad_norm": 0.0156345646828413,
      "learning_rate": 1.1831680000000001e-05,
      "loss": 0.0192,
      "step": 38290
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.04796481505036354,
      "learning_rate": 1.1829546666666666e-05,
      "loss": 0.0004,
      "step": 38300
    },
    {
      "epoch": 1.22592,
      "grad_norm": 0.011929023079574108,
      "learning_rate": 1.1827413333333335e-05,
      "loss": 0.0004,
      "step": 38310
    },
    {
      "epoch": 1.22624,
      "grad_norm": 0.005590416956692934,
      "learning_rate": 1.182528e-05,
      "loss": 0.0004,
      "step": 38320
    },
    {
      "epoch": 1.22656,
      "grad_norm": 0.004101256374269724,
      "learning_rate": 1.1823146666666667e-05,
      "loss": 0.0002,
      "step": 38330
    },
    {
      "epoch": 1.22688,
      "grad_norm": 0.004051235504448414,
      "learning_rate": 1.1821013333333333e-05,
      "loss": 0.0003,
      "step": 38340
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.004790221806615591,
      "learning_rate": 1.1818880000000002e-05,
      "loss": 0.0526,
      "step": 38350
    },
    {
      "epoch": 1.22752,
      "grad_norm": 0.004645980894565582,
      "learning_rate": 1.1816746666666667e-05,
      "loss": 0.0003,
      "step": 38360
    },
    {
      "epoch": 1.22784,
      "grad_norm": 0.0100765535607934,
      "learning_rate": 1.1814613333333334e-05,
      "loss": 0.0003,
      "step": 38370
    },
    {
      "epoch": 1.22816,
      "grad_norm": 0.004726226441562176,
      "learning_rate": 1.1812480000000001e-05,
      "loss": 0.0025,
      "step": 38380
    },
    {
      "epoch": 1.22848,
      "grad_norm": 0.00405627116560936,
      "learning_rate": 1.1810346666666668e-05,
      "loss": 0.0003,
      "step": 38390
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.004433748312294483,
      "learning_rate": 1.1808213333333334e-05,
      "loss": 0.0006,
      "step": 38400
    },
    {
      "epoch": 1.22912,
      "grad_norm": 0.008407820016145706,
      "learning_rate": 1.1806080000000002e-05,
      "loss": 0.0723,
      "step": 38410
    },
    {
      "epoch": 1.22944,
      "grad_norm": 0.0048783039674162865,
      "learning_rate": 1.1803946666666668e-05,
      "loss": 0.0127,
      "step": 38420
    },
    {
      "epoch": 1.22976,
      "grad_norm": 5.625582218170166,
      "learning_rate": 1.1801813333333335e-05,
      "loss": 0.0139,
      "step": 38430
    },
    {
      "epoch": 1.23008,
      "grad_norm": 0.009665758349001408,
      "learning_rate": 1.179968e-05,
      "loss": 0.0091,
      "step": 38440
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.004511602688580751,
      "learning_rate": 1.1797546666666669e-05,
      "loss": 0.0008,
      "step": 38450
    },
    {
      "epoch": 1.23072,
      "grad_norm": 0.012418399564921856,
      "learning_rate": 1.1795413333333334e-05,
      "loss": 0.0005,
      "step": 38460
    },
    {
      "epoch": 1.23104,
      "grad_norm": 0.0062125977128744125,
      "learning_rate": 1.179328e-05,
      "loss": 0.0003,
      "step": 38470
    },
    {
      "epoch": 1.23136,
      "grad_norm": 0.005301837343722582,
      "learning_rate": 1.1791146666666668e-05,
      "loss": 0.0003,
      "step": 38480
    },
    {
      "epoch": 1.2316799999999999,
      "grad_norm": 0.009326049126684666,
      "learning_rate": 1.1789013333333334e-05,
      "loss": 0.0003,
      "step": 38490
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.006298505235463381,
      "learning_rate": 1.1786880000000001e-05,
      "loss": 0.0003,
      "step": 38500
    },
    {
      "epoch": 1.23232,
      "grad_norm": 0.007035740185528994,
      "learning_rate": 1.1784746666666666e-05,
      "loss": 0.0003,
      "step": 38510
    },
    {
      "epoch": 1.23264,
      "grad_norm": 0.09215454012155533,
      "learning_rate": 1.1782613333333335e-05,
      "loss": 0.0003,
      "step": 38520
    },
    {
      "epoch": 1.23296,
      "grad_norm": 0.005587094929069281,
      "learning_rate": 1.178048e-05,
      "loss": 0.0521,
      "step": 38530
    },
    {
      "epoch": 1.23328,
      "grad_norm": 0.005582911428064108,
      "learning_rate": 1.1778346666666667e-05,
      "loss": 0.046,
      "step": 38540
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.008588305674493313,
      "learning_rate": 1.1776213333333335e-05,
      "loss": 0.0521,
      "step": 38550
    },
    {
      "epoch": 1.23392,
      "grad_norm": 0.005479129962623119,
      "learning_rate": 1.1774080000000002e-05,
      "loss": 0.0003,
      "step": 38560
    },
    {
      "epoch": 1.23424,
      "grad_norm": 0.002702387049794197,
      "learning_rate": 1.1771946666666667e-05,
      "loss": 0.0004,
      "step": 38570
    },
    {
      "epoch": 1.23456,
      "grad_norm": 0.007250855211168528,
      "learning_rate": 1.1769813333333336e-05,
      "loss": 0.0048,
      "step": 38580
    },
    {
      "epoch": 1.23488,
      "grad_norm": 0.01416891347616911,
      "learning_rate": 1.1767680000000001e-05,
      "loss": 0.0023,
      "step": 38590
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.01298387348651886,
      "learning_rate": 1.1765546666666667e-05,
      "loss": 0.003,
      "step": 38600
    },
    {
      "epoch": 1.23552,
      "grad_norm": 0.014739595353603363,
      "learning_rate": 1.1763413333333334e-05,
      "loss": 0.0003,
      "step": 38610
    },
    {
      "epoch": 1.23584,
      "grad_norm": 0.004304953850805759,
      "learning_rate": 1.1761280000000002e-05,
      "loss": 0.0005,
      "step": 38620
    },
    {
      "epoch": 1.23616,
      "grad_norm": 0.007043355610221624,
      "learning_rate": 1.1759146666666668e-05,
      "loss": 0.0003,
      "step": 38630
    },
    {
      "epoch": 1.23648,
      "grad_norm": 0.03571389988064766,
      "learning_rate": 1.1757013333333333e-05,
      "loss": 0.0312,
      "step": 38640
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.00451525766402483,
      "learning_rate": 1.1754880000000002e-05,
      "loss": 0.0022,
      "step": 38650
    },
    {
      "epoch": 1.23712,
      "grad_norm": 0.0040781800635159016,
      "learning_rate": 1.1752746666666667e-05,
      "loss": 0.0014,
      "step": 38660
    },
    {
      "epoch": 1.23744,
      "grad_norm": 0.013901024125516415,
      "learning_rate": 1.1750613333333334e-05,
      "loss": 0.0436,
      "step": 38670
    },
    {
      "epoch": 1.23776,
      "grad_norm": 0.026716474443674088,
      "learning_rate": 1.174848e-05,
      "loss": 0.0005,
      "step": 38680
    },
    {
      "epoch": 1.23808,
      "grad_norm": 0.009696733206510544,
      "learning_rate": 1.1746346666666668e-05,
      "loss": 0.0003,
      "step": 38690
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.006515222601592541,
      "learning_rate": 1.1744213333333334e-05,
      "loss": 0.0257,
      "step": 38700
    },
    {
      "epoch": 1.23872,
      "grad_norm": 0.008989449590444565,
      "learning_rate": 1.1742080000000001e-05,
      "loss": 0.0011,
      "step": 38710
    },
    {
      "epoch": 1.23904,
      "grad_norm": 1.838922142982483,
      "learning_rate": 1.1739946666666668e-05,
      "loss": 0.034,
      "step": 38720
    },
    {
      "epoch": 1.23936,
      "grad_norm": 0.144492506980896,
      "learning_rate": 1.1737813333333335e-05,
      "loss": 0.0009,
      "step": 38730
    },
    {
      "epoch": 1.23968,
      "grad_norm": 0.004287614952772856,
      "learning_rate": 1.173568e-05,
      "loss": 0.0004,
      "step": 38740
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.7793127298355103,
      "learning_rate": 1.173354666666667e-05,
      "loss": 0.0036,
      "step": 38750
    },
    {
      "epoch": 1.24032,
      "grad_norm": 0.016618626192212105,
      "learning_rate": 1.1731413333333335e-05,
      "loss": 0.0003,
      "step": 38760
    },
    {
      "epoch": 1.24064,
      "grad_norm": 0.0032961247488856316,
      "learning_rate": 1.172928e-05,
      "loss": 0.0622,
      "step": 38770
    },
    {
      "epoch": 1.24096,
      "grad_norm": 0.013057584874331951,
      "learning_rate": 1.1727146666666667e-05,
      "loss": 0.0005,
      "step": 38780
    },
    {
      "epoch": 1.24128,
      "grad_norm": 0.011775143444538116,
      "learning_rate": 1.1725013333333334e-05,
      "loss": 0.0011,
      "step": 38790
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.005611533764749765,
      "learning_rate": 1.1722880000000001e-05,
      "loss": 0.0034,
      "step": 38800
    },
    {
      "epoch": 1.24192,
      "grad_norm": 0.007494308985769749,
      "learning_rate": 1.1720746666666667e-05,
      "loss": 0.0004,
      "step": 38810
    },
    {
      "epoch": 1.24224,
      "grad_norm": 0.006254983600229025,
      "learning_rate": 1.1718613333333335e-05,
      "loss": 0.0003,
      "step": 38820
    },
    {
      "epoch": 1.24256,
      "grad_norm": 0.01705329120159149,
      "learning_rate": 1.171648e-05,
      "loss": 0.0007,
      "step": 38830
    },
    {
      "epoch": 1.24288,
      "grad_norm": 0.03170491382479668,
      "learning_rate": 1.1714346666666668e-05,
      "loss": 0.0017,
      "step": 38840
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.0072936611250042915,
      "learning_rate": 1.1712213333333333e-05,
      "loss": 0.0933,
      "step": 38850
    },
    {
      "epoch": 1.24352,
      "grad_norm": 12.345905303955078,
      "learning_rate": 1.1710080000000002e-05,
      "loss": 0.0257,
      "step": 38860
    },
    {
      "epoch": 1.24384,
      "grad_norm": 0.008021138608455658,
      "learning_rate": 1.1707946666666667e-05,
      "loss": 0.0004,
      "step": 38870
    },
    {
      "epoch": 1.24416,
      "grad_norm": 0.006290564779192209,
      "learning_rate": 1.1705813333333333e-05,
      "loss": 0.0003,
      "step": 38880
    },
    {
      "epoch": 1.24448,
      "grad_norm": 0.006348208524286747,
      "learning_rate": 1.1703680000000001e-05,
      "loss": 0.0003,
      "step": 38890
    },
    {
      "epoch": 1.2448,
      "grad_norm": 2.5291991233825684,
      "learning_rate": 1.1701546666666668e-05,
      "loss": 0.0038,
      "step": 38900
    },
    {
      "epoch": 1.24512,
      "grad_norm": 2.1973791122436523,
      "learning_rate": 1.1699413333333334e-05,
      "loss": 0.0036,
      "step": 38910
    },
    {
      "epoch": 1.2454399999999999,
      "grad_norm": 0.007393052335828543,
      "learning_rate": 1.1697280000000003e-05,
      "loss": 0.0499,
      "step": 38920
    },
    {
      "epoch": 1.24576,
      "grad_norm": 0.008431683294475079,
      "learning_rate": 1.1695146666666668e-05,
      "loss": 0.0193,
      "step": 38930
    },
    {
      "epoch": 1.24608,
      "grad_norm": 0.009977879002690315,
      "learning_rate": 1.1693013333333333e-05,
      "loss": 0.0004,
      "step": 38940
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.01783817820250988,
      "learning_rate": 1.169088e-05,
      "loss": 0.0018,
      "step": 38950
    },
    {
      "epoch": 1.24672,
      "grad_norm": 0.011749588884413242,
      "learning_rate": 1.1688746666666668e-05,
      "loss": 0.0004,
      "step": 38960
    },
    {
      "epoch": 1.24704,
      "grad_norm": 0.017471058294177055,
      "learning_rate": 1.1686613333333335e-05,
      "loss": 0.0006,
      "step": 38970
    },
    {
      "epoch": 1.24736,
      "grad_norm": 0.012493654154241085,
      "learning_rate": 1.168448e-05,
      "loss": 0.0007,
      "step": 38980
    },
    {
      "epoch": 1.24768,
      "grad_norm": 0.007570957764983177,
      "learning_rate": 1.1682346666666669e-05,
      "loss": 0.0483,
      "step": 38990
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.024121759459376335,
      "learning_rate": 1.1680213333333334e-05,
      "loss": 0.0007,
      "step": 39000
    },
    {
      "epoch": 1.24832,
      "grad_norm": 0.010654217563569546,
      "learning_rate": 1.1678080000000001e-05,
      "loss": 0.0005,
      "step": 39010
    },
    {
      "epoch": 1.24864,
      "grad_norm": 0.0042280894704163074,
      "learning_rate": 1.1675946666666667e-05,
      "loss": 0.0035,
      "step": 39020
    },
    {
      "epoch": 1.24896,
      "grad_norm": 0.005627290345728397,
      "learning_rate": 1.1673813333333335e-05,
      "loss": 0.0007,
      "step": 39030
    },
    {
      "epoch": 1.24928,
      "grad_norm": 0.19649167358875275,
      "learning_rate": 1.167168e-05,
      "loss": 0.0007,
      "step": 39040
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.003376515582203865,
      "learning_rate": 1.1669546666666666e-05,
      "loss": 0.0005,
      "step": 39050
    },
    {
      "epoch": 1.24992,
      "grad_norm": 0.11096716672182083,
      "learning_rate": 1.1667413333333335e-05,
      "loss": 0.053,
      "step": 39060
    },
    {
      "epoch": 1.25024,
      "grad_norm": 0.002491595922037959,
      "learning_rate": 1.166528e-05,
      "loss": 0.0006,
      "step": 39070
    },
    {
      "epoch": 1.2505600000000001,
      "grad_norm": 0.006391966715455055,
      "learning_rate": 1.1663146666666667e-05,
      "loss": 0.0436,
      "step": 39080
    },
    {
      "epoch": 1.25088,
      "grad_norm": 0.012994767166674137,
      "learning_rate": 1.1661013333333336e-05,
      "loss": 0.0369,
      "step": 39090
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.05605674535036087,
      "learning_rate": 1.1658880000000001e-05,
      "loss": 0.0006,
      "step": 39100
    },
    {
      "epoch": 1.25152,
      "grad_norm": 0.015613379888236523,
      "learning_rate": 1.1656746666666667e-05,
      "loss": 0.0066,
      "step": 39110
    },
    {
      "epoch": 1.25184,
      "grad_norm": 0.006780647207051516,
      "learning_rate": 1.1654613333333334e-05,
      "loss": 0.0004,
      "step": 39120
    },
    {
      "epoch": 1.25216,
      "grad_norm": 0.0034196150954812765,
      "learning_rate": 1.1652480000000001e-05,
      "loss": 0.0005,
      "step": 39130
    },
    {
      "epoch": 1.25248,
      "grad_norm": 0.012822980992496014,
      "learning_rate": 1.1650346666666668e-05,
      "loss": 0.0005,
      "step": 39140
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.03347751870751381,
      "learning_rate": 1.1648213333333333e-05,
      "loss": 0.0014,
      "step": 39150
    },
    {
      "epoch": 1.25312,
      "grad_norm": 0.010013294406235218,
      "learning_rate": 1.1646080000000002e-05,
      "loss": 0.0009,
      "step": 39160
    },
    {
      "epoch": 1.2534399999999999,
      "grad_norm": 0.015097850002348423,
      "learning_rate": 1.1643946666666668e-05,
      "loss": 0.0004,
      "step": 39170
    },
    {
      "epoch": 1.25376,
      "grad_norm": 0.011538696475327015,
      "learning_rate": 1.1641813333333335e-05,
      "loss": 0.0032,
      "step": 39180
    },
    {
      "epoch": 1.25408,
      "grad_norm": 0.011144361458718777,
      "learning_rate": 1.163968e-05,
      "loss": 0.0422,
      "step": 39190
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.00958880502730608,
      "learning_rate": 1.1637546666666669e-05,
      "loss": 0.0011,
      "step": 39200
    },
    {
      "epoch": 1.25472,
      "grad_norm": 0.020582808181643486,
      "learning_rate": 1.1635413333333334e-05,
      "loss": 0.0005,
      "step": 39210
    },
    {
      "epoch": 1.25504,
      "grad_norm": 0.01334172673523426,
      "learning_rate": 1.163328e-05,
      "loss": 0.0257,
      "step": 39220
    },
    {
      "epoch": 1.25536,
      "grad_norm": 0.03346911817789078,
      "learning_rate": 1.1631146666666668e-05,
      "loss": 0.001,
      "step": 39230
    },
    {
      "epoch": 1.25568,
      "grad_norm": 0.013736196793615818,
      "learning_rate": 1.1629013333333334e-05,
      "loss": 0.0109,
      "step": 39240
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.023473283275961876,
      "learning_rate": 1.162688e-05,
      "loss": 0.0007,
      "step": 39250
    },
    {
      "epoch": 1.25632,
      "grad_norm": 0.009957154281437397,
      "learning_rate": 1.1624746666666668e-05,
      "loss": 0.0007,
      "step": 39260
    },
    {
      "epoch": 1.25664,
      "grad_norm": 0.008213632740080357,
      "learning_rate": 1.1622613333333335e-05,
      "loss": 0.0185,
      "step": 39270
    },
    {
      "epoch": 1.25696,
      "grad_norm": 0.004805487580597401,
      "learning_rate": 1.162048e-05,
      "loss": 0.001,
      "step": 39280
    },
    {
      "epoch": 1.25728,
      "grad_norm": 0.0042798640206456184,
      "learning_rate": 1.1618346666666667e-05,
      "loss": 0.0011,
      "step": 39290
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.008362350054085255,
      "learning_rate": 1.1616213333333334e-05,
      "loss": 0.0003,
      "step": 39300
    },
    {
      "epoch": 1.25792,
      "grad_norm": 0.015119550749659538,
      "learning_rate": 1.1614080000000001e-05,
      "loss": 0.0003,
      "step": 39310
    },
    {
      "epoch": 1.25824,
      "grad_norm": 0.0610615573823452,
      "learning_rate": 1.1611946666666667e-05,
      "loss": 0.001,
      "step": 39320
    },
    {
      "epoch": 1.2585600000000001,
      "grad_norm": 0.007488262839615345,
      "learning_rate": 1.1609813333333336e-05,
      "loss": 0.0022,
      "step": 39330
    },
    {
      "epoch": 1.25888,
      "grad_norm": 0.010814760811626911,
      "learning_rate": 1.1607680000000001e-05,
      "loss": 0.0004,
      "step": 39340
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.00968471635133028,
      "learning_rate": 1.1605546666666666e-05,
      "loss": 0.0265,
      "step": 39350
    },
    {
      "epoch": 1.25952,
      "grad_norm": 0.004643505439162254,
      "learning_rate": 1.1603413333333333e-05,
      "loss": 0.0005,
      "step": 39360
    },
    {
      "epoch": 1.25984,
      "grad_norm": 0.008902699686586857,
      "learning_rate": 1.1601280000000002e-05,
      "loss": 0.001,
      "step": 39370
    },
    {
      "epoch": 1.26016,
      "grad_norm": 0.003408080665394664,
      "learning_rate": 1.1599146666666668e-05,
      "loss": 0.0003,
      "step": 39380
    },
    {
      "epoch": 1.26048,
      "grad_norm": 0.006620744243264198,
      "learning_rate": 1.1597013333333333e-05,
      "loss": 0.0003,
      "step": 39390
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.02691498212516308,
      "learning_rate": 1.1594880000000002e-05,
      "loss": 0.0013,
      "step": 39400
    },
    {
      "epoch": 1.26112,
      "grad_norm": 0.01020559761673212,
      "learning_rate": 1.1592746666666667e-05,
      "loss": 0.0002,
      "step": 39410
    },
    {
      "epoch": 1.26144,
      "grad_norm": 0.007596265058964491,
      "learning_rate": 1.1590613333333334e-05,
      "loss": 0.0003,
      "step": 39420
    },
    {
      "epoch": 1.26176,
      "grad_norm": 0.005858247634023428,
      "learning_rate": 1.1588480000000001e-05,
      "loss": 0.0003,
      "step": 39430
    },
    {
      "epoch": 1.26208,
      "grad_norm": 0.004306203220039606,
      "learning_rate": 1.1586346666666668e-05,
      "loss": 0.0003,
      "step": 39440
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.004275432787835598,
      "learning_rate": 1.1584213333333334e-05,
      "loss": 0.0003,
      "step": 39450
    },
    {
      "epoch": 1.26272,
      "grad_norm": 0.009108036756515503,
      "learning_rate": 1.158208e-05,
      "loss": 0.0007,
      "step": 39460
    },
    {
      "epoch": 1.26304,
      "grad_norm": 0.0025734880473464727,
      "learning_rate": 1.1579946666666668e-05,
      "loss": 0.0002,
      "step": 39470
    },
    {
      "epoch": 1.26336,
      "grad_norm": 0.0061573428101837635,
      "learning_rate": 1.1577813333333335e-05,
      "loss": 0.0002,
      "step": 39480
    },
    {
      "epoch": 1.26368,
      "grad_norm": 0.0018424297450110316,
      "learning_rate": 1.157568e-05,
      "loss": 0.0002,
      "step": 39490
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.0047152102924883366,
      "learning_rate": 1.1573546666666669e-05,
      "loss": 0.001,
      "step": 39500
    },
    {
      "epoch": 1.26432,
      "grad_norm": 0.005590509623289108,
      "learning_rate": 1.1571413333333334e-05,
      "loss": 0.0092,
      "step": 39510
    },
    {
      "epoch": 1.26464,
      "grad_norm": 0.009054278023540974,
      "learning_rate": 1.156928e-05,
      "loss": 0.0596,
      "step": 39520
    },
    {
      "epoch": 1.2649599999999999,
      "grad_norm": 0.015344484709203243,
      "learning_rate": 1.1567146666666667e-05,
      "loss": 0.0006,
      "step": 39530
    },
    {
      "epoch": 1.26528,
      "grad_norm": 0.005556919611990452,
      "learning_rate": 1.1565013333333334e-05,
      "loss": 0.0451,
      "step": 39540
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.012190264649689198,
      "learning_rate": 1.1562880000000001e-05,
      "loss": 0.0012,
      "step": 39550
    },
    {
      "epoch": 1.26592,
      "grad_norm": 0.005504750180989504,
      "learning_rate": 1.1560746666666666e-05,
      "loss": 0.0003,
      "step": 39560
    },
    {
      "epoch": 1.26624,
      "grad_norm": 0.0059265936724841595,
      "learning_rate": 1.1558613333333335e-05,
      "loss": 0.0003,
      "step": 39570
    },
    {
      "epoch": 1.2665600000000001,
      "grad_norm": 0.019310591742396355,
      "learning_rate": 1.155648e-05,
      "loss": 0.0003,
      "step": 39580
    },
    {
      "epoch": 1.26688,
      "grad_norm": 0.010081707499921322,
      "learning_rate": 1.1554346666666668e-05,
      "loss": 0.0005,
      "step": 39590
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.003496316494420171,
      "learning_rate": 1.1552213333333335e-05,
      "loss": 0.0004,
      "step": 39600
    },
    {
      "epoch": 1.26752,
      "grad_norm": 0.025667773559689522,
      "learning_rate": 1.1550080000000002e-05,
      "loss": 0.0005,
      "step": 39610
    },
    {
      "epoch": 1.26784,
      "grad_norm": 0.027990175411105156,
      "learning_rate": 1.1547946666666667e-05,
      "loss": 0.0004,
      "step": 39620
    },
    {
      "epoch": 1.26816,
      "grad_norm": 0.012111346237361431,
      "learning_rate": 1.1545813333333332e-05,
      "loss": 0.0218,
      "step": 39630
    },
    {
      "epoch": 1.26848,
      "grad_norm": 0.00751267047598958,
      "learning_rate": 1.1543680000000001e-05,
      "loss": 0.0003,
      "step": 39640
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.010372642427682877,
      "learning_rate": 1.1541546666666668e-05,
      "loss": 0.0583,
      "step": 39650
    },
    {
      "epoch": 1.26912,
      "grad_norm": 0.00635945238173008,
      "learning_rate": 1.1539413333333334e-05,
      "loss": 0.0502,
      "step": 39660
    },
    {
      "epoch": 1.26944,
      "grad_norm": 0.004289436154067516,
      "learning_rate": 1.1537280000000002e-05,
      "loss": 0.0011,
      "step": 39670
    },
    {
      "epoch": 1.26976,
      "grad_norm": 0.008413215167820454,
      "learning_rate": 1.1535146666666668e-05,
      "loss": 0.001,
      "step": 39680
    },
    {
      "epoch": 1.27008,
      "grad_norm": 0.008704140782356262,
      "learning_rate": 1.1533013333333333e-05,
      "loss": 0.0003,
      "step": 39690
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.002489660633727908,
      "learning_rate": 1.153088e-05,
      "loss": 0.0016,
      "step": 39700
    },
    {
      "epoch": 1.27072,
      "grad_norm": 0.004240037873387337,
      "learning_rate": 1.1528746666666667e-05,
      "loss": 0.0002,
      "step": 39710
    },
    {
      "epoch": 1.27104,
      "grad_norm": 0.004911933094263077,
      "learning_rate": 1.1526613333333334e-05,
      "loss": 0.0003,
      "step": 39720
    },
    {
      "epoch": 1.27136,
      "grad_norm": 0.004421612713485956,
      "learning_rate": 1.152448e-05,
      "loss": 0.0003,
      "step": 39730
    },
    {
      "epoch": 1.27168,
      "grad_norm": 0.0047930758446455,
      "learning_rate": 1.1522346666666669e-05,
      "loss": 0.0003,
      "step": 39740
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.006311432924121618,
      "learning_rate": 1.1520213333333334e-05,
      "loss": 0.0002,
      "step": 39750
    },
    {
      "epoch": 1.2723200000000001,
      "grad_norm": 0.003817893797531724,
      "learning_rate": 1.1518080000000001e-05,
      "loss": 0.0002,
      "step": 39760
    },
    {
      "epoch": 1.27264,
      "grad_norm": 0.0028966416139155626,
      "learning_rate": 1.1515946666666668e-05,
      "loss": 0.0002,
      "step": 39770
    },
    {
      "epoch": 1.2729599999999999,
      "grad_norm": 0.014155660755932331,
      "learning_rate": 1.1513813333333335e-05,
      "loss": 0.0028,
      "step": 39780
    },
    {
      "epoch": 1.27328,
      "grad_norm": 0.004706922918558121,
      "learning_rate": 1.151168e-05,
      "loss": 0.0003,
      "step": 39790
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.012080579064786434,
      "learning_rate": 1.1509546666666666e-05,
      "loss": 0.0003,
      "step": 39800
    },
    {
      "epoch": 1.27392,
      "grad_norm": 0.006994652561843395,
      "learning_rate": 1.1507413333333335e-05,
      "loss": 0.0003,
      "step": 39810
    },
    {
      "epoch": 1.27424,
      "grad_norm": 0.010095080360770226,
      "learning_rate": 1.150528e-05,
      "loss": 0.0003,
      "step": 39820
    },
    {
      "epoch": 1.2745600000000001,
      "grad_norm": 0.003824335290119052,
      "learning_rate": 1.1503146666666667e-05,
      "loss": 0.0108,
      "step": 39830
    },
    {
      "epoch": 1.27488,
      "grad_norm": 0.01272224448621273,
      "learning_rate": 1.1501013333333336e-05,
      "loss": 0.0003,
      "step": 39840
    },
    {
      "epoch": 1.2752,
      "grad_norm": 1.7910288572311401,
      "learning_rate": 1.1498880000000001e-05,
      "loss": 0.0236,
      "step": 39850
    },
    {
      "epoch": 1.27552,
      "grad_norm": 0.016641387715935707,
      "learning_rate": 1.1496746666666667e-05,
      "loss": 0.0003,
      "step": 39860
    },
    {
      "epoch": 1.27584,
      "grad_norm": 0.007123214658349752,
      "learning_rate": 1.1494613333333334e-05,
      "loss": 0.0004,
      "step": 39870
    },
    {
      "epoch": 1.27616,
      "grad_norm": 0.018285484984517097,
      "learning_rate": 1.149248e-05,
      "loss": 0.0005,
      "step": 39880
    },
    {
      "epoch": 1.27648,
      "grad_norm": 0.0059060510247945786,
      "learning_rate": 1.1490346666666668e-05,
      "loss": 0.0003,
      "step": 39890
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.005130961537361145,
      "learning_rate": 1.1488213333333333e-05,
      "loss": 0.0527,
      "step": 39900
    },
    {
      "epoch": 1.27712,
      "grad_norm": 0.003773205913603306,
      "learning_rate": 1.1486080000000002e-05,
      "loss": 0.0002,
      "step": 39910
    },
    {
      "epoch": 1.27744,
      "grad_norm": 0.005576925817877054,
      "learning_rate": 1.1483946666666667e-05,
      "loss": 0.0003,
      "step": 39920
    },
    {
      "epoch": 1.27776,
      "grad_norm": 0.002986770821735263,
      "learning_rate": 1.1481813333333334e-05,
      "loss": 0.0002,
      "step": 39930
    },
    {
      "epoch": 1.27808,
      "grad_norm": 0.008100743405520916,
      "learning_rate": 1.1479680000000002e-05,
      "loss": 0.0003,
      "step": 39940
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.003204851411283016,
      "learning_rate": 1.1477546666666669e-05,
      "loss": 0.0004,
      "step": 39950
    },
    {
      "epoch": 1.27872,
      "grad_norm": 0.006963040214031935,
      "learning_rate": 1.1475413333333334e-05,
      "loss": 0.0003,
      "step": 39960
    },
    {
      "epoch": 1.27904,
      "grad_norm": 0.01261508371680975,
      "learning_rate": 1.147328e-05,
      "loss": 0.0013,
      "step": 39970
    },
    {
      "epoch": 1.27936,
      "grad_norm": 0.007012492977082729,
      "learning_rate": 1.1471146666666668e-05,
      "loss": 0.0009,
      "step": 39980
    },
    {
      "epoch": 1.27968,
      "grad_norm": 0.007412411272525787,
      "learning_rate": 1.1469013333333333e-05,
      "loss": 0.0534,
      "step": 39990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.004685826599597931,
      "learning_rate": 1.146688e-05,
      "loss": 0.0008,
      "step": 40000
    },
    {
      "epoch": 1.2803200000000001,
      "grad_norm": 0.00856869202107191,
      "learning_rate": 1.1464746666666668e-05,
      "loss": 0.0007,
      "step": 40010
    },
    {
      "epoch": 1.28064,
      "grad_norm": 0.0130704864859581,
      "learning_rate": 1.1462613333333335e-05,
      "loss": 0.0003,
      "step": 40020
    },
    {
      "epoch": 1.2809599999999999,
      "grad_norm": 0.013729584403336048,
      "learning_rate": 1.146048e-05,
      "loss": 0.0436,
      "step": 40030
    },
    {
      "epoch": 1.28128,
      "grad_norm": 0.007635429501533508,
      "learning_rate": 1.1458346666666667e-05,
      "loss": 0.0003,
      "step": 40040
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.005091758910566568,
      "learning_rate": 1.1456213333333334e-05,
      "loss": 0.0003,
      "step": 40050
    },
    {
      "epoch": 1.28192,
      "grad_norm": 0.0031811015214771032,
      "learning_rate": 1.1454080000000001e-05,
      "loss": 0.0003,
      "step": 40060
    },
    {
      "epoch": 1.28224,
      "grad_norm": 0.0035338480956852436,
      "learning_rate": 1.1451946666666667e-05,
      "loss": 0.0003,
      "step": 40070
    },
    {
      "epoch": 1.28256,
      "grad_norm": 0.007452379912137985,
      "learning_rate": 1.1449813333333335e-05,
      "loss": 0.0003,
      "step": 40080
    },
    {
      "epoch": 1.28288,
      "grad_norm": 0.009684208780527115,
      "learning_rate": 1.144768e-05,
      "loss": 0.0754,
      "step": 40090
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.011056173592805862,
      "learning_rate": 1.1445546666666666e-05,
      "loss": 0.0004,
      "step": 40100
    },
    {
      "epoch": 1.28352,
      "grad_norm": 0.008127821609377861,
      "learning_rate": 1.1443413333333335e-05,
      "loss": 0.0005,
      "step": 40110
    },
    {
      "epoch": 1.28384,
      "grad_norm": 0.003946726676076651,
      "learning_rate": 1.1441280000000002e-05,
      "loss": 0.0212,
      "step": 40120
    },
    {
      "epoch": 1.28416,
      "grad_norm": 0.005876102019101381,
      "learning_rate": 1.1439146666666667e-05,
      "loss": 0.0005,
      "step": 40130
    },
    {
      "epoch": 1.28448,
      "grad_norm": 0.007495233789086342,
      "learning_rate": 1.1437013333333333e-05,
      "loss": 0.0007,
      "step": 40140
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.0076836938969790936,
      "learning_rate": 1.1434880000000002e-05,
      "loss": 0.0004,
      "step": 40150
    },
    {
      "epoch": 1.28512,
      "grad_norm": 0.02051635831594467,
      "learning_rate": 1.1432746666666667e-05,
      "loss": 0.0006,
      "step": 40160
    },
    {
      "epoch": 1.28544,
      "grad_norm": 0.010269051417708397,
      "learning_rate": 1.1430613333333334e-05,
      "loss": 0.0428,
      "step": 40170
    },
    {
      "epoch": 1.28576,
      "grad_norm": 0.018572529777884483,
      "learning_rate": 1.1428480000000001e-05,
      "loss": 0.002,
      "step": 40180
    },
    {
      "epoch": 1.2860800000000001,
      "grad_norm": 0.00976251158863306,
      "learning_rate": 1.1426346666666668e-05,
      "loss": 0.0004,
      "step": 40190
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.006217231508344412,
      "learning_rate": 1.1424213333333334e-05,
      "loss": 0.0011,
      "step": 40200
    },
    {
      "epoch": 1.2867199999999999,
      "grad_norm": 0.006383365951478481,
      "learning_rate": 1.142208e-05,
      "loss": 0.0004,
      "step": 40210
    },
    {
      "epoch": 1.28704,
      "grad_norm": 0.0507352240383625,
      "learning_rate": 1.1419946666666668e-05,
      "loss": 0.0006,
      "step": 40220
    },
    {
      "epoch": 1.28736,
      "grad_norm": 0.017163150012493134,
      "learning_rate": 1.1417813333333335e-05,
      "loss": 0.0008,
      "step": 40230
    },
    {
      "epoch": 1.28768,
      "grad_norm": 0.007319946773350239,
      "learning_rate": 1.141568e-05,
      "loss": 0.0004,
      "step": 40240
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.013724946416914463,
      "learning_rate": 1.1413546666666669e-05,
      "loss": 0.001,
      "step": 40250
    },
    {
      "epoch": 1.2883200000000001,
      "grad_norm": 0.01093384250998497,
      "learning_rate": 1.1411413333333334e-05,
      "loss": 0.0019,
      "step": 40260
    },
    {
      "epoch": 1.28864,
      "grad_norm": 0.00421961210668087,
      "learning_rate": 1.140928e-05,
      "loss": 0.0004,
      "step": 40270
    },
    {
      "epoch": 1.2889599999999999,
      "grad_norm": 0.005277769174426794,
      "learning_rate": 1.1407146666666668e-05,
      "loss": 0.0005,
      "step": 40280
    },
    {
      "epoch": 1.28928,
      "grad_norm": 0.003585065482184291,
      "learning_rate": 1.1405013333333334e-05,
      "loss": 0.0006,
      "step": 40290
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.005170969292521477,
      "learning_rate": 1.140288e-05,
      "loss": 0.001,
      "step": 40300
    },
    {
      "epoch": 1.28992,
      "grad_norm": 0.006519399117678404,
      "learning_rate": 1.1400746666666666e-05,
      "loss": 0.0002,
      "step": 40310
    },
    {
      "epoch": 1.29024,
      "grad_norm": 0.007629930507391691,
      "learning_rate": 1.1398613333333335e-05,
      "loss": 0.0003,
      "step": 40320
    },
    {
      "epoch": 1.29056,
      "grad_norm": 0.0019148743012920022,
      "learning_rate": 1.139648e-05,
      "loss": 0.0004,
      "step": 40330
    },
    {
      "epoch": 1.29088,
      "grad_norm": 0.00732304435223341,
      "learning_rate": 1.1394346666666667e-05,
      "loss": 0.0003,
      "step": 40340
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.008423751220107079,
      "learning_rate": 1.1392213333333335e-05,
      "loss": 0.0003,
      "step": 40350
    },
    {
      "epoch": 1.29152,
      "grad_norm": 0.0054685636423528194,
      "learning_rate": 1.1390080000000002e-05,
      "loss": 0.0003,
      "step": 40360
    },
    {
      "epoch": 1.29184,
      "grad_norm": 0.0021403233986347914,
      "learning_rate": 1.1387946666666667e-05,
      "loss": 0.0002,
      "step": 40370
    },
    {
      "epoch": 1.29216,
      "grad_norm": 0.0075990501791238785,
      "learning_rate": 1.1385813333333332e-05,
      "loss": 0.0003,
      "step": 40380
    },
    {
      "epoch": 1.29248,
      "grad_norm": 0.07687662541866302,
      "learning_rate": 1.1383680000000001e-05,
      "loss": 0.0258,
      "step": 40390
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.012780395336449146,
      "learning_rate": 1.1381546666666668e-05,
      "loss": 0.0497,
      "step": 40400
    },
    {
      "epoch": 1.29312,
      "grad_norm": 0.008644001558423042,
      "learning_rate": 1.1379413333333334e-05,
      "loss": 0.0003,
      "step": 40410
    },
    {
      "epoch": 1.29344,
      "grad_norm": 0.004684717860072851,
      "learning_rate": 1.1377280000000002e-05,
      "loss": 0.0241,
      "step": 40420
    },
    {
      "epoch": 1.29376,
      "grad_norm": 0.008300705812871456,
      "learning_rate": 1.1375146666666668e-05,
      "loss": 0.0005,
      "step": 40430
    },
    {
      "epoch": 1.2940800000000001,
      "grad_norm": 0.00531336385756731,
      "learning_rate": 1.1373013333333333e-05,
      "loss": 0.0003,
      "step": 40440
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.048923786729574203,
      "learning_rate": 1.1370880000000002e-05,
      "loss": 0.0052,
      "step": 40450
    },
    {
      "epoch": 1.2947199999999999,
      "grad_norm": 0.01244038250297308,
      "learning_rate": 1.1368746666666667e-05,
      "loss": 0.0696,
      "step": 40460
    },
    {
      "epoch": 1.29504,
      "grad_norm": 0.005326043348759413,
      "learning_rate": 1.1366613333333334e-05,
      "loss": 0.0006,
      "step": 40470
    },
    {
      "epoch": 1.29536,
      "grad_norm": 0.004052875097841024,
      "learning_rate": 1.136448e-05,
      "loss": 0.0003,
      "step": 40480
    },
    {
      "epoch": 1.29568,
      "grad_norm": 0.004994748160243034,
      "learning_rate": 1.1362346666666668e-05,
      "loss": 0.0004,
      "step": 40490
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.007861717604100704,
      "learning_rate": 1.1360213333333334e-05,
      "loss": 0.0406,
      "step": 40500
    },
    {
      "epoch": 1.29632,
      "grad_norm": 0.006654683500528336,
      "learning_rate": 1.1358080000000001e-05,
      "loss": 0.0004,
      "step": 40510
    },
    {
      "epoch": 1.29664,
      "grad_norm": 0.0068906559608876705,
      "learning_rate": 1.1355946666666668e-05,
      "loss": 0.0238,
      "step": 40520
    },
    {
      "epoch": 1.29696,
      "grad_norm": 0.009877131320536137,
      "learning_rate": 1.1353813333333335e-05,
      "loss": 0.0004,
      "step": 40530
    },
    {
      "epoch": 1.29728,
      "grad_norm": 0.012911397032439709,
      "learning_rate": 1.135168e-05,
      "loss": 0.0034,
      "step": 40540
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.004994683433324099,
      "learning_rate": 1.1349546666666666e-05,
      "loss": 0.0492,
      "step": 40550
    },
    {
      "epoch": 1.29792,
      "grad_norm": 0.0057517606765031815,
      "learning_rate": 1.1347413333333335e-05,
      "loss": 0.0006,
      "step": 40560
    },
    {
      "epoch": 1.29824,
      "grad_norm": 0.01761450618505478,
      "learning_rate": 1.134528e-05,
      "loss": 0.0004,
      "step": 40570
    },
    {
      "epoch": 1.29856,
      "grad_norm": 0.009453225880861282,
      "learning_rate": 1.1343146666666667e-05,
      "loss": 0.0005,
      "step": 40580
    },
    {
      "epoch": 1.29888,
      "grad_norm": 0.012431158684194088,
      "learning_rate": 1.1341013333333336e-05,
      "loss": 0.0481,
      "step": 40590
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.0048658279702067375,
      "learning_rate": 1.1338880000000001e-05,
      "loss": 0.0007,
      "step": 40600
    },
    {
      "epoch": 1.29952,
      "grad_norm": 3.661773920059204,
      "learning_rate": 1.1336746666666666e-05,
      "loss": 0.0038,
      "step": 40610
    },
    {
      "epoch": 1.29984,
      "grad_norm": 0.01438760757446289,
      "learning_rate": 1.1334613333333335e-05,
      "loss": 0.0532,
      "step": 40620
    },
    {
      "epoch": 1.30016,
      "grad_norm": 0.015936337411403656,
      "learning_rate": 1.133248e-05,
      "loss": 0.0547,
      "step": 40630
    },
    {
      "epoch": 1.30048,
      "grad_norm": 0.021271593868732452,
      "learning_rate": 1.1330346666666668e-05,
      "loss": 0.0101,
      "step": 40640
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.00926434900611639,
      "learning_rate": 1.1328213333333333e-05,
      "loss": 0.0286,
      "step": 40650
    },
    {
      "epoch": 1.30112,
      "grad_norm": 0.008254408836364746,
      "learning_rate": 1.1326080000000002e-05,
      "loss": 0.0005,
      "step": 40660
    },
    {
      "epoch": 1.30144,
      "grad_norm": 0.010194806382060051,
      "learning_rate": 1.1323946666666667e-05,
      "loss": 0.0005,
      "step": 40670
    },
    {
      "epoch": 1.30176,
      "grad_norm": 0.007291393354535103,
      "learning_rate": 1.1321813333333334e-05,
      "loss": 0.0007,
      "step": 40680
    },
    {
      "epoch": 1.3020800000000001,
      "grad_norm": 0.009581738151609898,
      "learning_rate": 1.1319680000000001e-05,
      "loss": 0.0006,
      "step": 40690
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.007807385642081499,
      "learning_rate": 1.1317546666666668e-05,
      "loss": 0.0005,
      "step": 40700
    },
    {
      "epoch": 1.3027199999999999,
      "grad_norm": 0.01387711614370346,
      "learning_rate": 1.1315413333333334e-05,
      "loss": 0.0005,
      "step": 40710
    },
    {
      "epoch": 1.30304,
      "grad_norm": 0.009524322114884853,
      "learning_rate": 1.131328e-05,
      "loss": 0.0006,
      "step": 40720
    },
    {
      "epoch": 1.30336,
      "grad_norm": 0.007320384029299021,
      "learning_rate": 1.1311146666666668e-05,
      "loss": 0.0028,
      "step": 40730
    },
    {
      "epoch": 1.30368,
      "grad_norm": 0.0029235256370157003,
      "learning_rate": 1.1309013333333333e-05,
      "loss": 0.0004,
      "step": 40740
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.006066435482352972,
      "learning_rate": 1.130688e-05,
      "loss": 0.0273,
      "step": 40750
    },
    {
      "epoch": 1.30432,
      "grad_norm": 0.010407580994069576,
      "learning_rate": 1.1304746666666667e-05,
      "loss": 0.0036,
      "step": 40760
    },
    {
      "epoch": 1.30464,
      "grad_norm": 0.03289433941245079,
      "learning_rate": 1.1302613333333335e-05,
      "loss": 0.0013,
      "step": 40770
    },
    {
      "epoch": 1.30496,
      "grad_norm": 0.012906023301184177,
      "learning_rate": 1.130048e-05,
      "loss": 0.0005,
      "step": 40780
    },
    {
      "epoch": 1.30528,
      "grad_norm": 0.00837169960141182,
      "learning_rate": 1.1298346666666669e-05,
      "loss": 0.0003,
      "step": 40790
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.009965891018509865,
      "learning_rate": 1.1296213333333334e-05,
      "loss": 0.0006,
      "step": 40800
    },
    {
      "epoch": 1.30592,
      "grad_norm": 0.007969096302986145,
      "learning_rate": 1.1294080000000001e-05,
      "loss": 0.0206,
      "step": 40810
    },
    {
      "epoch": 1.30624,
      "grad_norm": 0.02726064808666706,
      "learning_rate": 1.1291946666666667e-05,
      "loss": 0.035,
      "step": 40820
    },
    {
      "epoch": 1.30656,
      "grad_norm": 0.004952144809067249,
      "learning_rate": 1.1289813333333335e-05,
      "loss": 0.0007,
      "step": 40830
    },
    {
      "epoch": 1.30688,
      "grad_norm": 0.014112900011241436,
      "learning_rate": 1.128768e-05,
      "loss": 0.0004,
      "step": 40840
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.012083913199603558,
      "learning_rate": 1.1285546666666666e-05,
      "loss": 0.0003,
      "step": 40850
    },
    {
      "epoch": 1.30752,
      "grad_norm": 0.0070506553165614605,
      "learning_rate": 1.1283413333333335e-05,
      "loss": 0.0008,
      "step": 40860
    },
    {
      "epoch": 1.3078400000000001,
      "grad_norm": 0.004852338694036007,
      "learning_rate": 1.1281280000000002e-05,
      "loss": 0.0004,
      "step": 40870
    },
    {
      "epoch": 1.30816,
      "grad_norm": 0.02580767497420311,
      "learning_rate": 1.1279146666666667e-05,
      "loss": 0.0532,
      "step": 40880
    },
    {
      "epoch": 1.3084799999999999,
      "grad_norm": 0.006174966227263212,
      "learning_rate": 1.1277013333333333e-05,
      "loss": 0.0008,
      "step": 40890
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.005079497117549181,
      "learning_rate": 1.1274880000000001e-05,
      "loss": 0.0003,
      "step": 40900
    },
    {
      "epoch": 1.30912,
      "grad_norm": 0.008169599808752537,
      "learning_rate": 1.1272746666666667e-05,
      "loss": 0.0004,
      "step": 40910
    },
    {
      "epoch": 1.30944,
      "grad_norm": 0.02010078728199005,
      "learning_rate": 1.1270613333333334e-05,
      "loss": 0.0632,
      "step": 40920
    },
    {
      "epoch": 1.30976,
      "grad_norm": 2.7929093837738037,
      "learning_rate": 1.1268480000000001e-05,
      "loss": 0.0058,
      "step": 40930
    },
    {
      "epoch": 1.3100800000000001,
      "grad_norm": 0.006929636467248201,
      "learning_rate": 1.1266346666666668e-05,
      "loss": 0.0006,
      "step": 40940
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.007962465286254883,
      "learning_rate": 1.1264213333333333e-05,
      "loss": 0.0007,
      "step": 40950
    },
    {
      "epoch": 1.3107199999999999,
      "grad_norm": 0.005761264357715845,
      "learning_rate": 1.1262080000000002e-05,
      "loss": 0.0005,
      "step": 40960
    },
    {
      "epoch": 1.31104,
      "grad_norm": 0.006954858545213938,
      "learning_rate": 1.1259946666666668e-05,
      "loss": 0.0005,
      "step": 40970
    },
    {
      "epoch": 1.31136,
      "grad_norm": 0.00494924234226346,
      "learning_rate": 1.1257813333333335e-05,
      "loss": 0.0003,
      "step": 40980
    },
    {
      "epoch": 1.31168,
      "grad_norm": 0.005103146657347679,
      "learning_rate": 1.125568e-05,
      "loss": 0.0107,
      "step": 40990
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.006235367152839899,
      "learning_rate": 1.1253546666666669e-05,
      "loss": 0.0004,
      "step": 41000
    },
    {
      "epoch": 1.31232,
      "grad_norm": 0.003252269933000207,
      "learning_rate": 1.1251413333333334e-05,
      "loss": 0.0003,
      "step": 41010
    },
    {
      "epoch": 1.31264,
      "grad_norm": 0.005332054104655981,
      "learning_rate": 1.124928e-05,
      "loss": 0.025,
      "step": 41020
    },
    {
      "epoch": 1.31296,
      "grad_norm": 8.172688484191895,
      "learning_rate": 1.1247146666666668e-05,
      "loss": 0.0174,
      "step": 41030
    },
    {
      "epoch": 1.31328,
      "grad_norm": 0.008553322404623032,
      "learning_rate": 1.1245013333333334e-05,
      "loss": 0.0005,
      "step": 41040
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.003910758998245001,
      "learning_rate": 1.124288e-05,
      "loss": 0.0004,
      "step": 41050
    },
    {
      "epoch": 1.31392,
      "grad_norm": 0.008797230198979378,
      "learning_rate": 1.124074666666667e-05,
      "loss": 0.0003,
      "step": 41060
    },
    {
      "epoch": 1.31424,
      "grad_norm": 0.008530600927770138,
      "learning_rate": 1.1238613333333335e-05,
      "loss": 0.0004,
      "step": 41070
    },
    {
      "epoch": 1.31456,
      "grad_norm": 0.002614055061712861,
      "learning_rate": 1.123648e-05,
      "loss": 0.0003,
      "step": 41080
    },
    {
      "epoch": 1.31488,
      "grad_norm": 2.10416579246521,
      "learning_rate": 1.1234346666666667e-05,
      "loss": 0.0027,
      "step": 41090
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.004530793987214565,
      "learning_rate": 1.1232213333333334e-05,
      "loss": 0.0003,
      "step": 41100
    },
    {
      "epoch": 1.31552,
      "grad_norm": 0.0037382962182164192,
      "learning_rate": 1.1230080000000001e-05,
      "loss": 0.0029,
      "step": 41110
    },
    {
      "epoch": 1.3158400000000001,
      "grad_norm": 0.006096618715673685,
      "learning_rate": 1.1227946666666667e-05,
      "loss": 0.0385,
      "step": 41120
    },
    {
      "epoch": 1.31616,
      "grad_norm": 0.002636981662362814,
      "learning_rate": 1.1225813333333336e-05,
      "loss": 0.0002,
      "step": 41130
    },
    {
      "epoch": 1.3164799999999999,
      "grad_norm": 0.00425003794953227,
      "learning_rate": 1.1223680000000001e-05,
      "loss": 0.0002,
      "step": 41140
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.0028631463646888733,
      "learning_rate": 1.1221546666666668e-05,
      "loss": 0.0002,
      "step": 41150
    },
    {
      "epoch": 1.31712,
      "grad_norm": 0.006167520768940449,
      "learning_rate": 1.1219413333333333e-05,
      "loss": 0.0053,
      "step": 41160
    },
    {
      "epoch": 1.31744,
      "grad_norm": 0.004503624979406595,
      "learning_rate": 1.1217280000000002e-05,
      "loss": 0.0002,
      "step": 41170
    },
    {
      "epoch": 1.31776,
      "grad_norm": 2.6623375415802,
      "learning_rate": 1.1215146666666668e-05,
      "loss": 0.0137,
      "step": 41180
    },
    {
      "epoch": 1.31808,
      "grad_norm": 0.006293254904448986,
      "learning_rate": 1.1213013333333333e-05,
      "loss": 0.007,
      "step": 41190
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.07318876683712006,
      "learning_rate": 1.1210880000000002e-05,
      "loss": 0.045,
      "step": 41200
    },
    {
      "epoch": 1.31872,
      "grad_norm": 0.0063392398878932,
      "learning_rate": 1.1208746666666667e-05,
      "loss": 0.0028,
      "step": 41210
    },
    {
      "epoch": 1.31904,
      "grad_norm": 0.027186913415789604,
      "learning_rate": 1.1206613333333334e-05,
      "loss": 0.0039,
      "step": 41220
    },
    {
      "epoch": 1.31936,
      "grad_norm": 5.564007759094238,
      "learning_rate": 1.1204480000000001e-05,
      "loss": 0.0667,
      "step": 41230
    },
    {
      "epoch": 1.31968,
      "grad_norm": 0.1281498521566391,
      "learning_rate": 1.1202346666666668e-05,
      "loss": 0.0007,
      "step": 41240
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.012184721417725086,
      "learning_rate": 1.1200213333333334e-05,
      "loss": 0.0019,
      "step": 41250
    },
    {
      "epoch": 1.32032,
      "grad_norm": 0.8820112943649292,
      "learning_rate": 1.119808e-05,
      "loss": 0.0046,
      "step": 41260
    },
    {
      "epoch": 1.32064,
      "grad_norm": 0.004799215123057365,
      "learning_rate": 1.1195946666666668e-05,
      "loss": 0.0004,
      "step": 41270
    },
    {
      "epoch": 1.32096,
      "grad_norm": 0.030732877552509308,
      "learning_rate": 1.1193813333333335e-05,
      "loss": 0.0004,
      "step": 41280
    },
    {
      "epoch": 1.32128,
      "grad_norm": 0.02509630098938942,
      "learning_rate": 1.119168e-05,
      "loss": 0.0013,
      "step": 41290
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.00994807854294777,
      "learning_rate": 1.1189546666666669e-05,
      "loss": 0.0497,
      "step": 41300
    },
    {
      "epoch": 1.32192,
      "grad_norm": 0.01356938574463129,
      "learning_rate": 1.1187413333333334e-05,
      "loss": 0.0003,
      "step": 41310
    },
    {
      "epoch": 1.32224,
      "grad_norm": 0.012805924750864506,
      "learning_rate": 1.118528e-05,
      "loss": 0.0016,
      "step": 41320
    },
    {
      "epoch": 1.32256,
      "grad_norm": 0.006105857435613871,
      "learning_rate": 1.1183146666666667e-05,
      "loss": 0.001,
      "step": 41330
    },
    {
      "epoch": 1.32288,
      "grad_norm": 0.008381797932088375,
      "learning_rate": 1.1181013333333336e-05,
      "loss": 0.0004,
      "step": 41340
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.008623838424682617,
      "learning_rate": 1.1178880000000001e-05,
      "loss": 0.0005,
      "step": 41350
    },
    {
      "epoch": 1.32352,
      "grad_norm": 0.008932210505008698,
      "learning_rate": 1.1176746666666666e-05,
      "loss": 0.0004,
      "step": 41360
    },
    {
      "epoch": 1.3238400000000001,
      "grad_norm": 0.016813967376947403,
      "learning_rate": 1.1174613333333335e-05,
      "loss": 0.004,
      "step": 41370
    },
    {
      "epoch": 1.32416,
      "grad_norm": 0.05747535452246666,
      "learning_rate": 1.117248e-05,
      "loss": 0.0008,
      "step": 41380
    },
    {
      "epoch": 1.3244799999999999,
      "grad_norm": 0.009166388772428036,
      "learning_rate": 1.1170346666666668e-05,
      "loss": 0.0003,
      "step": 41390
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.004523656331002712,
      "learning_rate": 1.1168213333333335e-05,
      "loss": 0.0205,
      "step": 41400
    },
    {
      "epoch": 1.32512,
      "grad_norm": 0.010155639611184597,
      "learning_rate": 1.1166080000000002e-05,
      "loss": 0.0003,
      "step": 41410
    },
    {
      "epoch": 1.32544,
      "grad_norm": 0.03823554515838623,
      "learning_rate": 1.1163946666666667e-05,
      "loss": 0.0005,
      "step": 41420
    },
    {
      "epoch": 1.32576,
      "grad_norm": 0.0035376660525798798,
      "learning_rate": 1.1161813333333334e-05,
      "loss": 0.0062,
      "step": 41430
    },
    {
      "epoch": 1.32608,
      "grad_norm": 0.8528679013252258,
      "learning_rate": 1.1159680000000001e-05,
      "loss": 0.0011,
      "step": 41440
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.0033549368381500244,
      "learning_rate": 1.1157546666666668e-05,
      "loss": 0.0004,
      "step": 41450
    },
    {
      "epoch": 1.32672,
      "grad_norm": 0.004791783634573221,
      "learning_rate": 1.1155413333333334e-05,
      "loss": 0.0004,
      "step": 41460
    },
    {
      "epoch": 1.32704,
      "grad_norm": 0.010489901527762413,
      "learning_rate": 1.1153280000000002e-05,
      "loss": 0.0003,
      "step": 41470
    },
    {
      "epoch": 1.32736,
      "grad_norm": 0.004161061719059944,
      "learning_rate": 1.1151146666666668e-05,
      "loss": 0.0003,
      "step": 41480
    },
    {
      "epoch": 1.32768,
      "grad_norm": 0.010123231448233128,
      "learning_rate": 1.1149013333333333e-05,
      "loss": 0.0004,
      "step": 41490
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.023936204612255096,
      "learning_rate": 1.114688e-05,
      "loss": 0.0043,
      "step": 41500
    },
    {
      "epoch": 1.32832,
      "grad_norm": 0.0037882772739976645,
      "learning_rate": 1.1144746666666667e-05,
      "loss": 0.0066,
      "step": 41510
    },
    {
      "epoch": 1.32864,
      "grad_norm": 0.017982767894864082,
      "learning_rate": 1.1142613333333334e-05,
      "loss": 0.0013,
      "step": 41520
    },
    {
      "epoch": 1.32896,
      "grad_norm": 0.010010570287704468,
      "learning_rate": 1.114048e-05,
      "loss": 0.0004,
      "step": 41530
    },
    {
      "epoch": 1.32928,
      "grad_norm": 0.00580601254478097,
      "learning_rate": 1.1138346666666669e-05,
      "loss": 0.0016,
      "step": 41540
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.07085315138101578,
      "learning_rate": 1.1136213333333334e-05,
      "loss": 0.0003,
      "step": 41550
    },
    {
      "epoch": 1.32992,
      "grad_norm": 0.009569664485752583,
      "learning_rate": 1.1134080000000001e-05,
      "loss": 0.0185,
      "step": 41560
    },
    {
      "epoch": 1.3302399999999999,
      "grad_norm": 0.06549625843763351,
      "learning_rate": 1.1131946666666668e-05,
      "loss": 0.0007,
      "step": 41570
    },
    {
      "epoch": 1.33056,
      "grad_norm": 0.04801077023148537,
      "learning_rate": 1.1129813333333335e-05,
      "loss": 0.0004,
      "step": 41580
    },
    {
      "epoch": 1.33088,
      "grad_norm": 0.01059217844158411,
      "learning_rate": 1.112768e-05,
      "loss": 0.0003,
      "step": 41590
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.0057226140052080154,
      "learning_rate": 1.1125546666666666e-05,
      "loss": 0.0004,
      "step": 41600
    },
    {
      "epoch": 1.33152,
      "grad_norm": 0.01862867921590805,
      "learning_rate": 1.1123413333333335e-05,
      "loss": 0.001,
      "step": 41610
    },
    {
      "epoch": 1.3318400000000001,
      "grad_norm": 0.007485383190214634,
      "learning_rate": 1.1121280000000002e-05,
      "loss": 0.0004,
      "step": 41620
    },
    {
      "epoch": 1.33216,
      "grad_norm": 0.011809756979346275,
      "learning_rate": 1.1119146666666667e-05,
      "loss": 0.0005,
      "step": 41630
    },
    {
      "epoch": 1.3324799999999999,
      "grad_norm": 0.010813804343342781,
      "learning_rate": 1.1117013333333336e-05,
      "loss": 0.0009,
      "step": 41640
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.003638077061623335,
      "learning_rate": 1.1114880000000001e-05,
      "loss": 0.0005,
      "step": 41650
    },
    {
      "epoch": 1.33312,
      "grad_norm": 0.00953100435435772,
      "learning_rate": 1.1112746666666667e-05,
      "loss": 0.0004,
      "step": 41660
    },
    {
      "epoch": 1.33344,
      "grad_norm": 0.01992323435842991,
      "learning_rate": 1.1110613333333334e-05,
      "loss": 0.0002,
      "step": 41670
    },
    {
      "epoch": 1.33376,
      "grad_norm": 0.021397259086370468,
      "learning_rate": 1.110848e-05,
      "loss": 0.0463,
      "step": 41680
    },
    {
      "epoch": 1.33408,
      "grad_norm": 0.005169755779206753,
      "learning_rate": 1.1106346666666668e-05,
      "loss": 0.0004,
      "step": 41690
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.01828901283442974,
      "learning_rate": 1.1104213333333333e-05,
      "loss": 0.0003,
      "step": 41700
    },
    {
      "epoch": 1.33472,
      "grad_norm": 0.004323286470025778,
      "learning_rate": 1.1102080000000002e-05,
      "loss": 0.007,
      "step": 41710
    },
    {
      "epoch": 1.33504,
      "grad_norm": 0.002965104067698121,
      "learning_rate": 1.1099946666666667e-05,
      "loss": 0.0004,
      "step": 41720
    },
    {
      "epoch": 1.33536,
      "grad_norm": 0.13118422031402588,
      "learning_rate": 1.1097813333333334e-05,
      "loss": 0.0004,
      "step": 41730
    },
    {
      "epoch": 1.33568,
      "grad_norm": 0.004180444870144129,
      "learning_rate": 1.1095680000000001e-05,
      "loss": 0.0034,
      "step": 41740
    },
    {
      "epoch": 1.336,
      "grad_norm": 2.055281400680542,
      "learning_rate": 1.1093546666666669e-05,
      "loss": 0.017,
      "step": 41750
    },
    {
      "epoch": 1.33632,
      "grad_norm": 0.011025818064808846,
      "learning_rate": 1.1091413333333334e-05,
      "loss": 0.0008,
      "step": 41760
    },
    {
      "epoch": 1.33664,
      "grad_norm": 0.04585118964314461,
      "learning_rate": 1.108928e-05,
      "loss": 0.0004,
      "step": 41770
    },
    {
      "epoch": 1.33696,
      "grad_norm": 0.004752382170408964,
      "learning_rate": 1.1087146666666668e-05,
      "loss": 0.0005,
      "step": 41780
    },
    {
      "epoch": 1.33728,
      "grad_norm": 0.06661506742238998,
      "learning_rate": 1.1085013333333333e-05,
      "loss": 0.0003,
      "step": 41790
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.008084649220108986,
      "learning_rate": 1.108288e-05,
      "loss": 0.0007,
      "step": 41800
    },
    {
      "epoch": 1.33792,
      "grad_norm": 0.00468600494787097,
      "learning_rate": 1.108074666666667e-05,
      "loss": 0.0002,
      "step": 41810
    },
    {
      "epoch": 1.3382399999999999,
      "grad_norm": 0.00395719101652503,
      "learning_rate": 1.1078613333333335e-05,
      "loss": 0.0003,
      "step": 41820
    },
    {
      "epoch": 1.33856,
      "grad_norm": 0.004903798922896385,
      "learning_rate": 1.107648e-05,
      "loss": 0.0003,
      "step": 41830
    },
    {
      "epoch": 1.33888,
      "grad_norm": 0.006047836039215326,
      "learning_rate": 1.1074346666666667e-05,
      "loss": 0.0286,
      "step": 41840
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.0027394024655222893,
      "learning_rate": 1.1072213333333334e-05,
      "loss": 0.0035,
      "step": 41850
    },
    {
      "epoch": 1.33952,
      "grad_norm": 0.0037077264860272408,
      "learning_rate": 1.1070080000000001e-05,
      "loss": 0.0003,
      "step": 41860
    },
    {
      "epoch": 1.33984,
      "grad_norm": 0.003837346099317074,
      "learning_rate": 1.1067946666666667e-05,
      "loss": 0.012,
      "step": 41870
    },
    {
      "epoch": 1.34016,
      "grad_norm": 0.04891519248485565,
      "learning_rate": 1.1065813333333335e-05,
      "loss": 0.0004,
      "step": 41880
    },
    {
      "epoch": 1.34048,
      "grad_norm": 0.005374955013394356,
      "learning_rate": 1.106368e-05,
      "loss": 0.0007,
      "step": 41890
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.009690984152257442,
      "learning_rate": 1.1061546666666668e-05,
      "loss": 0.0004,
      "step": 41900
    },
    {
      "epoch": 1.34112,
      "grad_norm": 0.007067803759127855,
      "learning_rate": 1.1059413333333335e-05,
      "loss": 0.0003,
      "step": 41910
    },
    {
      "epoch": 1.34144,
      "grad_norm": 0.008055644109845161,
      "learning_rate": 1.1057280000000002e-05,
      "loss": 0.0003,
      "step": 41920
    },
    {
      "epoch": 1.34176,
      "grad_norm": 0.014006160199642181,
      "learning_rate": 1.1055146666666667e-05,
      "loss": 0.0007,
      "step": 41930
    },
    {
      "epoch": 1.34208,
      "grad_norm": 0.004335559904575348,
      "learning_rate": 1.1053013333333333e-05,
      "loss": 0.0254,
      "step": 41940
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.006755978800356388,
      "learning_rate": 1.1050880000000002e-05,
      "loss": 0.0211,
      "step": 41950
    },
    {
      "epoch": 1.34272,
      "grad_norm": 0.016491364687681198,
      "learning_rate": 1.1048746666666667e-05,
      "loss": 0.0327,
      "step": 41960
    },
    {
      "epoch": 1.34304,
      "grad_norm": 0.021551145240664482,
      "learning_rate": 1.1046613333333334e-05,
      "loss": 0.0003,
      "step": 41970
    },
    {
      "epoch": 1.34336,
      "grad_norm": 0.015124065801501274,
      "learning_rate": 1.1044480000000001e-05,
      "loss": 0.0005,
      "step": 41980
    },
    {
      "epoch": 1.34368,
      "grad_norm": 0.12806116044521332,
      "learning_rate": 1.1042346666666668e-05,
      "loss": 0.0005,
      "step": 41990
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.00704939803108573,
      "learning_rate": 1.1040213333333333e-05,
      "loss": 0.0063,
      "step": 42000
    },
    {
      "epoch": 1.34432,
      "grad_norm": 0.03353091701865196,
      "learning_rate": 1.103808e-05,
      "loss": 0.0004,
      "step": 42010
    },
    {
      "epoch": 1.34464,
      "grad_norm": 0.004981065634638071,
      "learning_rate": 1.1035946666666668e-05,
      "loss": 0.0269,
      "step": 42020
    },
    {
      "epoch": 1.34496,
      "grad_norm": 0.004872565157711506,
      "learning_rate": 1.1033813333333335e-05,
      "loss": 0.0453,
      "step": 42030
    },
    {
      "epoch": 1.34528,
      "grad_norm": 0.00386405223980546,
      "learning_rate": 1.103168e-05,
      "loss": 0.0003,
      "step": 42040
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.026783565059304237,
      "learning_rate": 1.1029546666666669e-05,
      "loss": 0.0003,
      "step": 42050
    },
    {
      "epoch": 1.34592,
      "grad_norm": 0.010969980619847775,
      "learning_rate": 1.1027413333333334e-05,
      "loss": 0.0003,
      "step": 42060
    },
    {
      "epoch": 1.3462399999999999,
      "grad_norm": 0.008666357956826687,
      "learning_rate": 1.102528e-05,
      "loss": 0.0003,
      "step": 42070
    },
    {
      "epoch": 1.34656,
      "grad_norm": 2.0612215995788574,
      "learning_rate": 1.1023146666666668e-05,
      "loss": 0.0669,
      "step": 42080
    },
    {
      "epoch": 1.34688,
      "grad_norm": 0.0043899910524487495,
      "learning_rate": 1.1021013333333335e-05,
      "loss": 0.0002,
      "step": 42090
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.017312105745077133,
      "learning_rate": 1.101888e-05,
      "loss": 0.0308,
      "step": 42100
    },
    {
      "epoch": 1.34752,
      "grad_norm": 0.004938184283673763,
      "learning_rate": 1.1016746666666666e-05,
      "loss": 0.0022,
      "step": 42110
    },
    {
      "epoch": 1.34784,
      "grad_norm": 0.010159032419323921,
      "learning_rate": 1.1014613333333335e-05,
      "loss": 0.0003,
      "step": 42120
    },
    {
      "epoch": 1.34816,
      "grad_norm": 0.0068879988975822926,
      "learning_rate": 1.101248e-05,
      "loss": 0.0537,
      "step": 42130
    },
    {
      "epoch": 1.34848,
      "grad_norm": 0.009793182834982872,
      "learning_rate": 1.1010346666666667e-05,
      "loss": 0.0004,
      "step": 42140
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.02457738295197487,
      "learning_rate": 1.1008213333333334e-05,
      "loss": 0.0005,
      "step": 42150
    },
    {
      "epoch": 1.34912,
      "grad_norm": 0.008786775171756744,
      "learning_rate": 1.1006080000000002e-05,
      "loss": 0.0004,
      "step": 42160
    },
    {
      "epoch": 1.34944,
      "grad_norm": 0.006686353590339422,
      "learning_rate": 1.1003946666666667e-05,
      "loss": 0.0002,
      "step": 42170
    },
    {
      "epoch": 1.34976,
      "grad_norm": 0.012371007353067398,
      "learning_rate": 1.1001813333333334e-05,
      "loss": 0.0003,
      "step": 42180
    },
    {
      "epoch": 1.35008,
      "grad_norm": 0.03529169037938118,
      "learning_rate": 1.0999680000000001e-05,
      "loss": 0.0006,
      "step": 42190
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.041232239454984665,
      "learning_rate": 1.0997546666666668e-05,
      "loss": 0.0007,
      "step": 42200
    },
    {
      "epoch": 1.35072,
      "grad_norm": 0.006434394977986813,
      "learning_rate": 1.0995413333333333e-05,
      "loss": 0.0003,
      "step": 42210
    },
    {
      "epoch": 1.35104,
      "grad_norm": 0.009657111950218678,
      "learning_rate": 1.0993280000000002e-05,
      "loss": 0.0056,
      "step": 42220
    },
    {
      "epoch": 1.3513600000000001,
      "grad_norm": 0.012109666131436825,
      "learning_rate": 1.0991146666666668e-05,
      "loss": 0.0003,
      "step": 42230
    },
    {
      "epoch": 1.35168,
      "grad_norm": 0.0045764632523059845,
      "learning_rate": 1.0989013333333333e-05,
      "loss": 0.0003,
      "step": 42240
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.002147246617823839,
      "learning_rate": 1.0986880000000002e-05,
      "loss": 0.0003,
      "step": 42250
    },
    {
      "epoch": 1.35232,
      "grad_norm": 0.004948722664266825,
      "learning_rate": 1.0984746666666669e-05,
      "loss": 0.0102,
      "step": 42260
    },
    {
      "epoch": 1.35264,
      "grad_norm": 0.011972072534263134,
      "learning_rate": 1.0982613333333334e-05,
      "loss": 0.0468,
      "step": 42270
    },
    {
      "epoch": 1.35296,
      "grad_norm": 0.02980584278702736,
      "learning_rate": 1.098048e-05,
      "loss": 0.0005,
      "step": 42280
    },
    {
      "epoch": 1.35328,
      "grad_norm": 0.010808318853378296,
      "learning_rate": 1.0978346666666668e-05,
      "loss": 0.0003,
      "step": 42290
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.0025432072579860687,
      "learning_rate": 1.0976213333333334e-05,
      "loss": 0.0004,
      "step": 42300
    },
    {
      "epoch": 1.35392,
      "grad_norm": 0.006179644260555506,
      "learning_rate": 1.097408e-05,
      "loss": 0.0003,
      "step": 42310
    },
    {
      "epoch": 1.3542399999999999,
      "grad_norm": 0.0043618083000183105,
      "learning_rate": 1.0971946666666668e-05,
      "loss": 0.0414,
      "step": 42320
    },
    {
      "epoch": 1.35456,
      "grad_norm": 0.016338348388671875,
      "learning_rate": 1.0969813333333335e-05,
      "loss": 0.0004,
      "step": 42330
    },
    {
      "epoch": 1.35488,
      "grad_norm": 0.02346513234078884,
      "learning_rate": 1.096768e-05,
      "loss": 0.0004,
      "step": 42340
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.02924080565571785,
      "learning_rate": 1.0965546666666667e-05,
      "loss": 0.027,
      "step": 42350
    },
    {
      "epoch": 1.35552,
      "grad_norm": 0.011769422329962254,
      "learning_rate": 1.0963413333333334e-05,
      "loss": 0.0003,
      "step": 42360
    },
    {
      "epoch": 1.35584,
      "grad_norm": 0.010752327740192413,
      "learning_rate": 1.0961280000000002e-05,
      "loss": 0.0006,
      "step": 42370
    },
    {
      "epoch": 1.35616,
      "grad_norm": 0.005335408262908459,
      "learning_rate": 1.0959146666666667e-05,
      "loss": 0.0006,
      "step": 42380
    },
    {
      "epoch": 1.35648,
      "grad_norm": 0.008611337281763554,
      "learning_rate": 1.0957013333333336e-05,
      "loss": 0.0014,
      "step": 42390
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.024842191487550735,
      "learning_rate": 1.0954880000000001e-05,
      "loss": 0.0041,
      "step": 42400
    },
    {
      "epoch": 1.35712,
      "grad_norm": 0.006681352388113737,
      "learning_rate": 1.0952746666666666e-05,
      "loss": 0.0003,
      "step": 42410
    },
    {
      "epoch": 1.35744,
      "grad_norm": 0.014960359781980515,
      "learning_rate": 1.0950613333333335e-05,
      "loss": 0.0005,
      "step": 42420
    },
    {
      "epoch": 1.35776,
      "grad_norm": 0.005883053410798311,
      "learning_rate": 1.094848e-05,
      "loss": 0.0003,
      "step": 42430
    },
    {
      "epoch": 1.35808,
      "grad_norm": 0.02289191633462906,
      "learning_rate": 1.0946346666666668e-05,
      "loss": 0.0004,
      "step": 42440
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.00434897979721427,
      "learning_rate": 1.0944213333333333e-05,
      "loss": 0.0002,
      "step": 42450
    },
    {
      "epoch": 1.35872,
      "grad_norm": 0.025764770805835724,
      "learning_rate": 1.0942080000000002e-05,
      "loss": 0.0003,
      "step": 42460
    },
    {
      "epoch": 1.35904,
      "grad_norm": 0.005959253292530775,
      "learning_rate": 1.0939946666666667e-05,
      "loss": 0.0003,
      "step": 42470
    },
    {
      "epoch": 1.3593600000000001,
      "grad_norm": 0.004011192359030247,
      "learning_rate": 1.0937813333333334e-05,
      "loss": 0.0083,
      "step": 42480
    },
    {
      "epoch": 1.35968,
      "grad_norm": 0.007701610680669546,
      "learning_rate": 1.0935680000000001e-05,
      "loss": 0.0494,
      "step": 42490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.008265575394034386,
      "learning_rate": 1.0933546666666668e-05,
      "loss": 0.0003,
      "step": 42500
    },
    {
      "epoch": 1.36032,
      "grad_norm": 0.0181628055870533,
      "learning_rate": 1.0931413333333334e-05,
      "loss": 0.0006,
      "step": 42510
    },
    {
      "epoch": 1.36064,
      "grad_norm": 0.009515603072941303,
      "learning_rate": 1.0929279999999999e-05,
      "loss": 0.0003,
      "step": 42520
    },
    {
      "epoch": 1.36096,
      "grad_norm": 0.014285676181316376,
      "learning_rate": 1.0927146666666668e-05,
      "loss": 0.0006,
      "step": 42530
    },
    {
      "epoch": 1.36128,
      "grad_norm": 0.0057013253681361675,
      "learning_rate": 1.0925013333333335e-05,
      "loss": 0.0004,
      "step": 42540
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.008047807961702347,
      "learning_rate": 1.092288e-05,
      "loss": 0.0407,
      "step": 42550
    },
    {
      "epoch": 1.36192,
      "grad_norm": 0.004205621778964996,
      "learning_rate": 1.0920746666666669e-05,
      "loss": 0.0003,
      "step": 42560
    },
    {
      "epoch": 1.36224,
      "grad_norm": 0.005476127378642559,
      "learning_rate": 1.0918613333333334e-05,
      "loss": 0.0003,
      "step": 42570
    },
    {
      "epoch": 1.36256,
      "grad_norm": 0.006912090815603733,
      "learning_rate": 1.091648e-05,
      "loss": 0.012,
      "step": 42580
    },
    {
      "epoch": 1.36288,
      "grad_norm": 0.00401685107499361,
      "learning_rate": 1.0914346666666669e-05,
      "loss": 0.0003,
      "step": 42590
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.0031746546737849712,
      "learning_rate": 1.0912213333333334e-05,
      "loss": 0.0003,
      "step": 42600
    },
    {
      "epoch": 1.36352,
      "grad_norm": 0.019063791260123253,
      "learning_rate": 1.0910080000000001e-05,
      "loss": 0.0005,
      "step": 42610
    },
    {
      "epoch": 1.36384,
      "grad_norm": 0.004687968175858259,
      "learning_rate": 1.0907946666666666e-05,
      "loss": 0.0004,
      "step": 42620
    },
    {
      "epoch": 1.36416,
      "grad_norm": 0.021125202998518944,
      "learning_rate": 1.0905813333333335e-05,
      "loss": 0.0007,
      "step": 42630
    },
    {
      "epoch": 1.36448,
      "grad_norm": 0.008459257893264294,
      "learning_rate": 1.090368e-05,
      "loss": 0.0003,
      "step": 42640
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.004218586720526218,
      "learning_rate": 1.0901546666666668e-05,
      "loss": 0.0185,
      "step": 42650
    },
    {
      "epoch": 1.3651200000000001,
      "grad_norm": 0.009231332689523697,
      "learning_rate": 1.0899413333333335e-05,
      "loss": 0.0003,
      "step": 42660
    },
    {
      "epoch": 1.36544,
      "grad_norm": 0.03797917068004608,
      "learning_rate": 1.0897280000000002e-05,
      "loss": 0.0003,
      "step": 42670
    },
    {
      "epoch": 1.3657599999999999,
      "grad_norm": 0.0028921954799443483,
      "learning_rate": 1.0895146666666667e-05,
      "loss": 0.0007,
      "step": 42680
    },
    {
      "epoch": 1.36608,
      "grad_norm": 0.004836701788008213,
      "learning_rate": 1.0893013333333333e-05,
      "loss": 0.0003,
      "step": 42690
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.006059827748686075,
      "learning_rate": 1.0890880000000001e-05,
      "loss": 0.0002,
      "step": 42700
    },
    {
      "epoch": 1.36672,
      "grad_norm": 0.0040517160668969154,
      "learning_rate": 1.0888746666666667e-05,
      "loss": 0.0007,
      "step": 42710
    },
    {
      "epoch": 1.36704,
      "grad_norm": 0.0030603529885411263,
      "learning_rate": 1.0886613333333334e-05,
      "loss": 0.0615,
      "step": 42720
    },
    {
      "epoch": 1.3673600000000001,
      "grad_norm": 0.003179488703608513,
      "learning_rate": 1.0884480000000003e-05,
      "loss": 0.0003,
      "step": 42730
    },
    {
      "epoch": 1.36768,
      "grad_norm": 0.004383181221783161,
      "learning_rate": 1.0882346666666668e-05,
      "loss": 0.0002,
      "step": 42740
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.0031941223423928022,
      "learning_rate": 1.0880213333333333e-05,
      "loss": 0.0002,
      "step": 42750
    },
    {
      "epoch": 1.36832,
      "grad_norm": 0.004297635518014431,
      "learning_rate": 1.0878080000000002e-05,
      "loss": 0.0003,
      "step": 42760
    },
    {
      "epoch": 1.36864,
      "grad_norm": 0.38360100984573364,
      "learning_rate": 1.0875946666666667e-05,
      "loss": 0.0006,
      "step": 42770
    },
    {
      "epoch": 1.36896,
      "grad_norm": 0.018522996455430984,
      "learning_rate": 1.0873813333333335e-05,
      "loss": 0.0003,
      "step": 42780
    },
    {
      "epoch": 1.36928,
      "grad_norm": 0.0073701911605894566,
      "learning_rate": 1.087168e-05,
      "loss": 0.0002,
      "step": 42790
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.003121249610558152,
      "learning_rate": 1.0869546666666669e-05,
      "loss": 0.015,
      "step": 42800
    },
    {
      "epoch": 1.36992,
      "grad_norm": 0.0028374053072184324,
      "learning_rate": 1.0867413333333334e-05,
      "loss": 0.0002,
      "step": 42810
    },
    {
      "epoch": 1.37024,
      "grad_norm": 0.011008135043084621,
      "learning_rate": 1.0865280000000001e-05,
      "loss": 0.0003,
      "step": 42820
    },
    {
      "epoch": 1.37056,
      "grad_norm": 0.040220122784376144,
      "learning_rate": 1.0863146666666668e-05,
      "loss": 0.0311,
      "step": 42830
    },
    {
      "epoch": 1.37088,
      "grad_norm": 0.004746361169964075,
      "learning_rate": 1.0861013333333335e-05,
      "loss": 0.0002,
      "step": 42840
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.008588223718106747,
      "learning_rate": 1.085888e-05,
      "loss": 0.0007,
      "step": 42850
    },
    {
      "epoch": 1.37152,
      "grad_norm": 0.005256814416497946,
      "learning_rate": 1.0856746666666666e-05,
      "loss": 0.0002,
      "step": 42860
    },
    {
      "epoch": 1.37184,
      "grad_norm": 0.018792826682329178,
      "learning_rate": 1.0854613333333335e-05,
      "loss": 0.0003,
      "step": 42870
    },
    {
      "epoch": 1.37216,
      "grad_norm": 0.005368791054934263,
      "learning_rate": 1.085248e-05,
      "loss": 0.0008,
      "step": 42880
    },
    {
      "epoch": 1.37248,
      "grad_norm": 0.003522849641740322,
      "learning_rate": 1.0850346666666667e-05,
      "loss": 0.0027,
      "step": 42890
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.004767486359924078,
      "learning_rate": 1.0848213333333334e-05,
      "loss": 0.0002,
      "step": 42900
    },
    {
      "epoch": 1.3731200000000001,
      "grad_norm": 0.0017924250569194555,
      "learning_rate": 1.0846080000000001e-05,
      "loss": 0.0011,
      "step": 42910
    },
    {
      "epoch": 1.37344,
      "grad_norm": 0.004106931388378143,
      "learning_rate": 1.0843946666666667e-05,
      "loss": 0.0002,
      "step": 42920
    },
    {
      "epoch": 1.3737599999999999,
      "grad_norm": 0.0031769636552780867,
      "learning_rate": 1.0841813333333336e-05,
      "loss": 0.0002,
      "step": 42930
    },
    {
      "epoch": 1.37408,
      "grad_norm": 0.00830636452883482,
      "learning_rate": 1.0839680000000001e-05,
      "loss": 0.0002,
      "step": 42940
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.007553176023066044,
      "learning_rate": 1.0837546666666668e-05,
      "loss": 0.0006,
      "step": 42950
    },
    {
      "epoch": 1.37472,
      "grad_norm": 0.003350232494994998,
      "learning_rate": 1.0835413333333333e-05,
      "loss": 0.0003,
      "step": 42960
    },
    {
      "epoch": 1.37504,
      "grad_norm": 0.009934751316905022,
      "learning_rate": 1.0833280000000002e-05,
      "loss": 0.0002,
      "step": 42970
    },
    {
      "epoch": 1.3753600000000001,
      "grad_norm": 0.0026658321730792522,
      "learning_rate": 1.0831146666666667e-05,
      "loss": 0.0003,
      "step": 42980
    },
    {
      "epoch": 1.37568,
      "grad_norm": 0.0025970437563955784,
      "learning_rate": 1.0829013333333333e-05,
      "loss": 0.0409,
      "step": 42990
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.003012680681422353,
      "learning_rate": 1.0826880000000002e-05,
      "loss": 0.0002,
      "step": 43000
    },
    {
      "epoch": 1.37632,
      "grad_norm": 0.003045659279450774,
      "learning_rate": 1.0824746666666669e-05,
      "loss": 0.0002,
      "step": 43010
    },
    {
      "epoch": 1.37664,
      "grad_norm": 0.0041527943685650826,
      "learning_rate": 1.0822613333333334e-05,
      "loss": 0.0002,
      "step": 43020
    },
    {
      "epoch": 1.37696,
      "grad_norm": 0.0017689117230474949,
      "learning_rate": 1.082048e-05,
      "loss": 0.0104,
      "step": 43030
    },
    {
      "epoch": 1.37728,
      "grad_norm": 0.002790857804939151,
      "learning_rate": 1.0818346666666668e-05,
      "loss": 0.0002,
      "step": 43040
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.004558242857456207,
      "learning_rate": 1.0816213333333334e-05,
      "loss": 0.0002,
      "step": 43050
    },
    {
      "epoch": 1.37792,
      "grad_norm": 0.04603099077939987,
      "learning_rate": 1.081408e-05,
      "loss": 0.0004,
      "step": 43060
    },
    {
      "epoch": 1.37824,
      "grad_norm": 0.003715046914294362,
      "learning_rate": 1.0811946666666668e-05,
      "loss": 0.0002,
      "step": 43070
    },
    {
      "epoch": 1.37856,
      "grad_norm": 0.07068194448947906,
      "learning_rate": 1.0809813333333335e-05,
      "loss": 0.0003,
      "step": 43080
    },
    {
      "epoch": 1.37888,
      "grad_norm": 0.002328205853700638,
      "learning_rate": 1.080768e-05,
      "loss": 0.0007,
      "step": 43090
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.0031242077238857746,
      "learning_rate": 1.0805546666666669e-05,
      "loss": 0.0055,
      "step": 43100
    },
    {
      "epoch": 1.37952,
      "grad_norm": 0.0030396420042961836,
      "learning_rate": 1.0803413333333334e-05,
      "loss": 0.0002,
      "step": 43110
    },
    {
      "epoch": 1.37984,
      "grad_norm": 0.003497106721624732,
      "learning_rate": 1.0801280000000001e-05,
      "loss": 0.0002,
      "step": 43120
    },
    {
      "epoch": 1.38016,
      "grad_norm": 0.01436975970864296,
      "learning_rate": 1.0799146666666667e-05,
      "loss": 0.0002,
      "step": 43130
    },
    {
      "epoch": 1.38048,
      "grad_norm": 0.002603107364848256,
      "learning_rate": 1.0797013333333336e-05,
      "loss": 0.0032,
      "step": 43140
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.08763553202152252,
      "learning_rate": 1.0794880000000001e-05,
      "loss": 0.0027,
      "step": 43150
    },
    {
      "epoch": 1.3811200000000001,
      "grad_norm": 0.0031158896163105965,
      "learning_rate": 1.0792746666666666e-05,
      "loss": 0.0002,
      "step": 43160
    },
    {
      "epoch": 1.38144,
      "grad_norm": 0.003108074888586998,
      "learning_rate": 1.0790613333333335e-05,
      "loss": 0.0542,
      "step": 43170
    },
    {
      "epoch": 1.3817599999999999,
      "grad_norm": 0.002529011806473136,
      "learning_rate": 1.078848e-05,
      "loss": 0.0002,
      "step": 43180
    },
    {
      "epoch": 1.38208,
      "grad_norm": 0.004592574201524258,
      "learning_rate": 1.0786346666666667e-05,
      "loss": 0.0002,
      "step": 43190
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.007835178636014462,
      "learning_rate": 1.0784213333333333e-05,
      "loss": 0.0002,
      "step": 43200
    },
    {
      "epoch": 1.38272,
      "grad_norm": 0.0030175286810845137,
      "learning_rate": 1.0782080000000002e-05,
      "loss": 0.0112,
      "step": 43210
    },
    {
      "epoch": 1.38304,
      "grad_norm": 11.285359382629395,
      "learning_rate": 1.0779946666666667e-05,
      "loss": 0.0116,
      "step": 43220
    },
    {
      "epoch": 1.38336,
      "grad_norm": 0.0057191429659724236,
      "learning_rate": 1.0777813333333334e-05,
      "loss": 0.0194,
      "step": 43230
    },
    {
      "epoch": 1.38368,
      "grad_norm": 0.0060901097021996975,
      "learning_rate": 1.0775680000000001e-05,
      "loss": 0.0001,
      "step": 43240
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.005310087464749813,
      "learning_rate": 1.0773546666666668e-05,
      "loss": 0.0008,
      "step": 43250
    },
    {
      "epoch": 1.38432,
      "grad_norm": 0.8302909135818481,
      "learning_rate": 1.0771413333333334e-05,
      "loss": 0.0108,
      "step": 43260
    },
    {
      "epoch": 1.38464,
      "grad_norm": 0.0019013960845768452,
      "learning_rate": 1.0769280000000002e-05,
      "loss": 0.0002,
      "step": 43270
    },
    {
      "epoch": 1.38496,
      "grad_norm": 0.0031293691135942936,
      "learning_rate": 1.0767146666666668e-05,
      "loss": 0.0003,
      "step": 43280
    },
    {
      "epoch": 1.38528,
      "grad_norm": 0.0027294154278934,
      "learning_rate": 1.0765013333333335e-05,
      "loss": 0.0002,
      "step": 43290
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.003045585472136736,
      "learning_rate": 1.076288e-05,
      "loss": 0.0005,
      "step": 43300
    },
    {
      "epoch": 1.38592,
      "grad_norm": 0.0035289148800075054,
      "learning_rate": 1.0760746666666669e-05,
      "loss": 0.0001,
      "step": 43310
    },
    {
      "epoch": 1.38624,
      "grad_norm": 0.0033003357239067554,
      "learning_rate": 1.0758613333333334e-05,
      "loss": 0.0002,
      "step": 43320
    },
    {
      "epoch": 1.38656,
      "grad_norm": 0.011504975147545338,
      "learning_rate": 1.075648e-05,
      "loss": 0.0003,
      "step": 43330
    },
    {
      "epoch": 1.3868800000000001,
      "grad_norm": 0.0012692180462181568,
      "learning_rate": 1.0754346666666668e-05,
      "loss": 0.0019,
      "step": 43340
    },
    {
      "epoch": 1.3872,
      "grad_norm": 6.066393852233887,
      "learning_rate": 1.0752213333333334e-05,
      "loss": 0.0211,
      "step": 43350
    },
    {
      "epoch": 1.3875199999999999,
      "grad_norm": 0.004507812671363354,
      "learning_rate": 1.0750080000000001e-05,
      "loss": 0.0002,
      "step": 43360
    },
    {
      "epoch": 1.38784,
      "grad_norm": 0.003644806332886219,
      "learning_rate": 1.0747946666666666e-05,
      "loss": 0.0002,
      "step": 43370
    },
    {
      "epoch": 1.38816,
      "grad_norm": 0.0026274651754647493,
      "learning_rate": 1.0745813333333335e-05,
      "loss": 0.0126,
      "step": 43380
    },
    {
      "epoch": 1.38848,
      "grad_norm": 0.0015526103088632226,
      "learning_rate": 1.074368e-05,
      "loss": 0.0001,
      "step": 43390
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.004289519507437944,
      "learning_rate": 1.0741546666666668e-05,
      "loss": 0.017,
      "step": 43400
    },
    {
      "epoch": 1.3891200000000001,
      "grad_norm": 0.0026540160179138184,
      "learning_rate": 1.0739413333333335e-05,
      "loss": 0.0254,
      "step": 43410
    },
    {
      "epoch": 1.38944,
      "grad_norm": 0.004940106999129057,
      "learning_rate": 1.0737280000000002e-05,
      "loss": 0.0002,
      "step": 43420
    },
    {
      "epoch": 1.3897599999999999,
      "grad_norm": 0.006852473597973585,
      "learning_rate": 1.0735146666666667e-05,
      "loss": 0.0003,
      "step": 43430
    },
    {
      "epoch": 1.39008,
      "grad_norm": 0.0033195994328707457,
      "learning_rate": 1.0733013333333336e-05,
      "loss": 0.0001,
      "step": 43440
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.0025111513677984476,
      "learning_rate": 1.0730880000000001e-05,
      "loss": 0.0026,
      "step": 43450
    },
    {
      "epoch": 1.39072,
      "grad_norm": 0.015359616838395596,
      "learning_rate": 1.0728746666666667e-05,
      "loss": 0.0002,
      "step": 43460
    },
    {
      "epoch": 1.39104,
      "grad_norm": 0.004058142192661762,
      "learning_rate": 1.0726613333333334e-05,
      "loss": 0.0002,
      "step": 43470
    },
    {
      "epoch": 1.39136,
      "grad_norm": 0.0030426010489463806,
      "learning_rate": 1.0724480000000002e-05,
      "loss": 0.0001,
      "step": 43480
    },
    {
      "epoch": 1.39168,
      "grad_norm": 0.0037335779052227736,
      "learning_rate": 1.0722346666666668e-05,
      "loss": 0.0002,
      "step": 43490
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.001841468852944672,
      "learning_rate": 1.0720213333333333e-05,
      "loss": 0.0483,
      "step": 43500
    },
    {
      "epoch": 1.39232,
      "grad_norm": 0.0019287366885691881,
      "learning_rate": 1.0718080000000002e-05,
      "loss": 0.0002,
      "step": 43510
    },
    {
      "epoch": 1.39264,
      "grad_norm": 0.0022709618788212538,
      "learning_rate": 1.0715946666666667e-05,
      "loss": 0.0002,
      "step": 43520
    },
    {
      "epoch": 1.39296,
      "grad_norm": 0.005233997944742441,
      "learning_rate": 1.0713813333333334e-05,
      "loss": 0.0084,
      "step": 43530
    },
    {
      "epoch": 1.39328,
      "grad_norm": 0.0021978728473186493,
      "learning_rate": 1.071168e-05,
      "loss": 0.0002,
      "step": 43540
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.004361129831522703,
      "learning_rate": 1.0709546666666669e-05,
      "loss": 0.0002,
      "step": 43550
    },
    {
      "epoch": 1.39392,
      "grad_norm": 0.004463899880647659,
      "learning_rate": 1.0707413333333334e-05,
      "loss": 0.0001,
      "step": 43560
    },
    {
      "epoch": 1.39424,
      "grad_norm": 0.002424495294690132,
      "learning_rate": 1.0705280000000001e-05,
      "loss": 0.0001,
      "step": 43570
    },
    {
      "epoch": 1.39456,
      "grad_norm": 0.01586218923330307,
      "learning_rate": 1.0703146666666668e-05,
      "loss": 0.0002,
      "step": 43580
    },
    {
      "epoch": 1.3948800000000001,
      "grad_norm": 0.0038623639848083258,
      "learning_rate": 1.0701013333333335e-05,
      "loss": 0.0003,
      "step": 43590
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.001326478668488562,
      "learning_rate": 1.069888e-05,
      "loss": 0.0001,
      "step": 43600
    },
    {
      "epoch": 1.3955199999999999,
      "grad_norm": 0.013138381764292717,
      "learning_rate": 1.069674666666667e-05,
      "loss": 0.0002,
      "step": 43610
    },
    {
      "epoch": 1.39584,
      "grad_norm": 0.0029447514098137617,
      "learning_rate": 1.0694613333333335e-05,
      "loss": 0.0002,
      "step": 43620
    },
    {
      "epoch": 1.39616,
      "grad_norm": 5.687399864196777,
      "learning_rate": 1.069248e-05,
      "loss": 0.0742,
      "step": 43630
    },
    {
      "epoch": 1.39648,
      "grad_norm": 0.002773216227069497,
      "learning_rate": 1.0690346666666667e-05,
      "loss": 0.0001,
      "step": 43640
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.003897071583196521,
      "learning_rate": 1.0688213333333334e-05,
      "loss": 0.0004,
      "step": 43650
    },
    {
      "epoch": 1.39712,
      "grad_norm": 0.0021446000318974257,
      "learning_rate": 1.0686080000000001e-05,
      "loss": 0.0084,
      "step": 43660
    },
    {
      "epoch": 1.39744,
      "grad_norm": 0.0028650194872170687,
      "learning_rate": 1.0683946666666667e-05,
      "loss": 0.0005,
      "step": 43670
    },
    {
      "epoch": 1.39776,
      "grad_norm": 5.270150184631348,
      "learning_rate": 1.0681813333333335e-05,
      "loss": 0.0053,
      "step": 43680
    },
    {
      "epoch": 1.39808,
      "grad_norm": 0.0031217760406434536,
      "learning_rate": 1.067968e-05,
      "loss": 0.0546,
      "step": 43690
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.007685137912631035,
      "learning_rate": 1.0677546666666668e-05,
      "loss": 0.033,
      "step": 43700
    },
    {
      "epoch": 1.39872,
      "grad_norm": 0.004743811674416065,
      "learning_rate": 1.0675413333333333e-05,
      "loss": 0.0001,
      "step": 43710
    },
    {
      "epoch": 1.39904,
      "grad_norm": 0.007996800355613232,
      "learning_rate": 1.0673280000000002e-05,
      "loss": 0.0118,
      "step": 43720
    },
    {
      "epoch": 1.39936,
      "grad_norm": 0.004518421366810799,
      "learning_rate": 1.0671146666666667e-05,
      "loss": 0.0121,
      "step": 43730
    },
    {
      "epoch": 1.39968,
      "grad_norm": 1.8619534969329834,
      "learning_rate": 1.0669013333333333e-05,
      "loss": 0.0824,
      "step": 43740
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.009643984958529472,
      "learning_rate": 1.0666880000000001e-05,
      "loss": 0.0002,
      "step": 43750
    },
    {
      "epoch": 1.40032,
      "grad_norm": 0.006032724864780903,
      "learning_rate": 1.0664746666666669e-05,
      "loss": 0.0011,
      "step": 43760
    },
    {
      "epoch": 1.40064,
      "grad_norm": 0.004255744628608227,
      "learning_rate": 1.0662613333333334e-05,
      "loss": 0.0002,
      "step": 43770
    },
    {
      "epoch": 1.40096,
      "grad_norm": 0.008272090926766396,
      "learning_rate": 1.0660480000000003e-05,
      "loss": 0.0002,
      "step": 43780
    },
    {
      "epoch": 1.40128,
      "grad_norm": 0.003782355459406972,
      "learning_rate": 1.0658346666666668e-05,
      "loss": 0.0002,
      "step": 43790
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.002582504414021969,
      "learning_rate": 1.0656213333333333e-05,
      "loss": 0.0222,
      "step": 43800
    },
    {
      "epoch": 1.40192,
      "grad_norm": 0.0029228990897536278,
      "learning_rate": 1.065408e-05,
      "loss": 0.0003,
      "step": 43810
    },
    {
      "epoch": 1.40224,
      "grad_norm": 0.013962160795927048,
      "learning_rate": 1.0651946666666668e-05,
      "loss": 0.0003,
      "step": 43820
    },
    {
      "epoch": 1.40256,
      "grad_norm": 0.20287328958511353,
      "learning_rate": 1.0649813333333335e-05,
      "loss": 0.0005,
      "step": 43830
    },
    {
      "epoch": 1.4028800000000001,
      "grad_norm": 0.004829453304409981,
      "learning_rate": 1.064768e-05,
      "loss": 0.0003,
      "step": 43840
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.005593043752014637,
      "learning_rate": 1.0645546666666669e-05,
      "loss": 0.0299,
      "step": 43850
    },
    {
      "epoch": 1.4035199999999999,
      "grad_norm": 0.007460969965904951,
      "learning_rate": 1.0643413333333334e-05,
      "loss": 0.0002,
      "step": 43860
    },
    {
      "epoch": 1.40384,
      "grad_norm": 0.005917922127991915,
      "learning_rate": 1.0641280000000001e-05,
      "loss": 0.0004,
      "step": 43870
    },
    {
      "epoch": 1.40416,
      "grad_norm": 0.006046659778803587,
      "learning_rate": 1.0639146666666667e-05,
      "loss": 0.045,
      "step": 43880
    },
    {
      "epoch": 1.40448,
      "grad_norm": 0.005843811202794313,
      "learning_rate": 1.0637013333333335e-05,
      "loss": 0.0003,
      "step": 43890
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.007189152296632528,
      "learning_rate": 1.063488e-05,
      "loss": 0.0007,
      "step": 43900
    },
    {
      "epoch": 1.40512,
      "grad_norm": 0.0029782613273710012,
      "learning_rate": 1.0632746666666666e-05,
      "loss": 0.0002,
      "step": 43910
    },
    {
      "epoch": 1.40544,
      "grad_norm": 0.003641276154667139,
      "learning_rate": 1.0630613333333335e-05,
      "loss": 0.0001,
      "step": 43920
    },
    {
      "epoch": 1.40576,
      "grad_norm": 0.022027865052223206,
      "learning_rate": 1.062848e-05,
      "loss": 0.0003,
      "step": 43930
    },
    {
      "epoch": 1.40608,
      "grad_norm": 0.0018718382343649864,
      "learning_rate": 1.0626346666666667e-05,
      "loss": 0.0001,
      "step": 43940
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.004887779708951712,
      "learning_rate": 1.0624213333333336e-05,
      "loss": 0.0017,
      "step": 43950
    },
    {
      "epoch": 1.40672,
      "grad_norm": 0.003063056617975235,
      "learning_rate": 1.0622080000000001e-05,
      "loss": 0.0056,
      "step": 43960
    },
    {
      "epoch": 1.40704,
      "grad_norm": 0.02041112259030342,
      "learning_rate": 1.0619946666666667e-05,
      "loss": 0.0006,
      "step": 43970
    },
    {
      "epoch": 1.40736,
      "grad_norm": 0.002733709989115596,
      "learning_rate": 1.0617813333333334e-05,
      "loss": 0.0349,
      "step": 43980
    },
    {
      "epoch": 1.40768,
      "grad_norm": 0.003584193531423807,
      "learning_rate": 1.0615680000000001e-05,
      "loss": 0.0002,
      "step": 43990
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.005311174318194389,
      "learning_rate": 1.0613546666666668e-05,
      "loss": 0.0151,
      "step": 44000
    },
    {
      "epoch": 1.40832,
      "grad_norm": 0.010699898935854435,
      "learning_rate": 1.0611413333333333e-05,
      "loss": 0.0003,
      "step": 44010
    },
    {
      "epoch": 1.4086400000000001,
      "grad_norm": 0.006179310847073793,
      "learning_rate": 1.0609280000000002e-05,
      "loss": 0.0003,
      "step": 44020
    },
    {
      "epoch": 1.40896,
      "grad_norm": 0.006288048345595598,
      "learning_rate": 1.0607146666666668e-05,
      "loss": 0.047,
      "step": 44030
    },
    {
      "epoch": 1.4092799999999999,
      "grad_norm": 0.0020950648467987776,
      "learning_rate": 1.0605013333333335e-05,
      "loss": 0.0017,
      "step": 44040
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.9100403785705566,
      "learning_rate": 1.060288e-05,
      "loss": 0.0127,
      "step": 44050
    },
    {
      "epoch": 1.40992,
      "grad_norm": 0.003933591302484274,
      "learning_rate": 1.0600746666666669e-05,
      "loss": 0.0023,
      "step": 44060
    },
    {
      "epoch": 1.41024,
      "grad_norm": 0.007392860483378172,
      "learning_rate": 1.0598613333333334e-05,
      "loss": 0.0002,
      "step": 44070
    },
    {
      "epoch": 1.41056,
      "grad_norm": 0.002630108967423439,
      "learning_rate": 1.059648e-05,
      "loss": 0.0002,
      "step": 44080
    },
    {
      "epoch": 1.4108800000000001,
      "grad_norm": 0.10283514857292175,
      "learning_rate": 1.0594346666666668e-05,
      "loss": 0.0004,
      "step": 44090
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.00801682099699974,
      "learning_rate": 1.0592213333333334e-05,
      "loss": 0.0002,
      "step": 44100
    },
    {
      "epoch": 1.4115199999999999,
      "grad_norm": 0.0065767536871135235,
      "learning_rate": 1.059008e-05,
      "loss": 0.0002,
      "step": 44110
    },
    {
      "epoch": 1.41184,
      "grad_norm": 0.0049158683978021145,
      "learning_rate": 1.0587946666666668e-05,
      "loss": 0.0003,
      "step": 44120
    },
    {
      "epoch": 1.41216,
      "grad_norm": 0.0035485487896949053,
      "learning_rate": 1.0585813333333335e-05,
      "loss": 0.0002,
      "step": 44130
    },
    {
      "epoch": 1.41248,
      "grad_norm": 0.006663860287517309,
      "learning_rate": 1.058368e-05,
      "loss": 0.0002,
      "step": 44140
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.0029793332796543837,
      "learning_rate": 1.0581546666666667e-05,
      "loss": 0.0005,
      "step": 44150
    },
    {
      "epoch": 1.41312,
      "grad_norm": 0.0038970932364463806,
      "learning_rate": 1.0579413333333334e-05,
      "loss": 0.0002,
      "step": 44160
    },
    {
      "epoch": 1.41344,
      "grad_norm": 0.01260888297110796,
      "learning_rate": 1.0577280000000001e-05,
      "loss": 0.0003,
      "step": 44170
    },
    {
      "epoch": 1.41376,
      "grad_norm": 0.002092974493280053,
      "learning_rate": 1.0575146666666667e-05,
      "loss": 0.0001,
      "step": 44180
    },
    {
      "epoch": 1.41408,
      "grad_norm": 0.0029750606045126915,
      "learning_rate": 1.0573013333333336e-05,
      "loss": 0.0003,
      "step": 44190
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.003970346413552761,
      "learning_rate": 1.0570880000000001e-05,
      "loss": 0.0002,
      "step": 44200
    },
    {
      "epoch": 1.41472,
      "grad_norm": 0.002195638371631503,
      "learning_rate": 1.0568746666666666e-05,
      "loss": 0.0009,
      "step": 44210
    },
    {
      "epoch": 1.41504,
      "grad_norm": 0.0031260543037205935,
      "learning_rate": 1.0566613333333333e-05,
      "loss": 0.0002,
      "step": 44220
    },
    {
      "epoch": 1.41536,
      "grad_norm": 0.0016378661384806037,
      "learning_rate": 1.0564480000000002e-05,
      "loss": 0.0553,
      "step": 44230
    },
    {
      "epoch": 1.41568,
      "grad_norm": 0.005993833299726248,
      "learning_rate": 1.0562346666666668e-05,
      "loss": 0.0009,
      "step": 44240
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.003771259682253003,
      "learning_rate": 1.0560213333333333e-05,
      "loss": 0.0005,
      "step": 44250
    },
    {
      "epoch": 1.41632,
      "grad_norm": 0.0023386278189718723,
      "learning_rate": 1.0558080000000002e-05,
      "loss": 0.0002,
      "step": 44260
    },
    {
      "epoch": 1.4166400000000001,
      "grad_norm": 0.002783397678285837,
      "learning_rate": 1.0555946666666667e-05,
      "loss": 0.0001,
      "step": 44270
    },
    {
      "epoch": 1.41696,
      "grad_norm": 0.0043498799204826355,
      "learning_rate": 1.0553813333333334e-05,
      "loss": 0.0007,
      "step": 44280
    },
    {
      "epoch": 1.4172799999999999,
      "grad_norm": 0.0033760874066501856,
      "learning_rate": 1.0551680000000001e-05,
      "loss": 0.0666,
      "step": 44290
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.0023262077011168003,
      "learning_rate": 1.0549546666666668e-05,
      "loss": 0.0003,
      "step": 44300
    },
    {
      "epoch": 1.41792,
      "grad_norm": 0.0028832866810262203,
      "learning_rate": 1.0547413333333334e-05,
      "loss": 0.0002,
      "step": 44310
    },
    {
      "epoch": 1.41824,
      "grad_norm": 0.007648375350981951,
      "learning_rate": 1.054528e-05,
      "loss": 0.0002,
      "step": 44320
    },
    {
      "epoch": 1.41856,
      "grad_norm": 0.0048768711276352406,
      "learning_rate": 1.0543146666666668e-05,
      "loss": 0.0346,
      "step": 44330
    },
    {
      "epoch": 1.41888,
      "grad_norm": 0.004269376862794161,
      "learning_rate": 1.0541013333333335e-05,
      "loss": 0.0003,
      "step": 44340
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.00398286571726203,
      "learning_rate": 1.053888e-05,
      "loss": 0.0006,
      "step": 44350
    },
    {
      "epoch": 1.41952,
      "grad_norm": 0.0013348580105230212,
      "learning_rate": 1.0536746666666669e-05,
      "loss": 0.0003,
      "step": 44360
    },
    {
      "epoch": 1.41984,
      "grad_norm": 0.0021601475309580564,
      "learning_rate": 1.0534613333333334e-05,
      "loss": 0.0619,
      "step": 44370
    },
    {
      "epoch": 1.42016,
      "grad_norm": 0.002618372207507491,
      "learning_rate": 1.053248e-05,
      "loss": 0.0002,
      "step": 44380
    },
    {
      "epoch": 1.42048,
      "grad_norm": 0.001899334485642612,
      "learning_rate": 1.0530346666666667e-05,
      "loss": 0.0003,
      "step": 44390
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.004853267688304186,
      "learning_rate": 1.0528213333333334e-05,
      "loss": 0.0002,
      "step": 44400
    },
    {
      "epoch": 1.42112,
      "grad_norm": 0.003882385790348053,
      "learning_rate": 1.0526080000000001e-05,
      "loss": 0.0004,
      "step": 44410
    },
    {
      "epoch": 1.42144,
      "grad_norm": 0.0029585023876279593,
      "learning_rate": 1.0523946666666666e-05,
      "loss": 0.0003,
      "step": 44420
    },
    {
      "epoch": 1.42176,
      "grad_norm": 0.0035615970846265554,
      "learning_rate": 1.0521813333333335e-05,
      "loss": 0.0001,
      "step": 44430
    },
    {
      "epoch": 1.42208,
      "grad_norm": 0.006070219445973635,
      "learning_rate": 1.051968e-05,
      "loss": 0.0005,
      "step": 44440
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.010120810009539127,
      "learning_rate": 1.0517546666666668e-05,
      "loss": 0.0003,
      "step": 44450
    },
    {
      "epoch": 1.42272,
      "grad_norm": 0.21290460228919983,
      "learning_rate": 1.0515413333333335e-05,
      "loss": 0.0118,
      "step": 44460
    },
    {
      "epoch": 1.42304,
      "grad_norm": 0.0029813493601977825,
      "learning_rate": 1.0513280000000002e-05,
      "loss": 0.0002,
      "step": 44470
    },
    {
      "epoch": 1.42336,
      "grad_norm": 0.0031038112938404083,
      "learning_rate": 1.0511146666666667e-05,
      "loss": 0.0002,
      "step": 44480
    },
    {
      "epoch": 1.42368,
      "grad_norm": 0.005226267967373133,
      "learning_rate": 1.0509013333333333e-05,
      "loss": 0.0002,
      "step": 44490
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.003212416311725974,
      "learning_rate": 1.0506880000000001e-05,
      "loss": 0.0002,
      "step": 44500
    },
    {
      "epoch": 1.42432,
      "grad_norm": 0.0019510933198034763,
      "learning_rate": 1.0504746666666668e-05,
      "loss": 0.0002,
      "step": 44510
    },
    {
      "epoch": 1.4246400000000001,
      "grad_norm": 0.00460390280932188,
      "learning_rate": 1.0502613333333334e-05,
      "loss": 0.0002,
      "step": 44520
    },
    {
      "epoch": 1.42496,
      "grad_norm": 0.0031763615552335978,
      "learning_rate": 1.0500480000000003e-05,
      "loss": 0.0001,
      "step": 44530
    },
    {
      "epoch": 1.4252799999999999,
      "grad_norm": 0.0018902351148426533,
      "learning_rate": 1.0498346666666668e-05,
      "loss": 0.0002,
      "step": 44540
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.007446678355336189,
      "learning_rate": 1.0496213333333333e-05,
      "loss": 0.0016,
      "step": 44550
    },
    {
      "epoch": 1.42592,
      "grad_norm": 0.029271597042679787,
      "learning_rate": 1.049408e-05,
      "loss": 0.0002,
      "step": 44560
    },
    {
      "epoch": 1.42624,
      "grad_norm": 0.0037432152312248945,
      "learning_rate": 1.0491946666666667e-05,
      "loss": 0.0126,
      "step": 44570
    },
    {
      "epoch": 1.42656,
      "grad_norm": 0.002600276842713356,
      "learning_rate": 1.0489813333333334e-05,
      "loss": 0.0002,
      "step": 44580
    },
    {
      "epoch": 1.42688,
      "grad_norm": 0.019939685240387917,
      "learning_rate": 1.048768e-05,
      "loss": 0.0008,
      "step": 44590
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.001909066690132022,
      "learning_rate": 1.0485546666666669e-05,
      "loss": 0.0002,
      "step": 44600
    },
    {
      "epoch": 1.42752,
      "grad_norm": 0.010045369155704975,
      "learning_rate": 1.0483413333333334e-05,
      "loss": 0.0008,
      "step": 44610
    },
    {
      "epoch": 1.42784,
      "grad_norm": 0.003026328282430768,
      "learning_rate": 1.0481280000000001e-05,
      "loss": 0.0002,
      "step": 44620
    },
    {
      "epoch": 1.42816,
      "grad_norm": 0.0029785672668367624,
      "learning_rate": 1.0479146666666668e-05,
      "loss": 0.0001,
      "step": 44630
    },
    {
      "epoch": 1.42848,
      "grad_norm": 0.002960718935355544,
      "learning_rate": 1.0477013333333335e-05,
      "loss": 0.0002,
      "step": 44640
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.0021073182579129934,
      "learning_rate": 1.047488e-05,
      "loss": 0.0002,
      "step": 44650
    },
    {
      "epoch": 1.42912,
      "grad_norm": 0.003111194586381316,
      "learning_rate": 1.0472746666666666e-05,
      "loss": 0.0003,
      "step": 44660
    },
    {
      "epoch": 1.42944,
      "grad_norm": 0.003403027541935444,
      "learning_rate": 1.0470613333333335e-05,
      "loss": 0.0001,
      "step": 44670
    },
    {
      "epoch": 1.42976,
      "grad_norm": 0.003602698678150773,
      "learning_rate": 1.046848e-05,
      "loss": 0.0002,
      "step": 44680
    },
    {
      "epoch": 1.43008,
      "grad_norm": 0.007089623250067234,
      "learning_rate": 1.0466346666666667e-05,
      "loss": 0.0001,
      "step": 44690
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.002348331967368722,
      "learning_rate": 1.0464213333333336e-05,
      "loss": 0.0002,
      "step": 44700
    },
    {
      "epoch": 1.43072,
      "grad_norm": 0.03519614040851593,
      "learning_rate": 1.0462080000000001e-05,
      "loss": 0.0002,
      "step": 44710
    },
    {
      "epoch": 1.4310399999999999,
      "grad_norm": 0.0091025959700346,
      "learning_rate": 1.0459946666666667e-05,
      "loss": 0.0372,
      "step": 44720
    },
    {
      "epoch": 1.43136,
      "grad_norm": 0.0021236783359199762,
      "learning_rate": 1.0457813333333334e-05,
      "loss": 0.0385,
      "step": 44730
    },
    {
      "epoch": 1.43168,
      "grad_norm": 0.0028548238333314657,
      "learning_rate": 1.045568e-05,
      "loss": 0.0026,
      "step": 44740
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.001889512874186039,
      "learning_rate": 1.0453546666666668e-05,
      "loss": 0.0187,
      "step": 44750
    },
    {
      "epoch": 1.43232,
      "grad_norm": 0.0019835184793919325,
      "learning_rate": 1.0451413333333333e-05,
      "loss": 0.0002,
      "step": 44760
    },
    {
      "epoch": 1.4326400000000001,
      "grad_norm": 0.004669030196964741,
      "learning_rate": 1.0449280000000002e-05,
      "loss": 0.0171,
      "step": 44770
    },
    {
      "epoch": 1.43296,
      "grad_norm": 0.003003718564286828,
      "learning_rate": 1.0447146666666667e-05,
      "loss": 0.0001,
      "step": 44780
    },
    {
      "epoch": 1.4332799999999999,
      "grad_norm": 0.003340747207403183,
      "learning_rate": 1.0445013333333334e-05,
      "loss": 0.0435,
      "step": 44790
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.012319064699113369,
      "learning_rate": 1.0442880000000002e-05,
      "loss": 0.0002,
      "step": 44800
    },
    {
      "epoch": 1.43392,
      "grad_norm": 0.0022501491475850344,
      "learning_rate": 1.0440746666666669e-05,
      "loss": 0.0058,
      "step": 44810
    },
    {
      "epoch": 1.43424,
      "grad_norm": 0.002293637255206704,
      "learning_rate": 1.0438613333333334e-05,
      "loss": 0.0471,
      "step": 44820
    },
    {
      "epoch": 1.43456,
      "grad_norm": 0.0023759305477142334,
      "learning_rate": 1.043648e-05,
      "loss": 0.0004,
      "step": 44830
    },
    {
      "epoch": 1.43488,
      "grad_norm": 0.0024733569007366896,
      "learning_rate": 1.0434346666666668e-05,
      "loss": 0.0002,
      "step": 44840
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.0037118832115083933,
      "learning_rate": 1.0432213333333334e-05,
      "loss": 0.0005,
      "step": 44850
    },
    {
      "epoch": 1.43552,
      "grad_norm": 0.010717354714870453,
      "learning_rate": 1.043008e-05,
      "loss": 0.0002,
      "step": 44860
    },
    {
      "epoch": 1.43584,
      "grad_norm": 0.0035207003820687532,
      "learning_rate": 1.0427946666666668e-05,
      "loss": 0.0002,
      "step": 44870
    },
    {
      "epoch": 1.43616,
      "grad_norm": 0.0032963864505290985,
      "learning_rate": 1.0425813333333335e-05,
      "loss": 0.0002,
      "step": 44880
    },
    {
      "epoch": 1.43648,
      "grad_norm": 0.0033170455135405064,
      "learning_rate": 1.042368e-05,
      "loss": 0.0002,
      "step": 44890
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.005898208357393742,
      "learning_rate": 1.0421546666666667e-05,
      "loss": 0.0116,
      "step": 44900
    },
    {
      "epoch": 1.43712,
      "grad_norm": 0.001797627191990614,
      "learning_rate": 1.0419413333333334e-05,
      "loss": 0.0003,
      "step": 44910
    },
    {
      "epoch": 1.43744,
      "grad_norm": 0.0047717927955091,
      "learning_rate": 1.0417280000000001e-05,
      "loss": 0.0002,
      "step": 44920
    },
    {
      "epoch": 1.43776,
      "grad_norm": 0.004391817841678858,
      "learning_rate": 1.0415146666666667e-05,
      "loss": 0.0028,
      "step": 44930
    },
    {
      "epoch": 1.43808,
      "grad_norm": 0.029466548934578896,
      "learning_rate": 1.0413013333333335e-05,
      "loss": 0.0141,
      "step": 44940
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.0053757112473249435,
      "learning_rate": 1.0410880000000001e-05,
      "loss": 0.0001,
      "step": 44950
    },
    {
      "epoch": 1.43872,
      "grad_norm": 0.036546722054481506,
      "learning_rate": 1.0408746666666666e-05,
      "loss": 0.0076,
      "step": 44960
    },
    {
      "epoch": 1.4390399999999999,
      "grad_norm": 0.003439471824094653,
      "learning_rate": 1.0406613333333335e-05,
      "loss": 0.0003,
      "step": 44970
    },
    {
      "epoch": 1.43936,
      "grad_norm": 0.0048415884375572205,
      "learning_rate": 1.0404480000000002e-05,
      "loss": 0.0002,
      "step": 44980
    },
    {
      "epoch": 1.43968,
      "grad_norm": 0.055741146206855774,
      "learning_rate": 1.0402346666666667e-05,
      "loss": 0.0003,
      "step": 44990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.005461760796606541,
      "learning_rate": 1.0400213333333333e-05,
      "loss": 0.0003,
      "step": 45000
    },
    {
      "epoch": 1.44032,
      "grad_norm": 0.0025990663561969995,
      "learning_rate": 1.0398080000000002e-05,
      "loss": 0.0514,
      "step": 45010
    },
    {
      "epoch": 1.44064,
      "grad_norm": 0.002620707731693983,
      "learning_rate": 1.0395946666666667e-05,
      "loss": 0.0002,
      "step": 45020
    },
    {
      "epoch": 1.44096,
      "grad_norm": 0.004911447875201702,
      "learning_rate": 1.0393813333333334e-05,
      "loss": 0.0003,
      "step": 45030
    },
    {
      "epoch": 1.44128,
      "grad_norm": 0.0031929777469486,
      "learning_rate": 1.0391680000000001e-05,
      "loss": 0.0002,
      "step": 45040
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.0012880585854873061,
      "learning_rate": 1.0389546666666668e-05,
      "loss": 0.0003,
      "step": 45050
    },
    {
      "epoch": 1.44192,
      "grad_norm": 0.004655949771404266,
      "learning_rate": 1.0387413333333334e-05,
      "loss": 0.0236,
      "step": 45060
    },
    {
      "epoch": 1.44224,
      "grad_norm": 0.008219245821237564,
      "learning_rate": 1.038528e-05,
      "loss": 0.0002,
      "step": 45070
    },
    {
      "epoch": 1.44256,
      "grad_norm": 0.005864477716386318,
      "learning_rate": 1.0383146666666668e-05,
      "loss": 0.0002,
      "step": 45080
    },
    {
      "epoch": 1.44288,
      "grad_norm": 0.007150499150156975,
      "learning_rate": 1.0381013333333335e-05,
      "loss": 0.0003,
      "step": 45090
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.026892950758337975,
      "learning_rate": 1.037888e-05,
      "loss": 0.0007,
      "step": 45100
    },
    {
      "epoch": 1.44352,
      "grad_norm": 0.0032007466070353985,
      "learning_rate": 1.0376746666666669e-05,
      "loss": 0.0002,
      "step": 45110
    },
    {
      "epoch": 1.44384,
      "grad_norm": 0.0034189349971711636,
      "learning_rate": 1.0374613333333334e-05,
      "loss": 0.0002,
      "step": 45120
    },
    {
      "epoch": 1.44416,
      "grad_norm": 0.004608917515724897,
      "learning_rate": 1.037248e-05,
      "loss": 0.0785,
      "step": 45130
    },
    {
      "epoch": 1.44448,
      "grad_norm": 0.005600926000624895,
      "learning_rate": 1.0370346666666668e-05,
      "loss": 0.0002,
      "step": 45140
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.016277046874165535,
      "learning_rate": 1.0368213333333334e-05,
      "loss": 0.0032,
      "step": 45150
    },
    {
      "epoch": 1.44512,
      "grad_norm": 0.01782427728176117,
      "learning_rate": 1.0366080000000001e-05,
      "loss": 0.0009,
      "step": 45160
    },
    {
      "epoch": 1.44544,
      "grad_norm": 0.044052597135305405,
      "learning_rate": 1.0363946666666666e-05,
      "loss": 0.001,
      "step": 45170
    },
    {
      "epoch": 1.44576,
      "grad_norm": 0.007901687175035477,
      "learning_rate": 1.0361813333333335e-05,
      "loss": 0.0003,
      "step": 45180
    },
    {
      "epoch": 1.44608,
      "grad_norm": 0.004965567030012608,
      "learning_rate": 1.035968e-05,
      "loss": 0.0025,
      "step": 45190
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.003645994234830141,
      "learning_rate": 1.0357546666666667e-05,
      "loss": 0.0003,
      "step": 45200
    },
    {
      "epoch": 1.44672,
      "grad_norm": 6.060671329498291,
      "learning_rate": 1.0355413333333335e-05,
      "loss": 0.0684,
      "step": 45210
    },
    {
      "epoch": 1.4470399999999999,
      "grad_norm": 0.10924461483955383,
      "learning_rate": 1.0353280000000002e-05,
      "loss": 0.0003,
      "step": 45220
    },
    {
      "epoch": 1.44736,
      "grad_norm": 0.02854960225522518,
      "learning_rate": 1.0351146666666667e-05,
      "loss": 0.0003,
      "step": 45230
    },
    {
      "epoch": 1.44768,
      "grad_norm": 0.005347898695617914,
      "learning_rate": 1.0349013333333332e-05,
      "loss": 0.0003,
      "step": 45240
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.007592758163809776,
      "learning_rate": 1.0346880000000001e-05,
      "loss": 0.001,
      "step": 45250
    },
    {
      "epoch": 1.44832,
      "grad_norm": 0.006807785946875811,
      "learning_rate": 1.0344746666666668e-05,
      "loss": 0.0002,
      "step": 45260
    },
    {
      "epoch": 1.44864,
      "grad_norm": 0.0037849268410354853,
      "learning_rate": 1.0342613333333334e-05,
      "loss": 0.0015,
      "step": 45270
    },
    {
      "epoch": 1.44896,
      "grad_norm": 0.027389174327254295,
      "learning_rate": 1.0340480000000002e-05,
      "loss": 0.0003,
      "step": 45280
    },
    {
      "epoch": 1.44928,
      "grad_norm": 0.0030695816967636347,
      "learning_rate": 1.0338346666666668e-05,
      "loss": 0.0112,
      "step": 45290
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.0028549458365887403,
      "learning_rate": 1.0336213333333333e-05,
      "loss": 0.0002,
      "step": 45300
    },
    {
      "epoch": 1.44992,
      "grad_norm": 0.005945465061813593,
      "learning_rate": 1.0334080000000002e-05,
      "loss": 0.0002,
      "step": 45310
    },
    {
      "epoch": 1.45024,
      "grad_norm": 0.003912992309778929,
      "learning_rate": 1.0331946666666667e-05,
      "loss": 0.055,
      "step": 45320
    },
    {
      "epoch": 1.45056,
      "grad_norm": 0.0036603922490030527,
      "learning_rate": 1.0329813333333334e-05,
      "loss": 0.0002,
      "step": 45330
    },
    {
      "epoch": 1.45088,
      "grad_norm": 2.0524067878723145,
      "learning_rate": 1.032768e-05,
      "loss": 0.0405,
      "step": 45340
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.004803777206689119,
      "learning_rate": 1.0325546666666668e-05,
      "loss": 0.0003,
      "step": 45350
    },
    {
      "epoch": 1.45152,
      "grad_norm": 0.005732797086238861,
      "learning_rate": 1.0323413333333334e-05,
      "loss": 0.0009,
      "step": 45360
    },
    {
      "epoch": 1.45184,
      "grad_norm": 0.010150646790862083,
      "learning_rate": 1.0321280000000001e-05,
      "loss": 0.0004,
      "step": 45370
    },
    {
      "epoch": 1.4521600000000001,
      "grad_norm": 0.003352012485265732,
      "learning_rate": 1.0319146666666668e-05,
      "loss": 0.0002,
      "step": 45380
    },
    {
      "epoch": 1.45248,
      "grad_norm": 0.016981082037091255,
      "learning_rate": 1.0317013333333335e-05,
      "loss": 0.0055,
      "step": 45390
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.006548394449055195,
      "learning_rate": 1.031488e-05,
      "loss": 0.0003,
      "step": 45400
    },
    {
      "epoch": 1.45312,
      "grad_norm": 0.007864018902182579,
      "learning_rate": 1.0312746666666666e-05,
      "loss": 0.0003,
      "step": 45410
    },
    {
      "epoch": 1.45344,
      "grad_norm": 0.003755229990929365,
      "learning_rate": 1.0310613333333335e-05,
      "loss": 0.0002,
      "step": 45420
    },
    {
      "epoch": 1.45376,
      "grad_norm": 1.1535122394561768,
      "learning_rate": 1.030848e-05,
      "loss": 0.0012,
      "step": 45430
    },
    {
      "epoch": 1.45408,
      "grad_norm": 0.005791341420263052,
      "learning_rate": 1.0306346666666667e-05,
      "loss": 0.0002,
      "step": 45440
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.01029537059366703,
      "learning_rate": 1.0304213333333336e-05,
      "loss": 0.0349,
      "step": 45450
    },
    {
      "epoch": 1.45472,
      "grad_norm": 0.0028831937815994024,
      "learning_rate": 1.0302080000000001e-05,
      "loss": 0.0008,
      "step": 45460
    },
    {
      "epoch": 1.45504,
      "grad_norm": 0.004181111231446266,
      "learning_rate": 1.0299946666666667e-05,
      "loss": 0.0149,
      "step": 45470
    },
    {
      "epoch": 1.45536,
      "grad_norm": 0.010794545523822308,
      "learning_rate": 1.0297813333333335e-05,
      "loss": 0.0004,
      "step": 45480
    },
    {
      "epoch": 1.45568,
      "grad_norm": 0.00468225684016943,
      "learning_rate": 1.029568e-05,
      "loss": 0.0484,
      "step": 45490
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.00786096416413784,
      "learning_rate": 1.0293546666666668e-05,
      "loss": 0.0003,
      "step": 45500
    },
    {
      "epoch": 1.45632,
      "grad_norm": 0.012395196594297886,
      "learning_rate": 1.0291413333333333e-05,
      "loss": 0.0462,
      "step": 45510
    },
    {
      "epoch": 1.45664,
      "grad_norm": 0.004978597164154053,
      "learning_rate": 1.0289280000000002e-05,
      "loss": 0.0006,
      "step": 45520
    },
    {
      "epoch": 1.45696,
      "grad_norm": 0.007602327037602663,
      "learning_rate": 1.0287146666666667e-05,
      "loss": 0.0262,
      "step": 45530
    },
    {
      "epoch": 1.45728,
      "grad_norm": 0.02391963079571724,
      "learning_rate": 1.0285013333333334e-05,
      "loss": 0.0522,
      "step": 45540
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.011867922730743885,
      "learning_rate": 1.0282880000000001e-05,
      "loss": 0.0004,
      "step": 45550
    },
    {
      "epoch": 1.45792,
      "grad_norm": 0.0044893259182572365,
      "learning_rate": 1.0280746666666668e-05,
      "loss": 0.0383,
      "step": 45560
    },
    {
      "epoch": 1.45824,
      "grad_norm": 0.001984203467145562,
      "learning_rate": 1.0278613333333334e-05,
      "loss": 0.0005,
      "step": 45570
    },
    {
      "epoch": 1.45856,
      "grad_norm": 0.005869022570550442,
      "learning_rate": 1.027648e-05,
      "loss": 0.0003,
      "step": 45580
    },
    {
      "epoch": 1.45888,
      "grad_norm": 0.005427186843007803,
      "learning_rate": 1.0274346666666668e-05,
      "loss": 0.0003,
      "step": 45590
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.04408030956983566,
      "learning_rate": 1.0272213333333333e-05,
      "loss": 0.0016,
      "step": 45600
    },
    {
      "epoch": 1.45952,
      "grad_norm": 0.0057778614573180676,
      "learning_rate": 1.027008e-05,
      "loss": 0.0198,
      "step": 45610
    },
    {
      "epoch": 1.45984,
      "grad_norm": 0.006665189750492573,
      "learning_rate": 1.0267946666666668e-05,
      "loss": 0.0011,
      "step": 45620
    },
    {
      "epoch": 1.4601600000000001,
      "grad_norm": 0.005404983181506395,
      "learning_rate": 1.0265813333333335e-05,
      "loss": 0.0009,
      "step": 45630
    },
    {
      "epoch": 1.46048,
      "grad_norm": 0.024233372882008553,
      "learning_rate": 1.026368e-05,
      "loss": 0.0004,
      "step": 45640
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.004567325580865145,
      "learning_rate": 1.0261546666666669e-05,
      "loss": 0.0011,
      "step": 45650
    },
    {
      "epoch": 1.46112,
      "grad_norm": 0.002779464703053236,
      "learning_rate": 1.0259413333333334e-05,
      "loss": 0.0002,
      "step": 45660
    },
    {
      "epoch": 1.46144,
      "grad_norm": 0.00391205120831728,
      "learning_rate": 1.0257280000000001e-05,
      "loss": 0.0002,
      "step": 45670
    },
    {
      "epoch": 1.46176,
      "grad_norm": 0.007083795964717865,
      "learning_rate": 1.0255146666666667e-05,
      "loss": 0.0002,
      "step": 45680
    },
    {
      "epoch": 1.46208,
      "grad_norm": 0.011229068972170353,
      "learning_rate": 1.0253013333333335e-05,
      "loss": 0.0482,
      "step": 45690
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.006219044327735901,
      "learning_rate": 1.025088e-05,
      "loss": 0.0004,
      "step": 45700
    },
    {
      "epoch": 1.46272,
      "grad_norm": 0.0037446145433932543,
      "learning_rate": 1.0248746666666666e-05,
      "loss": 0.0008,
      "step": 45710
    },
    {
      "epoch": 1.46304,
      "grad_norm": 0.005444208160042763,
      "learning_rate": 1.0246613333333335e-05,
      "loss": 0.0003,
      "step": 45720
    },
    {
      "epoch": 1.46336,
      "grad_norm": 0.007185694295912981,
      "learning_rate": 1.0244480000000002e-05,
      "loss": 0.0003,
      "step": 45730
    },
    {
      "epoch": 1.46368,
      "grad_norm": 0.04353969544172287,
      "learning_rate": 1.0242346666666667e-05,
      "loss": 0.0004,
      "step": 45740
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.011358220130205154,
      "learning_rate": 1.0240213333333333e-05,
      "loss": 0.0007,
      "step": 45750
    },
    {
      "epoch": 1.46432,
      "grad_norm": 0.00612305523827672,
      "learning_rate": 1.0238080000000001e-05,
      "loss": 0.0003,
      "step": 45760
    },
    {
      "epoch": 1.46464,
      "grad_norm": 0.0040869633667171,
      "learning_rate": 1.0235946666666667e-05,
      "loss": 0.0003,
      "step": 45770
    },
    {
      "epoch": 1.46496,
      "grad_norm": 0.0070577338337898254,
      "learning_rate": 1.0233813333333334e-05,
      "loss": 0.0003,
      "step": 45780
    },
    {
      "epoch": 1.46528,
      "grad_norm": 0.006187853403389454,
      "learning_rate": 1.0231680000000001e-05,
      "loss": 0.0007,
      "step": 45790
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.016062133014202118,
      "learning_rate": 1.0229546666666668e-05,
      "loss": 0.0375,
      "step": 45800
    },
    {
      "epoch": 1.4659200000000001,
      "grad_norm": 0.010190877132117748,
      "learning_rate": 1.0227413333333333e-05,
      "loss": 0.0006,
      "step": 45810
    },
    {
      "epoch": 1.46624,
      "grad_norm": 0.00418351823464036,
      "learning_rate": 1.0225280000000002e-05,
      "loss": 0.0004,
      "step": 45820
    },
    {
      "epoch": 1.4665599999999999,
      "grad_norm": 0.0291821900755167,
      "learning_rate": 1.0223146666666668e-05,
      "loss": 0.0004,
      "step": 45830
    },
    {
      "epoch": 1.46688,
      "grad_norm": 0.030196402221918106,
      "learning_rate": 1.0221013333333335e-05,
      "loss": 0.0003,
      "step": 45840
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.009685924276709557,
      "learning_rate": 1.021888e-05,
      "loss": 0.0003,
      "step": 45850
    },
    {
      "epoch": 1.46752,
      "grad_norm": 0.027304353192448616,
      "learning_rate": 1.0216746666666669e-05,
      "loss": 0.0003,
      "step": 45860
    },
    {
      "epoch": 1.46784,
      "grad_norm": 0.007014559581875801,
      "learning_rate": 1.0214613333333334e-05,
      "loss": 0.0003,
      "step": 45870
    },
    {
      "epoch": 1.4681600000000001,
      "grad_norm": 0.005586468148976564,
      "learning_rate": 1.021248e-05,
      "loss": 0.0218,
      "step": 45880
    },
    {
      "epoch": 1.46848,
      "grad_norm": 0.0052503496408462524,
      "learning_rate": 1.0210346666666668e-05,
      "loss": 0.0003,
      "step": 45890
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.005043498240411282,
      "learning_rate": 1.0208213333333334e-05,
      "loss": 0.0169,
      "step": 45900
    },
    {
      "epoch": 1.46912,
      "grad_norm": 0.02619241178035736,
      "learning_rate": 1.020608e-05,
      "loss": 0.0003,
      "step": 45910
    },
    {
      "epoch": 1.46944,
      "grad_norm": 0.00615764269605279,
      "learning_rate": 1.0203946666666666e-05,
      "loss": 0.0462,
      "step": 45920
    },
    {
      "epoch": 1.46976,
      "grad_norm": 0.005303848069161177,
      "learning_rate": 1.0201813333333335e-05,
      "loss": 0.0003,
      "step": 45930
    },
    {
      "epoch": 1.47008,
      "grad_norm": 0.005305492784827948,
      "learning_rate": 1.019968e-05,
      "loss": 0.0005,
      "step": 45940
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.011511904187500477,
      "learning_rate": 1.0197546666666667e-05,
      "loss": 0.0015,
      "step": 45950
    },
    {
      "epoch": 1.47072,
      "grad_norm": 0.007959157228469849,
      "learning_rate": 1.0195413333333334e-05,
      "loss": 0.0003,
      "step": 45960
    },
    {
      "epoch": 1.47104,
      "grad_norm": 0.011721109040081501,
      "learning_rate": 1.0193280000000001e-05,
      "loss": 0.0247,
      "step": 45970
    },
    {
      "epoch": 1.47136,
      "grad_norm": 3.5924267768859863,
      "learning_rate": 1.0191146666666667e-05,
      "loss": 0.0523,
      "step": 45980
    },
    {
      "epoch": 1.47168,
      "grad_norm": 0.008635793812572956,
      "learning_rate": 1.0189013333333336e-05,
      "loss": 0.0004,
      "step": 45990
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.00866634864360094,
      "learning_rate": 1.0186880000000001e-05,
      "loss": 0.0004,
      "step": 46000
    },
    {
      "epoch": 1.47232,
      "grad_norm": 0.005412080325186253,
      "learning_rate": 1.0184746666666668e-05,
      "loss": 0.0175,
      "step": 46010
    },
    {
      "epoch": 1.47264,
      "grad_norm": 0.024662433192133904,
      "learning_rate": 1.0182613333333333e-05,
      "loss": 0.0006,
      "step": 46020
    },
    {
      "epoch": 1.47296,
      "grad_norm": 0.004416489973664284,
      "learning_rate": 1.0180480000000002e-05,
      "loss": 0.0004,
      "step": 46030
    },
    {
      "epoch": 1.47328,
      "grad_norm": 0.005611767992377281,
      "learning_rate": 1.0178346666666668e-05,
      "loss": 0.0003,
      "step": 46040
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.012143771164119244,
      "learning_rate": 1.0176213333333333e-05,
      "loss": 0.0261,
      "step": 46050
    },
    {
      "epoch": 1.4739200000000001,
      "grad_norm": 3.77592134475708,
      "learning_rate": 1.0174080000000002e-05,
      "loss": 0.0041,
      "step": 46060
    },
    {
      "epoch": 1.47424,
      "grad_norm": 0.015810538083314896,
      "learning_rate": 1.0171946666666667e-05,
      "loss": 0.001,
      "step": 46070
    },
    {
      "epoch": 1.4745599999999999,
      "grad_norm": 0.00903003104031086,
      "learning_rate": 1.0169813333333334e-05,
      "loss": 0.0497,
      "step": 46080
    },
    {
      "epoch": 1.47488,
      "grad_norm": 0.008185124956071377,
      "learning_rate": 1.016768e-05,
      "loss": 0.0002,
      "step": 46090
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.008381384424865246,
      "learning_rate": 1.0165546666666668e-05,
      "loss": 0.0002,
      "step": 46100
    },
    {
      "epoch": 1.47552,
      "grad_norm": 0.007559916470199823,
      "learning_rate": 1.0163413333333334e-05,
      "loss": 0.0003,
      "step": 46110
    },
    {
      "epoch": 1.47584,
      "grad_norm": 0.005482912063598633,
      "learning_rate": 1.016128e-05,
      "loss": 0.0215,
      "step": 46120
    },
    {
      "epoch": 1.4761600000000001,
      "grad_norm": 0.009301961399614811,
      "learning_rate": 1.0159146666666668e-05,
      "loss": 0.0003,
      "step": 46130
    },
    {
      "epoch": 1.47648,
      "grad_norm": 0.004371116869151592,
      "learning_rate": 1.0157013333333335e-05,
      "loss": 0.0267,
      "step": 46140
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.012444154359400272,
      "learning_rate": 1.015488e-05,
      "loss": 0.0081,
      "step": 46150
    },
    {
      "epoch": 1.47712,
      "grad_norm": 0.011183837428689003,
      "learning_rate": 1.0152746666666669e-05,
      "loss": 0.0016,
      "step": 46160
    },
    {
      "epoch": 1.47744,
      "grad_norm": 0.08882059901952744,
      "learning_rate": 1.0150613333333334e-05,
      "loss": 0.0007,
      "step": 46170
    },
    {
      "epoch": 1.47776,
      "grad_norm": 0.10577435046434402,
      "learning_rate": 1.014848e-05,
      "loss": 0.047,
      "step": 46180
    },
    {
      "epoch": 1.47808,
      "grad_norm": 0.011985084041953087,
      "learning_rate": 1.0146346666666667e-05,
      "loss": 0.0008,
      "step": 46190
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.008651797659695148,
      "learning_rate": 1.0144213333333336e-05,
      "loss": 0.0004,
      "step": 46200
    },
    {
      "epoch": 1.47872,
      "grad_norm": 0.023746030405163765,
      "learning_rate": 1.0142080000000001e-05,
      "loss": 0.0003,
      "step": 46210
    },
    {
      "epoch": 1.47904,
      "grad_norm": 0.06720806658267975,
      "learning_rate": 1.0139946666666666e-05,
      "loss": 0.0007,
      "step": 46220
    },
    {
      "epoch": 1.47936,
      "grad_norm": 0.0065756007097661495,
      "learning_rate": 1.0137813333333335e-05,
      "loss": 0.0003,
      "step": 46230
    },
    {
      "epoch": 1.47968,
      "grad_norm": 0.006535782013088465,
      "learning_rate": 1.013568e-05,
      "loss": 0.0003,
      "step": 46240
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.00254784501157701,
      "learning_rate": 1.0133546666666668e-05,
      "loss": 0.0004,
      "step": 46250
    },
    {
      "epoch": 1.48032,
      "grad_norm": 0.004571991972625256,
      "learning_rate": 1.0131413333333333e-05,
      "loss": 0.0004,
      "step": 46260
    },
    {
      "epoch": 1.48064,
      "grad_norm": 4.950806617736816,
      "learning_rate": 1.0129280000000002e-05,
      "loss": 0.0306,
      "step": 46270
    },
    {
      "epoch": 1.48096,
      "grad_norm": 0.00536331394687295,
      "learning_rate": 1.0127146666666667e-05,
      "loss": 0.0002,
      "step": 46280
    },
    {
      "epoch": 1.48128,
      "grad_norm": 0.006966746877878904,
      "learning_rate": 1.0125013333333334e-05,
      "loss": 0.0046,
      "step": 46290
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.004591700155287981,
      "learning_rate": 1.0122880000000001e-05,
      "loss": 0.0006,
      "step": 46300
    },
    {
      "epoch": 1.4819200000000001,
      "grad_norm": 0.21836471557617188,
      "learning_rate": 1.0120746666666668e-05,
      "loss": 0.0006,
      "step": 46310
    },
    {
      "epoch": 1.48224,
      "grad_norm": 0.010559207759797573,
      "learning_rate": 1.0118613333333334e-05,
      "loss": 0.0005,
      "step": 46320
    },
    {
      "epoch": 1.4825599999999999,
      "grad_norm": 0.007316687144339085,
      "learning_rate": 1.0116480000000002e-05,
      "loss": 0.0004,
      "step": 46330
    },
    {
      "epoch": 1.48288,
      "grad_norm": 0.020904405042529106,
      "learning_rate": 1.0114346666666668e-05,
      "loss": 0.0003,
      "step": 46340
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.004990788176655769,
      "learning_rate": 1.0112213333333333e-05,
      "loss": 0.0421,
      "step": 46350
    },
    {
      "epoch": 1.48352,
      "grad_norm": 0.0043441662564873695,
      "learning_rate": 1.011008e-05,
      "loss": 0.0012,
      "step": 46360
    },
    {
      "epoch": 1.48384,
      "grad_norm": 0.007466073147952557,
      "learning_rate": 1.0107946666666667e-05,
      "loss": 0.0003,
      "step": 46370
    },
    {
      "epoch": 1.48416,
      "grad_norm": 0.016458356752991676,
      "learning_rate": 1.0105813333333334e-05,
      "loss": 0.007,
      "step": 46380
    },
    {
      "epoch": 1.48448,
      "grad_norm": 0.004341771360486746,
      "learning_rate": 1.010368e-05,
      "loss": 0.0003,
      "step": 46390
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.005434065591543913,
      "learning_rate": 1.0101546666666669e-05,
      "loss": 0.0018,
      "step": 46400
    },
    {
      "epoch": 1.48512,
      "grad_norm": 0.0032228995114564896,
      "learning_rate": 1.0099413333333334e-05,
      "loss": 0.0044,
      "step": 46410
    },
    {
      "epoch": 1.48544,
      "grad_norm": 0.9728506803512573,
      "learning_rate": 1.0097280000000001e-05,
      "loss": 0.0057,
      "step": 46420
    },
    {
      "epoch": 1.48576,
      "grad_norm": 0.0034600922372192144,
      "learning_rate": 1.0095146666666666e-05,
      "loss": 0.0002,
      "step": 46430
    },
    {
      "epoch": 1.48608,
      "grad_norm": 0.01175847090780735,
      "learning_rate": 1.0093013333333335e-05,
      "loss": 0.0696,
      "step": 46440
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.03518117219209671,
      "learning_rate": 1.009088e-05,
      "loss": 0.0003,
      "step": 46450
    },
    {
      "epoch": 1.48672,
      "grad_norm": 0.001973348204046488,
      "learning_rate": 1.0088746666666666e-05,
      "loss": 0.0002,
      "step": 46460
    },
    {
      "epoch": 1.48704,
      "grad_norm": 0.0062613217160105705,
      "learning_rate": 1.0086613333333335e-05,
      "loss": 0.0192,
      "step": 46470
    },
    {
      "epoch": 1.48736,
      "grad_norm": 0.0018099386943504214,
      "learning_rate": 1.0084480000000002e-05,
      "loss": 0.0031,
      "step": 46480
    },
    {
      "epoch": 1.4876800000000001,
      "grad_norm": 0.0059663220308721066,
      "learning_rate": 1.0082346666666667e-05,
      "loss": 0.0008,
      "step": 46490
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.00785029400140047,
      "learning_rate": 1.0080213333333336e-05,
      "loss": 0.0024,
      "step": 46500
    },
    {
      "epoch": 1.4883199999999999,
      "grad_norm": 0.005950658116489649,
      "learning_rate": 1.0078080000000001e-05,
      "loss": 0.0005,
      "step": 46510
    },
    {
      "epoch": 1.48864,
      "grad_norm": 0.0021065438631922007,
      "learning_rate": 1.0075946666666667e-05,
      "loss": 0.0004,
      "step": 46520
    },
    {
      "epoch": 1.48896,
      "grad_norm": 4.872379302978516,
      "learning_rate": 1.0073813333333334e-05,
      "loss": 0.0799,
      "step": 46530
    },
    {
      "epoch": 1.48928,
      "grad_norm": 0.0038621872663497925,
      "learning_rate": 1.007168e-05,
      "loss": 0.0015,
      "step": 46540
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.0028538256883621216,
      "learning_rate": 1.0069546666666668e-05,
      "loss": 0.0004,
      "step": 46550
    },
    {
      "epoch": 1.4899200000000001,
      "grad_norm": 0.19563211500644684,
      "learning_rate": 1.0067413333333333e-05,
      "loss": 0.0007,
      "step": 46560
    },
    {
      "epoch": 1.49024,
      "grad_norm": 0.007570832036435604,
      "learning_rate": 1.0065280000000002e-05,
      "loss": 0.0003,
      "step": 46570
    },
    {
      "epoch": 1.4905599999999999,
      "grad_norm": 0.0038557129446417093,
      "learning_rate": 1.0063146666666667e-05,
      "loss": 0.0002,
      "step": 46580
    },
    {
      "epoch": 1.49088,
      "grad_norm": 0.0031116215977817774,
      "learning_rate": 1.0061013333333334e-05,
      "loss": 0.0003,
      "step": 46590
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.009521435014903545,
      "learning_rate": 1.005888e-05,
      "loss": 0.0002,
      "step": 46600
    },
    {
      "epoch": 1.49152,
      "grad_norm": 0.0023855245672166348,
      "learning_rate": 1.0056746666666669e-05,
      "loss": 0.001,
      "step": 46610
    },
    {
      "epoch": 1.49184,
      "grad_norm": 0.0036260054912418127,
      "learning_rate": 1.0054613333333334e-05,
      "loss": 0.0004,
      "step": 46620
    },
    {
      "epoch": 1.49216,
      "grad_norm": 0.0037683919072151184,
      "learning_rate": 1.005248e-05,
      "loss": 0.0003,
      "step": 46630
    },
    {
      "epoch": 1.49248,
      "grad_norm": 0.12341664731502533,
      "learning_rate": 1.0050346666666668e-05,
      "loss": 0.0003,
      "step": 46640
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.06656336784362793,
      "learning_rate": 1.0048213333333333e-05,
      "loss": 0.0007,
      "step": 46650
    },
    {
      "epoch": 1.49312,
      "grad_norm": 0.007404161151498556,
      "learning_rate": 1.004608e-05,
      "loss": 0.0056,
      "step": 46660
    },
    {
      "epoch": 1.49344,
      "grad_norm": 0.004707830958068371,
      "learning_rate": 1.004394666666667e-05,
      "loss": 0.0003,
      "step": 46670
    },
    {
      "epoch": 1.49376,
      "grad_norm": 0.0032910997979342937,
      "learning_rate": 1.0041813333333335e-05,
      "loss": 0.0003,
      "step": 46680
    },
    {
      "epoch": 1.49408,
      "grad_norm": 0.0031593558378517628,
      "learning_rate": 1.003968e-05,
      "loss": 0.0003,
      "step": 46690
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.005151454359292984,
      "learning_rate": 1.0037546666666667e-05,
      "loss": 0.0004,
      "step": 46700
    },
    {
      "epoch": 1.49472,
      "grad_norm": 0.08538662642240524,
      "learning_rate": 1.0035413333333334e-05,
      "loss": 0.0004,
      "step": 46710
    },
    {
      "epoch": 1.49504,
      "grad_norm": 0.001957620494067669,
      "learning_rate": 1.0033280000000001e-05,
      "loss": 0.0003,
      "step": 46720
    },
    {
      "epoch": 1.49536,
      "grad_norm": 0.005583307705819607,
      "learning_rate": 1.0031146666666667e-05,
      "loss": 0.0114,
      "step": 46730
    },
    {
      "epoch": 1.4956800000000001,
      "grad_norm": 0.0055032651871442795,
      "learning_rate": 1.0029013333333335e-05,
      "loss": 0.0349,
      "step": 46740
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.005034359637647867,
      "learning_rate": 1.002688e-05,
      "loss": 0.0062,
      "step": 46750
    },
    {
      "epoch": 1.4963199999999999,
      "grad_norm": 0.0065273139625787735,
      "learning_rate": 1.0024746666666668e-05,
      "loss": 0.0016,
      "step": 46760
    },
    {
      "epoch": 1.49664,
      "grad_norm": 0.00544650387018919,
      "learning_rate": 1.0022613333333333e-05,
      "loss": 0.0042,
      "step": 46770
    },
    {
      "epoch": 1.49696,
      "grad_norm": 0.004408343695104122,
      "learning_rate": 1.0020480000000002e-05,
      "loss": 0.0002,
      "step": 46780
    },
    {
      "epoch": 1.49728,
      "grad_norm": 0.004028230905532837,
      "learning_rate": 1.0018346666666667e-05,
      "loss": 0.0004,
      "step": 46790
    },
    {
      "epoch": 1.4976,
      "grad_norm": 6.056106090545654,
      "learning_rate": 1.0016213333333333e-05,
      "loss": 0.0094,
      "step": 46800
    },
    {
      "epoch": 1.49792,
      "grad_norm": 0.04781829193234444,
      "learning_rate": 1.0014080000000002e-05,
      "loss": 0.0004,
      "step": 46810
    },
    {
      "epoch": 1.49824,
      "grad_norm": 0.00554261077195406,
      "learning_rate": 1.0011946666666667e-05,
      "loss": 0.0002,
      "step": 46820
    },
    {
      "epoch": 1.49856,
      "grad_norm": 0.004463504534214735,
      "learning_rate": 1.0009813333333334e-05,
      "loss": 0.0002,
      "step": 46830
    },
    {
      "epoch": 1.49888,
      "grad_norm": 0.0024717729538679123,
      "learning_rate": 1.0007680000000001e-05,
      "loss": 0.0002,
      "step": 46840
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.0047281766310334206,
      "learning_rate": 1.0005546666666668e-05,
      "loss": 0.0002,
      "step": 46850
    },
    {
      "epoch": 1.49952,
      "grad_norm": 0.005009240470826626,
      "learning_rate": 1.0003413333333334e-05,
      "loss": 0.0002,
      "step": 46860
    },
    {
      "epoch": 1.49984,
      "grad_norm": 0.004157923627644777,
      "learning_rate": 1.000128e-05,
      "loss": 0.0009,
      "step": 46870
    },
    {
      "epoch": 1.5001600000000002,
      "grad_norm": 0.0038468700367957354,
      "learning_rate": 9.999146666666668e-06,
      "loss": 0.0028,
      "step": 46880
    },
    {
      "epoch": 1.50048,
      "grad_norm": 0.003389225108548999,
      "learning_rate": 9.997013333333335e-06,
      "loss": 0.0002,
      "step": 46890
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.003228371264412999,
      "learning_rate": 9.994880000000002e-06,
      "loss": 0.0132,
      "step": 46900
    },
    {
      "epoch": 1.50112,
      "grad_norm": 0.07906865328550339,
      "learning_rate": 9.992746666666667e-06,
      "loss": 0.0004,
      "step": 46910
    },
    {
      "epoch": 1.50144,
      "grad_norm": 0.004747496917843819,
      "learning_rate": 9.990613333333334e-06,
      "loss": 0.0003,
      "step": 46920
    },
    {
      "epoch": 1.50176,
      "grad_norm": 0.010727373883128166,
      "learning_rate": 9.98848e-06,
      "loss": 0.0002,
      "step": 46930
    },
    {
      "epoch": 1.5020799999999999,
      "grad_norm": 0.00877207238227129,
      "learning_rate": 9.986346666666667e-06,
      "loss": 0.0002,
      "step": 46940
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.0026031634770333767,
      "learning_rate": 9.984213333333335e-06,
      "loss": 0.0003,
      "step": 46950
    },
    {
      "epoch": 1.50272,
      "grad_norm": 0.0018030684441328049,
      "learning_rate": 9.98208e-06,
      "loss": 0.0004,
      "step": 46960
    },
    {
      "epoch": 1.50304,
      "grad_norm": 0.010625626891851425,
      "learning_rate": 9.979946666666668e-06,
      "loss": 0.0002,
      "step": 46970
    },
    {
      "epoch": 1.50336,
      "grad_norm": 0.007607177831232548,
      "learning_rate": 9.977813333333333e-06,
      "loss": 0.0053,
      "step": 46980
    },
    {
      "epoch": 1.5036800000000001,
      "grad_norm": 0.0030418294481933117,
      "learning_rate": 9.97568e-06,
      "loss": 0.0002,
      "step": 46990
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.006476040929555893,
      "learning_rate": 9.973546666666667e-06,
      "loss": 0.0482,
      "step": 47000
    },
    {
      "epoch": 1.5043199999999999,
      "grad_norm": 0.0032608078327029943,
      "learning_rate": 9.971413333333334e-06,
      "loss": 0.0002,
      "step": 47010
    },
    {
      "epoch": 1.50464,
      "grad_norm": 0.004454903304576874,
      "learning_rate": 9.969280000000002e-06,
      "loss": 0.0003,
      "step": 47020
    },
    {
      "epoch": 1.50496,
      "grad_norm": 0.1197829470038414,
      "learning_rate": 9.967146666666667e-06,
      "loss": 0.0004,
      "step": 47030
    },
    {
      "epoch": 1.50528,
      "grad_norm": 0.005551814567297697,
      "learning_rate": 9.965013333333334e-06,
      "loss": 0.0002,
      "step": 47040
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.0077063352800905704,
      "learning_rate": 9.962880000000001e-06,
      "loss": 0.0004,
      "step": 47050
    },
    {
      "epoch": 1.5059200000000001,
      "grad_norm": 0.005523958709090948,
      "learning_rate": 9.960746666666668e-06,
      "loss": 0.0499,
      "step": 47060
    },
    {
      "epoch": 1.50624,
      "grad_norm": 0.0063067153096199036,
      "learning_rate": 9.958613333333335e-06,
      "loss": 0.0006,
      "step": 47070
    },
    {
      "epoch": 1.50656,
      "grad_norm": 0.006190980784595013,
      "learning_rate": 9.95648e-06,
      "loss": 0.0003,
      "step": 47080
    },
    {
      "epoch": 1.50688,
      "grad_norm": 0.0031750525813549757,
      "learning_rate": 9.954346666666668e-06,
      "loss": 0.0002,
      "step": 47090
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.006305682007223368,
      "learning_rate": 9.952213333333333e-06,
      "loss": 0.0105,
      "step": 47100
    },
    {
      "epoch": 1.50752,
      "grad_norm": 0.01504595298320055,
      "learning_rate": 9.95008e-06,
      "loss": 0.0002,
      "step": 47110
    },
    {
      "epoch": 1.5078399999999998,
      "grad_norm": 0.004696009214967489,
      "learning_rate": 9.947946666666667e-06,
      "loss": 0.0003,
      "step": 47120
    },
    {
      "epoch": 1.50816,
      "grad_norm": 0.005567560438066721,
      "learning_rate": 9.945813333333334e-06,
      "loss": 0.0007,
      "step": 47130
    },
    {
      "epoch": 1.50848,
      "grad_norm": 0.004104933235794306,
      "learning_rate": 9.943680000000001e-06,
      "loss": 0.0018,
      "step": 47140
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.008098345249891281,
      "learning_rate": 9.941546666666667e-06,
      "loss": 0.0002,
      "step": 47150
    },
    {
      "epoch": 1.50912,
      "grad_norm": 0.004604588262736797,
      "learning_rate": 9.939413333333334e-06,
      "loss": 0.0062,
      "step": 47160
    },
    {
      "epoch": 1.5094400000000001,
      "grad_norm": 0.004714223090559244,
      "learning_rate": 9.937280000000001e-06,
      "loss": 0.0005,
      "step": 47170
    },
    {
      "epoch": 1.50976,
      "grad_norm": 0.011186927556991577,
      "learning_rate": 9.935146666666668e-06,
      "loss": 0.0002,
      "step": 47180
    },
    {
      "epoch": 1.5100799999999999,
      "grad_norm": 0.005715194158256054,
      "learning_rate": 9.933013333333335e-06,
      "loss": 0.0002,
      "step": 47190
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.009563318453729153,
      "learning_rate": 9.93088e-06,
      "loss": 0.0002,
      "step": 47200
    },
    {
      "epoch": 1.51072,
      "grad_norm": 0.010299774818122387,
      "learning_rate": 9.928746666666667e-06,
      "loss": 0.0002,
      "step": 47210
    },
    {
      "epoch": 1.51104,
      "grad_norm": 0.003972191829234362,
      "learning_rate": 9.926613333333333e-06,
      "loss": 0.0005,
      "step": 47220
    },
    {
      "epoch": 1.51136,
      "grad_norm": 0.0052892048843204975,
      "learning_rate": 9.924480000000002e-06,
      "loss": 0.0002,
      "step": 47230
    },
    {
      "epoch": 1.5116800000000001,
      "grad_norm": 0.020494939759373665,
      "learning_rate": 9.922346666666669e-06,
      "loss": 0.0002,
      "step": 47240
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.7069405317306519,
      "learning_rate": 9.920213333333334e-06,
      "loss": 0.0051,
      "step": 47250
    },
    {
      "epoch": 1.5123199999999999,
      "grad_norm": 0.0057655638083815575,
      "learning_rate": 9.918080000000001e-06,
      "loss": 0.0002,
      "step": 47260
    },
    {
      "epoch": 1.51264,
      "grad_norm": 0.003611962776631117,
      "learning_rate": 9.915946666666666e-06,
      "loss": 0.0002,
      "step": 47270
    },
    {
      "epoch": 1.51296,
      "grad_norm": 0.003576903138309717,
      "learning_rate": 9.913813333333334e-06,
      "loss": 0.0005,
      "step": 47280
    },
    {
      "epoch": 1.51328,
      "grad_norm": 0.003641572082415223,
      "learning_rate": 9.91168e-06,
      "loss": 0.0002,
      "step": 47290
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.00618093041703105,
      "learning_rate": 9.909546666666668e-06,
      "loss": 0.0003,
      "step": 47300
    },
    {
      "epoch": 1.5139200000000002,
      "grad_norm": 0.004708894528448582,
      "learning_rate": 9.907413333333335e-06,
      "loss": 0.0002,
      "step": 47310
    },
    {
      "epoch": 1.51424,
      "grad_norm": 0.0019902028143405914,
      "learning_rate": 9.90528e-06,
      "loss": 0.0005,
      "step": 47320
    },
    {
      "epoch": 1.51456,
      "grad_norm": 0.0038752276450395584,
      "learning_rate": 9.903146666666667e-06,
      "loss": 0.0371,
      "step": 47330
    },
    {
      "epoch": 1.51488,
      "grad_norm": 0.0023342850618064404,
      "learning_rate": 9.901013333333334e-06,
      "loss": 0.0002,
      "step": 47340
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.0032889079302549362,
      "learning_rate": 9.898880000000001e-06,
      "loss": 0.0079,
      "step": 47350
    },
    {
      "epoch": 1.51552,
      "grad_norm": 0.001107937889173627,
      "learning_rate": 9.896746666666668e-06,
      "loss": 0.0171,
      "step": 47360
    },
    {
      "epoch": 1.5158399999999999,
      "grad_norm": 0.0038142993580549955,
      "learning_rate": 9.894613333333334e-06,
      "loss": 0.0002,
      "step": 47370
    },
    {
      "epoch": 1.51616,
      "grad_norm": 0.0020987677853554487,
      "learning_rate": 9.892480000000001e-06,
      "loss": 0.0002,
      "step": 47380
    },
    {
      "epoch": 1.51648,
      "grad_norm": 0.01703745312988758,
      "learning_rate": 9.890346666666666e-06,
      "loss": 0.0002,
      "step": 47390
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.006909809075295925,
      "learning_rate": 9.888213333333333e-06,
      "loss": 0.0002,
      "step": 47400
    },
    {
      "epoch": 1.51712,
      "grad_norm": 0.003438674146309495,
      "learning_rate": 9.88608e-06,
      "loss": 0.0103,
      "step": 47410
    },
    {
      "epoch": 1.5174400000000001,
      "grad_norm": 0.004340865649282932,
      "learning_rate": 9.883946666666667e-06,
      "loss": 0.0003,
      "step": 47420
    },
    {
      "epoch": 1.51776,
      "grad_norm": 0.004544463008642197,
      "learning_rate": 9.881813333333335e-06,
      "loss": 0.0002,
      "step": 47430
    },
    {
      "epoch": 1.5180799999999999,
      "grad_norm": 0.004157878924161196,
      "learning_rate": 9.87968e-06,
      "loss": 0.0413,
      "step": 47440
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.017075657844543457,
      "learning_rate": 9.877546666666667e-06,
      "loss": 0.0175,
      "step": 47450
    },
    {
      "epoch": 1.51872,
      "grad_norm": 0.004290297627449036,
      "learning_rate": 9.875413333333334e-06,
      "loss": 0.0002,
      "step": 47460
    },
    {
      "epoch": 1.51904,
      "grad_norm": 0.004026609938591719,
      "learning_rate": 9.873280000000001e-06,
      "loss": 0.0001,
      "step": 47470
    },
    {
      "epoch": 1.51936,
      "grad_norm": 0.008029951713979244,
      "learning_rate": 9.871146666666668e-06,
      "loss": 0.0003,
      "step": 47480
    },
    {
      "epoch": 1.5196800000000001,
      "grad_norm": 0.006803759373724461,
      "learning_rate": 9.869013333333334e-06,
      "loss": 0.0002,
      "step": 47490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.0029938500374555588,
      "learning_rate": 9.86688e-06,
      "loss": 0.0002,
      "step": 47500
    },
    {
      "epoch": 1.52032,
      "grad_norm": 0.0064336564391851425,
      "learning_rate": 9.864746666666668e-06,
      "loss": 0.0003,
      "step": 47510
    },
    {
      "epoch": 1.52064,
      "grad_norm": 0.05651584640145302,
      "learning_rate": 9.862613333333335e-06,
      "loss": 0.0004,
      "step": 47520
    },
    {
      "epoch": 1.52096,
      "grad_norm": 0.0048016891814768314,
      "learning_rate": 9.860480000000002e-06,
      "loss": 0.0002,
      "step": 47530
    },
    {
      "epoch": 1.52128,
      "grad_norm": 0.004118356388062239,
      "learning_rate": 9.858346666666667e-06,
      "loss": 0.0004,
      "step": 47540
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.0036973890382796526,
      "learning_rate": 9.856213333333334e-06,
      "loss": 0.0005,
      "step": 47550
    },
    {
      "epoch": 1.5219200000000002,
      "grad_norm": 0.0038279229775071144,
      "learning_rate": 9.85408e-06,
      "loss": 0.0003,
      "step": 47560
    },
    {
      "epoch": 1.52224,
      "grad_norm": 0.010033654049038887,
      "learning_rate": 9.851946666666667e-06,
      "loss": 0.0003,
      "step": 47570
    },
    {
      "epoch": 1.52256,
      "grad_norm": 0.0066971685737371445,
      "learning_rate": 9.849813333333334e-06,
      "loss": 0.0002,
      "step": 47580
    },
    {
      "epoch": 1.52288,
      "grad_norm": 0.018018599599599838,
      "learning_rate": 9.847680000000001e-06,
      "loss": 0.0002,
      "step": 47590
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.032807234674692154,
      "learning_rate": 9.845546666666668e-06,
      "loss": 0.0003,
      "step": 47600
    },
    {
      "epoch": 1.52352,
      "grad_norm": 0.003399066859856248,
      "learning_rate": 9.843413333333333e-06,
      "loss": 0.0003,
      "step": 47610
    },
    {
      "epoch": 1.5238399999999999,
      "grad_norm": 0.002728099003434181,
      "learning_rate": 9.84128e-06,
      "loss": 0.0002,
      "step": 47620
    },
    {
      "epoch": 1.52416,
      "grad_norm": 0.004042172338813543,
      "learning_rate": 9.839146666666667e-06,
      "loss": 0.0001,
      "step": 47630
    },
    {
      "epoch": 1.52448,
      "grad_norm": 0.012674400582909584,
      "learning_rate": 9.837013333333335e-06,
      "loss": 0.0004,
      "step": 47640
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.0027608457021415234,
      "learning_rate": 9.834880000000002e-06,
      "loss": 0.006,
      "step": 47650
    },
    {
      "epoch": 1.52512,
      "grad_norm": 0.0017730860272422433,
      "learning_rate": 9.832746666666667e-06,
      "loss": 0.0002,
      "step": 47660
    },
    {
      "epoch": 1.5254400000000001,
      "grad_norm": 0.0011820963118225336,
      "learning_rate": 9.830613333333334e-06,
      "loss": 0.0516,
      "step": 47670
    },
    {
      "epoch": 1.52576,
      "grad_norm": 0.0054619200527668,
      "learning_rate": 9.828480000000001e-06,
      "loss": 0.0002,
      "step": 47680
    },
    {
      "epoch": 1.5260799999999999,
      "grad_norm": 0.004045728128403425,
      "learning_rate": 9.826346666666667e-06,
      "loss": 0.0002,
      "step": 47690
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.003953164909034967,
      "learning_rate": 9.824213333333335e-06,
      "loss": 0.0006,
      "step": 47700
    },
    {
      "epoch": 1.52672,
      "grad_norm": 0.5918987393379211,
      "learning_rate": 9.82208e-06,
      "loss": 0.0574,
      "step": 47710
    },
    {
      "epoch": 1.52704,
      "grad_norm": 0.002123194746673107,
      "learning_rate": 9.819946666666668e-06,
      "loss": 0.0006,
      "step": 47720
    },
    {
      "epoch": 1.52736,
      "grad_norm": 0.003616142552345991,
      "learning_rate": 9.817813333333333e-06,
      "loss": 0.0075,
      "step": 47730
    },
    {
      "epoch": 1.5276800000000001,
      "grad_norm": 0.04192168265581131,
      "learning_rate": 9.81568e-06,
      "loss": 0.0018,
      "step": 47740
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.007451301906257868,
      "learning_rate": 9.813546666666667e-06,
      "loss": 0.0002,
      "step": 47750
    },
    {
      "epoch": 1.52832,
      "grad_norm": 0.007047224324196577,
      "learning_rate": 9.811413333333334e-06,
      "loss": 0.0005,
      "step": 47760
    },
    {
      "epoch": 1.52864,
      "grad_norm": 0.007463994901627302,
      "learning_rate": 9.809280000000001e-06,
      "loss": 0.0432,
      "step": 47770
    },
    {
      "epoch": 1.52896,
      "grad_norm": 0.3975673019886017,
      "learning_rate": 9.807146666666667e-06,
      "loss": 0.0004,
      "step": 47780
    },
    {
      "epoch": 1.52928,
      "grad_norm": 0.009288371540606022,
      "learning_rate": 9.805013333333334e-06,
      "loss": 0.011,
      "step": 47790
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.005709477700293064,
      "learning_rate": 9.802880000000001e-06,
      "loss": 0.0002,
      "step": 47800
    },
    {
      "epoch": 1.52992,
      "grad_norm": 0.005832413677126169,
      "learning_rate": 9.800746666666668e-06,
      "loss": 0.0003,
      "step": 47810
    },
    {
      "epoch": 1.53024,
      "grad_norm": 0.0061357999220490456,
      "learning_rate": 9.798613333333335e-06,
      "loss": 0.0007,
      "step": 47820
    },
    {
      "epoch": 1.53056,
      "grad_norm": 0.004078886471688747,
      "learning_rate": 9.79648e-06,
      "loss": 0.0002,
      "step": 47830
    },
    {
      "epoch": 1.53088,
      "grad_norm": 0.0038594293873757124,
      "learning_rate": 9.794346666666668e-06,
      "loss": 0.0011,
      "step": 47840
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.0060888174921274185,
      "learning_rate": 9.792213333333335e-06,
      "loss": 0.0009,
      "step": 47850
    },
    {
      "epoch": 1.53152,
      "grad_norm": 0.0035797685850411654,
      "learning_rate": 9.79008e-06,
      "loss": 0.045,
      "step": 47860
    },
    {
      "epoch": 1.5318399999999999,
      "grad_norm": 0.009882503189146519,
      "learning_rate": 9.787946666666667e-06,
      "loss": 0.0002,
      "step": 47870
    },
    {
      "epoch": 1.53216,
      "grad_norm": 0.0036754985339939594,
      "learning_rate": 9.785813333333334e-06,
      "loss": 0.0003,
      "step": 47880
    },
    {
      "epoch": 1.53248,
      "grad_norm": 0.003486891044303775,
      "learning_rate": 9.783680000000001e-06,
      "loss": 0.013,
      "step": 47890
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.004376261495053768,
      "learning_rate": 9.781546666666667e-06,
      "loss": 0.0003,
      "step": 47900
    },
    {
      "epoch": 1.53312,
      "grad_norm": 0.007473520003259182,
      "learning_rate": 9.779413333333334e-06,
      "loss": 0.044,
      "step": 47910
    },
    {
      "epoch": 1.5334400000000001,
      "grad_norm": 0.016807029023766518,
      "learning_rate": 9.77728e-06,
      "loss": 0.0636,
      "step": 47920
    },
    {
      "epoch": 1.53376,
      "grad_norm": 0.004304929170757532,
      "learning_rate": 9.775146666666668e-06,
      "loss": 0.0382,
      "step": 47930
    },
    {
      "epoch": 1.5340799999999999,
      "grad_norm": 0.001948544173501432,
      "learning_rate": 9.773013333333335e-06,
      "loss": 0.0004,
      "step": 47940
    },
    {
      "epoch": 1.5344,
      "grad_norm": 3.8746142387390137,
      "learning_rate": 9.77088e-06,
      "loss": 0.0036,
      "step": 47950
    },
    {
      "epoch": 1.53472,
      "grad_norm": 19.377426147460938,
      "learning_rate": 9.768746666666667e-06,
      "loss": 0.0322,
      "step": 47960
    },
    {
      "epoch": 1.53504,
      "grad_norm": 0.01895967312157154,
      "learning_rate": 9.766613333333334e-06,
      "loss": 0.0005,
      "step": 47970
    },
    {
      "epoch": 1.5353599999999998,
      "grad_norm": 0.0031184805557131767,
      "learning_rate": 9.764480000000001e-06,
      "loss": 0.0027,
      "step": 47980
    },
    {
      "epoch": 1.5356800000000002,
      "grad_norm": 0.004198591690510511,
      "learning_rate": 9.762346666666668e-06,
      "loss": 0.0002,
      "step": 47990
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.012582176364958286,
      "learning_rate": 9.760213333333334e-06,
      "loss": 0.0006,
      "step": 48000
    },
    {
      "epoch": 1.53632,
      "grad_norm": 0.009322769939899445,
      "learning_rate": 9.758080000000001e-06,
      "loss": 0.0002,
      "step": 48010
    },
    {
      "epoch": 1.53664,
      "grad_norm": 0.006594357080757618,
      "learning_rate": 9.755946666666668e-06,
      "loss": 0.0254,
      "step": 48020
    },
    {
      "epoch": 1.53696,
      "grad_norm": 0.003623449709266424,
      "learning_rate": 9.753813333333333e-06,
      "loss": 0.0002,
      "step": 48030
    },
    {
      "epoch": 1.53728,
      "grad_norm": 0.015682587400078773,
      "learning_rate": 9.75168e-06,
      "loss": 0.0003,
      "step": 48040
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.003969287499785423,
      "learning_rate": 9.749546666666668e-06,
      "loss": 0.0003,
      "step": 48050
    },
    {
      "epoch": 1.53792,
      "grad_norm": 0.004484253935515881,
      "learning_rate": 9.747413333333335e-06,
      "loss": 0.0252,
      "step": 48060
    },
    {
      "epoch": 1.53824,
      "grad_norm": 0.0039672646671533585,
      "learning_rate": 9.74528e-06,
      "loss": 0.0002,
      "step": 48070
    },
    {
      "epoch": 1.53856,
      "grad_norm": 0.009134882129728794,
      "learning_rate": 9.743146666666667e-06,
      "loss": 0.0002,
      "step": 48080
    },
    {
      "epoch": 1.53888,
      "grad_norm": 0.008492485620081425,
      "learning_rate": 9.741013333333334e-06,
      "loss": 0.0003,
      "step": 48090
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.017608724534511566,
      "learning_rate": 9.738880000000001e-06,
      "loss": 0.0002,
      "step": 48100
    },
    {
      "epoch": 1.53952,
      "grad_norm": 0.0023614356759935617,
      "learning_rate": 9.736746666666668e-06,
      "loss": 0.0002,
      "step": 48110
    },
    {
      "epoch": 1.5398399999999999,
      "grad_norm": 0.0014103344874456525,
      "learning_rate": 9.734613333333334e-06,
      "loss": 0.0188,
      "step": 48120
    },
    {
      "epoch": 1.54016,
      "grad_norm": 0.007348617538809776,
      "learning_rate": 9.73248e-06,
      "loss": 0.0376,
      "step": 48130
    },
    {
      "epoch": 1.54048,
      "grad_norm": 0.0036450750194489956,
      "learning_rate": 9.730346666666668e-06,
      "loss": 0.0002,
      "step": 48140
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.004652249161154032,
      "learning_rate": 9.728213333333333e-06,
      "loss": 0.0003,
      "step": 48150
    },
    {
      "epoch": 1.54112,
      "grad_norm": 0.004331985022872686,
      "learning_rate": 9.726080000000002e-06,
      "loss": 0.0003,
      "step": 48160
    },
    {
      "epoch": 1.5414400000000001,
      "grad_norm": 0.0033568525686860085,
      "learning_rate": 9.723946666666667e-06,
      "loss": 0.0004,
      "step": 48170
    },
    {
      "epoch": 1.54176,
      "grad_norm": 0.005942906718701124,
      "learning_rate": 9.721813333333334e-06,
      "loss": 0.0105,
      "step": 48180
    },
    {
      "epoch": 1.54208,
      "grad_norm": 0.0032366665545850992,
      "learning_rate": 9.719680000000001e-06,
      "loss": 0.0003,
      "step": 48190
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.0043028732761740685,
      "learning_rate": 9.717546666666667e-06,
      "loss": 0.0003,
      "step": 48200
    },
    {
      "epoch": 1.54272,
      "grad_norm": 0.0031843078322708607,
      "learning_rate": 9.715413333333334e-06,
      "loss": 0.0009,
      "step": 48210
    },
    {
      "epoch": 1.54304,
      "grad_norm": 0.005189807619899511,
      "learning_rate": 9.713280000000001e-06,
      "loss": 0.0023,
      "step": 48220
    },
    {
      "epoch": 1.5433599999999998,
      "grad_norm": 0.02398136630654335,
      "learning_rate": 9.711146666666668e-06,
      "loss": 0.0003,
      "step": 48230
    },
    {
      "epoch": 1.5436800000000002,
      "grad_norm": 0.0027448367327451706,
      "learning_rate": 9.709013333333333e-06,
      "loss": 0.0047,
      "step": 48240
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.002639482729136944,
      "learning_rate": 9.70688e-06,
      "loss": 0.0276,
      "step": 48250
    },
    {
      "epoch": 1.54432,
      "grad_norm": 0.0050042858347296715,
      "learning_rate": 9.704746666666668e-06,
      "loss": 0.0418,
      "step": 48260
    },
    {
      "epoch": 1.54464,
      "grad_norm": 0.004304252099245787,
      "learning_rate": 9.702613333333335e-06,
      "loss": 0.0003,
      "step": 48270
    },
    {
      "epoch": 1.54496,
      "grad_norm": 0.004861466120928526,
      "learning_rate": 9.700480000000002e-06,
      "loss": 0.02,
      "step": 48280
    },
    {
      "epoch": 1.54528,
      "grad_norm": 0.01543563324958086,
      "learning_rate": 9.698346666666667e-06,
      "loss": 0.0004,
      "step": 48290
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.0027429431211203337,
      "learning_rate": 9.696213333333334e-06,
      "loss": 0.0002,
      "step": 48300
    },
    {
      "epoch": 1.54592,
      "grad_norm": 0.0028090288396924734,
      "learning_rate": 9.694080000000001e-06,
      "loss": 0.0002,
      "step": 48310
    },
    {
      "epoch": 1.54624,
      "grad_norm": 0.003332920838147402,
      "learning_rate": 9.691946666666667e-06,
      "loss": 0.0003,
      "step": 48320
    },
    {
      "epoch": 1.54656,
      "grad_norm": 0.003272343659773469,
      "learning_rate": 9.689813333333334e-06,
      "loss": 0.0006,
      "step": 48330
    },
    {
      "epoch": 1.54688,
      "grad_norm": 0.006124739069491625,
      "learning_rate": 9.68768e-06,
      "loss": 0.0005,
      "step": 48340
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.8597967028617859,
      "learning_rate": 9.685546666666668e-06,
      "loss": 0.0014,
      "step": 48350
    },
    {
      "epoch": 1.54752,
      "grad_norm": 0.006047185976058245,
      "learning_rate": 9.683413333333335e-06,
      "loss": 0.0049,
      "step": 48360
    },
    {
      "epoch": 1.5478399999999999,
      "grad_norm": 0.015814341604709625,
      "learning_rate": 9.68128e-06,
      "loss": 0.0022,
      "step": 48370
    },
    {
      "epoch": 1.54816,
      "grad_norm": 0.003167488146573305,
      "learning_rate": 9.679146666666667e-06,
      "loss": 0.0002,
      "step": 48380
    },
    {
      "epoch": 1.54848,
      "grad_norm": 0.005874568596482277,
      "learning_rate": 9.677013333333334e-06,
      "loss": 0.0003,
      "step": 48390
    },
    {
      "epoch": 1.5488,
      "grad_norm": 3.2302629947662354,
      "learning_rate": 9.674880000000001e-06,
      "loss": 0.0047,
      "step": 48400
    },
    {
      "epoch": 1.54912,
      "grad_norm": 0.008327527903020382,
      "learning_rate": 9.672746666666667e-06,
      "loss": 0.0002,
      "step": 48410
    },
    {
      "epoch": 1.5494400000000002,
      "grad_norm": 0.005426030606031418,
      "learning_rate": 9.670613333333334e-06,
      "loss": 0.0002,
      "step": 48420
    },
    {
      "epoch": 1.54976,
      "grad_norm": 0.00266950111836195,
      "learning_rate": 9.668480000000001e-06,
      "loss": 0.0002,
      "step": 48430
    },
    {
      "epoch": 1.55008,
      "grad_norm": 0.0021846669260412455,
      "learning_rate": 9.666346666666668e-06,
      "loss": 0.0002,
      "step": 48440
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.002511656144633889,
      "learning_rate": 9.664213333333335e-06,
      "loss": 0.0003,
      "step": 48450
    },
    {
      "epoch": 1.55072,
      "grad_norm": 0.007256396114826202,
      "learning_rate": 9.66208e-06,
      "loss": 0.016,
      "step": 48460
    },
    {
      "epoch": 1.55104,
      "grad_norm": 0.0028432346880435944,
      "learning_rate": 9.659946666666668e-06,
      "loss": 0.0001,
      "step": 48470
    },
    {
      "epoch": 1.5513599999999999,
      "grad_norm": 0.002445533638820052,
      "learning_rate": 9.657813333333335e-06,
      "loss": 0.0003,
      "step": 48480
    },
    {
      "epoch": 1.55168,
      "grad_norm": 0.003985384479165077,
      "learning_rate": 9.65568e-06,
      "loss": 0.0003,
      "step": 48490
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.0012624819064512849,
      "learning_rate": 9.653546666666667e-06,
      "loss": 0.0003,
      "step": 48500
    },
    {
      "epoch": 1.55232,
      "grad_norm": 0.004366505891084671,
      "learning_rate": 9.651413333333334e-06,
      "loss": 0.0006,
      "step": 48510
    },
    {
      "epoch": 1.55264,
      "grad_norm": 0.005363092757761478,
      "learning_rate": 9.649280000000001e-06,
      "loss": 0.0002,
      "step": 48520
    },
    {
      "epoch": 1.5529600000000001,
      "grad_norm": 0.002797067165374756,
      "learning_rate": 9.647146666666668e-06,
      "loss": 0.0002,
      "step": 48530
    },
    {
      "epoch": 1.55328,
      "grad_norm": 0.02443038485944271,
      "learning_rate": 9.645013333333334e-06,
      "loss": 0.0004,
      "step": 48540
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.010966851375997066,
      "learning_rate": 9.64288e-06,
      "loss": 0.0003,
      "step": 48550
    },
    {
      "epoch": 1.55392,
      "grad_norm": 0.003449030453339219,
      "learning_rate": 9.640746666666668e-06,
      "loss": 0.0358,
      "step": 48560
    },
    {
      "epoch": 1.55424,
      "grad_norm": 0.0046031661331653595,
      "learning_rate": 9.638613333333335e-06,
      "loss": 0.0004,
      "step": 48570
    },
    {
      "epoch": 1.55456,
      "grad_norm": 0.005196335259824991,
      "learning_rate": 9.63648e-06,
      "loss": 0.0004,
      "step": 48580
    },
    {
      "epoch": 1.55488,
      "grad_norm": 0.01261595543473959,
      "learning_rate": 9.634346666666667e-06,
      "loss": 0.0002,
      "step": 48590
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.0010109817376360297,
      "learning_rate": 9.632213333333334e-06,
      "loss": 0.0009,
      "step": 48600
    },
    {
      "epoch": 1.55552,
      "grad_norm": 0.005134198814630508,
      "learning_rate": 9.63008e-06,
      "loss": 0.0002,
      "step": 48610
    },
    {
      "epoch": 1.55584,
      "grad_norm": 0.004466019105166197,
      "learning_rate": 9.627946666666667e-06,
      "loss": 0.0004,
      "step": 48620
    },
    {
      "epoch": 1.55616,
      "grad_norm": 0.07239911705255508,
      "learning_rate": 9.625813333333334e-06,
      "loss": 0.0003,
      "step": 48630
    },
    {
      "epoch": 1.55648,
      "grad_norm": 0.012323002330958843,
      "learning_rate": 9.623680000000001e-06,
      "loss": 0.0104,
      "step": 48640
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.003488050075247884,
      "learning_rate": 9.621546666666668e-06,
      "loss": 0.0002,
      "step": 48650
    },
    {
      "epoch": 1.5571199999999998,
      "grad_norm": 0.0017556938109919429,
      "learning_rate": 9.619413333333333e-06,
      "loss": 0.0003,
      "step": 48660
    },
    {
      "epoch": 1.5574400000000002,
      "grad_norm": 0.00253142137080431,
      "learning_rate": 9.61728e-06,
      "loss": 0.0002,
      "step": 48670
    },
    {
      "epoch": 1.55776,
      "grad_norm": 0.009554417803883553,
      "learning_rate": 9.615146666666668e-06,
      "loss": 0.0404,
      "step": 48680
    },
    {
      "epoch": 1.55808,
      "grad_norm": 0.0033633620478212833,
      "learning_rate": 9.613013333333335e-06,
      "loss": 0.0002,
      "step": 48690
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.07442010939121246,
      "learning_rate": 9.610880000000002e-06,
      "loss": 0.0003,
      "step": 48700
    },
    {
      "epoch": 1.55872,
      "grad_norm": 0.003403799142688513,
      "learning_rate": 9.608746666666667e-06,
      "loss": 0.0001,
      "step": 48710
    },
    {
      "epoch": 1.55904,
      "grad_norm": 0.0036576413549482822,
      "learning_rate": 9.606613333333334e-06,
      "loss": 0.0005,
      "step": 48720
    },
    {
      "epoch": 1.5593599999999999,
      "grad_norm": 0.0013870022958144546,
      "learning_rate": 9.604480000000001e-06,
      "loss": 0.0001,
      "step": 48730
    },
    {
      "epoch": 1.55968,
      "grad_norm": 0.003821604186668992,
      "learning_rate": 9.602346666666668e-06,
      "loss": 0.0593,
      "step": 48740
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.0027075086254626513,
      "learning_rate": 9.600213333333334e-06,
      "loss": 0.0239,
      "step": 48750
    },
    {
      "epoch": 1.56032,
      "grad_norm": 0.00504205422475934,
      "learning_rate": 9.59808e-06,
      "loss": 0.0008,
      "step": 48760
    },
    {
      "epoch": 1.56064,
      "grad_norm": 0.006047022994607687,
      "learning_rate": 9.595946666666668e-06,
      "loss": 0.0032,
      "step": 48770
    },
    {
      "epoch": 1.5609600000000001,
      "grad_norm": 0.004172054119408131,
      "learning_rate": 9.593813333333333e-06,
      "loss": 0.0002,
      "step": 48780
    },
    {
      "epoch": 1.56128,
      "grad_norm": 0.014544075354933739,
      "learning_rate": 9.59168e-06,
      "loss": 0.0013,
      "step": 48790
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.006403594743460417,
      "learning_rate": 9.589546666666667e-06,
      "loss": 0.0002,
      "step": 48800
    },
    {
      "epoch": 1.56192,
      "grad_norm": 0.0048347474075853825,
      "learning_rate": 9.587413333333334e-06,
      "loss": 0.0005,
      "step": 48810
    },
    {
      "epoch": 1.56224,
      "grad_norm": 0.0027145238127559423,
      "learning_rate": 9.585280000000002e-06,
      "loss": 0.0002,
      "step": 48820
    },
    {
      "epoch": 1.56256,
      "grad_norm": 0.006038496270775795,
      "learning_rate": 9.583146666666667e-06,
      "loss": 0.0003,
      "step": 48830
    },
    {
      "epoch": 1.56288,
      "grad_norm": 0.007467940915375948,
      "learning_rate": 9.581013333333334e-06,
      "loss": 0.0014,
      "step": 48840
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.01644636131823063,
      "learning_rate": 9.578880000000001e-06,
      "loss": 0.0003,
      "step": 48850
    },
    {
      "epoch": 1.56352,
      "grad_norm": 0.018091656267642975,
      "learning_rate": 9.576746666666668e-06,
      "loss": 0.0002,
      "step": 48860
    },
    {
      "epoch": 1.56384,
      "grad_norm": 0.0032171292696148157,
      "learning_rate": 9.574613333333335e-06,
      "loss": 0.0105,
      "step": 48870
    },
    {
      "epoch": 1.56416,
      "grad_norm": 0.002388486871495843,
      "learning_rate": 9.57248e-06,
      "loss": 0.0002,
      "step": 48880
    },
    {
      "epoch": 1.56448,
      "grad_norm": 0.004185404162853956,
      "learning_rate": 9.570346666666668e-06,
      "loss": 0.0002,
      "step": 48890
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.03612789139151573,
      "learning_rate": 9.568213333333333e-06,
      "loss": 0.0003,
      "step": 48900
    },
    {
      "epoch": 1.5651199999999998,
      "grad_norm": 0.002470096806064248,
      "learning_rate": 9.566080000000002e-06,
      "loss": 0.0001,
      "step": 48910
    },
    {
      "epoch": 1.5654400000000002,
      "grad_norm": 0.004571815952658653,
      "learning_rate": 9.563946666666667e-06,
      "loss": 0.0001,
      "step": 48920
    },
    {
      "epoch": 1.56576,
      "grad_norm": 0.0560256764292717,
      "learning_rate": 9.561813333333334e-06,
      "loss": 0.0003,
      "step": 48930
    },
    {
      "epoch": 1.56608,
      "grad_norm": 0.004618230741471052,
      "learning_rate": 9.559680000000001e-06,
      "loss": 0.0023,
      "step": 48940
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.017417866736650467,
      "learning_rate": 9.557546666666667e-06,
      "loss": 0.0509,
      "step": 48950
    },
    {
      "epoch": 1.5667200000000001,
      "grad_norm": 0.028684213757514954,
      "learning_rate": 9.555413333333334e-06,
      "loss": 0.0002,
      "step": 48960
    },
    {
      "epoch": 1.56704,
      "grad_norm": 0.006149429827928543,
      "learning_rate": 9.55328e-06,
      "loss": 0.0016,
      "step": 48970
    },
    {
      "epoch": 1.5673599999999999,
      "grad_norm": 0.0017524982104077935,
      "learning_rate": 9.551146666666668e-06,
      "loss": 0.0561,
      "step": 48980
    },
    {
      "epoch": 1.56768,
      "grad_norm": 0.003351856954395771,
      "learning_rate": 9.549013333333335e-06,
      "loss": 0.0003,
      "step": 48990
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.006296655163168907,
      "learning_rate": 9.54688e-06,
      "loss": 0.0013,
      "step": 49000
    },
    {
      "epoch": 1.56832,
      "grad_norm": 0.006604771129786968,
      "learning_rate": 9.544746666666667e-06,
      "loss": 0.0013,
      "step": 49010
    },
    {
      "epoch": 1.56864,
      "grad_norm": 0.006357882171869278,
      "learning_rate": 9.542613333333334e-06,
      "loss": 0.0002,
      "step": 49020
    },
    {
      "epoch": 1.5689600000000001,
      "grad_norm": 0.003461230080574751,
      "learning_rate": 9.540480000000002e-06,
      "loss": 0.0485,
      "step": 49030
    },
    {
      "epoch": 1.56928,
      "grad_norm": 0.010121968574821949,
      "learning_rate": 9.538346666666669e-06,
      "loss": 0.0006,
      "step": 49040
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 2.8908298015594482,
      "learning_rate": 9.536213333333334e-06,
      "loss": 0.039,
      "step": 49050
    },
    {
      "epoch": 1.56992,
      "grad_norm": 0.0017747378442436457,
      "learning_rate": 9.534080000000001e-06,
      "loss": 0.0002,
      "step": 49060
    },
    {
      "epoch": 1.57024,
      "grad_norm": 0.0028537330217659473,
      "learning_rate": 9.531946666666666e-06,
      "loss": 0.0457,
      "step": 49070
    },
    {
      "epoch": 1.57056,
      "grad_norm": 0.007844535633921623,
      "learning_rate": 9.529813333333333e-06,
      "loss": 0.047,
      "step": 49080
    },
    {
      "epoch": 1.57088,
      "grad_norm": 4.2112717628479,
      "learning_rate": 9.52768e-06,
      "loss": 0.0079,
      "step": 49090
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.002953249728307128,
      "learning_rate": 9.525546666666668e-06,
      "loss": 0.0025,
      "step": 49100
    },
    {
      "epoch": 1.57152,
      "grad_norm": 0.010491491295397282,
      "learning_rate": 9.523413333333335e-06,
      "loss": 0.0004,
      "step": 49110
    },
    {
      "epoch": 1.57184,
      "grad_norm": 0.3772801160812378,
      "learning_rate": 9.52128e-06,
      "loss": 0.0007,
      "step": 49120
    },
    {
      "epoch": 1.57216,
      "grad_norm": 0.0037920374888926744,
      "learning_rate": 9.519146666666667e-06,
      "loss": 0.0002,
      "step": 49130
    },
    {
      "epoch": 1.57248,
      "grad_norm": 0.006870900746434927,
      "learning_rate": 9.517013333333334e-06,
      "loss": 0.0003,
      "step": 49140
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.004998188000172377,
      "learning_rate": 9.514880000000001e-06,
      "loss": 0.0022,
      "step": 49150
    },
    {
      "epoch": 1.5731199999999999,
      "grad_norm": 0.013339696452021599,
      "learning_rate": 9.512746666666668e-06,
      "loss": 0.0007,
      "step": 49160
    },
    {
      "epoch": 1.57344,
      "grad_norm": 0.013814476318657398,
      "learning_rate": 9.510613333333334e-06,
      "loss": 0.0565,
      "step": 49170
    },
    {
      "epoch": 1.57376,
      "grad_norm": 0.006626132410019636,
      "learning_rate": 9.50848e-06,
      "loss": 0.0003,
      "step": 49180
    },
    {
      "epoch": 1.57408,
      "grad_norm": 0.003705051261931658,
      "learning_rate": 9.506346666666668e-06,
      "loss": 0.0002,
      "step": 49190
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.006634700112044811,
      "learning_rate": 9.504213333333335e-06,
      "loss": 0.0002,
      "step": 49200
    },
    {
      "epoch": 1.5747200000000001,
      "grad_norm": 0.005613354034721851,
      "learning_rate": 9.502080000000002e-06,
      "loss": 0.0004,
      "step": 49210
    },
    {
      "epoch": 1.57504,
      "grad_norm": 0.010176722891628742,
      "learning_rate": 9.499946666666667e-06,
      "loss": 0.0007,
      "step": 49220
    },
    {
      "epoch": 1.5753599999999999,
      "grad_norm": 0.006456353701651096,
      "learning_rate": 9.497813333333334e-06,
      "loss": 0.0005,
      "step": 49230
    },
    {
      "epoch": 1.57568,
      "grad_norm": 0.006669643335044384,
      "learning_rate": 9.49568e-06,
      "loss": 0.001,
      "step": 49240
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.0058620464988052845,
      "learning_rate": 9.493546666666667e-06,
      "loss": 0.0004,
      "step": 49250
    },
    {
      "epoch": 1.57632,
      "grad_norm": 0.0019223593408241868,
      "learning_rate": 9.491413333333334e-06,
      "loss": 0.0648,
      "step": 49260
    },
    {
      "epoch": 1.57664,
      "grad_norm": 0.003070270409807563,
      "learning_rate": 9.489280000000001e-06,
      "loss": 0.0467,
      "step": 49270
    },
    {
      "epoch": 1.5769600000000001,
      "grad_norm": 0.004486378747969866,
      "learning_rate": 9.487146666666668e-06,
      "loss": 0.0004,
      "step": 49280
    },
    {
      "epoch": 1.57728,
      "grad_norm": 0.0063873520120978355,
      "learning_rate": 9.485013333333334e-06,
      "loss": 0.0004,
      "step": 49290
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.005230897106230259,
      "learning_rate": 9.48288e-06,
      "loss": 0.0003,
      "step": 49300
    },
    {
      "epoch": 1.57792,
      "grad_norm": 0.0032296169083565474,
      "learning_rate": 9.480746666666668e-06,
      "loss": 0.0013,
      "step": 49310
    },
    {
      "epoch": 1.57824,
      "grad_norm": 0.003029415849596262,
      "learning_rate": 9.478613333333335e-06,
      "loss": 0.0003,
      "step": 49320
    },
    {
      "epoch": 1.57856,
      "grad_norm": 0.009390721097588539,
      "learning_rate": 9.476480000000002e-06,
      "loss": 0.0003,
      "step": 49330
    },
    {
      "epoch": 1.5788799999999998,
      "grad_norm": 0.004664991050958633,
      "learning_rate": 9.474346666666667e-06,
      "loss": 0.0074,
      "step": 49340
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.006544906180351973,
      "learning_rate": 9.472213333333334e-06,
      "loss": 0.0003,
      "step": 49350
    },
    {
      "epoch": 1.57952,
      "grad_norm": 0.004357003606855869,
      "learning_rate": 9.47008e-06,
      "loss": 0.0004,
      "step": 49360
    },
    {
      "epoch": 1.57984,
      "grad_norm": 0.009730314835906029,
      "learning_rate": 9.467946666666667e-06,
      "loss": 0.0497,
      "step": 49370
    },
    {
      "epoch": 1.58016,
      "grad_norm": 0.010760693810880184,
      "learning_rate": 9.465813333333335e-06,
      "loss": 0.0003,
      "step": 49380
    },
    {
      "epoch": 1.58048,
      "grad_norm": 0.004476435482501984,
      "learning_rate": 9.46368e-06,
      "loss": 0.0004,
      "step": 49390
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.008002132177352905,
      "learning_rate": 9.461546666666668e-06,
      "loss": 0.0003,
      "step": 49400
    },
    {
      "epoch": 1.5811199999999999,
      "grad_norm": 0.006046403665095568,
      "learning_rate": 9.459413333333333e-06,
      "loss": 0.0004,
      "step": 49410
    },
    {
      "epoch": 1.58144,
      "grad_norm": 0.004960102029144764,
      "learning_rate": 9.45728e-06,
      "loss": 0.0003,
      "step": 49420
    },
    {
      "epoch": 1.58176,
      "grad_norm": 0.004990747664123774,
      "learning_rate": 9.455146666666667e-06,
      "loss": 0.0004,
      "step": 49430
    },
    {
      "epoch": 1.58208,
      "grad_norm": 0.006103270687162876,
      "learning_rate": 9.453013333333335e-06,
      "loss": 0.0204,
      "step": 49440
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.01778987981379032,
      "learning_rate": 9.450880000000002e-06,
      "loss": 0.0003,
      "step": 49450
    },
    {
      "epoch": 1.5827200000000001,
      "grad_norm": 0.008119072765111923,
      "learning_rate": 9.448746666666667e-06,
      "loss": 0.0003,
      "step": 49460
    },
    {
      "epoch": 1.58304,
      "grad_norm": 0.005776767153292894,
      "learning_rate": 9.446613333333334e-06,
      "loss": 0.0489,
      "step": 49470
    },
    {
      "epoch": 1.5833599999999999,
      "grad_norm": 0.0016883950447663665,
      "learning_rate": 9.444480000000001e-06,
      "loss": 0.0005,
      "step": 49480
    },
    {
      "epoch": 1.58368,
      "grad_norm": 0.02283615618944168,
      "learning_rate": 9.442346666666668e-06,
      "loss": 0.0004,
      "step": 49490
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.008755307644605637,
      "learning_rate": 9.440213333333335e-06,
      "loss": 0.0004,
      "step": 49500
    },
    {
      "epoch": 1.58432,
      "grad_norm": 0.025592515245079994,
      "learning_rate": 9.43808e-06,
      "loss": 0.0004,
      "step": 49510
    },
    {
      "epoch": 1.58464,
      "grad_norm": 0.01126229390501976,
      "learning_rate": 9.435946666666668e-06,
      "loss": 0.0003,
      "step": 49520
    },
    {
      "epoch": 1.5849600000000001,
      "grad_norm": 4.622482776641846,
      "learning_rate": 9.433813333333333e-06,
      "loss": 0.0559,
      "step": 49530
    },
    {
      "epoch": 1.58528,
      "grad_norm": 0.011194405145943165,
      "learning_rate": 9.43168e-06,
      "loss": 0.0439,
      "step": 49540
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.010199359618127346,
      "learning_rate": 9.429546666666667e-06,
      "loss": 0.0004,
      "step": 49550
    },
    {
      "epoch": 1.58592,
      "grad_norm": 0.022266266867518425,
      "learning_rate": 9.427413333333334e-06,
      "loss": 0.0004,
      "step": 49560
    },
    {
      "epoch": 1.58624,
      "grad_norm": 0.06315051019191742,
      "learning_rate": 9.425280000000001e-06,
      "loss": 0.0007,
      "step": 49570
    },
    {
      "epoch": 1.58656,
      "grad_norm": 0.0055666291154921055,
      "learning_rate": 9.423146666666667e-06,
      "loss": 0.0007,
      "step": 49580
    },
    {
      "epoch": 1.5868799999999998,
      "grad_norm": 0.006737112067639828,
      "learning_rate": 9.421013333333334e-06,
      "loss": 0.0015,
      "step": 49590
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.004461660515516996,
      "learning_rate": 9.418880000000001e-06,
      "loss": 0.0003,
      "step": 49600
    },
    {
      "epoch": 1.58752,
      "grad_norm": 0.009131340309977531,
      "learning_rate": 9.416746666666668e-06,
      "loss": 0.0041,
      "step": 49610
    },
    {
      "epoch": 1.58784,
      "grad_norm": 0.0033190359827131033,
      "learning_rate": 9.414613333333335e-06,
      "loss": 0.0016,
      "step": 49620
    },
    {
      "epoch": 1.58816,
      "grad_norm": 0.00732787698507309,
      "learning_rate": 9.41248e-06,
      "loss": 0.0006,
      "step": 49630
    },
    {
      "epoch": 1.5884800000000001,
      "grad_norm": 0.00607072189450264,
      "learning_rate": 9.410346666666667e-06,
      "loss": 0.0005,
      "step": 49640
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.01144336350262165,
      "learning_rate": 9.408213333333333e-06,
      "loss": 0.0283,
      "step": 49650
    },
    {
      "epoch": 1.5891199999999999,
      "grad_norm": 0.012539252638816833,
      "learning_rate": 9.406080000000002e-06,
      "loss": 0.0003,
      "step": 49660
    },
    {
      "epoch": 1.58944,
      "grad_norm": 0.004197244998067617,
      "learning_rate": 9.403946666666669e-06,
      "loss": 0.0003,
      "step": 49670
    },
    {
      "epoch": 1.58976,
      "grad_norm": 0.006150187458842993,
      "learning_rate": 9.401813333333334e-06,
      "loss": 0.0008,
      "step": 49680
    },
    {
      "epoch": 1.59008,
      "grad_norm": 0.04015793651342392,
      "learning_rate": 9.399680000000001e-06,
      "loss": 0.0042,
      "step": 49690
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.00841351505368948,
      "learning_rate": 9.397546666666666e-06,
      "loss": 0.0003,
      "step": 49700
    },
    {
      "epoch": 1.5907200000000001,
      "grad_norm": 0.007509298622608185,
      "learning_rate": 9.395413333333334e-06,
      "loss": 0.0003,
      "step": 49710
    },
    {
      "epoch": 1.59104,
      "grad_norm": 0.014722219668328762,
      "learning_rate": 9.39328e-06,
      "loss": 0.0009,
      "step": 49720
    },
    {
      "epoch": 1.5913599999999999,
      "grad_norm": 0.007099074777215719,
      "learning_rate": 9.391146666666668e-06,
      "loss": 0.0003,
      "step": 49730
    },
    {
      "epoch": 1.59168,
      "grad_norm": 0.004983975552022457,
      "learning_rate": 9.389013333333335e-06,
      "loss": 0.0003,
      "step": 49740
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.0041550081223249435,
      "learning_rate": 9.38688e-06,
      "loss": 0.0002,
      "step": 49750
    },
    {
      "epoch": 1.59232,
      "grad_norm": 0.010577746666967869,
      "learning_rate": 9.384746666666667e-06,
      "loss": 0.0003,
      "step": 49760
    },
    {
      "epoch": 1.5926399999999998,
      "grad_norm": 0.007058390881866217,
      "learning_rate": 9.382613333333334e-06,
      "loss": 0.0003,
      "step": 49770
    },
    {
      "epoch": 1.5929600000000002,
      "grad_norm": 0.010229429230093956,
      "learning_rate": 9.380480000000001e-06,
      "loss": 0.0002,
      "step": 49780
    },
    {
      "epoch": 1.59328,
      "grad_norm": 0.004341316409409046,
      "learning_rate": 9.378346666666668e-06,
      "loss": 0.0003,
      "step": 49790
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.005694200750440359,
      "learning_rate": 9.376213333333334e-06,
      "loss": 0.0002,
      "step": 49800
    },
    {
      "epoch": 1.59392,
      "grad_norm": 0.004818145185709,
      "learning_rate": 9.374080000000001e-06,
      "loss": 0.0003,
      "step": 49810
    },
    {
      "epoch": 1.59424,
      "grad_norm": 0.008615406230092049,
      "learning_rate": 9.371946666666666e-06,
      "loss": 0.0005,
      "step": 49820
    },
    {
      "epoch": 1.59456,
      "grad_norm": 0.002054512267932296,
      "learning_rate": 9.369813333333333e-06,
      "loss": 0.0003,
      "step": 49830
    },
    {
      "epoch": 1.5948799999999999,
      "grad_norm": 0.006656952202320099,
      "learning_rate": 9.36768e-06,
      "loss": 0.0003,
      "step": 49840
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.007498031947761774,
      "learning_rate": 9.365546666666667e-06,
      "loss": 0.0003,
      "step": 49850
    },
    {
      "epoch": 1.59552,
      "grad_norm": 0.004752556327730417,
      "learning_rate": 9.363413333333335e-06,
      "loss": 0.0261,
      "step": 49860
    },
    {
      "epoch": 1.59584,
      "grad_norm": 0.005942919757217169,
      "learning_rate": 9.36128e-06,
      "loss": 0.0006,
      "step": 49870
    },
    {
      "epoch": 1.59616,
      "grad_norm": 0.0034343537408858538,
      "learning_rate": 9.359146666666667e-06,
      "loss": 0.0002,
      "step": 49880
    },
    {
      "epoch": 1.5964800000000001,
      "grad_norm": 0.012922827154397964,
      "learning_rate": 9.357013333333334e-06,
      "loss": 0.0003,
      "step": 49890
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.023606430739164352,
      "learning_rate": 9.354880000000001e-06,
      "loss": 0.0004,
      "step": 49900
    },
    {
      "epoch": 1.5971199999999999,
      "grad_norm": 0.0029135793447494507,
      "learning_rate": 9.352746666666668e-06,
      "loss": 0.0002,
      "step": 49910
    },
    {
      "epoch": 1.59744,
      "grad_norm": 0.0024696013424545527,
      "learning_rate": 9.350613333333334e-06,
      "loss": 0.0239,
      "step": 49920
    },
    {
      "epoch": 1.59776,
      "grad_norm": 0.013321571052074432,
      "learning_rate": 9.34848e-06,
      "loss": 0.0044,
      "step": 49930
    },
    {
      "epoch": 1.59808,
      "grad_norm": 0.0017687393119558692,
      "learning_rate": 9.346346666666668e-06,
      "loss": 0.0003,
      "step": 49940
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.007799516431987286,
      "learning_rate": 9.344213333333335e-06,
      "loss": 0.0022,
      "step": 49950
    },
    {
      "epoch": 1.5987200000000001,
      "grad_norm": 0.09904240071773529,
      "learning_rate": 9.342080000000002e-06,
      "loss": 0.0004,
      "step": 49960
    },
    {
      "epoch": 1.59904,
      "grad_norm": 0.0026900451630353928,
      "learning_rate": 9.339946666666667e-06,
      "loss": 0.0002,
      "step": 49970
    },
    {
      "epoch": 1.59936,
      "grad_norm": 0.006601002532988787,
      "learning_rate": 9.337813333333334e-06,
      "loss": 0.0024,
      "step": 49980
    },
    {
      "epoch": 1.59968,
      "grad_norm": 0.0032906087581068277,
      "learning_rate": 9.33568e-06,
      "loss": 0.0004,
      "step": 49990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.0027347353752702475,
      "learning_rate": 9.333546666666667e-06,
      "loss": 0.0002,
      "step": 50000
    },
    {
      "epoch": 1.60032,
      "grad_norm": 0.001571441302075982,
      "learning_rate": 9.331413333333334e-06,
      "loss": 0.0006,
      "step": 50010
    },
    {
      "epoch": 1.6006399999999998,
      "grad_norm": 0.004182994365692139,
      "learning_rate": 9.329280000000001e-06,
      "loss": 0.0499,
      "step": 50020
    },
    {
      "epoch": 1.6009600000000002,
      "grad_norm": 0.0036357110366225243,
      "learning_rate": 9.327146666666668e-06,
      "loss": 0.0004,
      "step": 50030
    },
    {
      "epoch": 1.60128,
      "grad_norm": 0.0040373895317316055,
      "learning_rate": 9.325013333333333e-06,
      "loss": 0.0193,
      "step": 50040
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.011899866163730621,
      "learning_rate": 9.32288e-06,
      "loss": 0.023,
      "step": 50050
    },
    {
      "epoch": 1.60192,
      "grad_norm": 0.002523620380088687,
      "learning_rate": 9.320746666666668e-06,
      "loss": 0.0004,
      "step": 50060
    },
    {
      "epoch": 1.60224,
      "grad_norm": 0.015405071899294853,
      "learning_rate": 9.318613333333335e-06,
      "loss": 0.0003,
      "step": 50070
    },
    {
      "epoch": 1.60256,
      "grad_norm": 0.014017965644598007,
      "learning_rate": 9.316480000000002e-06,
      "loss": 0.0002,
      "step": 50080
    },
    {
      "epoch": 1.6028799999999999,
      "grad_norm": 0.003804656909778714,
      "learning_rate": 9.314346666666667e-06,
      "loss": 0.0002,
      "step": 50090
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.0011197335552424192,
      "learning_rate": 9.312213333333334e-06,
      "loss": 0.0432,
      "step": 50100
    },
    {
      "epoch": 1.60352,
      "grad_norm": 0.004297128412872553,
      "learning_rate": 9.31008e-06,
      "loss": 0.05,
      "step": 50110
    },
    {
      "epoch": 1.60384,
      "grad_norm": 1.2585586309432983,
      "learning_rate": 9.307946666666667e-06,
      "loss": 0.002,
      "step": 50120
    },
    {
      "epoch": 1.60416,
      "grad_norm": 0.07604244351387024,
      "learning_rate": 9.305813333333335e-06,
      "loss": 0.1015,
      "step": 50130
    },
    {
      "epoch": 1.6044800000000001,
      "grad_norm": 0.010778605937957764,
      "learning_rate": 9.30368e-06,
      "loss": 0.0005,
      "step": 50140
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.0046208580024540424,
      "learning_rate": 9.301546666666668e-06,
      "loss": 0.0003,
      "step": 50150
    },
    {
      "epoch": 1.6051199999999999,
      "grad_norm": 0.0032123797573149204,
      "learning_rate": 9.299413333333333e-06,
      "loss": 0.001,
      "step": 50160
    },
    {
      "epoch": 1.60544,
      "grad_norm": 0.006049686577171087,
      "learning_rate": 9.29728e-06,
      "loss": 0.0282,
      "step": 50170
    },
    {
      "epoch": 1.60576,
      "grad_norm": 6.789313793182373,
      "learning_rate": 9.295146666666667e-06,
      "loss": 0.0453,
      "step": 50180
    },
    {
      "epoch": 1.60608,
      "grad_norm": 0.0063527547754347324,
      "learning_rate": 9.293013333333334e-06,
      "loss": 0.0004,
      "step": 50190
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.003517135512083769,
      "learning_rate": 9.290880000000001e-06,
      "loss": 0.0006,
      "step": 50200
    },
    {
      "epoch": 1.6067200000000001,
      "grad_norm": 0.0074096801690757275,
      "learning_rate": 9.288746666666667e-06,
      "loss": 0.0402,
      "step": 50210
    },
    {
      "epoch": 1.60704,
      "grad_norm": 0.017055563628673553,
      "learning_rate": 9.286613333333334e-06,
      "loss": 0.0004,
      "step": 50220
    },
    {
      "epoch": 1.60736,
      "grad_norm": 0.007904012687504292,
      "learning_rate": 9.284480000000001e-06,
      "loss": 0.0005,
      "step": 50230
    },
    {
      "epoch": 1.60768,
      "grad_norm": 0.0065903677605092525,
      "learning_rate": 9.282346666666668e-06,
      "loss": 0.0003,
      "step": 50240
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.014363243244588375,
      "learning_rate": 9.280213333333335e-06,
      "loss": 0.0003,
      "step": 50250
    },
    {
      "epoch": 1.60832,
      "grad_norm": 0.012831874191761017,
      "learning_rate": 9.27808e-06,
      "loss": 0.0065,
      "step": 50260
    },
    {
      "epoch": 1.6086399999999998,
      "grad_norm": 0.007763016037642956,
      "learning_rate": 9.275946666666668e-06,
      "loss": 0.0003,
      "step": 50270
    },
    {
      "epoch": 1.60896,
      "grad_norm": 0.00550864776596427,
      "learning_rate": 9.273813333333333e-06,
      "loss": 0.0004,
      "step": 50280
    },
    {
      "epoch": 1.60928,
      "grad_norm": 0.0033811330795288086,
      "learning_rate": 9.27168e-06,
      "loss": 0.0003,
      "step": 50290
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.0036645017098635435,
      "learning_rate": 9.269546666666667e-06,
      "loss": 0.0002,
      "step": 50300
    },
    {
      "epoch": 1.60992,
      "grad_norm": 0.003968638833612204,
      "learning_rate": 9.267413333333334e-06,
      "loss": 0.0006,
      "step": 50310
    },
    {
      "epoch": 1.6102400000000001,
      "grad_norm": 0.0076482961885631084,
      "learning_rate": 9.265280000000001e-06,
      "loss": 0.0003,
      "step": 50320
    },
    {
      "epoch": 1.61056,
      "grad_norm": 0.004214570391923189,
      "learning_rate": 9.263146666666667e-06,
      "loss": 0.0004,
      "step": 50330
    },
    {
      "epoch": 1.6108799999999999,
      "grad_norm": 0.004392419941723347,
      "learning_rate": 9.261013333333334e-06,
      "loss": 0.0156,
      "step": 50340
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.004186663776636124,
      "learning_rate": 9.25888e-06,
      "loss": 0.0004,
      "step": 50350
    },
    {
      "epoch": 1.61152,
      "grad_norm": 0.005289599299430847,
      "learning_rate": 9.256746666666668e-06,
      "loss": 0.0366,
      "step": 50360
    },
    {
      "epoch": 1.61184,
      "grad_norm": 0.007996814325451851,
      "learning_rate": 9.254613333333335e-06,
      "loss": 0.0004,
      "step": 50370
    },
    {
      "epoch": 1.61216,
      "grad_norm": 0.002861288608983159,
      "learning_rate": 9.25248e-06,
      "loss": 0.001,
      "step": 50380
    },
    {
      "epoch": 1.6124800000000001,
      "grad_norm": 0.0029108189046382904,
      "learning_rate": 9.250346666666667e-06,
      "loss": 0.0002,
      "step": 50390
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.005060795694589615,
      "learning_rate": 9.248213333333334e-06,
      "loss": 0.041,
      "step": 50400
    },
    {
      "epoch": 1.6131199999999999,
      "grad_norm": 0.005645361263304949,
      "learning_rate": 9.246080000000001e-06,
      "loss": 0.0035,
      "step": 50410
    },
    {
      "epoch": 1.61344,
      "grad_norm": 0.005037371534854174,
      "learning_rate": 9.243946666666669e-06,
      "loss": 0.051,
      "step": 50420
    },
    {
      "epoch": 1.61376,
      "grad_norm": 0.0026477405335754156,
      "learning_rate": 9.241813333333334e-06,
      "loss": 0.0023,
      "step": 50430
    },
    {
      "epoch": 1.61408,
      "grad_norm": 0.36693301796913147,
      "learning_rate": 9.239680000000001e-06,
      "loss": 0.0176,
      "step": 50440
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.006511158775538206,
      "learning_rate": 9.237546666666666e-06,
      "loss": 0.0003,
      "step": 50450
    },
    {
      "epoch": 1.6147200000000002,
      "grad_norm": 0.01161691639572382,
      "learning_rate": 9.235413333333333e-06,
      "loss": 0.0002,
      "step": 50460
    },
    {
      "epoch": 1.61504,
      "grad_norm": 0.005993950180709362,
      "learning_rate": 9.23328e-06,
      "loss": 0.0236,
      "step": 50470
    },
    {
      "epoch": 1.61536,
      "grad_norm": 0.005355425179004669,
      "learning_rate": 9.231146666666668e-06,
      "loss": 0.0004,
      "step": 50480
    },
    {
      "epoch": 1.61568,
      "grad_norm": 0.1777428388595581,
      "learning_rate": 9.229013333333335e-06,
      "loss": 0.0006,
      "step": 50490
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.005063620861619711,
      "learning_rate": 9.22688e-06,
      "loss": 0.0488,
      "step": 50500
    },
    {
      "epoch": 1.61632,
      "grad_norm": 0.06449360400438309,
      "learning_rate": 9.224746666666667e-06,
      "loss": 0.0006,
      "step": 50510
    },
    {
      "epoch": 1.6166399999999999,
      "grad_norm": 0.004464436788111925,
      "learning_rate": 9.222613333333334e-06,
      "loss": 0.0453,
      "step": 50520
    },
    {
      "epoch": 1.61696,
      "grad_norm": 0.006238958332687616,
      "learning_rate": 9.220480000000001e-06,
      "loss": 0.0005,
      "step": 50530
    },
    {
      "epoch": 1.61728,
      "grad_norm": 0.005054418463259935,
      "learning_rate": 9.218346666666668e-06,
      "loss": 0.0003,
      "step": 50540
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.09014362841844559,
      "learning_rate": 9.216213333333334e-06,
      "loss": 0.0005,
      "step": 50550
    },
    {
      "epoch": 1.61792,
      "grad_norm": 0.009301683865487576,
      "learning_rate": 9.21408e-06,
      "loss": 0.0149,
      "step": 50560
    },
    {
      "epoch": 1.6182400000000001,
      "grad_norm": 0.015229077078402042,
      "learning_rate": 9.211946666666668e-06,
      "loss": 0.0006,
      "step": 50570
    },
    {
      "epoch": 1.61856,
      "grad_norm": 0.014757448807358742,
      "learning_rate": 9.209813333333333e-06,
      "loss": 0.0003,
      "step": 50580
    },
    {
      "epoch": 1.6188799999999999,
      "grad_norm": 0.007508210372179747,
      "learning_rate": 9.20768e-06,
      "loss": 0.0005,
      "step": 50590
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.011764135211706161,
      "learning_rate": 9.205546666666667e-06,
      "loss": 0.0006,
      "step": 50600
    },
    {
      "epoch": 1.61952,
      "grad_norm": 0.008340463973581791,
      "learning_rate": 9.203413333333334e-06,
      "loss": 0.0008,
      "step": 50610
    },
    {
      "epoch": 1.61984,
      "grad_norm": 0.10594548285007477,
      "learning_rate": 9.20128e-06,
      "loss": 0.0011,
      "step": 50620
    },
    {
      "epoch": 1.62016,
      "grad_norm": 1.415873646736145,
      "learning_rate": 9.199146666666667e-06,
      "loss": 0.0013,
      "step": 50630
    },
    {
      "epoch": 1.6204800000000001,
      "grad_norm": 0.1768413782119751,
      "learning_rate": 9.197013333333334e-06,
      "loss": 0.0015,
      "step": 50640
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.01130601204931736,
      "learning_rate": 9.194880000000001e-06,
      "loss": 0.0003,
      "step": 50650
    },
    {
      "epoch": 1.62112,
      "grad_norm": 0.004947936162352562,
      "learning_rate": 9.192746666666668e-06,
      "loss": 0.0003,
      "step": 50660
    },
    {
      "epoch": 1.62144,
      "grad_norm": 0.006369834765791893,
      "learning_rate": 9.190613333333333e-06,
      "loss": 0.0003,
      "step": 50670
    },
    {
      "epoch": 1.62176,
      "grad_norm": 2.996471405029297,
      "learning_rate": 9.18848e-06,
      "loss": 0.0742,
      "step": 50680
    },
    {
      "epoch": 1.62208,
      "grad_norm": 0.00538435485213995,
      "learning_rate": 9.186346666666668e-06,
      "loss": 0.0419,
      "step": 50690
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.04304550588130951,
      "learning_rate": 9.184213333333335e-06,
      "loss": 0.0005,
      "step": 50700
    },
    {
      "epoch": 1.6227200000000002,
      "grad_norm": 0.00796758383512497,
      "learning_rate": 9.182080000000002e-06,
      "loss": 0.0009,
      "step": 50710
    },
    {
      "epoch": 1.62304,
      "grad_norm": 0.004111196380108595,
      "learning_rate": 9.179946666666667e-06,
      "loss": 0.0004,
      "step": 50720
    },
    {
      "epoch": 1.62336,
      "grad_norm": 0.012819857336580753,
      "learning_rate": 9.177813333333334e-06,
      "loss": 0.0073,
      "step": 50730
    },
    {
      "epoch": 1.62368,
      "grad_norm": 0.00759848952293396,
      "learning_rate": 9.175680000000001e-06,
      "loss": 0.0004,
      "step": 50740
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.006688481196761131,
      "learning_rate": 9.173546666666667e-06,
      "loss": 0.0004,
      "step": 50750
    },
    {
      "epoch": 1.62432,
      "grad_norm": 0.006325472146272659,
      "learning_rate": 9.171413333333334e-06,
      "loss": 0.0006,
      "step": 50760
    },
    {
      "epoch": 1.6246399999999999,
      "grad_norm": 0.007473169360309839,
      "learning_rate": 9.16928e-06,
      "loss": 0.0053,
      "step": 50770
    },
    {
      "epoch": 1.62496,
      "grad_norm": 0.012475527822971344,
      "learning_rate": 9.167146666666668e-06,
      "loss": 0.0006,
      "step": 50780
    },
    {
      "epoch": 1.62528,
      "grad_norm": 0.07285425812005997,
      "learning_rate": 9.165013333333333e-06,
      "loss": 0.0006,
      "step": 50790
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.005861018318682909,
      "learning_rate": 9.16288e-06,
      "loss": 0.0002,
      "step": 50800
    },
    {
      "epoch": 1.62592,
      "grad_norm": 0.005528859794139862,
      "learning_rate": 9.160746666666667e-06,
      "loss": 0.0004,
      "step": 50810
    },
    {
      "epoch": 1.6262400000000001,
      "grad_norm": 0.00878608413040638,
      "learning_rate": 9.158613333333334e-06,
      "loss": 0.0003,
      "step": 50820
    },
    {
      "epoch": 1.62656,
      "grad_norm": 0.004644364584237337,
      "learning_rate": 9.156480000000001e-06,
      "loss": 0.0307,
      "step": 50830
    },
    {
      "epoch": 1.6268799999999999,
      "grad_norm": 0.005822355393320322,
      "learning_rate": 9.154346666666667e-06,
      "loss": 0.0003,
      "step": 50840
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.006602651905268431,
      "learning_rate": 9.152213333333334e-06,
      "loss": 0.0004,
      "step": 50850
    },
    {
      "epoch": 1.62752,
      "grad_norm": 0.023068001493811607,
      "learning_rate": 9.150080000000001e-06,
      "loss": 0.0004,
      "step": 50860
    },
    {
      "epoch": 1.62784,
      "grad_norm": 0.005748142022639513,
      "learning_rate": 9.147946666666666e-06,
      "loss": 0.0007,
      "step": 50870
    },
    {
      "epoch": 1.62816,
      "grad_norm": 0.004603190813213587,
      "learning_rate": 9.145813333333335e-06,
      "loss": 0.0004,
      "step": 50880
    },
    {
      "epoch": 1.6284800000000001,
      "grad_norm": 0.007221858482807875,
      "learning_rate": 9.14368e-06,
      "loss": 0.0005,
      "step": 50890
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.04294063523411751,
      "learning_rate": 9.141546666666668e-06,
      "loss": 0.0314,
      "step": 50900
    },
    {
      "epoch": 1.62912,
      "grad_norm": 0.007698382250964642,
      "learning_rate": 9.139413333333335e-06,
      "loss": 0.002,
      "step": 50910
    },
    {
      "epoch": 1.62944,
      "grad_norm": 0.01270626112818718,
      "learning_rate": 9.13728e-06,
      "loss": 0.0003,
      "step": 50920
    },
    {
      "epoch": 1.62976,
      "grad_norm": 0.014301477931439877,
      "learning_rate": 9.135146666666667e-06,
      "loss": 0.0003,
      "step": 50930
    },
    {
      "epoch": 1.63008,
      "grad_norm": 0.016306914389133453,
      "learning_rate": 9.133013333333334e-06,
      "loss": 0.0317,
      "step": 50940
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.004082550760358572,
      "learning_rate": 9.130880000000001e-06,
      "loss": 0.0002,
      "step": 50950
    },
    {
      "epoch": 1.63072,
      "grad_norm": 0.005035142879933119,
      "learning_rate": 9.128746666666667e-06,
      "loss": 0.0004,
      "step": 50960
    },
    {
      "epoch": 1.63104,
      "grad_norm": 0.0034520195331424475,
      "learning_rate": 9.126613333333334e-06,
      "loss": 0.0339,
      "step": 50970
    },
    {
      "epoch": 1.63136,
      "grad_norm": 0.006351271644234657,
      "learning_rate": 9.12448e-06,
      "loss": 0.0004,
      "step": 50980
    },
    {
      "epoch": 1.63168,
      "grad_norm": 0.009227528236806393,
      "learning_rate": 9.122346666666668e-06,
      "loss": 0.0073,
      "step": 50990
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.005072975531220436,
      "learning_rate": 9.120213333333335e-06,
      "loss": 0.0002,
      "step": 51000
    },
    {
      "epoch": 1.63232,
      "grad_norm": 0.004631721414625645,
      "learning_rate": 9.11808e-06,
      "loss": 0.0003,
      "step": 51010
    },
    {
      "epoch": 1.6326399999999999,
      "grad_norm": 0.012440231628715992,
      "learning_rate": 9.115946666666667e-06,
      "loss": 0.0005,
      "step": 51020
    },
    {
      "epoch": 1.63296,
      "grad_norm": 0.0022515361197292805,
      "learning_rate": 9.113813333333334e-06,
      "loss": 0.0005,
      "step": 51030
    },
    {
      "epoch": 1.63328,
      "grad_norm": 0.004066010005772114,
      "learning_rate": 9.11168e-06,
      "loss": 0.0002,
      "step": 51040
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.08552876859903336,
      "learning_rate": 9.109546666666667e-06,
      "loss": 0.0015,
      "step": 51050
    },
    {
      "epoch": 1.63392,
      "grad_norm": 0.004289029166102409,
      "learning_rate": 9.107413333333334e-06,
      "loss": 0.0002,
      "step": 51060
    },
    {
      "epoch": 1.6342400000000001,
      "grad_norm": 0.004241451155394316,
      "learning_rate": 9.105280000000001e-06,
      "loss": 0.0249,
      "step": 51070
    },
    {
      "epoch": 1.63456,
      "grad_norm": 0.006370909977704287,
      "learning_rate": 9.103146666666668e-06,
      "loss": 0.001,
      "step": 51080
    },
    {
      "epoch": 1.6348799999999999,
      "grad_norm": 0.0037893163971602917,
      "learning_rate": 9.101013333333333e-06,
      "loss": 0.0009,
      "step": 51090
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.003444439498707652,
      "learning_rate": 9.09888e-06,
      "loss": 0.0003,
      "step": 51100
    },
    {
      "epoch": 1.63552,
      "grad_norm": 0.003299736650660634,
      "learning_rate": 9.096746666666668e-06,
      "loss": 0.0002,
      "step": 51110
    },
    {
      "epoch": 1.63584,
      "grad_norm": 0.005147809162735939,
      "learning_rate": 9.094613333333335e-06,
      "loss": 0.0002,
      "step": 51120
    },
    {
      "epoch": 1.6361599999999998,
      "grad_norm": 0.007641392294317484,
      "learning_rate": 9.09248e-06,
      "loss": 0.0005,
      "step": 51130
    },
    {
      "epoch": 1.6364800000000002,
      "grad_norm": 0.005805543158203363,
      "learning_rate": 9.090346666666667e-06,
      "loss": 0.0006,
      "step": 51140
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.008534579537808895,
      "learning_rate": 9.088213333333334e-06,
      "loss": 0.0753,
      "step": 51150
    },
    {
      "epoch": 1.63712,
      "grad_norm": 0.0020487115252763033,
      "learning_rate": 9.086080000000001e-06,
      "loss": 0.0343,
      "step": 51160
    },
    {
      "epoch": 1.63744,
      "grad_norm": 0.0080187963321805,
      "learning_rate": 9.083946666666668e-06,
      "loss": 0.0003,
      "step": 51170
    },
    {
      "epoch": 1.63776,
      "grad_norm": 0.04157732054591179,
      "learning_rate": 9.081813333333334e-06,
      "loss": 0.0003,
      "step": 51180
    },
    {
      "epoch": 1.63808,
      "grad_norm": 0.0029730687383562326,
      "learning_rate": 9.07968e-06,
      "loss": 0.0002,
      "step": 51190
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.004998812917619944,
      "learning_rate": 9.077546666666668e-06,
      "loss": 0.0003,
      "step": 51200
    },
    {
      "epoch": 1.63872,
      "grad_norm": 0.02266271971166134,
      "learning_rate": 9.075413333333333e-06,
      "loss": 0.0005,
      "step": 51210
    },
    {
      "epoch": 1.63904,
      "grad_norm": 0.006166486535221338,
      "learning_rate": 9.07328e-06,
      "loss": 0.0007,
      "step": 51220
    },
    {
      "epoch": 1.63936,
      "grad_norm": 0.3588607609272003,
      "learning_rate": 9.071146666666667e-06,
      "loss": 0.0006,
      "step": 51230
    },
    {
      "epoch": 1.63968,
      "grad_norm": 0.10326341539621353,
      "learning_rate": 9.069013333333334e-06,
      "loss": 0.0533,
      "step": 51240
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.008848782628774643,
      "learning_rate": 9.066880000000002e-06,
      "loss": 0.0002,
      "step": 51250
    },
    {
      "epoch": 1.64032,
      "grad_norm": 0.0034378760028630495,
      "learning_rate": 9.064746666666667e-06,
      "loss": 0.0003,
      "step": 51260
    },
    {
      "epoch": 1.6406399999999999,
      "grad_norm": 0.013694746419787407,
      "learning_rate": 9.062613333333334e-06,
      "loss": 0.0469,
      "step": 51270
    },
    {
      "epoch": 1.64096,
      "grad_norm": 0.005306916777044535,
      "learning_rate": 9.060480000000001e-06,
      "loss": 0.0496,
      "step": 51280
    },
    {
      "epoch": 1.64128,
      "grad_norm": 0.007143331225961447,
      "learning_rate": 9.058346666666668e-06,
      "loss": 0.0016,
      "step": 51290
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.005090522114187479,
      "learning_rate": 9.056213333333333e-06,
      "loss": 0.0003,
      "step": 51300
    },
    {
      "epoch": 1.64192,
      "grad_norm": 0.004494727589190006,
      "learning_rate": 9.05408e-06,
      "loss": 0.001,
      "step": 51310
    },
    {
      "epoch": 1.6422400000000001,
      "grad_norm": 0.01117048878222704,
      "learning_rate": 9.051946666666668e-06,
      "loss": 0.0034,
      "step": 51320
    },
    {
      "epoch": 1.64256,
      "grad_norm": 0.00331888091750443,
      "learning_rate": 9.049813333333333e-06,
      "loss": 0.0003,
      "step": 51330
    },
    {
      "epoch": 1.64288,
      "grad_norm": 0.005178038962185383,
      "learning_rate": 9.04768e-06,
      "loss": 0.0004,
      "step": 51340
    },
    {
      "epoch": 1.6432,
      "grad_norm": 1.612131118774414,
      "learning_rate": 9.045546666666667e-06,
      "loss": 0.0019,
      "step": 51350
    },
    {
      "epoch": 1.64352,
      "grad_norm": 0.01037596445530653,
      "learning_rate": 9.043413333333334e-06,
      "loss": 0.0003,
      "step": 51360
    },
    {
      "epoch": 1.64384,
      "grad_norm": 0.014154129661619663,
      "learning_rate": 9.041280000000001e-06,
      "loss": 0.0003,
      "step": 51370
    },
    {
      "epoch": 1.6441599999999998,
      "grad_norm": 0.19692642986774445,
      "learning_rate": 9.039146666666667e-06,
      "loss": 0.0006,
      "step": 51380
    },
    {
      "epoch": 1.6444800000000002,
      "grad_norm": 0.008969355374574661,
      "learning_rate": 9.037013333333334e-06,
      "loss": 0.0003,
      "step": 51390
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.0026322989724576473,
      "learning_rate": 9.03488e-06,
      "loss": 0.0547,
      "step": 51400
    },
    {
      "epoch": 1.64512,
      "grad_norm": 0.004658760502934456,
      "learning_rate": 9.032746666666668e-06,
      "loss": 0.0003,
      "step": 51410
    },
    {
      "epoch": 1.64544,
      "grad_norm": 0.0031646958086639643,
      "learning_rate": 9.030613333333335e-06,
      "loss": 0.0008,
      "step": 51420
    },
    {
      "epoch": 1.6457600000000001,
      "grad_norm": 0.005750543903559446,
      "learning_rate": 9.02848e-06,
      "loss": 0.0002,
      "step": 51430
    },
    {
      "epoch": 1.64608,
      "grad_norm": 0.004563725087791681,
      "learning_rate": 9.026346666666667e-06,
      "loss": 0.0389,
      "step": 51440
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.008591747842729092,
      "learning_rate": 9.024213333333334e-06,
      "loss": 0.0003,
      "step": 51450
    },
    {
      "epoch": 1.64672,
      "grad_norm": 0.0045519801788032055,
      "learning_rate": 9.022080000000002e-06,
      "loss": 0.0002,
      "step": 51460
    },
    {
      "epoch": 1.64704,
      "grad_norm": 0.024806156754493713,
      "learning_rate": 9.019946666666667e-06,
      "loss": 0.0003,
      "step": 51470
    },
    {
      "epoch": 1.64736,
      "grad_norm": 0.007801741827279329,
      "learning_rate": 9.017813333333334e-06,
      "loss": 0.0003,
      "step": 51480
    },
    {
      "epoch": 1.64768,
      "grad_norm": 0.00445999950170517,
      "learning_rate": 9.015680000000001e-06,
      "loss": 0.0014,
      "step": 51490
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.011233359575271606,
      "learning_rate": 9.013546666666666e-06,
      "loss": 0.0002,
      "step": 51500
    },
    {
      "epoch": 1.64832,
      "grad_norm": 0.015787474811077118,
      "learning_rate": 9.011413333333334e-06,
      "loss": 0.0003,
      "step": 51510
    },
    {
      "epoch": 1.6486399999999999,
      "grad_norm": 0.0013917204923927784,
      "learning_rate": 9.00928e-06,
      "loss": 0.0519,
      "step": 51520
    },
    {
      "epoch": 1.64896,
      "grad_norm": 0.004354425240308046,
      "learning_rate": 9.007146666666668e-06,
      "loss": 0.0005,
      "step": 51530
    },
    {
      "epoch": 1.64928,
      "grad_norm": 0.003934894222766161,
      "learning_rate": 9.005013333333335e-06,
      "loss": 0.0113,
      "step": 51540
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.004419237840920687,
      "learning_rate": 9.00288e-06,
      "loss": 0.0003,
      "step": 51550
    },
    {
      "epoch": 1.64992,
      "grad_norm": 2.1932015419006348,
      "learning_rate": 9.000746666666667e-06,
      "loss": 0.0404,
      "step": 51560
    },
    {
      "epoch": 1.6502400000000002,
      "grad_norm": 0.008349996991455555,
      "learning_rate": 8.998613333333334e-06,
      "loss": 0.0003,
      "step": 51570
    },
    {
      "epoch": 1.65056,
      "grad_norm": 0.009198527783155441,
      "learning_rate": 8.996480000000001e-06,
      "loss": 0.0003,
      "step": 51580
    },
    {
      "epoch": 1.65088,
      "grad_norm": 0.0038345560897141695,
      "learning_rate": 8.994346666666668e-06,
      "loss": 0.0003,
      "step": 51590
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.011718080379068851,
      "learning_rate": 8.992213333333334e-06,
      "loss": 0.0004,
      "step": 51600
    },
    {
      "epoch": 1.65152,
      "grad_norm": 0.004242262803018093,
      "learning_rate": 8.99008e-06,
      "loss": 0.0005,
      "step": 51610
    },
    {
      "epoch": 1.65184,
      "grad_norm": 0.019661394879221916,
      "learning_rate": 8.987946666666666e-06,
      "loss": 0.0148,
      "step": 51620
    },
    {
      "epoch": 1.6521599999999999,
      "grad_norm": 0.004549134057015181,
      "learning_rate": 8.985813333333335e-06,
      "loss": 0.0079,
      "step": 51630
    },
    {
      "epoch": 1.65248,
      "grad_norm": 0.014864365570247173,
      "learning_rate": 8.983680000000002e-06,
      "loss": 0.0004,
      "step": 51640
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.008489295840263367,
      "learning_rate": 8.981546666666667e-06,
      "loss": 0.0003,
      "step": 51650
    },
    {
      "epoch": 1.65312,
      "grad_norm": 0.0028755315579473972,
      "learning_rate": 8.979413333333334e-06,
      "loss": 0.0004,
      "step": 51660
    },
    {
      "epoch": 1.65344,
      "grad_norm": 0.009302394464612007,
      "learning_rate": 8.97728e-06,
      "loss": 0.0002,
      "step": 51670
    },
    {
      "epoch": 1.6537600000000001,
      "grad_norm": 0.0032727320212870836,
      "learning_rate": 8.975146666666667e-06,
      "loss": 0.0002,
      "step": 51680
    },
    {
      "epoch": 1.65408,
      "grad_norm": 0.019692983478307724,
      "learning_rate": 8.973013333333334e-06,
      "loss": 0.0507,
      "step": 51690
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.04828980192542076,
      "learning_rate": 8.970880000000001e-06,
      "loss": 0.0193,
      "step": 51700
    },
    {
      "epoch": 1.65472,
      "grad_norm": 0.016587261110544205,
      "learning_rate": 8.968746666666668e-06,
      "loss": 0.0005,
      "step": 51710
    },
    {
      "epoch": 1.65504,
      "grad_norm": 0.0031545076053589582,
      "learning_rate": 8.966613333333334e-06,
      "loss": 0.0002,
      "step": 51720
    },
    {
      "epoch": 1.65536,
      "grad_norm": 0.0087540652602911,
      "learning_rate": 8.96448e-06,
      "loss": 0.0003,
      "step": 51730
    },
    {
      "epoch": 1.65568,
      "grad_norm": 0.005559895653277636,
      "learning_rate": 8.962346666666668e-06,
      "loss": 0.0003,
      "step": 51740
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.007506364490836859,
      "learning_rate": 8.960213333333335e-06,
      "loss": 0.0003,
      "step": 51750
    },
    {
      "epoch": 1.65632,
      "grad_norm": 0.008489979431033134,
      "learning_rate": 8.958080000000002e-06,
      "loss": 0.0012,
      "step": 51760
    },
    {
      "epoch": 1.65664,
      "grad_norm": 0.006757334806025028,
      "learning_rate": 8.955946666666667e-06,
      "loss": 0.0002,
      "step": 51770
    },
    {
      "epoch": 1.65696,
      "grad_norm": 0.0026384128723293543,
      "learning_rate": 8.953813333333334e-06,
      "loss": 0.0003,
      "step": 51780
    },
    {
      "epoch": 1.65728,
      "grad_norm": 0.00486193411052227,
      "learning_rate": 8.95168e-06,
      "loss": 0.0003,
      "step": 51790
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.004538174252957106,
      "learning_rate": 8.949546666666667e-06,
      "loss": 0.0007,
      "step": 51800
    },
    {
      "epoch": 1.6579199999999998,
      "grad_norm": 0.013166684657335281,
      "learning_rate": 8.947413333333334e-06,
      "loss": 0.0075,
      "step": 51810
    },
    {
      "epoch": 1.6582400000000002,
      "grad_norm": 0.003270697547122836,
      "learning_rate": 8.945280000000001e-06,
      "loss": 0.0003,
      "step": 51820
    },
    {
      "epoch": 1.65856,
      "grad_norm": 0.004831312224268913,
      "learning_rate": 8.943146666666668e-06,
      "loss": 0.0011,
      "step": 51830
    },
    {
      "epoch": 1.65888,
      "grad_norm": 0.0026204041205346584,
      "learning_rate": 8.941013333333333e-06,
      "loss": 0.0012,
      "step": 51840
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.004771788138896227,
      "learning_rate": 8.93888e-06,
      "loss": 0.0003,
      "step": 51850
    },
    {
      "epoch": 1.65952,
      "grad_norm": 0.01158280298113823,
      "learning_rate": 8.936746666666667e-06,
      "loss": 0.0004,
      "step": 51860
    },
    {
      "epoch": 1.65984,
      "grad_norm": 0.012001373805105686,
      "learning_rate": 8.934613333333335e-06,
      "loss": 0.0003,
      "step": 51870
    },
    {
      "epoch": 1.6601599999999999,
      "grad_norm": 0.00795136671513319,
      "learning_rate": 8.932480000000002e-06,
      "loss": 0.0005,
      "step": 51880
    },
    {
      "epoch": 1.66048,
      "grad_norm": 0.016375144943594933,
      "learning_rate": 8.930346666666667e-06,
      "loss": 0.0004,
      "step": 51890
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.011940614320337772,
      "learning_rate": 8.928213333333334e-06,
      "loss": 0.0527,
      "step": 51900
    },
    {
      "epoch": 1.66112,
      "grad_norm": 0.00539206201210618,
      "learning_rate": 8.926080000000001e-06,
      "loss": 0.0005,
      "step": 51910
    },
    {
      "epoch": 1.66144,
      "grad_norm": 0.008121082559227943,
      "learning_rate": 8.923946666666668e-06,
      "loss": 0.0003,
      "step": 51920
    },
    {
      "epoch": 1.6617600000000001,
      "grad_norm": 0.008980074897408485,
      "learning_rate": 8.921813333333335e-06,
      "loss": 0.0003,
      "step": 51930
    },
    {
      "epoch": 1.66208,
      "grad_norm": 0.02544321119785309,
      "learning_rate": 8.91968e-06,
      "loss": 0.0004,
      "step": 51940
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.002682869555428624,
      "learning_rate": 8.917546666666668e-06,
      "loss": 0.0005,
      "step": 51950
    },
    {
      "epoch": 1.66272,
      "grad_norm": 0.004498785361647606,
      "learning_rate": 8.915413333333333e-06,
      "loss": 0.0003,
      "step": 51960
    },
    {
      "epoch": 1.66304,
      "grad_norm": 0.004639197140932083,
      "learning_rate": 8.91328e-06,
      "loss": 0.0248,
      "step": 51970
    },
    {
      "epoch": 1.66336,
      "grad_norm": 0.008567195385694504,
      "learning_rate": 8.911146666666667e-06,
      "loss": 0.0003,
      "step": 51980
    },
    {
      "epoch": 1.66368,
      "grad_norm": 0.004757369868457317,
      "learning_rate": 8.909013333333334e-06,
      "loss": 0.0002,
      "step": 51990
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 2.268376588821411,
      "learning_rate": 8.906880000000001e-06,
      "loss": 0.0023,
      "step": 52000
    },
    {
      "epoch": 1.66432,
      "grad_norm": 0.0076041086576879025,
      "learning_rate": 8.904746666666667e-06,
      "loss": 0.0002,
      "step": 52010
    },
    {
      "epoch": 1.66464,
      "grad_norm": 0.004140540026128292,
      "learning_rate": 8.902613333333334e-06,
      "loss": 0.0002,
      "step": 52020
    },
    {
      "epoch": 1.66496,
      "grad_norm": 0.0039046301972121,
      "learning_rate": 8.900480000000001e-06,
      "loss": 0.0023,
      "step": 52030
    },
    {
      "epoch": 1.66528,
      "grad_norm": 0.007250163238495588,
      "learning_rate": 8.898346666666668e-06,
      "loss": 0.0065,
      "step": 52040
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.0231143981218338,
      "learning_rate": 8.896213333333335e-06,
      "loss": 0.0003,
      "step": 52050
    },
    {
      "epoch": 1.6659199999999998,
      "grad_norm": 0.0030218821484595537,
      "learning_rate": 8.89408e-06,
      "loss": 0.0002,
      "step": 52060
    },
    {
      "epoch": 1.6662400000000002,
      "grad_norm": 0.0038535038474947214,
      "learning_rate": 8.891946666666667e-06,
      "loss": 0.0003,
      "step": 52070
    },
    {
      "epoch": 1.66656,
      "grad_norm": 0.002622118918225169,
      "learning_rate": 8.889813333333333e-06,
      "loss": 0.0002,
      "step": 52080
    },
    {
      "epoch": 1.66688,
      "grad_norm": 0.004898272920399904,
      "learning_rate": 8.88768e-06,
      "loss": 0.0104,
      "step": 52090
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.0070190103724598885,
      "learning_rate": 8.885546666666669e-06,
      "loss": 0.0003,
      "step": 52100
    },
    {
      "epoch": 1.6675200000000001,
      "grad_norm": 0.010172986425459385,
      "learning_rate": 8.883413333333334e-06,
      "loss": 0.0002,
      "step": 52110
    },
    {
      "epoch": 1.66784,
      "grad_norm": 0.005180443171411753,
      "learning_rate": 8.881280000000001e-06,
      "loss": 0.0002,
      "step": 52120
    },
    {
      "epoch": 1.6681599999999999,
      "grad_norm": 0.00869526993483305,
      "learning_rate": 8.879146666666667e-06,
      "loss": 0.0003,
      "step": 52130
    },
    {
      "epoch": 1.66848,
      "grad_norm": 0.008737055584788322,
      "learning_rate": 8.877013333333334e-06,
      "loss": 0.0002,
      "step": 52140
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.8210005760192871,
      "learning_rate": 8.87488e-06,
      "loss": 0.0006,
      "step": 52150
    },
    {
      "epoch": 1.66912,
      "grad_norm": 0.004501603078097105,
      "learning_rate": 8.872746666666668e-06,
      "loss": 0.0005,
      "step": 52160
    },
    {
      "epoch": 1.66944,
      "grad_norm": 0.0030650135595351458,
      "learning_rate": 8.870613333333335e-06,
      "loss": 0.0002,
      "step": 52170
    },
    {
      "epoch": 1.6697600000000001,
      "grad_norm": 0.0042770029976964,
      "learning_rate": 8.86848e-06,
      "loss": 0.0002,
      "step": 52180
    },
    {
      "epoch": 1.67008,
      "grad_norm": 0.0041000922210514545,
      "learning_rate": 8.866346666666667e-06,
      "loss": 0.0002,
      "step": 52190
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.029818197712302208,
      "learning_rate": 8.864213333333334e-06,
      "loss": 0.0619,
      "step": 52200
    },
    {
      "epoch": 1.67072,
      "grad_norm": 0.0035036879125982523,
      "learning_rate": 8.862080000000001e-06,
      "loss": 0.0002,
      "step": 52210
    },
    {
      "epoch": 1.67104,
      "grad_norm": 0.001240332960151136,
      "learning_rate": 8.859946666666668e-06,
      "loss": 0.0003,
      "step": 52220
    },
    {
      "epoch": 1.67136,
      "grad_norm": 0.008773556910455227,
      "learning_rate": 8.857813333333334e-06,
      "loss": 0.0002,
      "step": 52230
    },
    {
      "epoch": 1.67168,
      "grad_norm": 0.002332448959350586,
      "learning_rate": 8.855680000000001e-06,
      "loss": 0.0003,
      "step": 52240
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.0039062146097421646,
      "learning_rate": 8.853546666666666e-06,
      "loss": 0.0003,
      "step": 52250
    },
    {
      "epoch": 1.67232,
      "grad_norm": 0.006571770645678043,
      "learning_rate": 8.851413333333333e-06,
      "loss": 0.0007,
      "step": 52260
    },
    {
      "epoch": 1.67264,
      "grad_norm": 0.9546639919281006,
      "learning_rate": 8.84928e-06,
      "loss": 0.0008,
      "step": 52270
    },
    {
      "epoch": 1.67296,
      "grad_norm": 0.016968196257948875,
      "learning_rate": 8.847146666666667e-06,
      "loss": 0.0002,
      "step": 52280
    },
    {
      "epoch": 1.67328,
      "grad_norm": 0.004146146588027477,
      "learning_rate": 8.845013333333335e-06,
      "loss": 0.0002,
      "step": 52290
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.005730365868657827,
      "learning_rate": 8.84288e-06,
      "loss": 0.0002,
      "step": 52300
    },
    {
      "epoch": 1.6739199999999999,
      "grad_norm": 0.008696561679244041,
      "learning_rate": 8.840746666666667e-06,
      "loss": 0.0004,
      "step": 52310
    },
    {
      "epoch": 1.67424,
      "grad_norm": 0.004126479849219322,
      "learning_rate": 8.838613333333334e-06,
      "loss": 0.0163,
      "step": 52320
    },
    {
      "epoch": 1.67456,
      "grad_norm": 0.004802039824426174,
      "learning_rate": 8.836480000000001e-06,
      "loss": 0.0006,
      "step": 52330
    },
    {
      "epoch": 1.67488,
      "grad_norm": 0.0041093723848462105,
      "learning_rate": 8.834346666666668e-06,
      "loss": 0.0002,
      "step": 52340
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.004539736546576023,
      "learning_rate": 8.832213333333334e-06,
      "loss": 0.0007,
      "step": 52350
    },
    {
      "epoch": 1.6755200000000001,
      "grad_norm": 0.004461285658180714,
      "learning_rate": 8.83008e-06,
      "loss": 0.0003,
      "step": 52360
    },
    {
      "epoch": 1.67584,
      "grad_norm": 0.43631693720817566,
      "learning_rate": 8.827946666666666e-06,
      "loss": 0.0014,
      "step": 52370
    },
    {
      "epoch": 1.6761599999999999,
      "grad_norm": 0.0018638968467712402,
      "learning_rate": 8.825813333333335e-06,
      "loss": 0.0206,
      "step": 52380
    },
    {
      "epoch": 1.67648,
      "grad_norm": 7.34559965133667,
      "learning_rate": 8.823680000000002e-06,
      "loss": 0.0268,
      "step": 52390
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.006302328314632177,
      "learning_rate": 8.821546666666667e-06,
      "loss": 0.001,
      "step": 52400
    },
    {
      "epoch": 1.67712,
      "grad_norm": 0.011186668649315834,
      "learning_rate": 8.819413333333334e-06,
      "loss": 0.0238,
      "step": 52410
    },
    {
      "epoch": 1.67744,
      "grad_norm": 0.0028711198829114437,
      "learning_rate": 8.81728e-06,
      "loss": 0.0003,
      "step": 52420
    },
    {
      "epoch": 1.6777600000000001,
      "grad_norm": 0.003243585117161274,
      "learning_rate": 8.815146666666667e-06,
      "loss": 0.0001,
      "step": 52430
    },
    {
      "epoch": 1.67808,
      "grad_norm": 0.003933120984584093,
      "learning_rate": 8.813013333333334e-06,
      "loss": 0.0002,
      "step": 52440
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.0047063929960131645,
      "learning_rate": 8.810880000000001e-06,
      "loss": 0.0002,
      "step": 52450
    },
    {
      "epoch": 1.67872,
      "grad_norm": 0.00464043440297246,
      "learning_rate": 8.808746666666668e-06,
      "loss": 0.0002,
      "step": 52460
    },
    {
      "epoch": 1.67904,
      "grad_norm": 0.003375415923073888,
      "learning_rate": 8.806613333333333e-06,
      "loss": 0.0002,
      "step": 52470
    },
    {
      "epoch": 1.67936,
      "grad_norm": 0.002320587635040283,
      "learning_rate": 8.80448e-06,
      "loss": 0.0002,
      "step": 52480
    },
    {
      "epoch": 1.6796799999999998,
      "grad_norm": 0.0030067963525652885,
      "learning_rate": 8.802346666666668e-06,
      "loss": 0.001,
      "step": 52490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.004587307572364807,
      "learning_rate": 8.800213333333335e-06,
      "loss": 0.0003,
      "step": 52500
    },
    {
      "epoch": 1.68032,
      "grad_norm": 0.0073211961425840855,
      "learning_rate": 8.798080000000002e-06,
      "loss": 0.0002,
      "step": 52510
    },
    {
      "epoch": 1.68064,
      "grad_norm": 0.0038433868903666735,
      "learning_rate": 8.795946666666667e-06,
      "loss": 0.0002,
      "step": 52520
    },
    {
      "epoch": 1.68096,
      "grad_norm": 0.0016765464097261429,
      "learning_rate": 8.793813333333334e-06,
      "loss": 0.0052,
      "step": 52530
    },
    {
      "epoch": 1.68128,
      "grad_norm": 0.001555867725983262,
      "learning_rate": 8.79168e-06,
      "loss": 0.0002,
      "step": 52540
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.006243572104722261,
      "learning_rate": 8.789546666666667e-06,
      "loss": 0.0002,
      "step": 52550
    },
    {
      "epoch": 1.6819199999999999,
      "grad_norm": 0.007328646723181009,
      "learning_rate": 8.787413333333334e-06,
      "loss": 0.0001,
      "step": 52560
    },
    {
      "epoch": 1.68224,
      "grad_norm": 0.005836960393935442,
      "learning_rate": 8.78528e-06,
      "loss": 0.0479,
      "step": 52570
    },
    {
      "epoch": 1.68256,
      "grad_norm": 0.004615917801856995,
      "learning_rate": 8.783146666666668e-06,
      "loss": 0.0438,
      "step": 52580
    },
    {
      "epoch": 1.68288,
      "grad_norm": 0.005290599074214697,
      "learning_rate": 8.781013333333333e-06,
      "loss": 0.0002,
      "step": 52590
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.0035054434556514025,
      "learning_rate": 8.77888e-06,
      "loss": 0.0386,
      "step": 52600
    },
    {
      "epoch": 1.6835200000000001,
      "grad_norm": 0.0044498080387711525,
      "learning_rate": 8.776746666666667e-06,
      "loss": 0.0185,
      "step": 52610
    },
    {
      "epoch": 1.68384,
      "grad_norm": 0.002036390360444784,
      "learning_rate": 8.774613333333334e-06,
      "loss": 0.0004,
      "step": 52620
    },
    {
      "epoch": 1.6841599999999999,
      "grad_norm": 0.0037698561791330576,
      "learning_rate": 8.772480000000001e-06,
      "loss": 0.0002,
      "step": 52630
    },
    {
      "epoch": 1.68448,
      "grad_norm": 0.2061305046081543,
      "learning_rate": 8.770346666666667e-06,
      "loss": 0.0005,
      "step": 52640
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.009790982119739056,
      "learning_rate": 8.768213333333334e-06,
      "loss": 0.0027,
      "step": 52650
    },
    {
      "epoch": 1.68512,
      "grad_norm": 0.005215000826865435,
      "learning_rate": 8.766080000000001e-06,
      "loss": 0.0487,
      "step": 52660
    },
    {
      "epoch": 1.68544,
      "grad_norm": 0.00271396292373538,
      "learning_rate": 8.763946666666668e-06,
      "loss": 0.0003,
      "step": 52670
    },
    {
      "epoch": 1.6857600000000001,
      "grad_norm": 0.0036028120666742325,
      "learning_rate": 8.761813333333335e-06,
      "loss": 0.0001,
      "step": 52680
    },
    {
      "epoch": 1.68608,
      "grad_norm": 0.0052056084387004375,
      "learning_rate": 8.75968e-06,
      "loss": 0.0002,
      "step": 52690
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.0021403534337878227,
      "learning_rate": 8.757546666666668e-06,
      "loss": 0.0002,
      "step": 52700
    },
    {
      "epoch": 1.68672,
      "grad_norm": 0.0027842686977237463,
      "learning_rate": 8.755413333333333e-06,
      "loss": 0.0002,
      "step": 52710
    },
    {
      "epoch": 1.68704,
      "grad_norm": 0.00817252229899168,
      "learning_rate": 8.75328e-06,
      "loss": 0.0002,
      "step": 52720
    },
    {
      "epoch": 1.68736,
      "grad_norm": 0.004783605691045523,
      "learning_rate": 8.751146666666667e-06,
      "loss": 0.001,
      "step": 52730
    },
    {
      "epoch": 1.6876799999999998,
      "grad_norm": 0.01163558941334486,
      "learning_rate": 8.749013333333334e-06,
      "loss": 0.0004,
      "step": 52740
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.0036312416195869446,
      "learning_rate": 8.746880000000001e-06,
      "loss": 0.0002,
      "step": 52750
    },
    {
      "epoch": 1.68832,
      "grad_norm": 0.005624426528811455,
      "learning_rate": 8.744746666666667e-06,
      "loss": 0.0002,
      "step": 52760
    },
    {
      "epoch": 1.68864,
      "grad_norm": 0.0031947719398885965,
      "learning_rate": 8.742613333333334e-06,
      "loss": 0.0004,
      "step": 52770
    },
    {
      "epoch": 1.68896,
      "grad_norm": 0.2718973457813263,
      "learning_rate": 8.74048e-06,
      "loss": 0.0005,
      "step": 52780
    },
    {
      "epoch": 1.6892800000000001,
      "grad_norm": 0.010907667689025402,
      "learning_rate": 8.738346666666668e-06,
      "loss": 0.0002,
      "step": 52790
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.006399042438715696,
      "learning_rate": 8.736213333333335e-06,
      "loss": 0.0003,
      "step": 52800
    },
    {
      "epoch": 1.6899199999999999,
      "grad_norm": 0.006701779551804066,
      "learning_rate": 8.73408e-06,
      "loss": 0.0002,
      "step": 52810
    },
    {
      "epoch": 1.69024,
      "grad_norm": 0.006220635958015919,
      "learning_rate": 8.731946666666667e-06,
      "loss": 0.0012,
      "step": 52820
    },
    {
      "epoch": 1.69056,
      "grad_norm": 0.0036184857599437237,
      "learning_rate": 8.729813333333334e-06,
      "loss": 0.0002,
      "step": 52830
    },
    {
      "epoch": 1.69088,
      "grad_norm": 0.02173970639705658,
      "learning_rate": 8.72768e-06,
      "loss": 0.0522,
      "step": 52840
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.0021910788491368294,
      "learning_rate": 8.725546666666669e-06,
      "loss": 0.0001,
      "step": 52850
    },
    {
      "epoch": 1.6915200000000001,
      "grad_norm": 0.004609829746186733,
      "learning_rate": 8.723413333333334e-06,
      "loss": 0.0002,
      "step": 52860
    },
    {
      "epoch": 1.69184,
      "grad_norm": 0.006634517107158899,
      "learning_rate": 8.721280000000001e-06,
      "loss": 0.0189,
      "step": 52870
    },
    {
      "epoch": 1.6921599999999999,
      "grad_norm": 0.001997317187488079,
      "learning_rate": 8.719146666666666e-06,
      "loss": 0.0002,
      "step": 52880
    },
    {
      "epoch": 1.69248,
      "grad_norm": 0.0032511302269995213,
      "learning_rate": 8.717013333333333e-06,
      "loss": 0.0436,
      "step": 52890
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.004086979664862156,
      "learning_rate": 8.71488e-06,
      "loss": 0.0066,
      "step": 52900
    },
    {
      "epoch": 1.69312,
      "grad_norm": 0.005628167185932398,
      "learning_rate": 8.712746666666668e-06,
      "loss": 0.0003,
      "step": 52910
    },
    {
      "epoch": 1.6934399999999998,
      "grad_norm": 0.005020244978368282,
      "learning_rate": 8.710613333333335e-06,
      "loss": 0.0002,
      "step": 52920
    },
    {
      "epoch": 1.6937600000000002,
      "grad_norm": 0.007536024320870638,
      "learning_rate": 8.70848e-06,
      "loss": 0.0565,
      "step": 52930
    },
    {
      "epoch": 1.69408,
      "grad_norm": 0.0015312556643038988,
      "learning_rate": 8.706346666666667e-06,
      "loss": 0.0002,
      "step": 52940
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.009781865403056145,
      "learning_rate": 8.704213333333334e-06,
      "loss": 0.0002,
      "step": 52950
    },
    {
      "epoch": 1.69472,
      "grad_norm": 0.13274797797203064,
      "learning_rate": 8.702080000000001e-06,
      "loss": 0.0018,
      "step": 52960
    },
    {
      "epoch": 1.69504,
      "grad_norm": 0.0022200928069651127,
      "learning_rate": 8.699946666666668e-06,
      "loss": 0.0202,
      "step": 52970
    },
    {
      "epoch": 1.69536,
      "grad_norm": 0.002901931991800666,
      "learning_rate": 8.697813333333334e-06,
      "loss": 0.0003,
      "step": 52980
    },
    {
      "epoch": 1.6956799999999999,
      "grad_norm": 6.208376884460449,
      "learning_rate": 8.69568e-06,
      "loss": 0.0705,
      "step": 52990
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.0037337683606892824,
      "learning_rate": 8.693546666666668e-06,
      "loss": 0.0002,
      "step": 53000
    },
    {
      "epoch": 1.69632,
      "grad_norm": 0.007140833418816328,
      "learning_rate": 8.691413333333333e-06,
      "loss": 0.0003,
      "step": 53010
    },
    {
      "epoch": 1.69664,
      "grad_norm": 0.005763458553701639,
      "learning_rate": 8.68928e-06,
      "loss": 0.0004,
      "step": 53020
    },
    {
      "epoch": 1.69696,
      "grad_norm": 0.017702676355838776,
      "learning_rate": 8.687146666666667e-06,
      "loss": 0.0008,
      "step": 53030
    },
    {
      "epoch": 1.6972800000000001,
      "grad_norm": 0.012075511738657951,
      "learning_rate": 8.685013333333334e-06,
      "loss": 0.0002,
      "step": 53040
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.004003683105111122,
      "learning_rate": 8.68288e-06,
      "loss": 0.0171,
      "step": 53050
    },
    {
      "epoch": 1.6979199999999999,
      "grad_norm": 0.00856238417327404,
      "learning_rate": 8.680746666666667e-06,
      "loss": 0.0002,
      "step": 53060
    },
    {
      "epoch": 1.69824,
      "grad_norm": 0.005692491307854652,
      "learning_rate": 8.678613333333334e-06,
      "loss": 0.0003,
      "step": 53070
    },
    {
      "epoch": 1.69856,
      "grad_norm": 0.0043481518514454365,
      "learning_rate": 8.676480000000001e-06,
      "loss": 0.0003,
      "step": 53080
    },
    {
      "epoch": 1.69888,
      "grad_norm": 1.8026015758514404,
      "learning_rate": 8.674346666666668e-06,
      "loss": 0.0586,
      "step": 53090
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.011016652919352055,
      "learning_rate": 8.672213333333333e-06,
      "loss": 0.0002,
      "step": 53100
    },
    {
      "epoch": 1.6995200000000001,
      "grad_norm": 0.008286708034574986,
      "learning_rate": 8.67008e-06,
      "loss": 0.0003,
      "step": 53110
    },
    {
      "epoch": 1.69984,
      "grad_norm": 0.00672232685610652,
      "learning_rate": 8.667946666666668e-06,
      "loss": 0.0005,
      "step": 53120
    },
    {
      "epoch": 1.70016,
      "grad_norm": 0.003185114823281765,
      "learning_rate": 8.665813333333335e-06,
      "loss": 0.0002,
      "step": 53130
    },
    {
      "epoch": 1.70048,
      "grad_norm": 0.00349673954769969,
      "learning_rate": 8.663680000000002e-06,
      "loss": 0.0138,
      "step": 53140
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.003057739930227399,
      "learning_rate": 8.661546666666667e-06,
      "loss": 0.0002,
      "step": 53150
    },
    {
      "epoch": 1.70112,
      "grad_norm": 0.010552006773650646,
      "learning_rate": 8.659413333333334e-06,
      "loss": 0.0002,
      "step": 53160
    },
    {
      "epoch": 1.7014399999999998,
      "grad_norm": 0.004472852684557438,
      "learning_rate": 8.657280000000001e-06,
      "loss": 0.0145,
      "step": 53170
    },
    {
      "epoch": 1.7017600000000002,
      "grad_norm": 0.009464570321142673,
      "learning_rate": 8.655146666666667e-06,
      "loss": 0.0117,
      "step": 53180
    },
    {
      "epoch": 1.70208,
      "grad_norm": 0.004422514699399471,
      "learning_rate": 8.653013333333334e-06,
      "loss": 0.0003,
      "step": 53190
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.008637212216854095,
      "learning_rate": 8.65088e-06,
      "loss": 0.0002,
      "step": 53200
    },
    {
      "epoch": 1.70272,
      "grad_norm": 0.004608082585036755,
      "learning_rate": 8.648746666666668e-06,
      "loss": 0.0002,
      "step": 53210
    },
    {
      "epoch": 1.70304,
      "grad_norm": 0.010383148677647114,
      "learning_rate": 8.646613333333333e-06,
      "loss": 0.0002,
      "step": 53220
    },
    {
      "epoch": 1.70336,
      "grad_norm": 0.009346074424684048,
      "learning_rate": 8.64448e-06,
      "loss": 0.0003,
      "step": 53230
    },
    {
      "epoch": 1.7036799999999999,
      "grad_norm": 0.0022188115399330854,
      "learning_rate": 8.642346666666667e-06,
      "loss": 0.0003,
      "step": 53240
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.004194475710391998,
      "learning_rate": 8.640213333333334e-06,
      "loss": 0.0024,
      "step": 53250
    },
    {
      "epoch": 1.70432,
      "grad_norm": 0.005750725511461496,
      "learning_rate": 8.638080000000001e-06,
      "loss": 0.0465,
      "step": 53260
    },
    {
      "epoch": 1.70464,
      "grad_norm": 0.004478250630199909,
      "learning_rate": 8.635946666666667e-06,
      "loss": 0.0002,
      "step": 53270
    },
    {
      "epoch": 1.70496,
      "grad_norm": 0.0036115767434239388,
      "learning_rate": 8.633813333333334e-06,
      "loss": 0.0485,
      "step": 53280
    },
    {
      "epoch": 1.7052800000000001,
      "grad_norm": 0.006005999632179737,
      "learning_rate": 8.631680000000001e-06,
      "loss": 0.0002,
      "step": 53290
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.014126015827059746,
      "learning_rate": 8.629546666666666e-06,
      "loss": 0.0002,
      "step": 53300
    },
    {
      "epoch": 1.7059199999999999,
      "grad_norm": 0.0072428337298333645,
      "learning_rate": 8.627413333333333e-06,
      "loss": 0.0385,
      "step": 53310
    },
    {
      "epoch": 1.70624,
      "grad_norm": 0.006254843436181545,
      "learning_rate": 8.62528e-06,
      "loss": 0.0007,
      "step": 53320
    },
    {
      "epoch": 1.70656,
      "grad_norm": 0.004743858240544796,
      "learning_rate": 8.623146666666668e-06,
      "loss": 0.0002,
      "step": 53330
    },
    {
      "epoch": 1.70688,
      "grad_norm": 0.008970296941697598,
      "learning_rate": 8.621013333333335e-06,
      "loss": 0.0453,
      "step": 53340
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.007526855915784836,
      "learning_rate": 8.61888e-06,
      "loss": 0.0003,
      "step": 53350
    },
    {
      "epoch": 1.7075200000000001,
      "grad_norm": 0.004178674891591072,
      "learning_rate": 8.616746666666667e-06,
      "loss": 0.0003,
      "step": 53360
    },
    {
      "epoch": 1.70784,
      "grad_norm": 0.008839026093482971,
      "learning_rate": 8.614613333333334e-06,
      "loss": 0.0108,
      "step": 53370
    },
    {
      "epoch": 1.70816,
      "grad_norm": 0.00513896718621254,
      "learning_rate": 8.612480000000001e-06,
      "loss": 0.0003,
      "step": 53380
    },
    {
      "epoch": 1.70848,
      "grad_norm": 0.04013562574982643,
      "learning_rate": 8.610346666666667e-06,
      "loss": 0.0013,
      "step": 53390
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.007687557488679886,
      "learning_rate": 8.608213333333334e-06,
      "loss": 0.0003,
      "step": 53400
    },
    {
      "epoch": 1.70912,
      "grad_norm": 0.007534123957157135,
      "learning_rate": 8.60608e-06,
      "loss": 0.0002,
      "step": 53410
    },
    {
      "epoch": 1.7094399999999998,
      "grad_norm": 0.0035787872038781643,
      "learning_rate": 8.603946666666668e-06,
      "loss": 0.0021,
      "step": 53420
    },
    {
      "epoch": 1.70976,
      "grad_norm": 0.027277814224362373,
      "learning_rate": 8.601813333333335e-06,
      "loss": 0.0003,
      "step": 53430
    },
    {
      "epoch": 1.71008,
      "grad_norm": 3.326228380203247,
      "learning_rate": 8.59968e-06,
      "loss": 0.0629,
      "step": 53440
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.007142221089452505,
      "learning_rate": 8.597546666666667e-06,
      "loss": 0.0023,
      "step": 53450
    },
    {
      "epoch": 1.71072,
      "grad_norm": 0.014033397659659386,
      "learning_rate": 8.595413333333334e-06,
      "loss": 0.0004,
      "step": 53460
    },
    {
      "epoch": 1.7110400000000001,
      "grad_norm": 0.009842874482274055,
      "learning_rate": 8.59328e-06,
      "loss": 0.0372,
      "step": 53470
    },
    {
      "epoch": 1.71136,
      "grad_norm": 0.14390680193901062,
      "learning_rate": 8.591146666666667e-06,
      "loss": 0.0018,
      "step": 53480
    },
    {
      "epoch": 1.7116799999999999,
      "grad_norm": 0.008039196953177452,
      "learning_rate": 8.589013333333334e-06,
      "loss": 0.0002,
      "step": 53490
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.02152586542069912,
      "learning_rate": 8.586880000000001e-06,
      "loss": 0.0002,
      "step": 53500
    },
    {
      "epoch": 1.71232,
      "grad_norm": 0.04635399207472801,
      "learning_rate": 8.584746666666668e-06,
      "loss": 0.0139,
      "step": 53510
    },
    {
      "epoch": 1.71264,
      "grad_norm": 0.004066739231348038,
      "learning_rate": 8.582613333333333e-06,
      "loss": 0.0002,
      "step": 53520
    },
    {
      "epoch": 1.71296,
      "grad_norm": 0.13814575970172882,
      "learning_rate": 8.58048e-06,
      "loss": 0.0004,
      "step": 53530
    },
    {
      "epoch": 1.7132800000000001,
      "grad_norm": 0.0031567320693284273,
      "learning_rate": 8.578346666666668e-06,
      "loss": 0.0367,
      "step": 53540
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.00590998400002718,
      "learning_rate": 8.576213333333335e-06,
      "loss": 0.0009,
      "step": 53550
    },
    {
      "epoch": 1.7139199999999999,
      "grad_norm": 0.008305734023451805,
      "learning_rate": 8.57408e-06,
      "loss": 0.0002,
      "step": 53560
    },
    {
      "epoch": 1.71424,
      "grad_norm": 0.0032813018187880516,
      "learning_rate": 8.571946666666667e-06,
      "loss": 0.0003,
      "step": 53570
    },
    {
      "epoch": 1.71456,
      "grad_norm": 0.012512526474893093,
      "learning_rate": 8.569813333333334e-06,
      "loss": 0.0002,
      "step": 53580
    },
    {
      "epoch": 1.71488,
      "grad_norm": 0.006391347385942936,
      "learning_rate": 8.56768e-06,
      "loss": 0.0002,
      "step": 53590
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.10626880079507828,
      "learning_rate": 8.565546666666668e-06,
      "loss": 0.0003,
      "step": 53600
    },
    {
      "epoch": 1.7155200000000002,
      "grad_norm": 0.022772038355469704,
      "learning_rate": 8.563413333333334e-06,
      "loss": 0.0004,
      "step": 53610
    },
    {
      "epoch": 1.71584,
      "grad_norm": 0.04083370417356491,
      "learning_rate": 8.56128e-06,
      "loss": 0.0003,
      "step": 53620
    },
    {
      "epoch": 1.71616,
      "grad_norm": 0.007724160328507423,
      "learning_rate": 8.559146666666668e-06,
      "loss": 0.0003,
      "step": 53630
    },
    {
      "epoch": 1.71648,
      "grad_norm": 0.00130605383310467,
      "learning_rate": 8.557013333333333e-06,
      "loss": 0.0002,
      "step": 53640
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.006244961638003588,
      "learning_rate": 8.55488e-06,
      "loss": 0.0003,
      "step": 53650
    },
    {
      "epoch": 1.71712,
      "grad_norm": 0.008672531694173813,
      "learning_rate": 8.552746666666667e-06,
      "loss": 0.0536,
      "step": 53660
    },
    {
      "epoch": 1.7174399999999999,
      "grad_norm": 0.019476180896162987,
      "learning_rate": 8.550613333333334e-06,
      "loss": 0.0033,
      "step": 53670
    },
    {
      "epoch": 1.71776,
      "grad_norm": 0.01004685740917921,
      "learning_rate": 8.548480000000002e-06,
      "loss": 0.0008,
      "step": 53680
    },
    {
      "epoch": 1.71808,
      "grad_norm": 0.007025762926787138,
      "learning_rate": 8.546346666666667e-06,
      "loss": 0.0002,
      "step": 53690
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.004207308869808912,
      "learning_rate": 8.544213333333334e-06,
      "loss": 0.0004,
      "step": 53700
    },
    {
      "epoch": 1.71872,
      "grad_norm": 0.2906426787376404,
      "learning_rate": 8.542080000000001e-06,
      "loss": 0.0472,
      "step": 53710
    },
    {
      "epoch": 1.7190400000000001,
      "grad_norm": 0.003460609121248126,
      "learning_rate": 8.539946666666668e-06,
      "loss": 0.0002,
      "step": 53720
    },
    {
      "epoch": 1.71936,
      "grad_norm": 0.006821488030254841,
      "learning_rate": 8.537813333333334e-06,
      "loss": 0.0002,
      "step": 53730
    },
    {
      "epoch": 1.7196799999999999,
      "grad_norm": 0.006953533738851547,
      "learning_rate": 8.53568e-06,
      "loss": 0.0002,
      "step": 53740
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.004879561718553305,
      "learning_rate": 8.533546666666668e-06,
      "loss": 0.0002,
      "step": 53750
    },
    {
      "epoch": 1.72032,
      "grad_norm": 0.25593307614326477,
      "learning_rate": 8.531413333333333e-06,
      "loss": 0.0008,
      "step": 53760
    },
    {
      "epoch": 1.72064,
      "grad_norm": 0.009308436885476112,
      "learning_rate": 8.52928e-06,
      "loss": 0.0002,
      "step": 53770
    },
    {
      "epoch": 1.72096,
      "grad_norm": 0.003858005627989769,
      "learning_rate": 8.527146666666667e-06,
      "loss": 0.0002,
      "step": 53780
    },
    {
      "epoch": 1.7212800000000001,
      "grad_norm": 0.0068258000537753105,
      "learning_rate": 8.525013333333334e-06,
      "loss": 0.0004,
      "step": 53790
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.003985194023698568,
      "learning_rate": 8.522880000000001e-06,
      "loss": 0.017,
      "step": 53800
    },
    {
      "epoch": 1.72192,
      "grad_norm": 0.026906726881861687,
      "learning_rate": 8.520746666666667e-06,
      "loss": 0.0019,
      "step": 53810
    },
    {
      "epoch": 1.72224,
      "grad_norm": 0.004451772663742304,
      "learning_rate": 8.518613333333334e-06,
      "loss": 0.0004,
      "step": 53820
    },
    {
      "epoch": 1.72256,
      "grad_norm": 0.0010399764869362116,
      "learning_rate": 8.51648e-06,
      "loss": 0.0056,
      "step": 53830
    },
    {
      "epoch": 1.72288,
      "grad_norm": 0.003943843767046928,
      "learning_rate": 8.514346666666668e-06,
      "loss": 0.0002,
      "step": 53840
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.0061569963581860065,
      "learning_rate": 8.512213333333335e-06,
      "loss": 0.0003,
      "step": 53850
    },
    {
      "epoch": 1.7235200000000002,
      "grad_norm": 0.002185044577345252,
      "learning_rate": 8.51008e-06,
      "loss": 0.0003,
      "step": 53860
    },
    {
      "epoch": 1.72384,
      "grad_norm": 0.003124137641862035,
      "learning_rate": 8.507946666666667e-06,
      "loss": 0.0004,
      "step": 53870
    },
    {
      "epoch": 1.72416,
      "grad_norm": 0.00673477491363883,
      "learning_rate": 8.505813333333334e-06,
      "loss": 0.0003,
      "step": 53880
    },
    {
      "epoch": 1.72448,
      "grad_norm": 0.0038281057495623827,
      "learning_rate": 8.503680000000002e-06,
      "loss": 0.0002,
      "step": 53890
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.003033472690731287,
      "learning_rate": 8.501546666666667e-06,
      "loss": 0.0046,
      "step": 53900
    },
    {
      "epoch": 1.72512,
      "grad_norm": 0.008778992109000683,
      "learning_rate": 8.499413333333334e-06,
      "loss": 0.0041,
      "step": 53910
    },
    {
      "epoch": 1.7254399999999999,
      "grad_norm": 0.0022688047029078007,
      "learning_rate": 8.497280000000001e-06,
      "loss": 0.0003,
      "step": 53920
    },
    {
      "epoch": 1.72576,
      "grad_norm": 0.006804943084716797,
      "learning_rate": 8.495146666666666e-06,
      "loss": 0.0322,
      "step": 53930
    },
    {
      "epoch": 1.72608,
      "grad_norm": 0.002822061302140355,
      "learning_rate": 8.493013333333334e-06,
      "loss": 0.0002,
      "step": 53940
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.010402621701359749,
      "learning_rate": 8.49088e-06,
      "loss": 0.0004,
      "step": 53950
    },
    {
      "epoch": 1.72672,
      "grad_norm": 0.004038636106997728,
      "learning_rate": 8.488746666666668e-06,
      "loss": 0.0011,
      "step": 53960
    },
    {
      "epoch": 1.7270400000000001,
      "grad_norm": 0.005861124023795128,
      "learning_rate": 8.486613333333335e-06,
      "loss": 0.0466,
      "step": 53970
    },
    {
      "epoch": 1.72736,
      "grad_norm": 0.009917526505887508,
      "learning_rate": 8.48448e-06,
      "loss": 0.0006,
      "step": 53980
    },
    {
      "epoch": 1.7276799999999999,
      "grad_norm": 0.004380920436233282,
      "learning_rate": 8.482346666666667e-06,
      "loss": 0.0003,
      "step": 53990
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.0037981821224093437,
      "learning_rate": 8.480213333333334e-06,
      "loss": 0.0003,
      "step": 54000
    },
    {
      "epoch": 1.72832,
      "grad_norm": 0.008525751531124115,
      "learning_rate": 8.478080000000001e-06,
      "loss": 0.0002,
      "step": 54010
    },
    {
      "epoch": 1.72864,
      "grad_norm": 0.055051039904356,
      "learning_rate": 8.475946666666668e-06,
      "loss": 0.0003,
      "step": 54020
    },
    {
      "epoch": 1.72896,
      "grad_norm": 0.04740377888083458,
      "learning_rate": 8.473813333333334e-06,
      "loss": 0.0019,
      "step": 54030
    },
    {
      "epoch": 1.7292800000000002,
      "grad_norm": 0.0038381682243198156,
      "learning_rate": 8.471680000000001e-06,
      "loss": 0.0002,
      "step": 54040
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.0025244138669222593,
      "learning_rate": 8.469546666666666e-06,
      "loss": 0.0002,
      "step": 54050
    },
    {
      "epoch": 1.72992,
      "grad_norm": 0.004415868315845728,
      "learning_rate": 8.467413333333335e-06,
      "loss": 0.0003,
      "step": 54060
    },
    {
      "epoch": 1.73024,
      "grad_norm": 0.003483637236058712,
      "learning_rate": 8.46528e-06,
      "loss": 0.0002,
      "step": 54070
    },
    {
      "epoch": 1.73056,
      "grad_norm": 0.017182186245918274,
      "learning_rate": 8.463146666666667e-06,
      "loss": 0.0005,
      "step": 54080
    },
    {
      "epoch": 1.73088,
      "grad_norm": 0.003910680301487446,
      "learning_rate": 8.461013333333335e-06,
      "loss": 0.0002,
      "step": 54090
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.002301369560882449,
      "learning_rate": 8.45888e-06,
      "loss": 0.0332,
      "step": 54100
    },
    {
      "epoch": 1.73152,
      "grad_norm": 0.004534047096967697,
      "learning_rate": 8.456746666666667e-06,
      "loss": 0.0002,
      "step": 54110
    },
    {
      "epoch": 1.73184,
      "grad_norm": 0.003990259487181902,
      "learning_rate": 8.454613333333334e-06,
      "loss": 0.0002,
      "step": 54120
    },
    {
      "epoch": 1.73216,
      "grad_norm": 0.010110573843121529,
      "learning_rate": 8.452480000000001e-06,
      "loss": 0.0583,
      "step": 54130
    },
    {
      "epoch": 1.73248,
      "grad_norm": 0.0027821348048746586,
      "learning_rate": 8.450346666666668e-06,
      "loss": 0.0508,
      "step": 54140
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.0028129713609814644,
      "learning_rate": 8.448213333333334e-06,
      "loss": 0.0013,
      "step": 54150
    },
    {
      "epoch": 1.73312,
      "grad_norm": 0.004465328063815832,
      "learning_rate": 8.44608e-06,
      "loss": 0.0003,
      "step": 54160
    },
    {
      "epoch": 1.7334399999999999,
      "grad_norm": 0.005010019056499004,
      "learning_rate": 8.443946666666668e-06,
      "loss": 0.0002,
      "step": 54170
    },
    {
      "epoch": 1.73376,
      "grad_norm": 0.003869574284180999,
      "learning_rate": 8.441813333333335e-06,
      "loss": 0.0002,
      "step": 54180
    },
    {
      "epoch": 1.73408,
      "grad_norm": 0.003395914100110531,
      "learning_rate": 8.439680000000002e-06,
      "loss": 0.0027,
      "step": 54190
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.008195348083972931,
      "learning_rate": 8.437546666666667e-06,
      "loss": 0.0007,
      "step": 54200
    },
    {
      "epoch": 1.73472,
      "grad_norm": 0.22313395142555237,
      "learning_rate": 8.435413333333334e-06,
      "loss": 0.0442,
      "step": 54210
    },
    {
      "epoch": 1.7350400000000001,
      "grad_norm": 0.006547620519995689,
      "learning_rate": 8.43328e-06,
      "loss": 0.0002,
      "step": 54220
    },
    {
      "epoch": 1.73536,
      "grad_norm": 0.0043359496630728245,
      "learning_rate": 8.431146666666667e-06,
      "loss": 0.0001,
      "step": 54230
    },
    {
      "epoch": 1.73568,
      "grad_norm": 0.008241074159741402,
      "learning_rate": 8.429013333333334e-06,
      "loss": 0.0085,
      "step": 54240
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.002355299424380064,
      "learning_rate": 8.426880000000001e-06,
      "loss": 0.0003,
      "step": 54250
    },
    {
      "epoch": 1.73632,
      "grad_norm": 0.031922597438097,
      "learning_rate": 8.424746666666668e-06,
      "loss": 0.0003,
      "step": 54260
    },
    {
      "epoch": 1.73664,
      "grad_norm": 0.001960040768608451,
      "learning_rate": 8.422613333333333e-06,
      "loss": 0.0005,
      "step": 54270
    },
    {
      "epoch": 1.7369599999999998,
      "grad_norm": 0.0024461878929287195,
      "learning_rate": 8.42048e-06,
      "loss": 0.0002,
      "step": 54280
    },
    {
      "epoch": 1.7372800000000002,
      "grad_norm": 0.0037741530686616898,
      "learning_rate": 8.418346666666667e-06,
      "loss": 0.0002,
      "step": 54290
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.003815133823081851,
      "learning_rate": 8.416213333333335e-06,
      "loss": 0.0191,
      "step": 54300
    },
    {
      "epoch": 1.73792,
      "grad_norm": 1.8938735723495483,
      "learning_rate": 8.414080000000002e-06,
      "loss": 0.0513,
      "step": 54310
    },
    {
      "epoch": 1.73824,
      "grad_norm": 0.005878511816263199,
      "learning_rate": 8.411946666666667e-06,
      "loss": 0.001,
      "step": 54320
    },
    {
      "epoch": 1.73856,
      "grad_norm": 0.027309119701385498,
      "learning_rate": 8.409813333333334e-06,
      "loss": 0.0002,
      "step": 54330
    },
    {
      "epoch": 1.73888,
      "grad_norm": 0.005221369210630655,
      "learning_rate": 8.407680000000001e-06,
      "loss": 0.0002,
      "step": 54340
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.0027416374068707228,
      "learning_rate": 8.405546666666668e-06,
      "loss": 0.0306,
      "step": 54350
    },
    {
      "epoch": 1.73952,
      "grad_norm": 0.005033194553107023,
      "learning_rate": 8.403413333333335e-06,
      "loss": 0.0002,
      "step": 54360
    },
    {
      "epoch": 1.73984,
      "grad_norm": 0.004940134938806295,
      "learning_rate": 8.40128e-06,
      "loss": 0.0005,
      "step": 54370
    },
    {
      "epoch": 1.74016,
      "grad_norm": 0.005277435295283794,
      "learning_rate": 8.399146666666668e-06,
      "loss": 0.0003,
      "step": 54380
    },
    {
      "epoch": 1.74048,
      "grad_norm": 0.00434171361848712,
      "learning_rate": 8.397013333333333e-06,
      "loss": 0.0003,
      "step": 54390
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.007920957170426846,
      "learning_rate": 8.39488e-06,
      "loss": 0.0007,
      "step": 54400
    },
    {
      "epoch": 1.74112,
      "grad_norm": 0.006287822499871254,
      "learning_rate": 8.392746666666667e-06,
      "loss": 0.0079,
      "step": 54410
    },
    {
      "epoch": 1.7414399999999999,
      "grad_norm": 0.007120912428945303,
      "learning_rate": 8.390613333333334e-06,
      "loss": 0.0003,
      "step": 54420
    },
    {
      "epoch": 1.74176,
      "grad_norm": 0.0070826271548867226,
      "learning_rate": 8.388480000000001e-06,
      "loss": 0.0002,
      "step": 54430
    },
    {
      "epoch": 1.74208,
      "grad_norm": 0.002449981402605772,
      "learning_rate": 8.386346666666667e-06,
      "loss": 0.0004,
      "step": 54440
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.017165647819638252,
      "learning_rate": 8.384213333333334e-06,
      "loss": 0.0002,
      "step": 54450
    },
    {
      "epoch": 1.74272,
      "grad_norm": 0.003183845430612564,
      "learning_rate": 8.382080000000001e-06,
      "loss": 0.0002,
      "step": 54460
    },
    {
      "epoch": 1.7430400000000001,
      "grad_norm": 0.0029428421985358,
      "learning_rate": 8.379946666666668e-06,
      "loss": 0.0133,
      "step": 54470
    },
    {
      "epoch": 1.74336,
      "grad_norm": 0.2282380908727646,
      "learning_rate": 8.377813333333335e-06,
      "loss": 0.0554,
      "step": 54480
    },
    {
      "epoch": 1.74368,
      "grad_norm": 0.00569150922819972,
      "learning_rate": 8.37568e-06,
      "loss": 0.0003,
      "step": 54490
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.002640864811837673,
      "learning_rate": 8.373546666666667e-06,
      "loss": 0.0003,
      "step": 54500
    },
    {
      "epoch": 1.74432,
      "grad_norm": 0.0034096778836101294,
      "learning_rate": 8.371413333333333e-06,
      "loss": 0.0003,
      "step": 54510
    },
    {
      "epoch": 1.74464,
      "grad_norm": 0.004809993784874678,
      "learning_rate": 8.36928e-06,
      "loss": 0.0002,
      "step": 54520
    },
    {
      "epoch": 1.7449599999999998,
      "grad_norm": 0.006316616665571928,
      "learning_rate": 8.367146666666669e-06,
      "loss": 0.0003,
      "step": 54530
    },
    {
      "epoch": 1.7452800000000002,
      "grad_norm": 0.005636390298604965,
      "learning_rate": 8.365013333333334e-06,
      "loss": 0.0395,
      "step": 54540
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.0022476527374237776,
      "learning_rate": 8.362880000000001e-06,
      "loss": 0.0091,
      "step": 54550
    },
    {
      "epoch": 1.74592,
      "grad_norm": 0.0014435809571295977,
      "learning_rate": 8.360746666666667e-06,
      "loss": 0.0014,
      "step": 54560
    },
    {
      "epoch": 1.74624,
      "grad_norm": 0.007073670160025358,
      "learning_rate": 8.358613333333334e-06,
      "loss": 0.0005,
      "step": 54570
    },
    {
      "epoch": 1.7465600000000001,
      "grad_norm": 0.009868879802525043,
      "learning_rate": 8.35648e-06,
      "loss": 0.0002,
      "step": 54580
    },
    {
      "epoch": 1.74688,
      "grad_norm": 0.004270320758223534,
      "learning_rate": 8.354346666666668e-06,
      "loss": 0.0003,
      "step": 54590
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.00490350229665637,
      "learning_rate": 8.352213333333335e-06,
      "loss": 0.0318,
      "step": 54600
    },
    {
      "epoch": 1.74752,
      "grad_norm": 0.006523297633975744,
      "learning_rate": 8.35008e-06,
      "loss": 0.0003,
      "step": 54610
    },
    {
      "epoch": 1.74784,
      "grad_norm": 0.0059677185490727425,
      "learning_rate": 8.347946666666667e-06,
      "loss": 0.0003,
      "step": 54620
    },
    {
      "epoch": 1.74816,
      "grad_norm": 1.9449416399002075,
      "learning_rate": 8.345813333333334e-06,
      "loss": 0.042,
      "step": 54630
    },
    {
      "epoch": 1.74848,
      "grad_norm": 0.008542299270629883,
      "learning_rate": 8.343680000000001e-06,
      "loss": 0.0004,
      "step": 54640
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.0734485611319542,
      "learning_rate": 8.341546666666668e-06,
      "loss": 0.0005,
      "step": 54650
    },
    {
      "epoch": 1.74912,
      "grad_norm": 0.002470634179189801,
      "learning_rate": 8.339413333333334e-06,
      "loss": 0.0272,
      "step": 54660
    },
    {
      "epoch": 1.7494399999999999,
      "grad_norm": 0.011624456383287907,
      "learning_rate": 8.337280000000001e-06,
      "loss": 0.0003,
      "step": 54670
    },
    {
      "epoch": 1.74976,
      "grad_norm": 0.01725325733423233,
      "learning_rate": 8.335146666666666e-06,
      "loss": 0.0006,
      "step": 54680
    },
    {
      "epoch": 1.75008,
      "grad_norm": 11.267560958862305,
      "learning_rate": 8.333013333333333e-06,
      "loss": 0.0416,
      "step": 54690
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.006222009193152189,
      "learning_rate": 8.33088e-06,
      "loss": 0.0004,
      "step": 54700
    },
    {
      "epoch": 1.75072,
      "grad_norm": 0.01362746674567461,
      "learning_rate": 8.328746666666668e-06,
      "loss": 0.0005,
      "step": 54710
    },
    {
      "epoch": 1.7510400000000002,
      "grad_norm": 4.438151836395264,
      "learning_rate": 8.326613333333335e-06,
      "loss": 0.0331,
      "step": 54720
    },
    {
      "epoch": 1.75136,
      "grad_norm": 0.0048910509794950485,
      "learning_rate": 8.32448e-06,
      "loss": 0.0013,
      "step": 54730
    },
    {
      "epoch": 1.75168,
      "grad_norm": 0.004653489217162132,
      "learning_rate": 8.322346666666667e-06,
      "loss": 0.0118,
      "step": 54740
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.009328074753284454,
      "learning_rate": 8.320213333333334e-06,
      "loss": 0.0003,
      "step": 54750
    },
    {
      "epoch": 1.75232,
      "grad_norm": 0.004412426147609949,
      "learning_rate": 8.318080000000001e-06,
      "loss": 0.0004,
      "step": 54760
    },
    {
      "epoch": 1.75264,
      "grad_norm": 0.007207934278994799,
      "learning_rate": 8.315946666666668e-06,
      "loss": 0.0003,
      "step": 54770
    },
    {
      "epoch": 1.7529599999999999,
      "grad_norm": 0.0016503719380125403,
      "learning_rate": 8.313813333333334e-06,
      "loss": 0.0003,
      "step": 54780
    },
    {
      "epoch": 1.75328,
      "grad_norm": 0.013502911664545536,
      "learning_rate": 8.31168e-06,
      "loss": 0.0004,
      "step": 54790
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.0037284456193447113,
      "learning_rate": 8.309546666666666e-06,
      "loss": 0.0003,
      "step": 54800
    },
    {
      "epoch": 1.75392,
      "grad_norm": 0.011591204442083836,
      "learning_rate": 8.307413333333335e-06,
      "loss": 0.0002,
      "step": 54810
    },
    {
      "epoch": 1.75424,
      "grad_norm": 0.00439616059884429,
      "learning_rate": 8.305280000000002e-06,
      "loss": 0.0007,
      "step": 54820
    },
    {
      "epoch": 1.7545600000000001,
      "grad_norm": 0.004190986976027489,
      "learning_rate": 8.303146666666667e-06,
      "loss": 0.0003,
      "step": 54830
    },
    {
      "epoch": 1.75488,
      "grad_norm": 0.0035996288061141968,
      "learning_rate": 8.301013333333334e-06,
      "loss": 0.0002,
      "step": 54840
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.03494150936603546,
      "learning_rate": 8.29888e-06,
      "loss": 0.0002,
      "step": 54850
    },
    {
      "epoch": 1.75552,
      "grad_norm": 0.06902291625738144,
      "learning_rate": 8.296746666666667e-06,
      "loss": 0.0004,
      "step": 54860
    },
    {
      "epoch": 1.75584,
      "grad_norm": 0.0019243782153353095,
      "learning_rate": 8.294613333333334e-06,
      "loss": 0.0002,
      "step": 54870
    },
    {
      "epoch": 1.75616,
      "grad_norm": 0.002521939342841506,
      "learning_rate": 8.292480000000001e-06,
      "loss": 0.0002,
      "step": 54880
    },
    {
      "epoch": 1.75648,
      "grad_norm": 0.009770725853741169,
      "learning_rate": 8.290346666666668e-06,
      "loss": 0.0003,
      "step": 54890
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.015073622576892376,
      "learning_rate": 8.288213333333333e-06,
      "loss": 0.0004,
      "step": 54900
    },
    {
      "epoch": 1.75712,
      "grad_norm": 0.0037666158750653267,
      "learning_rate": 8.28608e-06,
      "loss": 0.0002,
      "step": 54910
    },
    {
      "epoch": 1.75744,
      "grad_norm": 0.0051690516993403435,
      "learning_rate": 8.283946666666668e-06,
      "loss": 0.0225,
      "step": 54920
    },
    {
      "epoch": 1.75776,
      "grad_norm": 0.0018016006797552109,
      "learning_rate": 8.281813333333335e-06,
      "loss": 0.0007,
      "step": 54930
    },
    {
      "epoch": 1.75808,
      "grad_norm": 0.005779443308711052,
      "learning_rate": 8.279680000000002e-06,
      "loss": 0.0002,
      "step": 54940
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.005131519865244627,
      "learning_rate": 8.277546666666667e-06,
      "loss": 0.0003,
      "step": 54950
    },
    {
      "epoch": 1.7587199999999998,
      "grad_norm": 0.004871504846960306,
      "learning_rate": 8.275413333333334e-06,
      "loss": 0.0002,
      "step": 54960
    },
    {
      "epoch": 1.7590400000000002,
      "grad_norm": 0.005631213076412678,
      "learning_rate": 8.27328e-06,
      "loss": 0.0003,
      "step": 54970
    },
    {
      "epoch": 1.75936,
      "grad_norm": 0.1276736557483673,
      "learning_rate": 8.271146666666667e-06,
      "loss": 0.0003,
      "step": 54980
    },
    {
      "epoch": 1.75968,
      "grad_norm": 0.004103257320821285,
      "learning_rate": 8.269013333333334e-06,
      "loss": 0.0003,
      "step": 54990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.00219598482362926,
      "learning_rate": 8.26688e-06,
      "loss": 0.0011,
      "step": 55000
    },
    {
      "epoch": 1.76032,
      "grad_norm": 0.006677329074591398,
      "learning_rate": 8.264746666666668e-06,
      "loss": 0.0002,
      "step": 55010
    },
    {
      "epoch": 1.76064,
      "grad_norm": 0.009094194509088993,
      "learning_rate": 8.262613333333333e-06,
      "loss": 0.0111,
      "step": 55020
    },
    {
      "epoch": 1.7609599999999999,
      "grad_norm": 0.01554181706160307,
      "learning_rate": 8.26048e-06,
      "loss": 0.0003,
      "step": 55030
    },
    {
      "epoch": 1.76128,
      "grad_norm": 0.0022380189038813114,
      "learning_rate": 8.258346666666667e-06,
      "loss": 0.0007,
      "step": 55040
    },
    {
      "epoch": 1.7616,
      "grad_norm": 6.3560285568237305,
      "learning_rate": 8.256213333333334e-06,
      "loss": 0.0221,
      "step": 55050
    },
    {
      "epoch": 1.76192,
      "grad_norm": 0.007910865359008312,
      "learning_rate": 8.254080000000001e-06,
      "loss": 0.0447,
      "step": 55060
    },
    {
      "epoch": 1.76224,
      "grad_norm": 0.004104763735085726,
      "learning_rate": 8.251946666666667e-06,
      "loss": 0.0002,
      "step": 55070
    },
    {
      "epoch": 1.7625600000000001,
      "grad_norm": 0.005240331869572401,
      "learning_rate": 8.249813333333334e-06,
      "loss": 0.0003,
      "step": 55080
    },
    {
      "epoch": 1.76288,
      "grad_norm": 0.0020993754733353853,
      "learning_rate": 8.247680000000001e-06,
      "loss": 0.0002,
      "step": 55090
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 8.466730117797852,
      "learning_rate": 8.245546666666668e-06,
      "loss": 0.0798,
      "step": 55100
    },
    {
      "epoch": 1.76352,
      "grad_norm": 0.006702281069010496,
      "learning_rate": 8.243413333333335e-06,
      "loss": 0.0005,
      "step": 55110
    },
    {
      "epoch": 1.76384,
      "grad_norm": 0.02001904882490635,
      "learning_rate": 8.24128e-06,
      "loss": 0.0002,
      "step": 55120
    },
    {
      "epoch": 1.76416,
      "grad_norm": 0.002760464558377862,
      "learning_rate": 8.239146666666668e-06,
      "loss": 0.0002,
      "step": 55130
    },
    {
      "epoch": 1.76448,
      "grad_norm": 0.005340953357517719,
      "learning_rate": 8.237013333333333e-06,
      "loss": 0.0002,
      "step": 55140
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.006468149833381176,
      "learning_rate": 8.23488e-06,
      "loss": 0.0002,
      "step": 55150
    },
    {
      "epoch": 1.76512,
      "grad_norm": 0.0015838987892493606,
      "learning_rate": 8.232746666666667e-06,
      "loss": 0.0002,
      "step": 55160
    },
    {
      "epoch": 1.76544,
      "grad_norm": 0.0072158160619437695,
      "learning_rate": 8.230613333333334e-06,
      "loss": 0.051,
      "step": 55170
    },
    {
      "epoch": 1.76576,
      "grad_norm": 0.007839184254407883,
      "learning_rate": 8.228480000000001e-06,
      "loss": 0.0001,
      "step": 55180
    },
    {
      "epoch": 1.76608,
      "grad_norm": 0.002810673089697957,
      "learning_rate": 8.226346666666667e-06,
      "loss": 0.0002,
      "step": 55190
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.004922507796436548,
      "learning_rate": 8.224213333333334e-06,
      "loss": 0.0002,
      "step": 55200
    },
    {
      "epoch": 1.7667199999999998,
      "grad_norm": 0.004755780100822449,
      "learning_rate": 8.22208e-06,
      "loss": 0.0002,
      "step": 55210
    },
    {
      "epoch": 1.7670400000000002,
      "grad_norm": 0.0026675786357373,
      "learning_rate": 8.219946666666668e-06,
      "loss": 0.0002,
      "step": 55220
    },
    {
      "epoch": 1.76736,
      "grad_norm": 0.004858371336013079,
      "learning_rate": 8.217813333333335e-06,
      "loss": 0.0002,
      "step": 55230
    },
    {
      "epoch": 1.76768,
      "grad_norm": 0.0058104800991714,
      "learning_rate": 8.21568e-06,
      "loss": 0.0002,
      "step": 55240
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.006208275444805622,
      "learning_rate": 8.213546666666667e-06,
      "loss": 0.0245,
      "step": 55250
    },
    {
      "epoch": 1.7683200000000001,
      "grad_norm": 0.004534597042948008,
      "learning_rate": 8.211413333333333e-06,
      "loss": 0.0002,
      "step": 55260
    },
    {
      "epoch": 1.76864,
      "grad_norm": 0.005365333519876003,
      "learning_rate": 8.20928e-06,
      "loss": 0.0007,
      "step": 55270
    },
    {
      "epoch": 1.7689599999999999,
      "grad_norm": 0.0032976511865854263,
      "learning_rate": 8.207146666666669e-06,
      "loss": 0.0002,
      "step": 55280
    },
    {
      "epoch": 1.76928,
      "grad_norm": 0.005089532118290663,
      "learning_rate": 8.205013333333334e-06,
      "loss": 0.0002,
      "step": 55290
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.004135642200708389,
      "learning_rate": 8.202880000000001e-06,
      "loss": 0.0002,
      "step": 55300
    },
    {
      "epoch": 1.76992,
      "grad_norm": 0.006816129665821791,
      "learning_rate": 8.200746666666666e-06,
      "loss": 0.0458,
      "step": 55310
    },
    {
      "epoch": 1.77024,
      "grad_norm": 0.008879837580025196,
      "learning_rate": 8.198613333333333e-06,
      "loss": 0.0004,
      "step": 55320
    },
    {
      "epoch": 1.7705600000000001,
      "grad_norm": 0.004864875692874193,
      "learning_rate": 8.19648e-06,
      "loss": 0.0003,
      "step": 55330
    },
    {
      "epoch": 1.77088,
      "grad_norm": 0.006835606414824724,
      "learning_rate": 8.194346666666668e-06,
      "loss": 0.0071,
      "step": 55340
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.0018423679284751415,
      "learning_rate": 8.192213333333335e-06,
      "loss": 0.0002,
      "step": 55350
    },
    {
      "epoch": 1.77152,
      "grad_norm": 0.01013042964041233,
      "learning_rate": 8.19008e-06,
      "loss": 0.0003,
      "step": 55360
    },
    {
      "epoch": 1.77184,
      "grad_norm": 0.007520511280745268,
      "learning_rate": 8.187946666666667e-06,
      "loss": 0.0426,
      "step": 55370
    },
    {
      "epoch": 1.77216,
      "grad_norm": 0.0029576867818832397,
      "learning_rate": 8.185813333333334e-06,
      "loss": 0.0002,
      "step": 55380
    },
    {
      "epoch": 1.77248,
      "grad_norm": 0.00580414617434144,
      "learning_rate": 8.183680000000001e-06,
      "loss": 0.0002,
      "step": 55390
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.008575446903705597,
      "learning_rate": 8.181546666666668e-06,
      "loss": 0.0007,
      "step": 55400
    },
    {
      "epoch": 1.77312,
      "grad_norm": 0.1319763958454132,
      "learning_rate": 8.179413333333334e-06,
      "loss": 0.0005,
      "step": 55410
    },
    {
      "epoch": 1.77344,
      "grad_norm": 0.005330407060682774,
      "learning_rate": 8.17728e-06,
      "loss": 0.0241,
      "step": 55420
    },
    {
      "epoch": 1.77376,
      "grad_norm": 0.09645377099514008,
      "learning_rate": 8.175146666666666e-06,
      "loss": 0.0019,
      "step": 55430
    },
    {
      "epoch": 1.77408,
      "grad_norm": 0.005866117775440216,
      "learning_rate": 8.173013333333333e-06,
      "loss": 0.0011,
      "step": 55440
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.003054769244045019,
      "learning_rate": 8.17088e-06,
      "loss": 0.0001,
      "step": 55450
    },
    {
      "epoch": 1.7747199999999999,
      "grad_norm": 0.005934175569564104,
      "learning_rate": 8.168746666666667e-06,
      "loss": 0.0002,
      "step": 55460
    },
    {
      "epoch": 1.77504,
      "grad_norm": 0.005656262394040823,
      "learning_rate": 8.166613333333334e-06,
      "loss": 0.0002,
      "step": 55470
    },
    {
      "epoch": 1.77536,
      "grad_norm": 0.004398699384182692,
      "learning_rate": 8.16448e-06,
      "loss": 0.0002,
      "step": 55480
    },
    {
      "epoch": 1.77568,
      "grad_norm": 0.010439692996442318,
      "learning_rate": 8.162346666666667e-06,
      "loss": 0.0003,
      "step": 55490
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.0017364625819027424,
      "learning_rate": 8.160213333333334e-06,
      "loss": 0.0008,
      "step": 55500
    },
    {
      "epoch": 1.7763200000000001,
      "grad_norm": 0.00253309216350317,
      "learning_rate": 8.158080000000001e-06,
      "loss": 0.0521,
      "step": 55510
    },
    {
      "epoch": 1.77664,
      "grad_norm": 0.00380462477914989,
      "learning_rate": 8.155946666666668e-06,
      "loss": 0.0002,
      "step": 55520
    },
    {
      "epoch": 1.7769599999999999,
      "grad_norm": 0.003474651137366891,
      "learning_rate": 8.153813333333333e-06,
      "loss": 0.0016,
      "step": 55530
    },
    {
      "epoch": 1.77728,
      "grad_norm": 0.004579851869493723,
      "learning_rate": 8.15168e-06,
      "loss": 0.0004,
      "step": 55540
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.005647727288305759,
      "learning_rate": 8.149546666666668e-06,
      "loss": 0.0004,
      "step": 55550
    },
    {
      "epoch": 1.77792,
      "grad_norm": 0.004204574506729841,
      "learning_rate": 8.147413333333335e-06,
      "loss": 0.0455,
      "step": 55560
    },
    {
      "epoch": 1.77824,
      "grad_norm": 0.004774682689458132,
      "learning_rate": 8.145280000000002e-06,
      "loss": 0.0002,
      "step": 55570
    },
    {
      "epoch": 1.7785600000000001,
      "grad_norm": 0.004848921205848455,
      "learning_rate": 8.143146666666667e-06,
      "loss": 0.0419,
      "step": 55580
    },
    {
      "epoch": 1.77888,
      "grad_norm": 0.00431946711614728,
      "learning_rate": 8.141013333333334e-06,
      "loss": 0.0002,
      "step": 55590
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.0025717143435031176,
      "learning_rate": 8.13888e-06,
      "loss": 0.0002,
      "step": 55600
    },
    {
      "epoch": 1.77952,
      "grad_norm": 0.002142261015251279,
      "learning_rate": 8.136746666666667e-06,
      "loss": 0.0008,
      "step": 55610
    },
    {
      "epoch": 1.77984,
      "grad_norm": 0.002869118470698595,
      "learning_rate": 8.134613333333334e-06,
      "loss": 0.0006,
      "step": 55620
    },
    {
      "epoch": 1.78016,
      "grad_norm": 0.008746511302888393,
      "learning_rate": 8.13248e-06,
      "loss": 0.0006,
      "step": 55630
    },
    {
      "epoch": 1.7804799999999998,
      "grad_norm": 0.006501056719571352,
      "learning_rate": 8.130346666666668e-06,
      "loss": 0.0002,
      "step": 55640
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.01750550977885723,
      "learning_rate": 8.128213333333333e-06,
      "loss": 0.0092,
      "step": 55650
    },
    {
      "epoch": 1.78112,
      "grad_norm": 0.01023650448769331,
      "learning_rate": 8.12608e-06,
      "loss": 0.0003,
      "step": 55660
    },
    {
      "epoch": 1.78144,
      "grad_norm": 0.005640968680381775,
      "learning_rate": 8.123946666666667e-06,
      "loss": 0.0013,
      "step": 55670
    },
    {
      "epoch": 1.78176,
      "grad_norm": 0.005894622765481472,
      "learning_rate": 8.121813333333334e-06,
      "loss": 0.0002,
      "step": 55680
    },
    {
      "epoch": 1.78208,
      "grad_norm": 0.007152807433158159,
      "learning_rate": 8.119680000000002e-06,
      "loss": 0.0023,
      "step": 55690
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.005729342345148325,
      "learning_rate": 8.117546666666667e-06,
      "loss": 0.0002,
      "step": 55700
    },
    {
      "epoch": 1.7827199999999999,
      "grad_norm": 2.908583402633667,
      "learning_rate": 8.115413333333334e-06,
      "loss": 0.0342,
      "step": 55710
    },
    {
      "epoch": 1.78304,
      "grad_norm": 0.004690525121986866,
      "learning_rate": 8.113280000000001e-06,
      "loss": 0.0014,
      "step": 55720
    },
    {
      "epoch": 1.78336,
      "grad_norm": 0.0021911172661930323,
      "learning_rate": 8.111146666666666e-06,
      "loss": 0.0002,
      "step": 55730
    },
    {
      "epoch": 1.78368,
      "grad_norm": 0.02231077291071415,
      "learning_rate": 8.109013333333333e-06,
      "loss": 0.0002,
      "step": 55740
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.005071277264505625,
      "learning_rate": 8.10688e-06,
      "loss": 0.0521,
      "step": 55750
    },
    {
      "epoch": 1.7843200000000001,
      "grad_norm": 0.003105153562501073,
      "learning_rate": 8.104746666666668e-06,
      "loss": 0.0005,
      "step": 55760
    },
    {
      "epoch": 1.78464,
      "grad_norm": 0.004409769084304571,
      "learning_rate": 8.102613333333333e-06,
      "loss": 0.0003,
      "step": 55770
    },
    {
      "epoch": 1.7849599999999999,
      "grad_norm": 0.029403459280729294,
      "learning_rate": 8.10048e-06,
      "loss": 0.0003,
      "step": 55780
    },
    {
      "epoch": 1.78528,
      "grad_norm": 0.006609395612031221,
      "learning_rate": 8.098346666666667e-06,
      "loss": 0.0002,
      "step": 55790
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.002899601822718978,
      "learning_rate": 8.096213333333334e-06,
      "loss": 0.0004,
      "step": 55800
    },
    {
      "epoch": 1.78592,
      "grad_norm": 0.007162569556385279,
      "learning_rate": 8.094080000000001e-06,
      "loss": 0.0491,
      "step": 55810
    },
    {
      "epoch": 1.78624,
      "grad_norm": 0.0071058026514947414,
      "learning_rate": 8.091946666666667e-06,
      "loss": 0.0004,
      "step": 55820
    },
    {
      "epoch": 1.7865600000000001,
      "grad_norm": 0.011408736929297447,
      "learning_rate": 8.089813333333334e-06,
      "loss": 0.0019,
      "step": 55830
    },
    {
      "epoch": 1.78688,
      "grad_norm": 0.00536700151860714,
      "learning_rate": 8.08768e-06,
      "loss": 0.0429,
      "step": 55840
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.0034666394349187613,
      "learning_rate": 8.085546666666668e-06,
      "loss": 0.0437,
      "step": 55850
    },
    {
      "epoch": 1.78752,
      "grad_norm": 0.006520948372781277,
      "learning_rate": 8.083413333333335e-06,
      "loss": 0.0002,
      "step": 55860
    },
    {
      "epoch": 1.78784,
      "grad_norm": 0.00885305367410183,
      "learning_rate": 8.08128e-06,
      "loss": 0.0002,
      "step": 55870
    },
    {
      "epoch": 1.78816,
      "grad_norm": 0.003930637612938881,
      "learning_rate": 8.079146666666667e-06,
      "loss": 0.0024,
      "step": 55880
    },
    {
      "epoch": 1.7884799999999998,
      "grad_norm": 0.003313455730676651,
      "learning_rate": 8.077013333333334e-06,
      "loss": 0.0054,
      "step": 55890
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.004403920844197273,
      "learning_rate": 8.07488e-06,
      "loss": 0.0002,
      "step": 55900
    },
    {
      "epoch": 1.78912,
      "grad_norm": 0.01478460244834423,
      "learning_rate": 8.072746666666667e-06,
      "loss": 0.0003,
      "step": 55910
    },
    {
      "epoch": 1.78944,
      "grad_norm": 0.003591711400076747,
      "learning_rate": 8.070613333333334e-06,
      "loss": 0.0002,
      "step": 55920
    },
    {
      "epoch": 1.78976,
      "grad_norm": 0.00821355078369379,
      "learning_rate": 8.068480000000001e-06,
      "loss": 0.0011,
      "step": 55930
    },
    {
      "epoch": 1.7900800000000001,
      "grad_norm": 0.00529355788603425,
      "learning_rate": 8.066346666666668e-06,
      "loss": 0.0364,
      "step": 55940
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.007880291901528835,
      "learning_rate": 8.064213333333334e-06,
      "loss": 0.0423,
      "step": 55950
    },
    {
      "epoch": 1.7907199999999999,
      "grad_norm": 0.01040896400809288,
      "learning_rate": 8.06208e-06,
      "loss": 0.0009,
      "step": 55960
    },
    {
      "epoch": 1.79104,
      "grad_norm": 0.003709684358909726,
      "learning_rate": 8.059946666666668e-06,
      "loss": 0.0005,
      "step": 55970
    },
    {
      "epoch": 1.79136,
      "grad_norm": 0.02710026502609253,
      "learning_rate": 8.057813333333335e-06,
      "loss": 0.0003,
      "step": 55980
    },
    {
      "epoch": 1.79168,
      "grad_norm": 0.00264527159743011,
      "learning_rate": 8.05568e-06,
      "loss": 0.0992,
      "step": 55990
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.04231526702642441,
      "learning_rate": 8.053546666666667e-06,
      "loss": 0.0004,
      "step": 56000
    },
    {
      "epoch": 1.7923200000000001,
      "grad_norm": 0.006220538634806871,
      "learning_rate": 8.051413333333334e-06,
      "loss": 0.0003,
      "step": 56010
    },
    {
      "epoch": 1.79264,
      "grad_norm": 0.002939029596745968,
      "learning_rate": 8.04928e-06,
      "loss": 0.0014,
      "step": 56020
    },
    {
      "epoch": 1.7929599999999999,
      "grad_norm": 0.005338911432772875,
      "learning_rate": 8.047146666666668e-06,
      "loss": 0.0003,
      "step": 56030
    },
    {
      "epoch": 1.79328,
      "grad_norm": 0.0032446905970573425,
      "learning_rate": 8.045013333333334e-06,
      "loss": 0.0004,
      "step": 56040
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.005308138206601143,
      "learning_rate": 8.04288e-06,
      "loss": 0.0003,
      "step": 56050
    },
    {
      "epoch": 1.79392,
      "grad_norm": 0.0039374129846692085,
      "learning_rate": 8.040746666666668e-06,
      "loss": 0.0002,
      "step": 56060
    },
    {
      "epoch": 1.7942399999999998,
      "grad_norm": 0.008333474397659302,
      "learning_rate": 8.038613333333333e-06,
      "loss": 0.0005,
      "step": 56070
    },
    {
      "epoch": 1.7945600000000002,
      "grad_norm": 0.006593783851712942,
      "learning_rate": 8.03648e-06,
      "loss": 0.0573,
      "step": 56080
    },
    {
      "epoch": 1.79488,
      "grad_norm": 0.007907779887318611,
      "learning_rate": 8.034346666666667e-06,
      "loss": 0.0007,
      "step": 56090
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.027338102459907532,
      "learning_rate": 8.032213333333334e-06,
      "loss": 0.0057,
      "step": 56100
    },
    {
      "epoch": 1.79552,
      "grad_norm": 0.003607493359595537,
      "learning_rate": 8.030080000000002e-06,
      "loss": 0.0006,
      "step": 56110
    },
    {
      "epoch": 1.79584,
      "grad_norm": 0.008355152793228626,
      "learning_rate": 8.027946666666667e-06,
      "loss": 0.0002,
      "step": 56120
    },
    {
      "epoch": 1.79616,
      "grad_norm": 0.004657386802136898,
      "learning_rate": 8.025813333333334e-06,
      "loss": 0.039,
      "step": 56130
    },
    {
      "epoch": 1.7964799999999999,
      "grad_norm": 0.00808516051620245,
      "learning_rate": 8.023680000000001e-06,
      "loss": 0.0023,
      "step": 56140
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.006662540137767792,
      "learning_rate": 8.021546666666668e-06,
      "loss": 0.0004,
      "step": 56150
    },
    {
      "epoch": 1.79712,
      "grad_norm": 0.006152806803584099,
      "learning_rate": 8.019413333333334e-06,
      "loss": 0.0003,
      "step": 56160
    },
    {
      "epoch": 1.79744,
      "grad_norm": 0.0033408356830477715,
      "learning_rate": 8.01728e-06,
      "loss": 0.0003,
      "step": 56170
    },
    {
      "epoch": 1.79776,
      "grad_norm": 0.008222024887800217,
      "learning_rate": 8.015146666666668e-06,
      "loss": 0.0003,
      "step": 56180
    },
    {
      "epoch": 1.7980800000000001,
      "grad_norm": 0.0031134055461734533,
      "learning_rate": 8.013013333333333e-06,
      "loss": 0.0005,
      "step": 56190
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.0036906860768795013,
      "learning_rate": 8.01088e-06,
      "loss": 0.0003,
      "step": 56200
    },
    {
      "epoch": 1.7987199999999999,
      "grad_norm": 0.007311561144888401,
      "learning_rate": 8.008746666666667e-06,
      "loss": 0.0267,
      "step": 56210
    },
    {
      "epoch": 1.79904,
      "grad_norm": 0.016923727467656136,
      "learning_rate": 8.006613333333334e-06,
      "loss": 0.0003,
      "step": 56220
    },
    {
      "epoch": 1.79936,
      "grad_norm": 0.005599077790975571,
      "learning_rate": 8.004480000000001e-06,
      "loss": 0.0004,
      "step": 56230
    },
    {
      "epoch": 1.79968,
      "grad_norm": 0.004616089165210724,
      "learning_rate": 8.002346666666667e-06,
      "loss": 0.0002,
      "step": 56240
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.0037566497921943665,
      "learning_rate": 8.000213333333334e-06,
      "loss": 0.0002,
      "step": 56250
    },
    {
      "epoch": 1.8003200000000001,
      "grad_norm": 0.017840759828686714,
      "learning_rate": 7.998080000000001e-06,
      "loss": 0.0003,
      "step": 56260
    },
    {
      "epoch": 1.80064,
      "grad_norm": 0.0035013926681131124,
      "learning_rate": 7.995946666666668e-06,
      "loss": 0.0003,
      "step": 56270
    },
    {
      "epoch": 1.80096,
      "grad_norm": 0.005408628843724728,
      "learning_rate": 7.993813333333335e-06,
      "loss": 0.0003,
      "step": 56280
    },
    {
      "epoch": 1.80128,
      "grad_norm": 0.00557246757671237,
      "learning_rate": 7.99168e-06,
      "loss": 0.001,
      "step": 56290
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.0029688093345612288,
      "learning_rate": 7.989546666666667e-06,
      "loss": 0.0003,
      "step": 56300
    },
    {
      "epoch": 1.80192,
      "grad_norm": 0.005527785047888756,
      "learning_rate": 7.987413333333335e-06,
      "loss": 0.0002,
      "step": 56310
    },
    {
      "epoch": 1.8022399999999998,
      "grad_norm": 0.009828795678913593,
      "learning_rate": 7.985280000000002e-06,
      "loss": 0.0003,
      "step": 56320
    },
    {
      "epoch": 1.8025600000000002,
      "grad_norm": 0.007869711145758629,
      "learning_rate": 7.983146666666667e-06,
      "loss": 0.0039,
      "step": 56330
    },
    {
      "epoch": 1.80288,
      "grad_norm": 0.004456650465726852,
      "learning_rate": 7.981013333333334e-06,
      "loss": 0.0211,
      "step": 56340
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.003272187663242221,
      "learning_rate": 7.978880000000001e-06,
      "loss": 0.0007,
      "step": 56350
    },
    {
      "epoch": 1.80352,
      "grad_norm": 0.007462974172085524,
      "learning_rate": 7.976746666666666e-06,
      "loss": 0.0002,
      "step": 56360
    },
    {
      "epoch": 1.80384,
      "grad_norm": 0.003175636986270547,
      "learning_rate": 7.974613333333334e-06,
      "loss": 0.0002,
      "step": 56370
    },
    {
      "epoch": 1.80416,
      "grad_norm": 0.005871556233614683,
      "learning_rate": 7.97248e-06,
      "loss": 0.0009,
      "step": 56380
    },
    {
      "epoch": 1.8044799999999999,
      "grad_norm": 0.010081169195473194,
      "learning_rate": 7.970346666666668e-06,
      "loss": 0.0342,
      "step": 56390
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.005000003147870302,
      "learning_rate": 7.968213333333335e-06,
      "loss": 0.006,
      "step": 56400
    },
    {
      "epoch": 1.80512,
      "grad_norm": 0.008977847173810005,
      "learning_rate": 7.96608e-06,
      "loss": 0.0002,
      "step": 56410
    },
    {
      "epoch": 1.80544,
      "grad_norm": 0.0035053263418376446,
      "learning_rate": 7.963946666666667e-06,
      "loss": 0.0006,
      "step": 56420
    },
    {
      "epoch": 1.80576,
      "grad_norm": 0.0025480324402451515,
      "learning_rate": 7.961813333333334e-06,
      "loss": 0.0003,
      "step": 56430
    },
    {
      "epoch": 1.8060800000000001,
      "grad_norm": 0.038461845368146896,
      "learning_rate": 7.959680000000001e-06,
      "loss": 0.0011,
      "step": 56440
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.006502739153802395,
      "learning_rate": 7.957546666666668e-06,
      "loss": 0.03,
      "step": 56450
    },
    {
      "epoch": 1.8067199999999999,
      "grad_norm": 0.002288223709911108,
      "learning_rate": 7.955413333333334e-06,
      "loss": 0.0005,
      "step": 56460
    },
    {
      "epoch": 1.80704,
      "grad_norm": 0.002391241490840912,
      "learning_rate": 7.953280000000001e-06,
      "loss": 0.018,
      "step": 56470
    },
    {
      "epoch": 1.80736,
      "grad_norm": 0.0067710354924201965,
      "learning_rate": 7.951146666666666e-06,
      "loss": 0.0182,
      "step": 56480
    },
    {
      "epoch": 1.80768,
      "grad_norm": 0.0032643843442201614,
      "learning_rate": 7.949013333333333e-06,
      "loss": 0.0005,
      "step": 56490
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.5395271182060242,
      "learning_rate": 7.94688e-06,
      "loss": 0.0006,
      "step": 56500
    },
    {
      "epoch": 1.8083200000000001,
      "grad_norm": 0.027641955763101578,
      "learning_rate": 7.944746666666667e-06,
      "loss": 0.0003,
      "step": 56510
    },
    {
      "epoch": 1.80864,
      "grad_norm": 0.005031145177781582,
      "learning_rate": 7.942613333333335e-06,
      "loss": 0.0066,
      "step": 56520
    },
    {
      "epoch": 1.80896,
      "grad_norm": 0.006112370174378157,
      "learning_rate": 7.94048e-06,
      "loss": 0.0003,
      "step": 56530
    },
    {
      "epoch": 1.80928,
      "grad_norm": 0.003436031984165311,
      "learning_rate": 7.938346666666667e-06,
      "loss": 0.0002,
      "step": 56540
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.003140825778245926,
      "learning_rate": 7.936213333333334e-06,
      "loss": 0.0003,
      "step": 56550
    },
    {
      "epoch": 1.80992,
      "grad_norm": 0.04610825702548027,
      "learning_rate": 7.934080000000001e-06,
      "loss": 0.0004,
      "step": 56560
    },
    {
      "epoch": 1.8102399999999998,
      "grad_norm": 0.007430236786603928,
      "learning_rate": 7.931946666666668e-06,
      "loss": 0.0315,
      "step": 56570
    },
    {
      "epoch": 1.81056,
      "grad_norm": 0.21958307921886444,
      "learning_rate": 7.929813333333334e-06,
      "loss": 0.0011,
      "step": 56580
    },
    {
      "epoch": 1.81088,
      "grad_norm": 0.004331173840910196,
      "learning_rate": 7.92768e-06,
      "loss": 0.0002,
      "step": 56590
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.0058175912126898766,
      "learning_rate": 7.925546666666668e-06,
      "loss": 0.0002,
      "step": 56600
    },
    {
      "epoch": 1.81152,
      "grad_norm": 0.0017638662829995155,
      "learning_rate": 7.923413333333335e-06,
      "loss": 0.0011,
      "step": 56610
    },
    {
      "epoch": 1.8118400000000001,
      "grad_norm": 0.003467653412371874,
      "learning_rate": 7.921280000000002e-06,
      "loss": 0.0002,
      "step": 56620
    },
    {
      "epoch": 1.81216,
      "grad_norm": 0.004941857885569334,
      "learning_rate": 7.919146666666667e-06,
      "loss": 0.0002,
      "step": 56630
    },
    {
      "epoch": 1.8124799999999999,
      "grad_norm": 0.005482402164489031,
      "learning_rate": 7.917013333333334e-06,
      "loss": 0.0002,
      "step": 56640
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.002932513365522027,
      "learning_rate": 7.91488e-06,
      "loss": 0.0117,
      "step": 56650
    },
    {
      "epoch": 1.81312,
      "grad_norm": 0.003757262835279107,
      "learning_rate": 7.912746666666667e-06,
      "loss": 0.0005,
      "step": 56660
    },
    {
      "epoch": 1.81344,
      "grad_norm": 0.005797384772449732,
      "learning_rate": 7.910613333333334e-06,
      "loss": 0.0003,
      "step": 56670
    },
    {
      "epoch": 1.81376,
      "grad_norm": 0.006267388351261616,
      "learning_rate": 7.908480000000001e-06,
      "loss": 0.0011,
      "step": 56680
    },
    {
      "epoch": 1.8140800000000001,
      "grad_norm": 0.0033196539152413607,
      "learning_rate": 7.906346666666668e-06,
      "loss": 0.0004,
      "step": 56690
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.0031177734490484,
      "learning_rate": 7.904213333333333e-06,
      "loss": 0.0003,
      "step": 56700
    },
    {
      "epoch": 1.8147199999999999,
      "grad_norm": 0.004095536656677723,
      "learning_rate": 7.90208e-06,
      "loss": 0.0002,
      "step": 56710
    },
    {
      "epoch": 1.81504,
      "grad_norm": 0.0035961982794106007,
      "learning_rate": 7.899946666666667e-06,
      "loss": 0.0504,
      "step": 56720
    },
    {
      "epoch": 1.81536,
      "grad_norm": 0.007460853084921837,
      "learning_rate": 7.897813333333335e-06,
      "loss": 0.0003,
      "step": 56730
    },
    {
      "epoch": 1.81568,
      "grad_norm": 0.009331556968390942,
      "learning_rate": 7.895680000000002e-06,
      "loss": 0.0003,
      "step": 56740
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.0024872799403965473,
      "learning_rate": 7.893546666666667e-06,
      "loss": 0.0004,
      "step": 56750
    },
    {
      "epoch": 1.8163200000000002,
      "grad_norm": 0.007376259192824364,
      "learning_rate": 7.891413333333334e-06,
      "loss": 0.0004,
      "step": 56760
    },
    {
      "epoch": 1.81664,
      "grad_norm": 0.01749606430530548,
      "learning_rate": 7.88928e-06,
      "loss": 0.0003,
      "step": 56770
    },
    {
      "epoch": 1.81696,
      "grad_norm": 0.0037680864334106445,
      "learning_rate": 7.887146666666668e-06,
      "loss": 0.0003,
      "step": 56780
    },
    {
      "epoch": 1.81728,
      "grad_norm": 0.003779269987717271,
      "learning_rate": 7.885013333333335e-06,
      "loss": 0.0003,
      "step": 56790
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.004296659026294947,
      "learning_rate": 7.88288e-06,
      "loss": 0.0002,
      "step": 56800
    },
    {
      "epoch": 1.81792,
      "grad_norm": 0.010173946619033813,
      "learning_rate": 7.880746666666668e-06,
      "loss": 0.0002,
      "step": 56810
    },
    {
      "epoch": 1.8182399999999999,
      "grad_norm": 0.004399726167321205,
      "learning_rate": 7.878613333333333e-06,
      "loss": 0.0003,
      "step": 56820
    },
    {
      "epoch": 1.81856,
      "grad_norm": 0.014828696846961975,
      "learning_rate": 7.87648e-06,
      "loss": 0.0002,
      "step": 56830
    },
    {
      "epoch": 1.81888,
      "grad_norm": 0.006810356862843037,
      "learning_rate": 7.874346666666667e-06,
      "loss": 0.0002,
      "step": 56840
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.007153823971748352,
      "learning_rate": 7.872213333333334e-06,
      "loss": 0.0002,
      "step": 56850
    },
    {
      "epoch": 1.81952,
      "grad_norm": 0.06515170633792877,
      "learning_rate": 7.870080000000001e-06,
      "loss": 0.0003,
      "step": 56860
    },
    {
      "epoch": 1.8198400000000001,
      "grad_norm": 0.003951990511268377,
      "learning_rate": 7.867946666666667e-06,
      "loss": 0.0002,
      "step": 56870
    },
    {
      "epoch": 1.82016,
      "grad_norm": 0.0033825342543423176,
      "learning_rate": 7.865813333333334e-06,
      "loss": 0.0003,
      "step": 56880
    },
    {
      "epoch": 1.8204799999999999,
      "grad_norm": 0.01841789484024048,
      "learning_rate": 7.863680000000001e-06,
      "loss": 0.0002,
      "step": 56890
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.0033601054456084967,
      "learning_rate": 7.861546666666668e-06,
      "loss": 0.0046,
      "step": 56900
    },
    {
      "epoch": 1.82112,
      "grad_norm": 0.007153031416237354,
      "learning_rate": 7.859413333333335e-06,
      "loss": 0.052,
      "step": 56910
    },
    {
      "epoch": 1.82144,
      "grad_norm": 0.07950032502412796,
      "learning_rate": 7.85728e-06,
      "loss": 0.0006,
      "step": 56920
    },
    {
      "epoch": 1.82176,
      "grad_norm": 0.005419564433395863,
      "learning_rate": 7.855146666666668e-06,
      "loss": 0.0002,
      "step": 56930
    },
    {
      "epoch": 1.8220800000000001,
      "grad_norm": 0.2698625326156616,
      "learning_rate": 7.853013333333333e-06,
      "loss": 0.0202,
      "step": 56940
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.003758578561246395,
      "learning_rate": 7.85088e-06,
      "loss": 0.0002,
      "step": 56950
    },
    {
      "epoch": 1.82272,
      "grad_norm": 0.004212631843984127,
      "learning_rate": 7.848746666666667e-06,
      "loss": 0.0002,
      "step": 56960
    },
    {
      "epoch": 1.82304,
      "grad_norm": 0.007350793574005365,
      "learning_rate": 7.846613333333334e-06,
      "loss": 0.0002,
      "step": 56970
    },
    {
      "epoch": 1.82336,
      "grad_norm": 0.0028187118005007505,
      "learning_rate": 7.844480000000001e-06,
      "loss": 0.0001,
      "step": 56980
    },
    {
      "epoch": 1.82368,
      "grad_norm": 0.004197482950985432,
      "learning_rate": 7.842346666666667e-06,
      "loss": 0.0002,
      "step": 56990
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.00852026604115963,
      "learning_rate": 7.840213333333334e-06,
      "loss": 0.0002,
      "step": 57000
    },
    {
      "epoch": 1.8243200000000002,
      "grad_norm": 0.002707456238567829,
      "learning_rate": 7.83808e-06,
      "loss": 0.0007,
      "step": 57010
    },
    {
      "epoch": 1.82464,
      "grad_norm": 0.08858850598335266,
      "learning_rate": 7.835946666666668e-06,
      "loss": 0.0003,
      "step": 57020
    },
    {
      "epoch": 1.82496,
      "grad_norm": 0.0040756771340966225,
      "learning_rate": 7.833813333333335e-06,
      "loss": 0.001,
      "step": 57030
    },
    {
      "epoch": 1.82528,
      "grad_norm": 0.007649445906281471,
      "learning_rate": 7.83168e-06,
      "loss": 0.0009,
      "step": 57040
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.0025640600360929966,
      "learning_rate": 7.829546666666667e-06,
      "loss": 0.0002,
      "step": 57050
    },
    {
      "epoch": 1.82592,
      "grad_norm": 0.008669998496770859,
      "learning_rate": 7.827413333333334e-06,
      "loss": 0.0003,
      "step": 57060
    },
    {
      "epoch": 1.8262399999999999,
      "grad_norm": 0.01266528107225895,
      "learning_rate": 7.825280000000001e-06,
      "loss": 0.0002,
      "step": 57070
    },
    {
      "epoch": 1.82656,
      "grad_norm": 0.0029107045847922564,
      "learning_rate": 7.823146666666668e-06,
      "loss": 0.0002,
      "step": 57080
    },
    {
      "epoch": 1.82688,
      "grad_norm": 0.00643412908539176,
      "learning_rate": 7.821013333333334e-06,
      "loss": 0.0005,
      "step": 57090
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.004112638533115387,
      "learning_rate": 7.818880000000001e-06,
      "loss": 0.0002,
      "step": 57100
    },
    {
      "epoch": 1.82752,
      "grad_norm": 0.021343916654586792,
      "learning_rate": 7.816746666666666e-06,
      "loss": 0.0002,
      "step": 57110
    },
    {
      "epoch": 1.8278400000000001,
      "grad_norm": 0.005184040870517492,
      "learning_rate": 7.814613333333333e-06,
      "loss": 0.0008,
      "step": 57120
    },
    {
      "epoch": 1.82816,
      "grad_norm": 0.002425235230475664,
      "learning_rate": 7.81248e-06,
      "loss": 0.1089,
      "step": 57130
    },
    {
      "epoch": 1.8284799999999999,
      "grad_norm": 0.003918929491192102,
      "learning_rate": 7.810346666666668e-06,
      "loss": 0.0001,
      "step": 57140
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.002941649407148361,
      "learning_rate": 7.808213333333335e-06,
      "loss": 0.0335,
      "step": 57150
    },
    {
      "epoch": 1.82912,
      "grad_norm": 3.809556007385254,
      "learning_rate": 7.80608e-06,
      "loss": 0.0051,
      "step": 57160
    },
    {
      "epoch": 1.82944,
      "grad_norm": 0.009023132734000683,
      "learning_rate": 7.803946666666667e-06,
      "loss": 0.0004,
      "step": 57170
    },
    {
      "epoch": 1.82976,
      "grad_norm": 0.002293019089847803,
      "learning_rate": 7.801813333333334e-06,
      "loss": 0.0002,
      "step": 57180
    },
    {
      "epoch": 1.8300800000000002,
      "grad_norm": 0.0043141040951013565,
      "learning_rate": 7.799680000000001e-06,
      "loss": 0.0113,
      "step": 57190
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.00444567296653986,
      "learning_rate": 7.797546666666668e-06,
      "loss": 0.0002,
      "step": 57200
    },
    {
      "epoch": 1.83072,
      "grad_norm": 0.006980495061725378,
      "learning_rate": 7.795413333333334e-06,
      "loss": 0.0519,
      "step": 57210
    },
    {
      "epoch": 1.83104,
      "grad_norm": 0.0030467528849840164,
      "learning_rate": 7.79328e-06,
      "loss": 0.0002,
      "step": 57220
    },
    {
      "epoch": 1.83136,
      "grad_norm": 0.004151425790041685,
      "learning_rate": 7.791146666666666e-06,
      "loss": 0.0002,
      "step": 57230
    },
    {
      "epoch": 1.83168,
      "grad_norm": 0.007644970435649157,
      "learning_rate": 7.789013333333333e-06,
      "loss": 0.0002,
      "step": 57240
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.002666679210960865,
      "learning_rate": 7.786880000000002e-06,
      "loss": 0.0002,
      "step": 57250
    },
    {
      "epoch": 1.83232,
      "grad_norm": 0.008052279241383076,
      "learning_rate": 7.784746666666667e-06,
      "loss": 0.001,
      "step": 57260
    },
    {
      "epoch": 1.83264,
      "grad_norm": 0.0025246532168239355,
      "learning_rate": 7.782613333333334e-06,
      "loss": 0.0002,
      "step": 57270
    },
    {
      "epoch": 1.83296,
      "grad_norm": 0.002986540785059333,
      "learning_rate": 7.78048e-06,
      "loss": 0.004,
      "step": 57280
    },
    {
      "epoch": 1.83328,
      "grad_norm": 0.003963288851082325,
      "learning_rate": 7.778346666666667e-06,
      "loss": 0.0002,
      "step": 57290
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.004122569691389799,
      "learning_rate": 7.776213333333334e-06,
      "loss": 0.0002,
      "step": 57300
    },
    {
      "epoch": 1.83392,
      "grad_norm": 0.038913872092962265,
      "learning_rate": 7.774080000000001e-06,
      "loss": 0.0003,
      "step": 57310
    },
    {
      "epoch": 1.8342399999999999,
      "grad_norm": 0.004446012899279594,
      "learning_rate": 7.771946666666668e-06,
      "loss": 0.0002,
      "step": 57320
    },
    {
      "epoch": 1.83456,
      "grad_norm": 0.003070530481636524,
      "learning_rate": 7.769813333333333e-06,
      "loss": 0.0379,
      "step": 57330
    },
    {
      "epoch": 1.83488,
      "grad_norm": 0.0037315250374376774,
      "learning_rate": 7.76768e-06,
      "loss": 0.0013,
      "step": 57340
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.004531573038548231,
      "learning_rate": 7.765546666666668e-06,
      "loss": 0.0163,
      "step": 57350
    },
    {
      "epoch": 1.83552,
      "grad_norm": 0.002269426127895713,
      "learning_rate": 7.763413333333335e-06,
      "loss": 0.0002,
      "step": 57360
    },
    {
      "epoch": 1.8358400000000001,
      "grad_norm": 0.007301187142729759,
      "learning_rate": 7.761280000000002e-06,
      "loss": 0.0004,
      "step": 57370
    },
    {
      "epoch": 1.83616,
      "grad_norm": 0.0032993850763887167,
      "learning_rate": 7.759146666666667e-06,
      "loss": 0.0002,
      "step": 57380
    },
    {
      "epoch": 1.83648,
      "grad_norm": 0.0025157227646559477,
      "learning_rate": 7.757013333333334e-06,
      "loss": 0.0002,
      "step": 57390
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.015725471079349518,
      "learning_rate": 7.75488e-06,
      "loss": 0.0002,
      "step": 57400
    },
    {
      "epoch": 1.83712,
      "grad_norm": 0.07700184732675552,
      "learning_rate": 7.752746666666667e-06,
      "loss": 0.0019,
      "step": 57410
    },
    {
      "epoch": 1.83744,
      "grad_norm": 0.0014944771537557244,
      "learning_rate": 7.750613333333334e-06,
      "loss": 0.0002,
      "step": 57420
    },
    {
      "epoch": 1.8377599999999998,
      "grad_norm": 0.01240677759051323,
      "learning_rate": 7.74848e-06,
      "loss": 0.0002,
      "step": 57430
    },
    {
      "epoch": 1.8380800000000002,
      "grad_norm": 7.086700439453125,
      "learning_rate": 7.746346666666668e-06,
      "loss": 0.0149,
      "step": 57440
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.003586497390642762,
      "learning_rate": 7.744213333333333e-06,
      "loss": 0.0004,
      "step": 57450
    },
    {
      "epoch": 1.83872,
      "grad_norm": 0.010015629231929779,
      "learning_rate": 7.74208e-06,
      "loss": 0.0003,
      "step": 57460
    },
    {
      "epoch": 1.83904,
      "grad_norm": 0.004426138941198587,
      "learning_rate": 7.739946666666667e-06,
      "loss": 0.0002,
      "step": 57470
    },
    {
      "epoch": 1.83936,
      "grad_norm": 0.0040566385723650455,
      "learning_rate": 7.737813333333334e-06,
      "loss": 0.0004,
      "step": 57480
    },
    {
      "epoch": 1.83968,
      "grad_norm": 0.00661716191098094,
      "learning_rate": 7.735680000000001e-06,
      "loss": 0.0002,
      "step": 57490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.00866538193076849,
      "learning_rate": 7.733546666666667e-06,
      "loss": 0.0446,
      "step": 57500
    },
    {
      "epoch": 1.84032,
      "grad_norm": 0.0080337543040514,
      "learning_rate": 7.731413333333334e-06,
      "loss": 0.0003,
      "step": 57510
    },
    {
      "epoch": 1.84064,
      "grad_norm": 0.003384213661774993,
      "learning_rate": 7.72928e-06,
      "loss": 0.0002,
      "step": 57520
    },
    {
      "epoch": 1.84096,
      "grad_norm": 0.005647700745612383,
      "learning_rate": 7.727146666666668e-06,
      "loss": 0.0303,
      "step": 57530
    },
    {
      "epoch": 1.84128,
      "grad_norm": 0.0018812112975865602,
      "learning_rate": 7.725013333333335e-06,
      "loss": 0.0002,
      "step": 57540
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.0016592990141361952,
      "learning_rate": 7.72288e-06,
      "loss": 0.0002,
      "step": 57550
    },
    {
      "epoch": 1.84192,
      "grad_norm": 0.009462865069508553,
      "learning_rate": 7.720746666666668e-06,
      "loss": 0.0002,
      "step": 57560
    },
    {
      "epoch": 1.8422399999999999,
      "grad_norm": 0.004412462934851646,
      "learning_rate": 7.718613333333333e-06,
      "loss": 0.0538,
      "step": 57570
    },
    {
      "epoch": 1.84256,
      "grad_norm": 0.004958538804203272,
      "learning_rate": 7.71648e-06,
      "loss": 0.0009,
      "step": 57580
    },
    {
      "epoch": 1.84288,
      "grad_norm": 0.00213255500420928,
      "learning_rate": 7.714346666666667e-06,
      "loss": 0.0178,
      "step": 57590
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.009360527619719505,
      "learning_rate": 7.712213333333334e-06,
      "loss": 0.0002,
      "step": 57600
    },
    {
      "epoch": 1.84352,
      "grad_norm": 0.00306364125572145,
      "learning_rate": 7.710080000000001e-06,
      "loss": 0.0106,
      "step": 57610
    },
    {
      "epoch": 1.8438400000000001,
      "grad_norm": 0.002269640564918518,
      "learning_rate": 7.707946666666667e-06,
      "loss": 0.0017,
      "step": 57620
    },
    {
      "epoch": 1.84416,
      "grad_norm": 0.017504708841443062,
      "learning_rate": 7.705813333333334e-06,
      "loss": 0.0009,
      "step": 57630
    },
    {
      "epoch": 1.84448,
      "grad_norm": 0.01534540206193924,
      "learning_rate": 7.70368e-06,
      "loss": 0.0225,
      "step": 57640
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.004863031208515167,
      "learning_rate": 7.701546666666668e-06,
      "loss": 0.0002,
      "step": 57650
    },
    {
      "epoch": 1.84512,
      "grad_norm": 0.002833415986970067,
      "learning_rate": 7.699413333333335e-06,
      "loss": 0.0002,
      "step": 57660
    },
    {
      "epoch": 1.84544,
      "grad_norm": 0.0030175766441971064,
      "learning_rate": 7.69728e-06,
      "loss": 0.0002,
      "step": 57670
    },
    {
      "epoch": 1.8457599999999998,
      "grad_norm": 0.0024092099629342556,
      "learning_rate": 7.695146666666667e-06,
      "loss": 0.0008,
      "step": 57680
    },
    {
      "epoch": 1.8460800000000002,
      "grad_norm": 0.13096602261066437,
      "learning_rate": 7.693013333333333e-06,
      "loss": 0.0005,
      "step": 57690
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.001873588771559298,
      "learning_rate": 7.69088e-06,
      "loss": 0.0003,
      "step": 57700
    },
    {
      "epoch": 1.84672,
      "grad_norm": 0.0032768959645181894,
      "learning_rate": 7.688746666666667e-06,
      "loss": 0.0408,
      "step": 57710
    },
    {
      "epoch": 1.84704,
      "grad_norm": 0.002387662883847952,
      "learning_rate": 7.686613333333334e-06,
      "loss": 0.0003,
      "step": 57720
    },
    {
      "epoch": 1.8473600000000001,
      "grad_norm": 0.004151717759668827,
      "learning_rate": 7.684480000000001e-06,
      "loss": 0.0003,
      "step": 57730
    },
    {
      "epoch": 1.84768,
      "grad_norm": 0.003973664250224829,
      "learning_rate": 7.682346666666666e-06,
      "loss": 0.0002,
      "step": 57740
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.00460880296304822,
      "learning_rate": 7.680213333333333e-06,
      "loss": 0.0008,
      "step": 57750
    },
    {
      "epoch": 1.84832,
      "grad_norm": 0.002553510246798396,
      "learning_rate": 7.67808e-06,
      "loss": 0.0002,
      "step": 57760
    },
    {
      "epoch": 1.84864,
      "grad_norm": 0.0210216473788023,
      "learning_rate": 7.675946666666668e-06,
      "loss": 0.0006,
      "step": 57770
    },
    {
      "epoch": 1.84896,
      "grad_norm": 0.007787789683789015,
      "learning_rate": 7.673813333333335e-06,
      "loss": 0.0002,
      "step": 57780
    },
    {
      "epoch": 1.84928,
      "grad_norm": 0.0035005807876586914,
      "learning_rate": 7.67168e-06,
      "loss": 0.0501,
      "step": 57790
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.006171694025397301,
      "learning_rate": 7.669546666666667e-06,
      "loss": 0.0002,
      "step": 57800
    },
    {
      "epoch": 1.84992,
      "grad_norm": 0.00585556123405695,
      "learning_rate": 7.667413333333334e-06,
      "loss": 0.0392,
      "step": 57810
    },
    {
      "epoch": 1.8502399999999999,
      "grad_norm": 0.0046777199022471905,
      "learning_rate": 7.665280000000001e-06,
      "loss": 0.0002,
      "step": 57820
    },
    {
      "epoch": 1.85056,
      "grad_norm": 0.004476638976484537,
      "learning_rate": 7.663146666666668e-06,
      "loss": 0.0004,
      "step": 57830
    },
    {
      "epoch": 1.85088,
      "grad_norm": 0.004649755544960499,
      "learning_rate": 7.661013333333334e-06,
      "loss": 0.0003,
      "step": 57840
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.01134274061769247,
      "learning_rate": 7.65888e-06,
      "loss": 0.0005,
      "step": 57850
    },
    {
      "epoch": 1.85152,
      "grad_norm": 0.0018455235986039042,
      "learning_rate": 7.656746666666666e-06,
      "loss": 0.0002,
      "step": 57860
    },
    {
      "epoch": 1.8518400000000002,
      "grad_norm": 0.0595647394657135,
      "learning_rate": 7.654613333333333e-06,
      "loss": 0.0003,
      "step": 57870
    },
    {
      "epoch": 1.85216,
      "grad_norm": 0.0020892953034490347,
      "learning_rate": 7.65248e-06,
      "loss": 0.0003,
      "step": 57880
    },
    {
      "epoch": 1.85248,
      "grad_norm": 0.0029624393209815025,
      "learning_rate": 7.650346666666667e-06,
      "loss": 0.0026,
      "step": 57890
    },
    {
      "epoch": 1.8528,
      "grad_norm": 1.7925649881362915,
      "learning_rate": 7.648213333333334e-06,
      "loss": 0.0021,
      "step": 57900
    },
    {
      "epoch": 1.85312,
      "grad_norm": 5.707179069519043,
      "learning_rate": 7.64608e-06,
      "loss": 0.0415,
      "step": 57910
    },
    {
      "epoch": 1.85344,
      "grad_norm": 0.006455076392740011,
      "learning_rate": 7.643946666666667e-06,
      "loss": 0.009,
      "step": 57920
    },
    {
      "epoch": 1.8537599999999999,
      "grad_norm": 0.06176013872027397,
      "learning_rate": 7.641813333333334e-06,
      "loss": 0.0003,
      "step": 57930
    },
    {
      "epoch": 1.85408,
      "grad_norm": 0.0040599205531179905,
      "learning_rate": 7.639680000000001e-06,
      "loss": 0.0002,
      "step": 57940
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.0033552981913089752,
      "learning_rate": 7.637546666666668e-06,
      "loss": 0.0001,
      "step": 57950
    },
    {
      "epoch": 1.85472,
      "grad_norm": 0.005840591620653868,
      "learning_rate": 7.635413333333333e-06,
      "loss": 0.0001,
      "step": 57960
    },
    {
      "epoch": 1.85504,
      "grad_norm": 0.0019186567515134811,
      "learning_rate": 7.63328e-06,
      "loss": 0.001,
      "step": 57970
    },
    {
      "epoch": 1.8553600000000001,
      "grad_norm": 0.0019910233095288277,
      "learning_rate": 7.631146666666668e-06,
      "loss": 0.0004,
      "step": 57980
    },
    {
      "epoch": 1.85568,
      "grad_norm": 0.0012643197551369667,
      "learning_rate": 7.629013333333334e-06,
      "loss": 0.0001,
      "step": 57990
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.01665128581225872,
      "learning_rate": 7.626880000000001e-06,
      "loss": 0.0002,
      "step": 58000
    },
    {
      "epoch": 1.85632,
      "grad_norm": 0.00742095522582531,
      "learning_rate": 7.624746666666667e-06,
      "loss": 0.0002,
      "step": 58010
    },
    {
      "epoch": 1.85664,
      "grad_norm": 0.002543025417253375,
      "learning_rate": 7.622613333333334e-06,
      "loss": 0.0006,
      "step": 58020
    },
    {
      "epoch": 1.85696,
      "grad_norm": 0.0026621033903211355,
      "learning_rate": 7.6204800000000004e-06,
      "loss": 0.0008,
      "step": 58030
    },
    {
      "epoch": 1.85728,
      "grad_norm": 0.004919376224279404,
      "learning_rate": 7.6183466666666675e-06,
      "loss": 0.0522,
      "step": 58040
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.004300092812627554,
      "learning_rate": 7.6162133333333346e-06,
      "loss": 0.0002,
      "step": 58050
    },
    {
      "epoch": 1.85792,
      "grad_norm": 0.002747346181422472,
      "learning_rate": 7.61408e-06,
      "loss": 0.0002,
      "step": 58060
    },
    {
      "epoch": 1.85824,
      "grad_norm": 0.025577086955308914,
      "learning_rate": 7.611946666666668e-06,
      "loss": 0.0025,
      "step": 58070
    },
    {
      "epoch": 1.85856,
      "grad_norm": 0.0025585831608623266,
      "learning_rate": 7.609813333333333e-06,
      "loss": 0.0005,
      "step": 58080
    },
    {
      "epoch": 1.85888,
      "grad_norm": 0.003562383586540818,
      "learning_rate": 7.60768e-06,
      "loss": 0.0004,
      "step": 58090
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.0018972385441884398,
      "learning_rate": 7.605546666666667e-06,
      "loss": 0.0245,
      "step": 58100
    },
    {
      "epoch": 1.8595199999999998,
      "grad_norm": 0.004237143788486719,
      "learning_rate": 7.603413333333334e-06,
      "loss": 0.0004,
      "step": 58110
    },
    {
      "epoch": 1.8598400000000002,
      "grad_norm": 0.0027126020286232233,
      "learning_rate": 7.601280000000001e-06,
      "loss": 0.0001,
      "step": 58120
    },
    {
      "epoch": 1.86016,
      "grad_norm": 0.004172022454440594,
      "learning_rate": 7.599146666666667e-06,
      "loss": 0.0507,
      "step": 58130
    },
    {
      "epoch": 1.86048,
      "grad_norm": 0.005204533226788044,
      "learning_rate": 7.597013333333334e-06,
      "loss": 0.0003,
      "step": 58140
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.005811905954033136,
      "learning_rate": 7.594880000000001e-06,
      "loss": 0.0008,
      "step": 58150
    },
    {
      "epoch": 1.86112,
      "grad_norm": 0.0055999294854700565,
      "learning_rate": 7.592746666666667e-06,
      "loss": 0.0001,
      "step": 58160
    },
    {
      "epoch": 1.86144,
      "grad_norm": 0.00411240803077817,
      "learning_rate": 7.590613333333334e-06,
      "loss": 0.0006,
      "step": 58170
    },
    {
      "epoch": 1.8617599999999999,
      "grad_norm": 0.005178408231586218,
      "learning_rate": 7.5884800000000006e-06,
      "loss": 0.0003,
      "step": 58180
    },
    {
      "epoch": 1.86208,
      "grad_norm": 0.002950033638626337,
      "learning_rate": 7.586346666666668e-06,
      "loss": 0.0006,
      "step": 58190
    },
    {
      "epoch": 1.8624,
      "grad_norm": 1.6539170742034912,
      "learning_rate": 7.584213333333333e-06,
      "loss": 0.0017,
      "step": 58200
    },
    {
      "epoch": 1.86272,
      "grad_norm": 0.0033870567567646503,
      "learning_rate": 7.582080000000001e-06,
      "loss": 0.0115,
      "step": 58210
    },
    {
      "epoch": 1.86304,
      "grad_norm": 0.18436795473098755,
      "learning_rate": 7.579946666666668e-06,
      "loss": 0.0502,
      "step": 58220
    },
    {
      "epoch": 1.8633600000000001,
      "grad_norm": 0.007070436608046293,
      "learning_rate": 7.577813333333333e-06,
      "loss": 0.0005,
      "step": 58230
    },
    {
      "epoch": 1.86368,
      "grad_norm": 0.0032896995544433594,
      "learning_rate": 7.5756800000000005e-06,
      "loss": 0.0002,
      "step": 58240
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.004449708852916956,
      "learning_rate": 7.573546666666667e-06,
      "loss": 0.0002,
      "step": 58250
    },
    {
      "epoch": 1.86432,
      "grad_norm": 4.79018497467041,
      "learning_rate": 7.571413333333334e-06,
      "loss": 0.0054,
      "step": 58260
    },
    {
      "epoch": 1.86464,
      "grad_norm": 0.0018041692674160004,
      "learning_rate": 7.569280000000001e-06,
      "loss": 0.0002,
      "step": 58270
    },
    {
      "epoch": 1.86496,
      "grad_norm": 0.020162757486104965,
      "learning_rate": 7.567146666666667e-06,
      "loss": 0.0004,
      "step": 58280
    },
    {
      "epoch": 1.86528,
      "grad_norm": 0.005287023726850748,
      "learning_rate": 7.565013333333334e-06,
      "loss": 0.0003,
      "step": 58290
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.001985028386116028,
      "learning_rate": 7.56288e-06,
      "loss": 0.0001,
      "step": 58300
    },
    {
      "epoch": 1.86592,
      "grad_norm": 0.002183441072702408,
      "learning_rate": 7.560746666666667e-06,
      "loss": 0.0002,
      "step": 58310
    },
    {
      "epoch": 1.86624,
      "grad_norm": 0.004430290311574936,
      "learning_rate": 7.5586133333333345e-06,
      "loss": 0.0013,
      "step": 58320
    },
    {
      "epoch": 1.86656,
      "grad_norm": 0.006337433122098446,
      "learning_rate": 7.556480000000001e-06,
      "loss": 0.003,
      "step": 58330
    },
    {
      "epoch": 1.86688,
      "grad_norm": 0.00185200956184417,
      "learning_rate": 7.554346666666668e-06,
      "loss": 0.0084,
      "step": 58340
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.0028583952225744724,
      "learning_rate": 7.552213333333334e-06,
      "loss": 0.022,
      "step": 58350
    },
    {
      "epoch": 1.8675199999999998,
      "grad_norm": 0.0051962267607450485,
      "learning_rate": 7.550080000000001e-06,
      "loss": 0.0143,
      "step": 58360
    },
    {
      "epoch": 1.86784,
      "grad_norm": 0.0037638074718415737,
      "learning_rate": 7.5479466666666664e-06,
      "loss": 0.0002,
      "step": 58370
    },
    {
      "epoch": 1.86816,
      "grad_norm": 0.004281930159777403,
      "learning_rate": 7.5458133333333335e-06,
      "loss": 0.018,
      "step": 58380
    },
    {
      "epoch": 1.86848,
      "grad_norm": 0.006709343288093805,
      "learning_rate": 7.543680000000001e-06,
      "loss": 0.0002,
      "step": 58390
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.0023167438339442015,
      "learning_rate": 7.541546666666667e-06,
      "loss": 0.0026,
      "step": 58400
    },
    {
      "epoch": 1.8691200000000001,
      "grad_norm": 0.0028589186258614063,
      "learning_rate": 7.539413333333334e-06,
      "loss": 0.0001,
      "step": 58410
    },
    {
      "epoch": 1.86944,
      "grad_norm": 0.003517280798405409,
      "learning_rate": 7.53728e-06,
      "loss": 0.0508,
      "step": 58420
    },
    {
      "epoch": 1.8697599999999999,
      "grad_norm": 0.032149504870176315,
      "learning_rate": 7.535146666666667e-06,
      "loss": 0.0002,
      "step": 58430
    },
    {
      "epoch": 1.87008,
      "grad_norm": 0.003955674357712269,
      "learning_rate": 7.533013333333334e-06,
      "loss": 0.0002,
      "step": 58440
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.0047838580794632435,
      "learning_rate": 7.5308800000000005e-06,
      "loss": 0.0002,
      "step": 58450
    },
    {
      "epoch": 1.87072,
      "grad_norm": 0.005126201547682285,
      "learning_rate": 7.5287466666666675e-06,
      "loss": 0.0003,
      "step": 58460
    },
    {
      "epoch": 1.87104,
      "grad_norm": 0.002234817948192358,
      "learning_rate": 7.526613333333334e-06,
      "loss": 0.0001,
      "step": 58470
    },
    {
      "epoch": 1.8713600000000001,
      "grad_norm": 0.0072636776603758335,
      "learning_rate": 7.524480000000001e-06,
      "loss": 0.0002,
      "step": 58480
    },
    {
      "epoch": 1.87168,
      "grad_norm": 0.004161133896559477,
      "learning_rate": 7.522346666666668e-06,
      "loss": 0.0003,
      "step": 58490
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.003578739007934928,
      "learning_rate": 7.520213333333334e-06,
      "loss": 0.0002,
      "step": 58500
    },
    {
      "epoch": 1.87232,
      "grad_norm": 0.012246692553162575,
      "learning_rate": 7.518080000000001e-06,
      "loss": 0.0002,
      "step": 58510
    },
    {
      "epoch": 1.87264,
      "grad_norm": 0.00978328101336956,
      "learning_rate": 7.515946666666667e-06,
      "loss": 0.0009,
      "step": 58520
    },
    {
      "epoch": 1.87296,
      "grad_norm": 0.007629660423845053,
      "learning_rate": 7.513813333333334e-06,
      "loss": 0.0005,
      "step": 58530
    },
    {
      "epoch": 1.8732799999999998,
      "grad_norm": 0.011236864142119884,
      "learning_rate": 7.51168e-06,
      "loss": 0.0004,
      "step": 58540
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.001734791905619204,
      "learning_rate": 7.509546666666667e-06,
      "loss": 0.0001,
      "step": 58550
    },
    {
      "epoch": 1.87392,
      "grad_norm": 0.0038074899930506945,
      "learning_rate": 7.507413333333334e-06,
      "loss": 0.0001,
      "step": 58560
    },
    {
      "epoch": 1.87424,
      "grad_norm": 0.015391788445413113,
      "learning_rate": 7.50528e-06,
      "loss": 0.0003,
      "step": 58570
    },
    {
      "epoch": 1.87456,
      "grad_norm": 0.0025663571432232857,
      "learning_rate": 7.503146666666667e-06,
      "loss": 0.0001,
      "step": 58580
    },
    {
      "epoch": 1.87488,
      "grad_norm": 0.0052319238893687725,
      "learning_rate": 7.5010133333333335e-06,
      "loss": 0.0002,
      "step": 58590
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.03876350820064545,
      "learning_rate": 7.498880000000001e-06,
      "loss": 0.0002,
      "step": 58600
    },
    {
      "epoch": 1.8755199999999999,
      "grad_norm": 0.013960978016257286,
      "learning_rate": 7.496746666666668e-06,
      "loss": 0.0002,
      "step": 58610
    },
    {
      "epoch": 1.87584,
      "grad_norm": 0.0027569083031266928,
      "learning_rate": 7.494613333333334e-06,
      "loss": 0.0001,
      "step": 58620
    },
    {
      "epoch": 1.87616,
      "grad_norm": 0.0017662602476775646,
      "learning_rate": 7.492480000000001e-06,
      "loss": 0.0004,
      "step": 58630
    },
    {
      "epoch": 1.87648,
      "grad_norm": 0.0022299133706837893,
      "learning_rate": 7.490346666666667e-06,
      "loss": 0.0002,
      "step": 58640
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.004179870709776878,
      "learning_rate": 7.488213333333334e-06,
      "loss": 0.0001,
      "step": 58650
    },
    {
      "epoch": 1.8771200000000001,
      "grad_norm": 0.0020211306400597095,
      "learning_rate": 7.486080000000001e-06,
      "loss": 0.0001,
      "step": 58660
    },
    {
      "epoch": 1.87744,
      "grad_norm": 0.0023013378959149122,
      "learning_rate": 7.483946666666667e-06,
      "loss": 0.0017,
      "step": 58670
    },
    {
      "epoch": 1.8777599999999999,
      "grad_norm": 0.0012835952220484614,
      "learning_rate": 7.481813333333335e-06,
      "loss": 0.0988,
      "step": 58680
    },
    {
      "epoch": 1.87808,
      "grad_norm": 0.00622401712462306,
      "learning_rate": 7.47968e-06,
      "loss": 0.0002,
      "step": 58690
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.0037482145708054304,
      "learning_rate": 7.477546666666667e-06,
      "loss": 0.0002,
      "step": 58700
    },
    {
      "epoch": 1.87872,
      "grad_norm": 0.014715862460434437,
      "learning_rate": 7.475413333333333e-06,
      "loss": 0.0002,
      "step": 58710
    },
    {
      "epoch": 1.87904,
      "grad_norm": 0.003305374411866069,
      "learning_rate": 7.47328e-06,
      "loss": 0.0028,
      "step": 58720
    },
    {
      "epoch": 1.8793600000000001,
      "grad_norm": 0.00341212865896523,
      "learning_rate": 7.4711466666666675e-06,
      "loss": 0.0002,
      "step": 58730
    },
    {
      "epoch": 1.87968,
      "grad_norm": 0.003897876013070345,
      "learning_rate": 7.469013333333334e-06,
      "loss": 0.0003,
      "step": 58740
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.004344921559095383,
      "learning_rate": 7.466880000000001e-06,
      "loss": 0.0002,
      "step": 58750
    },
    {
      "epoch": 1.88032,
      "grad_norm": 0.018991079181432724,
      "learning_rate": 7.464746666666667e-06,
      "loss": 0.0003,
      "step": 58760
    },
    {
      "epoch": 1.88064,
      "grad_norm": 0.0029647720512002707,
      "learning_rate": 7.462613333333334e-06,
      "loss": 0.0269,
      "step": 58770
    },
    {
      "epoch": 1.88096,
      "grad_norm": 0.0036001268308609724,
      "learning_rate": 7.460480000000001e-06,
      "loss": 0.0148,
      "step": 58780
    },
    {
      "epoch": 1.8812799999999998,
      "grad_norm": 0.003632253035902977,
      "learning_rate": 7.458346666666667e-06,
      "loss": 0.002,
      "step": 58790
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.0028408803045749664,
      "learning_rate": 7.456213333333334e-06,
      "loss": 0.0492,
      "step": 58800
    },
    {
      "epoch": 1.88192,
      "grad_norm": 0.0013558526989072561,
      "learning_rate": 7.45408e-06,
      "loss": 0.0002,
      "step": 58810
    },
    {
      "epoch": 1.88224,
      "grad_norm": 0.02972348965704441,
      "learning_rate": 7.451946666666668e-06,
      "loss": 0.0002,
      "step": 58820
    },
    {
      "epoch": 1.88256,
      "grad_norm": 0.0021714037284255028,
      "learning_rate": 7.449813333333335e-06,
      "loss": 0.0002,
      "step": 58830
    },
    {
      "epoch": 1.88288,
      "grad_norm": 0.016586238518357277,
      "learning_rate": 7.44768e-06,
      "loss": 0.0025,
      "step": 58840
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.006457192357629538,
      "learning_rate": 7.445546666666667e-06,
      "loss": 0.0001,
      "step": 58850
    },
    {
      "epoch": 1.8835199999999999,
      "grad_norm": 0.006415685173124075,
      "learning_rate": 7.4434133333333335e-06,
      "loss": 0.0001,
      "step": 58860
    },
    {
      "epoch": 1.88384,
      "grad_norm": 0.0032637142576277256,
      "learning_rate": 7.4412800000000005e-06,
      "loss": 0.0239,
      "step": 58870
    },
    {
      "epoch": 1.88416,
      "grad_norm": 0.00219979346729815,
      "learning_rate": 7.439146666666667e-06,
      "loss": 0.0005,
      "step": 58880
    },
    {
      "epoch": 1.88448,
      "grad_norm": 0.0019870775286108255,
      "learning_rate": 7.437013333333334e-06,
      "loss": 0.0002,
      "step": 58890
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.002150506479665637,
      "learning_rate": 7.434880000000001e-06,
      "loss": 0.0011,
      "step": 58900
    },
    {
      "epoch": 1.8851200000000001,
      "grad_norm": 0.0023899225052446127,
      "learning_rate": 7.432746666666667e-06,
      "loss": 0.0007,
      "step": 58910
    },
    {
      "epoch": 1.88544,
      "grad_norm": 0.005022452212870121,
      "learning_rate": 7.430613333333334e-06,
      "loss": 0.0002,
      "step": 58920
    },
    {
      "epoch": 1.8857599999999999,
      "grad_norm": 0.0023896144703030586,
      "learning_rate": 7.42848e-06,
      "loss": 0.0346,
      "step": 58930
    },
    {
      "epoch": 1.88608,
      "grad_norm": 0.004393167793750763,
      "learning_rate": 7.4263466666666675e-06,
      "loss": 0.0001,
      "step": 58940
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.0019729933701455593,
      "learning_rate": 7.4242133333333345e-06,
      "loss": 0.0002,
      "step": 58950
    },
    {
      "epoch": 1.88672,
      "grad_norm": 0.007618024945259094,
      "learning_rate": 7.422080000000001e-06,
      "loss": 0.0002,
      "step": 58960
    },
    {
      "epoch": 1.88704,
      "grad_norm": 0.0037222588434815407,
      "learning_rate": 7.419946666666668e-06,
      "loss": 0.0001,
      "step": 58970
    },
    {
      "epoch": 1.8873600000000001,
      "grad_norm": 0.0026204839814454317,
      "learning_rate": 7.417813333333333e-06,
      "loss": 0.0002,
      "step": 58980
    },
    {
      "epoch": 1.88768,
      "grad_norm": 0.0024726775009185076,
      "learning_rate": 7.41568e-06,
      "loss": 0.0001,
      "step": 58990
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.004041784908622503,
      "learning_rate": 7.413546666666667e-06,
      "loss": 0.0003,
      "step": 59000
    },
    {
      "epoch": 1.88832,
      "grad_norm": 1.7151718139648438,
      "learning_rate": 7.411413333333334e-06,
      "loss": 0.0033,
      "step": 59010
    },
    {
      "epoch": 1.88864,
      "grad_norm": 0.004158508498221636,
      "learning_rate": 7.409280000000001e-06,
      "loss": 0.0098,
      "step": 59020
    },
    {
      "epoch": 1.88896,
      "grad_norm": 7.967478275299072,
      "learning_rate": 7.407146666666667e-06,
      "loss": 0.0158,
      "step": 59030
    },
    {
      "epoch": 1.8892799999999998,
      "grad_norm": 0.0012384949950501323,
      "learning_rate": 7.405013333333334e-06,
      "loss": 0.0001,
      "step": 59040
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.0025551426224410534,
      "learning_rate": 7.40288e-06,
      "loss": 0.0132,
      "step": 59050
    },
    {
      "epoch": 1.88992,
      "grad_norm": 0.005151761695742607,
      "learning_rate": 7.400746666666667e-06,
      "loss": 0.0595,
      "step": 59060
    },
    {
      "epoch": 1.89024,
      "grad_norm": 0.0034736942034214735,
      "learning_rate": 7.398613333333334e-06,
      "loss": 0.0002,
      "step": 59070
    },
    {
      "epoch": 1.89056,
      "grad_norm": 0.003476641606539488,
      "learning_rate": 7.3964800000000005e-06,
      "loss": 0.0001,
      "step": 59080
    },
    {
      "epoch": 1.8908800000000001,
      "grad_norm": 0.002349322196096182,
      "learning_rate": 7.394346666666668e-06,
      "loss": 0.0002,
      "step": 59090
    },
    {
      "epoch": 1.8912,
      "grad_norm": 1.8405307531356812,
      "learning_rate": 7.392213333333334e-06,
      "loss": 0.0563,
      "step": 59100
    },
    {
      "epoch": 1.8915199999999999,
      "grad_norm": 0.0030248549301177263,
      "learning_rate": 7.390080000000001e-06,
      "loss": 0.0002,
      "step": 59110
    },
    {
      "epoch": 1.89184,
      "grad_norm": 0.002540119457989931,
      "learning_rate": 7.387946666666668e-06,
      "loss": 0.0001,
      "step": 59120
    },
    {
      "epoch": 1.89216,
      "grad_norm": 0.0024488160852342844,
      "learning_rate": 7.385813333333333e-06,
      "loss": 0.0086,
      "step": 59130
    },
    {
      "epoch": 1.89248,
      "grad_norm": 0.0012163458159193397,
      "learning_rate": 7.3836800000000004e-06,
      "loss": 0.0001,
      "step": 59140
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.026352480053901672,
      "learning_rate": 7.381546666666667e-06,
      "loss": 0.0002,
      "step": 59150
    },
    {
      "epoch": 1.8931200000000001,
      "grad_norm": 0.002729259431362152,
      "learning_rate": 7.379413333333334e-06,
      "loss": 0.0053,
      "step": 59160
    },
    {
      "epoch": 1.89344,
      "grad_norm": 0.004770650528371334,
      "learning_rate": 7.377280000000001e-06,
      "loss": 0.0004,
      "step": 59170
    },
    {
      "epoch": 1.8937599999999999,
      "grad_norm": 0.006496143061667681,
      "learning_rate": 7.375146666666667e-06,
      "loss": 0.0002,
      "step": 59180
    },
    {
      "epoch": 1.89408,
      "grad_norm": 0.005560163874179125,
      "learning_rate": 7.373013333333334e-06,
      "loss": 0.0001,
      "step": 59190
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.0020417904015630484,
      "learning_rate": 7.37088e-06,
      "loss": 0.0002,
      "step": 59200
    },
    {
      "epoch": 1.89472,
      "grad_norm": 0.0050983610562980175,
      "learning_rate": 7.368746666666667e-06,
      "loss": 0.0002,
      "step": 59210
    },
    {
      "epoch": 1.8950399999999998,
      "grad_norm": 0.002932645147666335,
      "learning_rate": 7.366613333333334e-06,
      "loss": 0.0001,
      "step": 59220
    },
    {
      "epoch": 1.8953600000000002,
      "grad_norm": 0.002373049734160304,
      "learning_rate": 7.364480000000001e-06,
      "loss": 0.022,
      "step": 59230
    },
    {
      "epoch": 1.89568,
      "grad_norm": 0.002478391630575061,
      "learning_rate": 7.362346666666668e-06,
      "loss": 0.0002,
      "step": 59240
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.006185648962855339,
      "learning_rate": 7.360213333333334e-06,
      "loss": 0.0002,
      "step": 59250
    },
    {
      "epoch": 1.89632,
      "grad_norm": 0.0071535357274115086,
      "learning_rate": 7.358080000000001e-06,
      "loss": 0.0591,
      "step": 59260
    },
    {
      "epoch": 1.89664,
      "grad_norm": 0.002103745937347412,
      "learning_rate": 7.3559466666666664e-06,
      "loss": 0.0006,
      "step": 59270
    },
    {
      "epoch": 1.89696,
      "grad_norm": 0.002007102593779564,
      "learning_rate": 7.3538133333333335e-06,
      "loss": 0.0172,
      "step": 59280
    },
    {
      "epoch": 1.8972799999999999,
      "grad_norm": 0.002630263799801469,
      "learning_rate": 7.351680000000001e-06,
      "loss": 0.0003,
      "step": 59290
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.0929877832531929,
      "learning_rate": 7.349546666666667e-06,
      "loss": 0.0006,
      "step": 59300
    },
    {
      "epoch": 1.89792,
      "grad_norm": 0.002159649273380637,
      "learning_rate": 7.347413333333334e-06,
      "loss": 0.0001,
      "step": 59310
    },
    {
      "epoch": 1.89824,
      "grad_norm": 0.002468324499204755,
      "learning_rate": 7.34528e-06,
      "loss": 0.0002,
      "step": 59320
    },
    {
      "epoch": 1.89856,
      "grad_norm": 0.09688124805688858,
      "learning_rate": 7.343146666666667e-06,
      "loss": 0.0006,
      "step": 59330
    },
    {
      "epoch": 1.8988800000000001,
      "grad_norm": 0.0036191935651004314,
      "learning_rate": 7.341013333333334e-06,
      "loss": 0.0002,
      "step": 59340
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.004057364538311958,
      "learning_rate": 7.3388800000000005e-06,
      "loss": 0.0009,
      "step": 59350
    },
    {
      "epoch": 1.8995199999999999,
      "grad_norm": 0.011827203445136547,
      "learning_rate": 7.3367466666666675e-06,
      "loss": 0.0003,
      "step": 59360
    },
    {
      "epoch": 1.89984,
      "grad_norm": 0.002613919088616967,
      "learning_rate": 7.334613333333334e-06,
      "loss": 0.0001,
      "step": 59370
    },
    {
      "epoch": 1.90016,
      "grad_norm": 0.0071272775530815125,
      "learning_rate": 7.332480000000001e-06,
      "loss": 0.0002,
      "step": 59380
    },
    {
      "epoch": 1.90048,
      "grad_norm": 0.0024822300765663385,
      "learning_rate": 7.330346666666667e-06,
      "loss": 0.0001,
      "step": 59390
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.001918522990308702,
      "learning_rate": 7.328213333333334e-06,
      "loss": 0.0464,
      "step": 59400
    },
    {
      "epoch": 1.9011200000000001,
      "grad_norm": 0.004426491912454367,
      "learning_rate": 7.326080000000001e-06,
      "loss": 0.0001,
      "step": 59410
    },
    {
      "epoch": 1.90144,
      "grad_norm": 0.038714636117219925,
      "learning_rate": 7.3239466666666666e-06,
      "loss": 0.0002,
      "step": 59420
    },
    {
      "epoch": 1.90176,
      "grad_norm": 0.0029178294353187084,
      "learning_rate": 7.3218133333333345e-06,
      "loss": 0.0002,
      "step": 59430
    },
    {
      "epoch": 1.90208,
      "grad_norm": 0.0029651785735040903,
      "learning_rate": 7.31968e-06,
      "loss": 0.0001,
      "step": 59440
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.12498591095209122,
      "learning_rate": 7.317546666666667e-06,
      "loss": 0.0003,
      "step": 59450
    },
    {
      "epoch": 1.90272,
      "grad_norm": 0.002931168768554926,
      "learning_rate": 7.315413333333334e-06,
      "loss": 0.0001,
      "step": 59460
    },
    {
      "epoch": 1.9030399999999998,
      "grad_norm": 0.00338184111751616,
      "learning_rate": 7.31328e-06,
      "loss": 0.0002,
      "step": 59470
    },
    {
      "epoch": 1.9033600000000002,
      "grad_norm": 0.0034806421026587486,
      "learning_rate": 7.311146666666667e-06,
      "loss": 0.0001,
      "step": 59480
    },
    {
      "epoch": 1.90368,
      "grad_norm": 0.0063445731066167355,
      "learning_rate": 7.3090133333333335e-06,
      "loss": 0.0002,
      "step": 59490
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.0011230309028178453,
      "learning_rate": 7.306880000000001e-06,
      "loss": 0.0001,
      "step": 59500
    },
    {
      "epoch": 1.90432,
      "grad_norm": 0.0019746741745620966,
      "learning_rate": 7.304746666666668e-06,
      "loss": 0.0002,
      "step": 59510
    },
    {
      "epoch": 1.90464,
      "grad_norm": 0.004598589614033699,
      "learning_rate": 7.302613333333334e-06,
      "loss": 0.0002,
      "step": 59520
    },
    {
      "epoch": 1.90496,
      "grad_norm": 0.0028059605974704027,
      "learning_rate": 7.300480000000001e-06,
      "loss": 0.0001,
      "step": 59530
    },
    {
      "epoch": 1.9052799999999999,
      "grad_norm": 0.006701755803078413,
      "learning_rate": 7.298346666666667e-06,
      "loss": 0.0002,
      "step": 59540
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.001258104806765914,
      "learning_rate": 7.296213333333334e-06,
      "loss": 0.0001,
      "step": 59550
    },
    {
      "epoch": 1.90592,
      "grad_norm": 0.004516082350164652,
      "learning_rate": 7.29408e-06,
      "loss": 0.0896,
      "step": 59560
    },
    {
      "epoch": 1.90624,
      "grad_norm": 0.003633719403296709,
      "learning_rate": 7.2919466666666675e-06,
      "loss": 0.0001,
      "step": 59570
    },
    {
      "epoch": 1.90656,
      "grad_norm": 0.2474326640367508,
      "learning_rate": 7.289813333333335e-06,
      "loss": 0.0005,
      "step": 59580
    },
    {
      "epoch": 1.9068800000000001,
      "grad_norm": 0.00398971326649189,
      "learning_rate": 7.28768e-06,
      "loss": 0.0109,
      "step": 59590
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.0033843463752418756,
      "learning_rate": 7.285546666666667e-06,
      "loss": 0.0003,
      "step": 59600
    },
    {
      "epoch": 1.9075199999999999,
      "grad_norm": 0.002568041905760765,
      "learning_rate": 7.283413333333333e-06,
      "loss": 0.0001,
      "step": 59610
    },
    {
      "epoch": 1.90784,
      "grad_norm": 0.0018033271189779043,
      "learning_rate": 7.28128e-06,
      "loss": 0.0002,
      "step": 59620
    },
    {
      "epoch": 1.90816,
      "grad_norm": 0.0028187085408717394,
      "learning_rate": 7.2791466666666674e-06,
      "loss": 0.0001,
      "step": 59630
    },
    {
      "epoch": 1.90848,
      "grad_norm": 0.0025123560335487127,
      "learning_rate": 7.277013333333334e-06,
      "loss": 0.0001,
      "step": 59640
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.003231561044231057,
      "learning_rate": 7.274880000000001e-06,
      "loss": 0.0002,
      "step": 59650
    },
    {
      "epoch": 1.9091200000000002,
      "grad_norm": 0.0015822426648810506,
      "learning_rate": 7.272746666666667e-06,
      "loss": 0.0002,
      "step": 59660
    },
    {
      "epoch": 1.90944,
      "grad_norm": 0.002999611897394061,
      "learning_rate": 7.270613333333334e-06,
      "loss": 0.0005,
      "step": 59670
    },
    {
      "epoch": 1.90976,
      "grad_norm": 0.0033451346680521965,
      "learning_rate": 7.268480000000001e-06,
      "loss": 0.0001,
      "step": 59680
    },
    {
      "epoch": 1.91008,
      "grad_norm": 0.002414878224954009,
      "learning_rate": 7.266346666666667e-06,
      "loss": 0.0001,
      "step": 59690
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.0034728029277175665,
      "learning_rate": 7.264213333333334e-06,
      "loss": 0.0507,
      "step": 59700
    },
    {
      "epoch": 1.91072,
      "grad_norm": 0.0019282393623143435,
      "learning_rate": 7.262080000000001e-06,
      "loss": 0.0001,
      "step": 59710
    },
    {
      "epoch": 1.9110399999999998,
      "grad_norm": 0.005224887281656265,
      "learning_rate": 7.259946666666668e-06,
      "loss": 0.0003,
      "step": 59720
    },
    {
      "epoch": 1.91136,
      "grad_norm": 0.0021943291649222374,
      "learning_rate": 7.257813333333333e-06,
      "loss": 0.0002,
      "step": 59730
    },
    {
      "epoch": 1.91168,
      "grad_norm": 0.0029480168595910072,
      "learning_rate": 7.25568e-06,
      "loss": 0.0002,
      "step": 59740
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.0029748815577477217,
      "learning_rate": 7.253546666666667e-06,
      "loss": 0.0002,
      "step": 59750
    },
    {
      "epoch": 1.91232,
      "grad_norm": 0.08400178700685501,
      "learning_rate": 7.2514133333333334e-06,
      "loss": 0.0003,
      "step": 59760
    },
    {
      "epoch": 1.9126400000000001,
      "grad_norm": 0.003455128986388445,
      "learning_rate": 7.2492800000000005e-06,
      "loss": 0.028,
      "step": 59770
    },
    {
      "epoch": 1.91296,
      "grad_norm": 0.007478042040020227,
      "learning_rate": 7.247146666666667e-06,
      "loss": 0.0002,
      "step": 59780
    },
    {
      "epoch": 1.9132799999999999,
      "grad_norm": 0.0055381725542247295,
      "learning_rate": 7.245013333333334e-06,
      "loss": 0.0002,
      "step": 59790
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.004527480341494083,
      "learning_rate": 7.242880000000001e-06,
      "loss": 0.0007,
      "step": 59800
    },
    {
      "epoch": 1.91392,
      "grad_norm": 0.002782115014269948,
      "learning_rate": 7.240746666666667e-06,
      "loss": 0.0001,
      "step": 59810
    },
    {
      "epoch": 1.91424,
      "grad_norm": 0.0024148437660187483,
      "learning_rate": 7.238613333333334e-06,
      "loss": 0.0531,
      "step": 59820
    },
    {
      "epoch": 1.91456,
      "grad_norm": 0.003296748735010624,
      "learning_rate": 7.23648e-06,
      "loss": 0.0002,
      "step": 59830
    },
    {
      "epoch": 1.9148800000000001,
      "grad_norm": 0.002625302877277136,
      "learning_rate": 7.2343466666666675e-06,
      "loss": 0.0011,
      "step": 59840
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.010197047144174576,
      "learning_rate": 7.2322133333333345e-06,
      "loss": 0.0004,
      "step": 59850
    },
    {
      "epoch": 1.91552,
      "grad_norm": 0.003182837041094899,
      "learning_rate": 7.230080000000001e-06,
      "loss": 0.0099,
      "step": 59860
    },
    {
      "epoch": 1.91584,
      "grad_norm": 0.008547735400497913,
      "learning_rate": 7.227946666666668e-06,
      "loss": 0.0002,
      "step": 59870
    },
    {
      "epoch": 1.91616,
      "grad_norm": 0.007455378770828247,
      "learning_rate": 7.225813333333333e-06,
      "loss": 0.0004,
      "step": 59880
    },
    {
      "epoch": 1.91648,
      "grad_norm": 0.0034804148599505424,
      "learning_rate": 7.223680000000001e-06,
      "loss": 0.0002,
      "step": 59890
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.002959189238026738,
      "learning_rate": 7.2215466666666665e-06,
      "loss": 0.0197,
      "step": 59900
    },
    {
      "epoch": 1.9171200000000002,
      "grad_norm": 0.002454641042277217,
      "learning_rate": 7.2194133333333336e-06,
      "loss": 0.0002,
      "step": 59910
    },
    {
      "epoch": 1.91744,
      "grad_norm": 0.007168113719671965,
      "learning_rate": 7.217280000000001e-06,
      "loss": 0.0715,
      "step": 59920
    },
    {
      "epoch": 1.91776,
      "grad_norm": 0.001516424585133791,
      "learning_rate": 7.215146666666667e-06,
      "loss": 0.0001,
      "step": 59930
    },
    {
      "epoch": 1.91808,
      "grad_norm": 0.003090135520324111,
      "learning_rate": 7.213013333333334e-06,
      "loss": 0.0314,
      "step": 59940
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.018334858119487762,
      "learning_rate": 7.21088e-06,
      "loss": 0.0006,
      "step": 59950
    },
    {
      "epoch": 1.91872,
      "grad_norm": 0.0024505460169166327,
      "learning_rate": 7.208746666666667e-06,
      "loss": 0.0003,
      "step": 59960
    },
    {
      "epoch": 1.9190399999999999,
      "grad_norm": 0.0070802983827888966,
      "learning_rate": 7.206613333333334e-06,
      "loss": 0.0002,
      "step": 59970
    },
    {
      "epoch": 1.91936,
      "grad_norm": 0.004783446900546551,
      "learning_rate": 7.2044800000000005e-06,
      "loss": 0.0002,
      "step": 59980
    },
    {
      "epoch": 1.91968,
      "grad_norm": 0.004580485634505749,
      "learning_rate": 7.202346666666668e-06,
      "loss": 0.0002,
      "step": 59990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.00967506691813469,
      "learning_rate": 7.200213333333334e-06,
      "loss": 0.0611,
      "step": 60000
    },
    {
      "epoch": 1.92032,
      "grad_norm": 2.2437779903411865,
      "learning_rate": 7.198080000000001e-06,
      "loss": 0.0519,
      "step": 60010
    },
    {
      "epoch": 1.9206400000000001,
      "grad_norm": 0.028499526903033257,
      "learning_rate": 7.195946666666668e-06,
      "loss": 0.0003,
      "step": 60020
    },
    {
      "epoch": 1.92096,
      "grad_norm": 0.013226205483078957,
      "learning_rate": 7.193813333333334e-06,
      "loss": 0.0009,
      "step": 60030
    },
    {
      "epoch": 1.9212799999999999,
      "grad_norm": 0.005766281392425299,
      "learning_rate": 7.191680000000001e-06,
      "loss": 0.0003,
      "step": 60040
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.0030601744074374437,
      "learning_rate": 7.189546666666667e-06,
      "loss": 0.0002,
      "step": 60050
    },
    {
      "epoch": 1.92192,
      "grad_norm": 0.022418055683374405,
      "learning_rate": 7.187413333333334e-06,
      "loss": 0.0004,
      "step": 60060
    },
    {
      "epoch": 1.92224,
      "grad_norm": 0.0061866482719779015,
      "learning_rate": 7.18528e-06,
      "loss": 0.0541,
      "step": 60070
    },
    {
      "epoch": 1.92256,
      "grad_norm": 0.015065492130815983,
      "learning_rate": 7.183146666666667e-06,
      "loss": 0.0003,
      "step": 60080
    },
    {
      "epoch": 1.9228800000000001,
      "grad_norm": 0.003889915067702532,
      "learning_rate": 7.181013333333334e-06,
      "loss": 0.0003,
      "step": 60090
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.00839000940322876,
      "learning_rate": 7.17888e-06,
      "loss": 0.0003,
      "step": 60100
    },
    {
      "epoch": 1.92352,
      "grad_norm": 0.006962149403989315,
      "learning_rate": 7.176746666666667e-06,
      "loss": 0.0002,
      "step": 60110
    },
    {
      "epoch": 1.92384,
      "grad_norm": 0.0075781261548399925,
      "learning_rate": 7.174613333333334e-06,
      "loss": 0.0154,
      "step": 60120
    },
    {
      "epoch": 1.92416,
      "grad_norm": 0.005095114931464195,
      "learning_rate": 7.172480000000001e-06,
      "loss": 0.0002,
      "step": 60130
    },
    {
      "epoch": 1.92448,
      "grad_norm": 0.004517811816185713,
      "learning_rate": 7.170346666666668e-06,
      "loss": 0.0008,
      "step": 60140
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.005016289185732603,
      "learning_rate": 7.168213333333334e-06,
      "loss": 0.0004,
      "step": 60150
    },
    {
      "epoch": 1.9251200000000002,
      "grad_norm": 0.007815786637365818,
      "learning_rate": 7.166080000000001e-06,
      "loss": 0.0002,
      "step": 60160
    },
    {
      "epoch": 1.92544,
      "grad_norm": 0.0038991812616586685,
      "learning_rate": 7.163946666666667e-06,
      "loss": 0.0005,
      "step": 60170
    },
    {
      "epoch": 1.92576,
      "grad_norm": 0.13525161147117615,
      "learning_rate": 7.161813333333334e-06,
      "loss": 0.001,
      "step": 60180
    },
    {
      "epoch": 1.92608,
      "grad_norm": 0.009065240621566772,
      "learning_rate": 7.159680000000001e-06,
      "loss": 0.0006,
      "step": 60190
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.0025156387127935886,
      "learning_rate": 7.157546666666667e-06,
      "loss": 0.0058,
      "step": 60200
    },
    {
      "epoch": 1.92672,
      "grad_norm": 3.69360089302063,
      "learning_rate": 7.155413333333334e-06,
      "loss": 0.0402,
      "step": 60210
    },
    {
      "epoch": 1.9270399999999999,
      "grad_norm": 0.005337567999958992,
      "learning_rate": 7.15328e-06,
      "loss": 0.0003,
      "step": 60220
    },
    {
      "epoch": 1.92736,
      "grad_norm": 0.00429570022970438,
      "learning_rate": 7.151146666666667e-06,
      "loss": 0.0002,
      "step": 60230
    },
    {
      "epoch": 1.92768,
      "grad_norm": 0.008341594599187374,
      "learning_rate": 7.149013333333334e-06,
      "loss": 0.0002,
      "step": 60240
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.0018433451186865568,
      "learning_rate": 7.1468800000000004e-06,
      "loss": 0.0014,
      "step": 60250
    },
    {
      "epoch": 1.92832,
      "grad_norm": 0.0024281516671180725,
      "learning_rate": 7.1447466666666675e-06,
      "loss": 0.0002,
      "step": 60260
    },
    {
      "epoch": 1.9286400000000001,
      "grad_norm": 0.0032219071872532368,
      "learning_rate": 7.142613333333334e-06,
      "loss": 0.0056,
      "step": 60270
    },
    {
      "epoch": 1.92896,
      "grad_norm": 0.004560815170407295,
      "learning_rate": 7.140480000000001e-06,
      "loss": 0.0002,
      "step": 60280
    },
    {
      "epoch": 1.9292799999999999,
      "grad_norm": 0.006269361823797226,
      "learning_rate": 7.138346666666667e-06,
      "loss": 0.0002,
      "step": 60290
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.003211667761206627,
      "learning_rate": 7.136213333333334e-06,
      "loss": 0.0005,
      "step": 60300
    },
    {
      "epoch": 1.92992,
      "grad_norm": 0.007746781688183546,
      "learning_rate": 7.134080000000001e-06,
      "loss": 0.0003,
      "step": 60310
    },
    {
      "epoch": 1.93024,
      "grad_norm": 0.0026142995338886976,
      "learning_rate": 7.131946666666667e-06,
      "loss": 0.0003,
      "step": 60320
    },
    {
      "epoch": 1.93056,
      "grad_norm": 0.008811280131340027,
      "learning_rate": 7.1298133333333345e-06,
      "loss": 0.0003,
      "step": 60330
    },
    {
      "epoch": 1.9308800000000002,
      "grad_norm": 0.004571257159113884,
      "learning_rate": 7.12768e-06,
      "loss": 0.0003,
      "step": 60340
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.008232117630541325,
      "learning_rate": 7.125546666666667e-06,
      "loss": 0.0002,
      "step": 60350
    },
    {
      "epoch": 1.93152,
      "grad_norm": 1.4132227897644043,
      "learning_rate": 7.123413333333335e-06,
      "loss": 0.0267,
      "step": 60360
    },
    {
      "epoch": 1.93184,
      "grad_norm": 0.003056438872590661,
      "learning_rate": 7.12128e-06,
      "loss": 0.0002,
      "step": 60370
    },
    {
      "epoch": 1.93216,
      "grad_norm": 0.005176398903131485,
      "learning_rate": 7.119146666666667e-06,
      "loss": 0.0006,
      "step": 60380
    },
    {
      "epoch": 1.93248,
      "grad_norm": 0.0033230942208319902,
      "learning_rate": 7.1170133333333335e-06,
      "loss": 0.0002,
      "step": 60390
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 1.071698784828186,
      "learning_rate": 7.1148800000000006e-06,
      "loss": 0.0012,
      "step": 60400
    },
    {
      "epoch": 1.93312,
      "grad_norm": 0.0016759888967499137,
      "learning_rate": 7.112746666666668e-06,
      "loss": 0.0003,
      "step": 60410
    },
    {
      "epoch": 1.93344,
      "grad_norm": 0.010491800494492054,
      "learning_rate": 7.110613333333334e-06,
      "loss": 0.0002,
      "step": 60420
    },
    {
      "epoch": 1.93376,
      "grad_norm": 0.004633889999240637,
      "learning_rate": 7.108480000000001e-06,
      "loss": 0.0002,
      "step": 60430
    },
    {
      "epoch": 1.93408,
      "grad_norm": 0.0030866083689033985,
      "learning_rate": 7.106346666666667e-06,
      "loss": 0.0007,
      "step": 60440
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.014890430495142937,
      "learning_rate": 7.104213333333334e-06,
      "loss": 0.0002,
      "step": 60450
    },
    {
      "epoch": 1.93472,
      "grad_norm": 0.0040153260342776775,
      "learning_rate": 7.1020800000000004e-06,
      "loss": 0.1003,
      "step": 60460
    },
    {
      "epoch": 1.9350399999999999,
      "grad_norm": 0.006255561485886574,
      "learning_rate": 7.0999466666666675e-06,
      "loss": 0.0003,
      "step": 60470
    },
    {
      "epoch": 1.93536,
      "grad_norm": 0.0018676286563277245,
      "learning_rate": 7.097813333333335e-06,
      "loss": 0.0002,
      "step": 60480
    },
    {
      "epoch": 1.93568,
      "grad_norm": 0.0023620426654815674,
      "learning_rate": 7.09568e-06,
      "loss": 0.0002,
      "step": 60490
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.003182640764862299,
      "learning_rate": 7.093546666666668e-06,
      "loss": 0.0004,
      "step": 60500
    },
    {
      "epoch": 1.93632,
      "grad_norm": 0.0020485343411564827,
      "learning_rate": 7.091413333333333e-06,
      "loss": 0.0002,
      "step": 60510
    },
    {
      "epoch": 1.9366400000000001,
      "grad_norm": 0.007524080108851194,
      "learning_rate": 7.08928e-06,
      "loss": 0.0002,
      "step": 60520
    },
    {
      "epoch": 1.93696,
      "grad_norm": 0.004005576483905315,
      "learning_rate": 7.087146666666667e-06,
      "loss": 0.0135,
      "step": 60530
    },
    {
      "epoch": 1.93728,
      "grad_norm": 0.004001688212156296,
      "learning_rate": 7.085013333333334e-06,
      "loss": 0.0002,
      "step": 60540
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.0027792600449174643,
      "learning_rate": 7.082880000000001e-06,
      "loss": 0.0002,
      "step": 60550
    },
    {
      "epoch": 1.93792,
      "grad_norm": 0.003398674773052335,
      "learning_rate": 7.080746666666667e-06,
      "loss": 0.0002,
      "step": 60560
    },
    {
      "epoch": 1.93824,
      "grad_norm": 0.0026930333115160465,
      "learning_rate": 7.078613333333334e-06,
      "loss": 0.0002,
      "step": 60570
    },
    {
      "epoch": 1.9385599999999998,
      "grad_norm": 0.002159395022317767,
      "learning_rate": 7.076480000000001e-06,
      "loss": 0.0003,
      "step": 60580
    },
    {
      "epoch": 1.9388800000000002,
      "grad_norm": 0.003356902627274394,
      "learning_rate": 7.074346666666667e-06,
      "loss": 0.101,
      "step": 60590
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.00320229958742857,
      "learning_rate": 7.072213333333334e-06,
      "loss": 0.0002,
      "step": 60600
    },
    {
      "epoch": 1.93952,
      "grad_norm": 0.012802192941308022,
      "learning_rate": 7.070080000000001e-06,
      "loss": 0.0003,
      "step": 60610
    },
    {
      "epoch": 1.93984,
      "grad_norm": 0.008575959131121635,
      "learning_rate": 7.067946666666668e-06,
      "loss": 0.0058,
      "step": 60620
    },
    {
      "epoch": 1.94016,
      "grad_norm": 0.006023840047419071,
      "learning_rate": 7.065813333333333e-06,
      "loss": 0.0003,
      "step": 60630
    },
    {
      "epoch": 1.94048,
      "grad_norm": 0.0068991417065262794,
      "learning_rate": 7.063680000000001e-06,
      "loss": 0.0002,
      "step": 60640
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.003335917368531227,
      "learning_rate": 7.061546666666668e-06,
      "loss": 0.0003,
      "step": 60650
    },
    {
      "epoch": 1.94112,
      "grad_norm": 0.0070601562038064,
      "learning_rate": 7.059413333333333e-06,
      "loss": 0.0317,
      "step": 60660
    },
    {
      "epoch": 1.94144,
      "grad_norm": 0.004703108221292496,
      "learning_rate": 7.0572800000000005e-06,
      "loss": 0.0002,
      "step": 60670
    },
    {
      "epoch": 1.94176,
      "grad_norm": 0.04592213034629822,
      "learning_rate": 7.055146666666667e-06,
      "loss": 0.0002,
      "step": 60680
    },
    {
      "epoch": 1.94208,
      "grad_norm": 0.00413604686036706,
      "learning_rate": 7.053013333333334e-06,
      "loss": 0.0593,
      "step": 60690
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.003523890394717455,
      "learning_rate": 7.050880000000001e-06,
      "loss": 0.023,
      "step": 60700
    },
    {
      "epoch": 1.94272,
      "grad_norm": 0.004729702603071928,
      "learning_rate": 7.048746666666667e-06,
      "loss": 0.0441,
      "step": 60710
    },
    {
      "epoch": 1.9430399999999999,
      "grad_norm": 0.002923441817983985,
      "learning_rate": 7.046613333333334e-06,
      "loss": 0.0002,
      "step": 60720
    },
    {
      "epoch": 1.94336,
      "grad_norm": 0.005317253060638905,
      "learning_rate": 7.04448e-06,
      "loss": 0.0005,
      "step": 60730
    },
    {
      "epoch": 1.94368,
      "grad_norm": 0.004105994012206793,
      "learning_rate": 7.0423466666666674e-06,
      "loss": 0.0018,
      "step": 60740
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.003273637732490897,
      "learning_rate": 7.0402133333333345e-06,
      "loss": 0.026,
      "step": 60750
    },
    {
      "epoch": 1.94432,
      "grad_norm": 0.011220700107514858,
      "learning_rate": 7.038080000000001e-06,
      "loss": 0.0003,
      "step": 60760
    },
    {
      "epoch": 1.9446400000000001,
      "grad_norm": 0.003474216442555189,
      "learning_rate": 7.035946666666668e-06,
      "loss": 0.0002,
      "step": 60770
    },
    {
      "epoch": 1.94496,
      "grad_norm": 0.05963945761322975,
      "learning_rate": 7.033813333333334e-06,
      "loss": 0.0004,
      "step": 60780
    },
    {
      "epoch": 1.94528,
      "grad_norm": 0.009750552475452423,
      "learning_rate": 7.031680000000001e-06,
      "loss": 0.0003,
      "step": 60790
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.018654942512512207,
      "learning_rate": 7.0295466666666665e-06,
      "loss": 0.0002,
      "step": 60800
    },
    {
      "epoch": 1.94592,
      "grad_norm": 0.9256840348243713,
      "learning_rate": 7.0274133333333335e-06,
      "loss": 0.0014,
      "step": 60810
    },
    {
      "epoch": 1.94624,
      "grad_norm": 0.006869423668831587,
      "learning_rate": 7.025280000000001e-06,
      "loss": 0.0003,
      "step": 60820
    },
    {
      "epoch": 1.9465599999999998,
      "grad_norm": 0.00583361741155386,
      "learning_rate": 7.023146666666667e-06,
      "loss": 0.0002,
      "step": 60830
    },
    {
      "epoch": 1.9468800000000002,
      "grad_norm": 0.005536160431802273,
      "learning_rate": 7.021013333333334e-06,
      "loss": 0.0002,
      "step": 60840
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.01974230818450451,
      "learning_rate": 7.01888e-06,
      "loss": 0.0002,
      "step": 60850
    },
    {
      "epoch": 1.94752,
      "grad_norm": 0.002939725760370493,
      "learning_rate": 7.016746666666667e-06,
      "loss": 0.0002,
      "step": 60860
    },
    {
      "epoch": 1.94784,
      "grad_norm": 0.004888325929641724,
      "learning_rate": 7.014613333333334e-06,
      "loss": 0.0002,
      "step": 60870
    },
    {
      "epoch": 1.9481600000000001,
      "grad_norm": 3.8123691082000732,
      "learning_rate": 7.0124800000000005e-06,
      "loss": 0.0034,
      "step": 60880
    },
    {
      "epoch": 1.94848,
      "grad_norm": 0.010405349545180798,
      "learning_rate": 7.0103466666666676e-06,
      "loss": 0.0002,
      "step": 60890
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.005998497828841209,
      "learning_rate": 7.008213333333334e-06,
      "loss": 0.0002,
      "step": 60900
    },
    {
      "epoch": 1.94912,
      "grad_norm": 0.00587755162268877,
      "learning_rate": 7.006080000000001e-06,
      "loss": 0.0002,
      "step": 60910
    },
    {
      "epoch": 1.94944,
      "grad_norm": 0.18306992948055267,
      "learning_rate": 7.003946666666668e-06,
      "loss": 0.0005,
      "step": 60920
    },
    {
      "epoch": 1.94976,
      "grad_norm": 0.004799763206392527,
      "learning_rate": 7.001813333333334e-06,
      "loss": 0.0008,
      "step": 60930
    },
    {
      "epoch": 1.95008,
      "grad_norm": 0.0036408600863069296,
      "learning_rate": 6.999680000000001e-06,
      "loss": 0.0003,
      "step": 60940
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.005339142866432667,
      "learning_rate": 6.997546666666667e-06,
      "loss": 0.0607,
      "step": 60950
    },
    {
      "epoch": 1.95072,
      "grad_norm": 0.0024315095506608486,
      "learning_rate": 6.995413333333334e-06,
      "loss": 0.0002,
      "step": 60960
    },
    {
      "epoch": 1.9510399999999999,
      "grad_norm": 0.009241842664778233,
      "learning_rate": 6.99328e-06,
      "loss": 0.0003,
      "step": 60970
    },
    {
      "epoch": 1.95136,
      "grad_norm": 0.025231584906578064,
      "learning_rate": 6.991146666666667e-06,
      "loss": 0.0003,
      "step": 60980
    },
    {
      "epoch": 1.95168,
      "grad_norm": 0.011750539764761925,
      "learning_rate": 6.989013333333334e-06,
      "loss": 0.0035,
      "step": 60990
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.004221634473651648,
      "learning_rate": 6.98688e-06,
      "loss": 0.0098,
      "step": 61000
    },
    {
      "epoch": 1.95232,
      "grad_norm": 0.006230677478015423,
      "learning_rate": 6.984746666666667e-06,
      "loss": 0.0003,
      "step": 61010
    },
    {
      "epoch": 1.9526400000000002,
      "grad_norm": 0.009137183427810669,
      "learning_rate": 6.9826133333333336e-06,
      "loss": 0.0007,
      "step": 61020
    },
    {
      "epoch": 1.95296,
      "grad_norm": 0.003988092765212059,
      "learning_rate": 6.980480000000001e-06,
      "loss": 0.0005,
      "step": 61030
    },
    {
      "epoch": 1.95328,
      "grad_norm": 0.005007629748433828,
      "learning_rate": 6.978346666666668e-06,
      "loss": 0.0577,
      "step": 61040
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.0084388954564929,
      "learning_rate": 6.976213333333334e-06,
      "loss": 0.0362,
      "step": 61050
    },
    {
      "epoch": 1.95392,
      "grad_norm": 0.009141501970589161,
      "learning_rate": 6.974080000000001e-06,
      "loss": 0.0897,
      "step": 61060
    },
    {
      "epoch": 1.95424,
      "grad_norm": 0.006254146341234446,
      "learning_rate": 6.971946666666667e-06,
      "loss": 0.0002,
      "step": 61070
    },
    {
      "epoch": 1.9545599999999999,
      "grad_norm": 0.006417339667677879,
      "learning_rate": 6.969813333333334e-06,
      "loss": 0.0007,
      "step": 61080
    },
    {
      "epoch": 1.95488,
      "grad_norm": 0.0062487199902534485,
      "learning_rate": 6.967680000000001e-06,
      "loss": 0.0003,
      "step": 61090
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.005247093737125397,
      "learning_rate": 6.965546666666667e-06,
      "loss": 0.0005,
      "step": 61100
    },
    {
      "epoch": 1.95552,
      "grad_norm": 0.21036973595619202,
      "learning_rate": 6.963413333333335e-06,
      "loss": 0.0014,
      "step": 61110
    },
    {
      "epoch": 1.95584,
      "grad_norm": 0.005273101385682821,
      "learning_rate": 6.96128e-06,
      "loss": 0.0003,
      "step": 61120
    },
    {
      "epoch": 1.9561600000000001,
      "grad_norm": 0.004104871302843094,
      "learning_rate": 6.959146666666667e-06,
      "loss": 0.0002,
      "step": 61130
    },
    {
      "epoch": 1.95648,
      "grad_norm": 1.372857928276062,
      "learning_rate": 6.957013333333333e-06,
      "loss": 0.0017,
      "step": 61140
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.009456818923354149,
      "learning_rate": 6.95488e-06,
      "loss": 0.0003,
      "step": 61150
    },
    {
      "epoch": 1.95712,
      "grad_norm": 0.003047809237614274,
      "learning_rate": 6.9527466666666675e-06,
      "loss": 0.0002,
      "step": 61160
    },
    {
      "epoch": 1.95744,
      "grad_norm": 0.005117122083902359,
      "learning_rate": 6.950613333333334e-06,
      "loss": 0.0003,
      "step": 61170
    },
    {
      "epoch": 1.95776,
      "grad_norm": 0.006502313073724508,
      "learning_rate": 6.948480000000001e-06,
      "loss": 0.0002,
      "step": 61180
    },
    {
      "epoch": 1.95808,
      "grad_norm": 0.005029319319874048,
      "learning_rate": 6.946346666666667e-06,
      "loss": 0.0003,
      "step": 61190
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.03424454480409622,
      "learning_rate": 6.944213333333334e-06,
      "loss": 0.0003,
      "step": 61200
    },
    {
      "epoch": 1.95872,
      "grad_norm": 7.230440616607666,
      "learning_rate": 6.942080000000001e-06,
      "loss": 0.0129,
      "step": 61210
    },
    {
      "epoch": 1.95904,
      "grad_norm": 0.0026342219207435846,
      "learning_rate": 6.939946666666667e-06,
      "loss": 0.0002,
      "step": 61220
    },
    {
      "epoch": 1.95936,
      "grad_norm": 0.003997146151959896,
      "learning_rate": 6.9378133333333344e-06,
      "loss": 0.0092,
      "step": 61230
    },
    {
      "epoch": 1.95968,
      "grad_norm": 0.003622804069891572,
      "learning_rate": 6.93568e-06,
      "loss": 0.0002,
      "step": 61240
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.005420953035354614,
      "learning_rate": 6.933546666666668e-06,
      "loss": 0.0002,
      "step": 61250
    },
    {
      "epoch": 1.9603199999999998,
      "grad_norm": 0.028516946360468864,
      "learning_rate": 6.931413333333335e-06,
      "loss": 0.0014,
      "step": 61260
    },
    {
      "epoch": 1.9606400000000002,
      "grad_norm": 0.010006667114794254,
      "learning_rate": 6.92928e-06,
      "loss": 0.001,
      "step": 61270
    },
    {
      "epoch": 1.96096,
      "grad_norm": 3.4689724445343018,
      "learning_rate": 6.927146666666667e-06,
      "loss": 0.0051,
      "step": 61280
    },
    {
      "epoch": 1.96128,
      "grad_norm": 0.027383647859096527,
      "learning_rate": 6.9250133333333335e-06,
      "loss": 0.0003,
      "step": 61290
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.0037059783935546875,
      "learning_rate": 6.9228800000000005e-06,
      "loss": 0.0002,
      "step": 61300
    },
    {
      "epoch": 1.96192,
      "grad_norm": 0.002962422790005803,
      "learning_rate": 6.920746666666667e-06,
      "loss": 0.0004,
      "step": 61310
    },
    {
      "epoch": 1.96224,
      "grad_norm": 0.012292132712900639,
      "learning_rate": 6.918613333333334e-06,
      "loss": 0.0005,
      "step": 61320
    },
    {
      "epoch": 1.9625599999999999,
      "grad_norm": 0.0036020714323967695,
      "learning_rate": 6.916480000000001e-06,
      "loss": 0.0004,
      "step": 61330
    },
    {
      "epoch": 1.96288,
      "grad_norm": 0.0034834963735193014,
      "learning_rate": 6.914346666666667e-06,
      "loss": 0.0002,
      "step": 61340
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.003209599293768406,
      "learning_rate": 6.912213333333334e-06,
      "loss": 0.0006,
      "step": 61350
    },
    {
      "epoch": 1.96352,
      "grad_norm": 0.00599417882040143,
      "learning_rate": 6.9100800000000004e-06,
      "loss": 0.0002,
      "step": 61360
    },
    {
      "epoch": 1.96384,
      "grad_norm": 0.005926125217229128,
      "learning_rate": 6.9079466666666675e-06,
      "loss": 0.0493,
      "step": 61370
    },
    {
      "epoch": 1.9641600000000001,
      "grad_norm": 0.0054921554401516914,
      "learning_rate": 6.9058133333333346e-06,
      "loss": 0.0008,
      "step": 61380
    },
    {
      "epoch": 1.96448,
      "grad_norm": 0.002135148271918297,
      "learning_rate": 6.903680000000001e-06,
      "loss": 0.0004,
      "step": 61390
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.0031536780297756195,
      "learning_rate": 6.901546666666668e-06,
      "loss": 0.0006,
      "step": 61400
    },
    {
      "epoch": 1.96512,
      "grad_norm": 0.00977324042469263,
      "learning_rate": 6.899413333333333e-06,
      "loss": 0.0513,
      "step": 61410
    },
    {
      "epoch": 1.96544,
      "grad_norm": 0.0019915036391466856,
      "learning_rate": 6.89728e-06,
      "loss": 0.0004,
      "step": 61420
    },
    {
      "epoch": 1.96576,
      "grad_norm": 0.005916169844567776,
      "learning_rate": 6.895146666666667e-06,
      "loss": 0.0003,
      "step": 61430
    },
    {
      "epoch": 1.96608,
      "grad_norm": 0.02077532187104225,
      "learning_rate": 6.893013333333334e-06,
      "loss": 0.0003,
      "step": 61440
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.0036203216295689344,
      "learning_rate": 6.890880000000001e-06,
      "loss": 0.0446,
      "step": 61450
    },
    {
      "epoch": 1.96672,
      "grad_norm": 0.005980625282973051,
      "learning_rate": 6.888746666666667e-06,
      "loss": 0.0003,
      "step": 61460
    },
    {
      "epoch": 1.96704,
      "grad_norm": 0.0018464167369529605,
      "learning_rate": 6.886613333333334e-06,
      "loss": 0.0003,
      "step": 61470
    },
    {
      "epoch": 1.96736,
      "grad_norm": 0.007273332215845585,
      "learning_rate": 6.88448e-06,
      "loss": 0.0012,
      "step": 61480
    },
    {
      "epoch": 1.96768,
      "grad_norm": 0.007369218394160271,
      "learning_rate": 6.882346666666667e-06,
      "loss": 0.0002,
      "step": 61490
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.004770930856466293,
      "learning_rate": 6.880213333333334e-06,
      "loss": 0.0002,
      "step": 61500
    },
    {
      "epoch": 1.9683199999999998,
      "grad_norm": 0.006632746197283268,
      "learning_rate": 6.8780800000000006e-06,
      "loss": 0.0005,
      "step": 61510
    },
    {
      "epoch": 1.96864,
      "grad_norm": 0.0050237649120390415,
      "learning_rate": 6.875946666666668e-06,
      "loss": 0.0102,
      "step": 61520
    },
    {
      "epoch": 1.96896,
      "grad_norm": 0.004032914526760578,
      "learning_rate": 6.873813333333334e-06,
      "loss": 0.0002,
      "step": 61530
    },
    {
      "epoch": 1.96928,
      "grad_norm": 0.00649282755330205,
      "learning_rate": 6.871680000000001e-06,
      "loss": 0.0002,
      "step": 61540
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.013668890111148357,
      "learning_rate": 6.869546666666668e-06,
      "loss": 0.0003,
      "step": 61550
    },
    {
      "epoch": 1.9699200000000001,
      "grad_norm": 0.004983977880328894,
      "learning_rate": 6.867413333333333e-06,
      "loss": 0.0003,
      "step": 61560
    },
    {
      "epoch": 1.97024,
      "grad_norm": 0.003213214688003063,
      "learning_rate": 6.8652800000000004e-06,
      "loss": 0.0529,
      "step": 61570
    },
    {
      "epoch": 1.9705599999999999,
      "grad_norm": 0.008900245651602745,
      "learning_rate": 6.863146666666667e-06,
      "loss": 0.0003,
      "step": 61580
    },
    {
      "epoch": 1.97088,
      "grad_norm": 0.01166254561394453,
      "learning_rate": 6.861013333333334e-06,
      "loss": 0.0004,
      "step": 61590
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.008842763490974903,
      "learning_rate": 6.858880000000001e-06,
      "loss": 0.0004,
      "step": 61600
    },
    {
      "epoch": 1.97152,
      "grad_norm": 0.03593236207962036,
      "learning_rate": 6.856746666666667e-06,
      "loss": 0.0003,
      "step": 61610
    },
    {
      "epoch": 1.97184,
      "grad_norm": 0.0036183150950819254,
      "learning_rate": 6.854613333333334e-06,
      "loss": 0.0023,
      "step": 61620
    },
    {
      "epoch": 1.9721600000000001,
      "grad_norm": 0.006674162577837706,
      "learning_rate": 6.85248e-06,
      "loss": 0.0004,
      "step": 61630
    },
    {
      "epoch": 1.97248,
      "grad_norm": 0.0036924476735293865,
      "learning_rate": 6.850346666666667e-06,
      "loss": 0.0092,
      "step": 61640
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.004588526673614979,
      "learning_rate": 6.848213333333334e-06,
      "loss": 0.0258,
      "step": 61650
    },
    {
      "epoch": 1.97312,
      "grad_norm": 0.09961720556020737,
      "learning_rate": 6.846080000000001e-06,
      "loss": 0.0006,
      "step": 61660
    },
    {
      "epoch": 1.97344,
      "grad_norm": 0.0106836361810565,
      "learning_rate": 6.843946666666668e-06,
      "loss": 0.0002,
      "step": 61670
    },
    {
      "epoch": 1.97376,
      "grad_norm": 0.0084659019485116,
      "learning_rate": 6.841813333333334e-06,
      "loss": 0.0002,
      "step": 61680
    },
    {
      "epoch": 1.9740799999999998,
      "grad_norm": 0.0021957841236144304,
      "learning_rate": 6.839680000000001e-06,
      "loss": 0.0015,
      "step": 61690
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 0.0029726552311331034,
      "learning_rate": 6.8375466666666664e-06,
      "loss": 0.0002,
      "step": 61700
    },
    {
      "epoch": 1.97472,
      "grad_norm": 0.008562041446566582,
      "learning_rate": 6.8354133333333335e-06,
      "loss": 0.0645,
      "step": 61710
    },
    {
      "epoch": 1.97504,
      "grad_norm": 1.8429619073867798,
      "learning_rate": 6.8332800000000014e-06,
      "loss": 0.0446,
      "step": 61720
    },
    {
      "epoch": 1.97536,
      "grad_norm": 0.0047883265651762486,
      "learning_rate": 6.831146666666667e-06,
      "loss": 0.0002,
      "step": 61730
    },
    {
      "epoch": 1.97568,
      "grad_norm": 0.01477993093430996,
      "learning_rate": 6.829013333333334e-06,
      "loss": 0.0002,
      "step": 61740
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.007824679836630821,
      "learning_rate": 6.82688e-06,
      "loss": 0.0002,
      "step": 61750
    },
    {
      "epoch": 1.9763199999999999,
      "grad_norm": 0.008744845166802406,
      "learning_rate": 6.824746666666667e-06,
      "loss": 0.0201,
      "step": 61760
    },
    {
      "epoch": 1.97664,
      "grad_norm": 0.0027896829415112734,
      "learning_rate": 6.822613333333334e-06,
      "loss": 0.0003,
      "step": 61770
    },
    {
      "epoch": 1.97696,
      "grad_norm": 0.0017867074348032475,
      "learning_rate": 6.8204800000000005e-06,
      "loss": 0.0002,
      "step": 61780
    },
    {
      "epoch": 1.97728,
      "grad_norm": 0.008043097332119942,
      "learning_rate": 6.8183466666666675e-06,
      "loss": 0.0002,
      "step": 61790
    },
    {
      "epoch": 1.9776,
      "grad_norm": 0.005170376971364021,
      "learning_rate": 6.816213333333334e-06,
      "loss": 0.0002,
      "step": 61800
    },
    {
      "epoch": 1.9779200000000001,
      "grad_norm": 0.0026968265883624554,
      "learning_rate": 6.814080000000001e-06,
      "loss": 0.0002,
      "step": 61810
    },
    {
      "epoch": 1.97824,
      "grad_norm": 0.004403664730489254,
      "learning_rate": 6.811946666666667e-06,
      "loss": 0.0002,
      "step": 61820
    },
    {
      "epoch": 1.9785599999999999,
      "grad_norm": 0.004779772832989693,
      "learning_rate": 6.809813333333334e-06,
      "loss": 0.0002,
      "step": 61830
    },
    {
      "epoch": 1.97888,
      "grad_norm": 0.5475085377693176,
      "learning_rate": 6.807680000000001e-06,
      "loss": 0.0008,
      "step": 61840
    },
    {
      "epoch": 1.9792,
      "grad_norm": 0.005888018757104874,
      "learning_rate": 6.805546666666667e-06,
      "loss": 0.0036,
      "step": 61850
    },
    {
      "epoch": 1.97952,
      "grad_norm": 0.0035392609424889088,
      "learning_rate": 6.8034133333333345e-06,
      "loss": 0.0527,
      "step": 61860
    },
    {
      "epoch": 1.97984,
      "grad_norm": 0.0030819401144981384,
      "learning_rate": 6.80128e-06,
      "loss": 0.0003,
      "step": 61870
    },
    {
      "epoch": 1.9801600000000001,
      "grad_norm": 0.016323789954185486,
      "learning_rate": 6.799146666666667e-06,
      "loss": 0.0008,
      "step": 61880
    },
    {
      "epoch": 1.98048,
      "grad_norm": 0.003655476728454232,
      "learning_rate": 6.797013333333334e-06,
      "loss": 0.0003,
      "step": 61890
    },
    {
      "epoch": 1.9808,
      "grad_norm": 0.001608154270797968,
      "learning_rate": 6.79488e-06,
      "loss": 0.0441,
      "step": 61900
    },
    {
      "epoch": 1.98112,
      "grad_norm": 0.0061302450485527515,
      "learning_rate": 6.792746666666667e-06,
      "loss": 0.0004,
      "step": 61910
    },
    {
      "epoch": 1.98144,
      "grad_norm": 0.0161750428378582,
      "learning_rate": 6.7906133333333335e-06,
      "loss": 0.0005,
      "step": 61920
    },
    {
      "epoch": 1.98176,
      "grad_norm": 0.011005898006260395,
      "learning_rate": 6.788480000000001e-06,
      "loss": 0.0005,
      "step": 61930
    },
    {
      "epoch": 1.9820799999999998,
      "grad_norm": 0.006260553374886513,
      "learning_rate": 6.786346666666668e-06,
      "loss": 0.0003,
      "step": 61940
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 0.0028993503656238317,
      "learning_rate": 6.784213333333334e-06,
      "loss": 0.0003,
      "step": 61950
    },
    {
      "epoch": 1.98272,
      "grad_norm": 0.01135275885462761,
      "learning_rate": 6.782080000000001e-06,
      "loss": 0.0003,
      "step": 61960
    },
    {
      "epoch": 1.98304,
      "grad_norm": 0.004845659714192152,
      "learning_rate": 6.779946666666667e-06,
      "loss": 0.0002,
      "step": 61970
    },
    {
      "epoch": 1.98336,
      "grad_norm": 0.012458228506147861,
      "learning_rate": 6.777813333333334e-06,
      "loss": 0.0003,
      "step": 61980
    },
    {
      "epoch": 1.98368,
      "grad_norm": 0.008487672545015812,
      "learning_rate": 6.77568e-06,
      "loss": 0.0003,
      "step": 61990
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.005990162491798401,
      "learning_rate": 6.7735466666666676e-06,
      "loss": 0.0002,
      "step": 62000
    },
    {
      "epoch": 1.9843199999999999,
      "grad_norm": 0.007640670984983444,
      "learning_rate": 6.771413333333335e-06,
      "loss": 0.0004,
      "step": 62010
    },
    {
      "epoch": 1.98464,
      "grad_norm": 0.004290607757866383,
      "learning_rate": 6.76928e-06,
      "loss": 0.0002,
      "step": 62020
    },
    {
      "epoch": 1.98496,
      "grad_norm": 0.005636675748974085,
      "learning_rate": 6.767146666666667e-06,
      "loss": 0.0003,
      "step": 62030
    },
    {
      "epoch": 1.98528,
      "grad_norm": 0.00802589487284422,
      "learning_rate": 6.765013333333333e-06,
      "loss": 0.0002,
      "step": 62040
    },
    {
      "epoch": 1.9856,
      "grad_norm": 0.005318285431712866,
      "learning_rate": 6.76288e-06,
      "loss": 0.0005,
      "step": 62050
    },
    {
      "epoch": 1.9859200000000001,
      "grad_norm": 0.003845139406621456,
      "learning_rate": 6.7607466666666674e-06,
      "loss": 0.0427,
      "step": 62060
    },
    {
      "epoch": 1.98624,
      "grad_norm": 0.004753262270241976,
      "learning_rate": 6.758613333333334e-06,
      "loss": 0.0003,
      "step": 62070
    },
    {
      "epoch": 1.9865599999999999,
      "grad_norm": 0.005206067115068436,
      "learning_rate": 6.756480000000001e-06,
      "loss": 0.0003,
      "step": 62080
    },
    {
      "epoch": 1.98688,
      "grad_norm": 0.0009771704208105803,
      "learning_rate": 6.754346666666667e-06,
      "loss": 0.0003,
      "step": 62090
    },
    {
      "epoch": 1.9872,
      "grad_norm": 0.0035515951458364725,
      "learning_rate": 6.752213333333334e-06,
      "loss": 0.0007,
      "step": 62100
    },
    {
      "epoch": 1.98752,
      "grad_norm": 0.007244857959449291,
      "learning_rate": 6.750080000000001e-06,
      "loss": 0.0081,
      "step": 62110
    },
    {
      "epoch": 1.98784,
      "grad_norm": 0.008653500117361546,
      "learning_rate": 6.747946666666667e-06,
      "loss": 0.0003,
      "step": 62120
    },
    {
      "epoch": 1.9881600000000001,
      "grad_norm": 0.01018819771707058,
      "learning_rate": 6.745813333333334e-06,
      "loss": 0.0227,
      "step": 62130
    },
    {
      "epoch": 1.98848,
      "grad_norm": 0.021202171221375465,
      "learning_rate": 6.743680000000001e-06,
      "loss": 0.0003,
      "step": 62140
    },
    {
      "epoch": 1.9888,
      "grad_norm": 0.016078343614935875,
      "learning_rate": 6.741546666666668e-06,
      "loss": 0.0004,
      "step": 62150
    },
    {
      "epoch": 1.98912,
      "grad_norm": 0.0073022437281906605,
      "learning_rate": 6.739413333333333e-06,
      "loss": 0.0003,
      "step": 62160
    },
    {
      "epoch": 1.98944,
      "grad_norm": 0.6550225019454956,
      "learning_rate": 6.73728e-06,
      "loss": 0.0473,
      "step": 62170
    },
    {
      "epoch": 1.98976,
      "grad_norm": 0.005119789857417345,
      "learning_rate": 6.735146666666667e-06,
      "loss": 0.0002,
      "step": 62180
    },
    {
      "epoch": 1.9900799999999998,
      "grad_norm": 0.004940325859934092,
      "learning_rate": 6.7330133333333334e-06,
      "loss": 0.0003,
      "step": 62190
    },
    {
      "epoch": 1.9904,
      "grad_norm": 0.005228274501860142,
      "learning_rate": 6.7308800000000005e-06,
      "loss": 0.0004,
      "step": 62200
    },
    {
      "epoch": 1.99072,
      "grad_norm": 0.00922243483364582,
      "learning_rate": 6.728746666666667e-06,
      "loss": 0.0003,
      "step": 62210
    },
    {
      "epoch": 1.99104,
      "grad_norm": 0.0010759680299088359,
      "learning_rate": 6.726613333333334e-06,
      "loss": 0.0002,
      "step": 62220
    },
    {
      "epoch": 1.99136,
      "grad_norm": 0.029474707320332527,
      "learning_rate": 6.724480000000001e-06,
      "loss": 0.0013,
      "step": 62230
    },
    {
      "epoch": 1.9916800000000001,
      "grad_norm": 0.008602204732596874,
      "learning_rate": 6.722346666666667e-06,
      "loss": 0.0003,
      "step": 62240
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.010439991019666195,
      "learning_rate": 6.720213333333334e-06,
      "loss": 0.0167,
      "step": 62250
    },
    {
      "epoch": 1.9923199999999999,
      "grad_norm": 0.004263296257704496,
      "learning_rate": 6.71808e-06,
      "loss": 0.0009,
      "step": 62260
    },
    {
      "epoch": 1.99264,
      "grad_norm": 0.004288915079087019,
      "learning_rate": 6.7159466666666675e-06,
      "loss": 0.0003,
      "step": 62270
    },
    {
      "epoch": 1.99296,
      "grad_norm": 0.006669968366622925,
      "learning_rate": 6.7138133333333345e-06,
      "loss": 0.0516,
      "step": 62280
    },
    {
      "epoch": 1.99328,
      "grad_norm": 0.03126133605837822,
      "learning_rate": 6.711680000000001e-06,
      "loss": 0.0004,
      "step": 62290
    },
    {
      "epoch": 1.9936,
      "grad_norm": 0.010629117488861084,
      "learning_rate": 6.709546666666668e-06,
      "loss": 0.0003,
      "step": 62300
    },
    {
      "epoch": 1.9939200000000001,
      "grad_norm": 0.002883851295337081,
      "learning_rate": 6.707413333333333e-06,
      "loss": 0.0003,
      "step": 62310
    },
    {
      "epoch": 1.99424,
      "grad_norm": 0.0023082599509507418,
      "learning_rate": 6.70528e-06,
      "loss": 0.0002,
      "step": 62320
    },
    {
      "epoch": 1.9945599999999999,
      "grad_norm": 0.004767916165292263,
      "learning_rate": 6.7031466666666665e-06,
      "loss": 0.0003,
      "step": 62330
    },
    {
      "epoch": 1.99488,
      "grad_norm": 0.011236491613090038,
      "learning_rate": 6.701013333333334e-06,
      "loss": 0.0004,
      "step": 62340
    },
    {
      "epoch": 1.9952,
      "grad_norm": 0.004817119333893061,
      "learning_rate": 6.698880000000001e-06,
      "loss": 0.0002,
      "step": 62350
    },
    {
      "epoch": 1.99552,
      "grad_norm": 0.006824750453233719,
      "learning_rate": 6.696746666666667e-06,
      "loss": 0.0003,
      "step": 62360
    },
    {
      "epoch": 1.9958399999999998,
      "grad_norm": 0.01863543875515461,
      "learning_rate": 6.694613333333334e-06,
      "loss": 0.0029,
      "step": 62370
    },
    {
      "epoch": 1.9961600000000002,
      "grad_norm": 0.005954832769930363,
      "learning_rate": 6.69248e-06,
      "loss": 0.0003,
      "step": 62380
    },
    {
      "epoch": 1.99648,
      "grad_norm": 0.006730804685503244,
      "learning_rate": 6.690346666666667e-06,
      "loss": 0.0013,
      "step": 62390
    },
    {
      "epoch": 1.9968,
      "grad_norm": 0.010222247801721096,
      "learning_rate": 6.688213333333334e-06,
      "loss": 0.0002,
      "step": 62400
    },
    {
      "epoch": 1.99712,
      "grad_norm": 0.008495794609189034,
      "learning_rate": 6.6860800000000005e-06,
      "loss": 0.0197,
      "step": 62410
    },
    {
      "epoch": 1.99744,
      "grad_norm": 0.006898391991853714,
      "learning_rate": 6.683946666666668e-06,
      "loss": 0.0008,
      "step": 62420
    },
    {
      "epoch": 1.99776,
      "grad_norm": 0.0022264865692704916,
      "learning_rate": 6.681813333333334e-06,
      "loss": 0.0002,
      "step": 62430
    },
    {
      "epoch": 1.9980799999999999,
      "grad_norm": 0.008182474412024021,
      "learning_rate": 6.679680000000001e-06,
      "loss": 0.0003,
      "step": 62440
    },
    {
      "epoch": 1.9984,
      "grad_norm": 0.008362636901438236,
      "learning_rate": 6.677546666666668e-06,
      "loss": 0.0002,
      "step": 62450
    },
    {
      "epoch": 1.99872,
      "grad_norm": 0.01712411269545555,
      "learning_rate": 6.675413333333333e-06,
      "loss": 0.0006,
      "step": 62460
    },
    {
      "epoch": 1.99904,
      "grad_norm": 0.004163495730608702,
      "learning_rate": 6.673280000000001e-06,
      "loss": 0.0015,
      "step": 62470
    },
    {
      "epoch": 1.99936,
      "grad_norm": 0.0046755727380514145,
      "learning_rate": 6.671146666666667e-06,
      "loss": 0.0004,
      "step": 62480
    },
    {
      "epoch": 1.9996800000000001,
      "grad_norm": 0.007446496747434139,
      "learning_rate": 6.669013333333334e-06,
      "loss": 0.0469,
      "step": 62490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.010076272301375866,
      "learning_rate": 6.66688e-06,
      "loss": 0.0002,
      "step": 62500
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.0013,
      "eval_f1": 0.0012999101169919106,
      "eval_loss": 10.330595016479492,
      "eval_precision": 0.0012956872851965571,
      "eval_recall": 0.0013047257871364143,
      "eval_runtime": 43.643,
      "eval_samples_per_second": 229.132,
      "eval_steps_per_second": 14.321,
      "step": 62500
    },
    {
      "epoch": 2.00032,
      "grad_norm": 0.002063085325062275,
      "learning_rate": 6.664746666666667e-06,
      "loss": 0.0002,
      "step": 62510
    },
    {
      "epoch": 2.00064,
      "grad_norm": 0.004368151072412729,
      "learning_rate": 6.662613333333334e-06,
      "loss": 0.0003,
      "step": 62520
    },
    {
      "epoch": 2.00096,
      "grad_norm": 0.009268248453736305,
      "learning_rate": 6.66048e-06,
      "loss": 0.0005,
      "step": 62530
    },
    {
      "epoch": 2.00128,
      "grad_norm": 0.005360221490263939,
      "learning_rate": 6.658346666666667e-06,
      "loss": 0.0002,
      "step": 62540
    },
    {
      "epoch": 2.0016,
      "grad_norm": 0.004466409794986248,
      "learning_rate": 6.656213333333334e-06,
      "loss": 0.0002,
      "step": 62550
    },
    {
      "epoch": 2.00192,
      "grad_norm": 0.005738757085055113,
      "learning_rate": 6.654080000000001e-06,
      "loss": 0.0003,
      "step": 62560
    },
    {
      "epoch": 2.00224,
      "grad_norm": 0.00608469545841217,
      "learning_rate": 6.651946666666668e-06,
      "loss": 0.0002,
      "step": 62570
    },
    {
      "epoch": 2.00256,
      "grad_norm": 0.003281115321442485,
      "learning_rate": 6.649813333333334e-06,
      "loss": 0.0003,
      "step": 62580
    },
    {
      "epoch": 2.00288,
      "grad_norm": 0.002683317521587014,
      "learning_rate": 6.647680000000001e-06,
      "loss": 0.0407,
      "step": 62590
    },
    {
      "epoch": 2.0032,
      "grad_norm": 0.010373680852353573,
      "learning_rate": 6.645546666666666e-06,
      "loss": 0.0004,
      "step": 62600
    },
    {
      "epoch": 2.00352,
      "grad_norm": 0.006422373931854963,
      "learning_rate": 6.643413333333334e-06,
      "loss": 0.0002,
      "step": 62610
    },
    {
      "epoch": 2.00384,
      "grad_norm": 0.005261747632175684,
      "learning_rate": 6.641280000000001e-06,
      "loss": 0.0004,
      "step": 62620
    },
    {
      "epoch": 2.00416,
      "grad_norm": 0.00523231690749526,
      "learning_rate": 6.639146666666667e-06,
      "loss": 0.0036,
      "step": 62630
    },
    {
      "epoch": 2.00448,
      "grad_norm": 0.004597949795424938,
      "learning_rate": 6.637013333333334e-06,
      "loss": 0.0032,
      "step": 62640
    },
    {
      "epoch": 2.0048,
      "grad_norm": 0.010835413821041584,
      "learning_rate": 6.63488e-06,
      "loss": 0.0006,
      "step": 62650
    },
    {
      "epoch": 2.00512,
      "grad_norm": 0.005829746834933758,
      "learning_rate": 6.632746666666667e-06,
      "loss": 0.0006,
      "step": 62660
    },
    {
      "epoch": 2.00544,
      "grad_norm": 0.0053010741248726845,
      "learning_rate": 6.630613333333333e-06,
      "loss": 0.0002,
      "step": 62670
    },
    {
      "epoch": 2.00576,
      "grad_norm": 0.006279560271650553,
      "learning_rate": 6.6284800000000004e-06,
      "loss": 0.0002,
      "step": 62680
    },
    {
      "epoch": 2.00608,
      "grad_norm": 0.004439631011337042,
      "learning_rate": 6.6263466666666675e-06,
      "loss": 0.0002,
      "step": 62690
    },
    {
      "epoch": 2.0064,
      "grad_norm": 0.003829132067039609,
      "learning_rate": 6.624213333333334e-06,
      "loss": 0.0793,
      "step": 62700
    },
    {
      "epoch": 2.00672,
      "grad_norm": 0.9165883660316467,
      "learning_rate": 6.622080000000001e-06,
      "loss": 0.0008,
      "step": 62710
    },
    {
      "epoch": 2.00704,
      "grad_norm": 0.001722652348689735,
      "learning_rate": 6.619946666666667e-06,
      "loss": 0.0002,
      "step": 62720
    },
    {
      "epoch": 2.00736,
      "grad_norm": 0.004760415758937597,
      "learning_rate": 6.617813333333334e-06,
      "loss": 0.0002,
      "step": 62730
    },
    {
      "epoch": 2.00768,
      "grad_norm": 0.0031917428132146597,
      "learning_rate": 6.615680000000001e-06,
      "loss": 0.0001,
      "step": 62740
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.004206985700875521,
      "learning_rate": 6.613546666666667e-06,
      "loss": 0.0002,
      "step": 62750
    },
    {
      "epoch": 2.00832,
      "grad_norm": 0.0032295689452439547,
      "learning_rate": 6.6114133333333345e-06,
      "loss": 0.0002,
      "step": 62760
    },
    {
      "epoch": 2.00864,
      "grad_norm": 0.005093827843666077,
      "learning_rate": 6.60928e-06,
      "loss": 0.0004,
      "step": 62770
    },
    {
      "epoch": 2.00896,
      "grad_norm": 0.0037967143580317497,
      "learning_rate": 6.607146666666667e-06,
      "loss": 0.0002,
      "step": 62780
    },
    {
      "epoch": 2.00928,
      "grad_norm": 0.0023322398774325848,
      "learning_rate": 6.605013333333334e-06,
      "loss": 0.0002,
      "step": 62790
    },
    {
      "epoch": 2.0096,
      "grad_norm": 0.7202374935150146,
      "learning_rate": 6.60288e-06,
      "loss": 0.0006,
      "step": 62800
    },
    {
      "epoch": 2.00992,
      "grad_norm": 0.003751071635633707,
      "learning_rate": 6.600746666666667e-06,
      "loss": 0.0002,
      "step": 62810
    },
    {
      "epoch": 2.01024,
      "grad_norm": 0.002059721387922764,
      "learning_rate": 6.5986133333333335e-06,
      "loss": 0.0001,
      "step": 62820
    },
    {
      "epoch": 2.01056,
      "grad_norm": 0.0047975704073905945,
      "learning_rate": 6.596480000000001e-06,
      "loss": 0.0001,
      "step": 62830
    },
    {
      "epoch": 2.0108800000000002,
      "grad_norm": 0.0025855465792119503,
      "learning_rate": 6.594346666666667e-06,
      "loss": 0.0002,
      "step": 62840
    },
    {
      "epoch": 2.0112,
      "grad_norm": 0.0031868410296738148,
      "learning_rate": 6.592213333333334e-06,
      "loss": 0.0002,
      "step": 62850
    },
    {
      "epoch": 2.01152,
      "grad_norm": 0.003374595893546939,
      "learning_rate": 6.590080000000001e-06,
      "loss": 0.0002,
      "step": 62860
    },
    {
      "epoch": 2.01184,
      "grad_norm": 0.004296975210309029,
      "learning_rate": 6.587946666666667e-06,
      "loss": 0.0002,
      "step": 62870
    },
    {
      "epoch": 2.01216,
      "grad_norm": 0.009789042174816132,
      "learning_rate": 6.585813333333334e-06,
      "loss": 0.0002,
      "step": 62880
    },
    {
      "epoch": 2.01248,
      "grad_norm": 0.0033482585567981005,
      "learning_rate": 6.5836800000000005e-06,
      "loss": 0.0002,
      "step": 62890
    },
    {
      "epoch": 2.0128,
      "grad_norm": 0.00427599623799324,
      "learning_rate": 6.5815466666666675e-06,
      "loss": 0.0021,
      "step": 62900
    },
    {
      "epoch": 2.01312,
      "grad_norm": 0.006367418449372053,
      "learning_rate": 6.579413333333335e-06,
      "loss": 0.0002,
      "step": 62910
    },
    {
      "epoch": 2.01344,
      "grad_norm": 0.00146763795055449,
      "learning_rate": 6.57728e-06,
      "loss": 0.0001,
      "step": 62920
    },
    {
      "epoch": 2.01376,
      "grad_norm": 0.00720952358096838,
      "learning_rate": 6.575146666666667e-06,
      "loss": 0.0536,
      "step": 62930
    },
    {
      "epoch": 2.01408,
      "grad_norm": 0.002216800581663847,
      "learning_rate": 6.573013333333333e-06,
      "loss": 0.0002,
      "step": 62940
    },
    {
      "epoch": 2.0144,
      "grad_norm": 0.009607124142348766,
      "learning_rate": 6.57088e-06,
      "loss": 0.0002,
      "step": 62950
    },
    {
      "epoch": 2.01472,
      "grad_norm": 0.0027303469832986593,
      "learning_rate": 6.568746666666667e-06,
      "loss": 0.0004,
      "step": 62960
    },
    {
      "epoch": 2.01504,
      "grad_norm": 0.003145824419334531,
      "learning_rate": 6.566613333333334e-06,
      "loss": 0.0001,
      "step": 62970
    },
    {
      "epoch": 2.01536,
      "grad_norm": 0.005398678593337536,
      "learning_rate": 6.564480000000001e-06,
      "loss": 0.0005,
      "step": 62980
    },
    {
      "epoch": 2.01568,
      "grad_norm": 0.0407489538192749,
      "learning_rate": 6.562346666666667e-06,
      "loss": 0.0003,
      "step": 62990
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.0073990123346447945,
      "learning_rate": 6.560213333333334e-06,
      "loss": 0.0002,
      "step": 63000
    },
    {
      "epoch": 2.01632,
      "grad_norm": 0.0022239084355533123,
      "learning_rate": 6.55808e-06,
      "loss": 0.0002,
      "step": 63010
    },
    {
      "epoch": 2.01664,
      "grad_norm": 0.006138201337307692,
      "learning_rate": 6.555946666666667e-06,
      "loss": 0.0002,
      "step": 63020
    },
    {
      "epoch": 2.01696,
      "grad_norm": 0.003911532927304506,
      "learning_rate": 6.553813333333334e-06,
      "loss": 0.0006,
      "step": 63030
    },
    {
      "epoch": 2.01728,
      "grad_norm": 0.0019688194151967764,
      "learning_rate": 6.551680000000001e-06,
      "loss": 0.0002,
      "step": 63040
    },
    {
      "epoch": 2.0176,
      "grad_norm": 0.0034155992325395346,
      "learning_rate": 6.549546666666668e-06,
      "loss": 0.0002,
      "step": 63050
    },
    {
      "epoch": 2.01792,
      "grad_norm": 1.7055689096450806,
      "learning_rate": 6.547413333333333e-06,
      "loss": 0.0542,
      "step": 63060
    },
    {
      "epoch": 2.01824,
      "grad_norm": 0.004970447160303593,
      "learning_rate": 6.54528e-06,
      "loss": 0.0431,
      "step": 63070
    },
    {
      "epoch": 2.01856,
      "grad_norm": 0.003585119266062975,
      "learning_rate": 6.543146666666668e-06,
      "loss": 0.0509,
      "step": 63080
    },
    {
      "epoch": 2.01888,
      "grad_norm": 0.007220466621220112,
      "learning_rate": 6.541013333333333e-06,
      "loss": 0.0002,
      "step": 63090
    },
    {
      "epoch": 2.0192,
      "grad_norm": 0.005773389711976051,
      "learning_rate": 6.5388800000000005e-06,
      "loss": 0.0002,
      "step": 63100
    },
    {
      "epoch": 2.01952,
      "grad_norm": 0.001183615531772375,
      "learning_rate": 6.536746666666667e-06,
      "loss": 0.0015,
      "step": 63110
    },
    {
      "epoch": 2.01984,
      "grad_norm": 0.005060161929577589,
      "learning_rate": 6.534613333333334e-06,
      "loss": 0.0003,
      "step": 63120
    },
    {
      "epoch": 2.02016,
      "grad_norm": 0.005393823608756065,
      "learning_rate": 6.532480000000001e-06,
      "loss": 0.0008,
      "step": 63130
    },
    {
      "epoch": 2.02048,
      "grad_norm": 0.002178852679207921,
      "learning_rate": 6.530346666666667e-06,
      "loss": 0.0004,
      "step": 63140
    },
    {
      "epoch": 2.0208,
      "grad_norm": 0.004273587837815285,
      "learning_rate": 6.528213333333334e-06,
      "loss": 0.0009,
      "step": 63150
    },
    {
      "epoch": 2.02112,
      "grad_norm": 0.01583241857588291,
      "learning_rate": 6.52608e-06,
      "loss": 0.0002,
      "step": 63160
    },
    {
      "epoch": 2.02144,
      "grad_norm": 0.014726321212947369,
      "learning_rate": 6.5239466666666674e-06,
      "loss": 0.0002,
      "step": 63170
    },
    {
      "epoch": 2.02176,
      "grad_norm": 0.004222408402711153,
      "learning_rate": 6.521813333333334e-06,
      "loss": 0.0002,
      "step": 63180
    },
    {
      "epoch": 2.02208,
      "grad_norm": 0.0031544985249638557,
      "learning_rate": 6.519680000000001e-06,
      "loss": 0.0465,
      "step": 63190
    },
    {
      "epoch": 2.0224,
      "grad_norm": 0.07339508086442947,
      "learning_rate": 6.517546666666668e-06,
      "loss": 0.0053,
      "step": 63200
    },
    {
      "epoch": 2.02272,
      "grad_norm": 0.0038467738777399063,
      "learning_rate": 6.515413333333333e-06,
      "loss": 0.0002,
      "step": 63210
    },
    {
      "epoch": 2.02304,
      "grad_norm": 0.006149617955088615,
      "learning_rate": 6.513280000000001e-06,
      "loss": 0.0158,
      "step": 63220
    },
    {
      "epoch": 2.02336,
      "grad_norm": 0.02423950657248497,
      "learning_rate": 6.5111466666666665e-06,
      "loss": 0.0003,
      "step": 63230
    },
    {
      "epoch": 2.02368,
      "grad_norm": 0.005803484935313463,
      "learning_rate": 6.5090133333333336e-06,
      "loss": 0.0002,
      "step": 63240
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.0011976268142461777,
      "learning_rate": 6.506880000000001e-06,
      "loss": 0.0002,
      "step": 63250
    },
    {
      "epoch": 2.02432,
      "grad_norm": 0.002537267282605171,
      "learning_rate": 6.504746666666667e-06,
      "loss": 0.0004,
      "step": 63260
    },
    {
      "epoch": 2.02464,
      "grad_norm": 0.0435999371111393,
      "learning_rate": 6.502613333333334e-06,
      "loss": 0.0436,
      "step": 63270
    },
    {
      "epoch": 2.02496,
      "grad_norm": 0.0018494075629860163,
      "learning_rate": 6.50048e-06,
      "loss": 0.0025,
      "step": 63280
    },
    {
      "epoch": 2.02528,
      "grad_norm": 0.0028107636608183384,
      "learning_rate": 6.498346666666667e-06,
      "loss": 0.0002,
      "step": 63290
    },
    {
      "epoch": 2.0256,
      "grad_norm": 0.006868475116789341,
      "learning_rate": 6.496213333333334e-06,
      "loss": 0.044,
      "step": 63300
    },
    {
      "epoch": 2.02592,
      "grad_norm": 0.004781805910170078,
      "learning_rate": 6.4940800000000005e-06,
      "loss": 0.0003,
      "step": 63310
    },
    {
      "epoch": 2.02624,
      "grad_norm": 0.004893154837191105,
      "learning_rate": 6.491946666666668e-06,
      "loss": 0.0003,
      "step": 63320
    },
    {
      "epoch": 2.02656,
      "grad_norm": 0.008235677145421505,
      "learning_rate": 6.489813333333334e-06,
      "loss": 0.0003,
      "step": 63330
    },
    {
      "epoch": 2.02688,
      "grad_norm": 0.006872999016195536,
      "learning_rate": 6.487680000000001e-06,
      "loss": 0.0004,
      "step": 63340
    },
    {
      "epoch": 2.0272,
      "grad_norm": 2.318328857421875,
      "learning_rate": 6.485546666666666e-06,
      "loss": 0.0401,
      "step": 63350
    },
    {
      "epoch": 2.02752,
      "grad_norm": 0.0038583085406571627,
      "learning_rate": 6.483413333333334e-06,
      "loss": 0.0003,
      "step": 63360
    },
    {
      "epoch": 2.02784,
      "grad_norm": 0.0025147097185254097,
      "learning_rate": 6.481280000000001e-06,
      "loss": 0.0602,
      "step": 63370
    },
    {
      "epoch": 2.02816,
      "grad_norm": 0.008701321668922901,
      "learning_rate": 6.479146666666667e-06,
      "loss": 0.0002,
      "step": 63380
    },
    {
      "epoch": 2.02848,
      "grad_norm": 0.1395798921585083,
      "learning_rate": 6.477013333333334e-06,
      "loss": 0.0006,
      "step": 63390
    },
    {
      "epoch": 2.0288,
      "grad_norm": 0.00873563066124916,
      "learning_rate": 6.47488e-06,
      "loss": 0.0488,
      "step": 63400
    },
    {
      "epoch": 2.02912,
      "grad_norm": 0.013226833194494247,
      "learning_rate": 6.472746666666667e-06,
      "loss": 0.0003,
      "step": 63410
    },
    {
      "epoch": 2.02944,
      "grad_norm": 0.003042274620383978,
      "learning_rate": 6.470613333333334e-06,
      "loss": 0.0005,
      "step": 63420
    },
    {
      "epoch": 2.02976,
      "grad_norm": 0.005825630854815245,
      "learning_rate": 6.46848e-06,
      "loss": 0.0003,
      "step": 63430
    },
    {
      "epoch": 2.03008,
      "grad_norm": 0.008704766631126404,
      "learning_rate": 6.466346666666667e-06,
      "loss": 0.0003,
      "step": 63440
    },
    {
      "epoch": 2.0304,
      "grad_norm": 0.006108531262725592,
      "learning_rate": 6.4642133333333336e-06,
      "loss": 0.0021,
      "step": 63450
    },
    {
      "epoch": 2.03072,
      "grad_norm": 0.007278199307620525,
      "learning_rate": 6.462080000000001e-06,
      "loss": 0.0285,
      "step": 63460
    },
    {
      "epoch": 2.03104,
      "grad_norm": 0.007381157949566841,
      "learning_rate": 6.459946666666668e-06,
      "loss": 0.0004,
      "step": 63470
    },
    {
      "epoch": 2.03136,
      "grad_norm": 0.007873174734413624,
      "learning_rate": 6.457813333333334e-06,
      "loss": 0.0002,
      "step": 63480
    },
    {
      "epoch": 2.03168,
      "grad_norm": 0.009676378220319748,
      "learning_rate": 6.455680000000001e-06,
      "loss": 0.0002,
      "step": 63490
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.004519511479884386,
      "learning_rate": 6.453546666666667e-06,
      "loss": 0.0004,
      "step": 63500
    },
    {
      "epoch": 2.03232,
      "grad_norm": 0.00583794666454196,
      "learning_rate": 6.451413333333334e-06,
      "loss": 0.0004,
      "step": 63510
    },
    {
      "epoch": 2.03264,
      "grad_norm": 0.015675563365221024,
      "learning_rate": 6.44928e-06,
      "loss": 0.0003,
      "step": 63520
    },
    {
      "epoch": 2.03296,
      "grad_norm": 0.006105305626988411,
      "learning_rate": 6.447146666666667e-06,
      "loss": 0.0004,
      "step": 63530
    },
    {
      "epoch": 2.03328,
      "grad_norm": 0.007763842586427927,
      "learning_rate": 6.445013333333334e-06,
      "loss": 0.0014,
      "step": 63540
    },
    {
      "epoch": 2.0336,
      "grad_norm": 0.004894021898508072,
      "learning_rate": 6.44288e-06,
      "loss": 0.0002,
      "step": 63550
    },
    {
      "epoch": 2.03392,
      "grad_norm": 0.01903863623738289,
      "learning_rate": 6.440746666666667e-06,
      "loss": 0.0003,
      "step": 63560
    },
    {
      "epoch": 2.03424,
      "grad_norm": 0.010433214716613293,
      "learning_rate": 6.438613333333333e-06,
      "loss": 0.0002,
      "step": 63570
    },
    {
      "epoch": 2.03456,
      "grad_norm": 0.004425801802426577,
      "learning_rate": 6.43648e-06,
      "loss": 0.0003,
      "step": 63580
    },
    {
      "epoch": 2.03488,
      "grad_norm": 0.0057392423041164875,
      "learning_rate": 6.4343466666666675e-06,
      "loss": 0.0003,
      "step": 63590
    },
    {
      "epoch": 2.0352,
      "grad_norm": 0.0026283259503543377,
      "learning_rate": 6.432213333333334e-06,
      "loss": 0.0003,
      "step": 63600
    },
    {
      "epoch": 2.03552,
      "grad_norm": 0.004903266206383705,
      "learning_rate": 6.430080000000001e-06,
      "loss": 0.0006,
      "step": 63610
    },
    {
      "epoch": 2.03584,
      "grad_norm": 0.003486695932224393,
      "learning_rate": 6.427946666666667e-06,
      "loss": 0.0002,
      "step": 63620
    },
    {
      "epoch": 2.03616,
      "grad_norm": 0.002922776620835066,
      "learning_rate": 6.425813333333334e-06,
      "loss": 0.0002,
      "step": 63630
    },
    {
      "epoch": 2.03648,
      "grad_norm": 3.2872209548950195,
      "learning_rate": 6.423680000000001e-06,
      "loss": 0.0169,
      "step": 63640
    },
    {
      "epoch": 2.0368,
      "grad_norm": 0.005845998879522085,
      "learning_rate": 6.421546666666667e-06,
      "loss": 0.0011,
      "step": 63650
    },
    {
      "epoch": 2.03712,
      "grad_norm": 0.0031559616327285767,
      "learning_rate": 6.4194133333333344e-06,
      "loss": 0.0005,
      "step": 63660
    },
    {
      "epoch": 2.03744,
      "grad_norm": 0.1777249276638031,
      "learning_rate": 6.41728e-06,
      "loss": 0.0061,
      "step": 63670
    },
    {
      "epoch": 2.03776,
      "grad_norm": 0.005548474844545126,
      "learning_rate": 6.415146666666667e-06,
      "loss": 0.0003,
      "step": 63680
    },
    {
      "epoch": 2.03808,
      "grad_norm": 0.0036408244632184505,
      "learning_rate": 6.413013333333333e-06,
      "loss": 0.0002,
      "step": 63690
    },
    {
      "epoch": 2.0384,
      "grad_norm": 0.004025080241262913,
      "learning_rate": 6.41088e-06,
      "loss": 0.0002,
      "step": 63700
    },
    {
      "epoch": 2.03872,
      "grad_norm": 0.007175077684223652,
      "learning_rate": 6.408746666666667e-06,
      "loss": 0.016,
      "step": 63710
    },
    {
      "epoch": 2.03904,
      "grad_norm": 0.00670367619022727,
      "learning_rate": 6.4066133333333335e-06,
      "loss": 0.0002,
      "step": 63720
    },
    {
      "epoch": 2.03936,
      "grad_norm": 0.004828206729143858,
      "learning_rate": 6.4044800000000006e-06,
      "loss": 0.0003,
      "step": 63730
    },
    {
      "epoch": 2.03968,
      "grad_norm": 0.14012028276920319,
      "learning_rate": 6.402346666666667e-06,
      "loss": 0.0003,
      "step": 63740
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.0049286410212516785,
      "learning_rate": 6.400213333333334e-06,
      "loss": 0.0003,
      "step": 63750
    },
    {
      "epoch": 2.04032,
      "grad_norm": 0.003525318345054984,
      "learning_rate": 6.398080000000001e-06,
      "loss": 0.0002,
      "step": 63760
    },
    {
      "epoch": 2.04064,
      "grad_norm": 0.010580010712146759,
      "learning_rate": 6.395946666666667e-06,
      "loss": 0.0002,
      "step": 63770
    },
    {
      "epoch": 2.04096,
      "grad_norm": 0.005563303362578154,
      "learning_rate": 6.393813333333334e-06,
      "loss": 0.0003,
      "step": 63780
    },
    {
      "epoch": 2.04128,
      "grad_norm": 0.00816656369715929,
      "learning_rate": 6.3916800000000004e-06,
      "loss": 0.0002,
      "step": 63790
    },
    {
      "epoch": 2.0416,
      "grad_norm": 0.007703147828578949,
      "learning_rate": 6.3895466666666675e-06,
      "loss": 0.0005,
      "step": 63800
    },
    {
      "epoch": 2.04192,
      "grad_norm": 0.011747661046683788,
      "learning_rate": 6.387413333333335e-06,
      "loss": 0.0002,
      "step": 63810
    },
    {
      "epoch": 2.04224,
      "grad_norm": 0.0033654659055173397,
      "learning_rate": 6.38528e-06,
      "loss": 0.0528,
      "step": 63820
    },
    {
      "epoch": 2.04256,
      "grad_norm": 0.004258373286575079,
      "learning_rate": 6.383146666666668e-06,
      "loss": 0.0003,
      "step": 63830
    },
    {
      "epoch": 2.04288,
      "grad_norm": 0.007413959130644798,
      "learning_rate": 6.381013333333333e-06,
      "loss": 0.0002,
      "step": 63840
    },
    {
      "epoch": 2.0432,
      "grad_norm": 0.00783846341073513,
      "learning_rate": 6.37888e-06,
      "loss": 0.0003,
      "step": 63850
    },
    {
      "epoch": 2.04352,
      "grad_norm": 0.005454023834317923,
      "learning_rate": 6.3767466666666665e-06,
      "loss": 0.0007,
      "step": 63860
    },
    {
      "epoch": 2.04384,
      "grad_norm": 0.007808514405041933,
      "learning_rate": 6.374613333333334e-06,
      "loss": 0.0004,
      "step": 63870
    },
    {
      "epoch": 2.04416,
      "grad_norm": 0.0032537500374019146,
      "learning_rate": 6.372480000000001e-06,
      "loss": 0.0002,
      "step": 63880
    },
    {
      "epoch": 2.04448,
      "grad_norm": 0.00530982343479991,
      "learning_rate": 6.370346666666667e-06,
      "loss": 0.0003,
      "step": 63890
    },
    {
      "epoch": 2.0448,
      "grad_norm": 0.004921520594507456,
      "learning_rate": 6.368213333333334e-06,
      "loss": 0.0002,
      "step": 63900
    },
    {
      "epoch": 2.04512,
      "grad_norm": 0.003577501280233264,
      "learning_rate": 6.36608e-06,
      "loss": 0.0635,
      "step": 63910
    },
    {
      "epoch": 2.04544,
      "grad_norm": 0.03113597258925438,
      "learning_rate": 6.363946666666667e-06,
      "loss": 0.0003,
      "step": 63920
    },
    {
      "epoch": 2.04576,
      "grad_norm": 0.0028193544130772352,
      "learning_rate": 6.361813333333334e-06,
      "loss": 0.0002,
      "step": 63930
    },
    {
      "epoch": 2.04608,
      "grad_norm": 0.008465343154966831,
      "learning_rate": 6.359680000000001e-06,
      "loss": 0.0003,
      "step": 63940
    },
    {
      "epoch": 2.0464,
      "grad_norm": 0.002844603266566992,
      "learning_rate": 6.357546666666668e-06,
      "loss": 0.0002,
      "step": 63950
    },
    {
      "epoch": 2.04672,
      "grad_norm": 0.00224918220192194,
      "learning_rate": 6.355413333333333e-06,
      "loss": 0.0138,
      "step": 63960
    },
    {
      "epoch": 2.04704,
      "grad_norm": 0.006188531406223774,
      "learning_rate": 6.353280000000001e-06,
      "loss": 0.0002,
      "step": 63970
    },
    {
      "epoch": 2.04736,
      "grad_norm": 0.002785162767395377,
      "learning_rate": 6.351146666666668e-06,
      "loss": 0.0004,
      "step": 63980
    },
    {
      "epoch": 2.04768,
      "grad_norm": 0.02680344134569168,
      "learning_rate": 6.349013333333333e-06,
      "loss": 0.0366,
      "step": 63990
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.007780905347317457,
      "learning_rate": 6.3468800000000005e-06,
      "loss": 0.0003,
      "step": 64000
    },
    {
      "epoch": 2.04832,
      "grad_norm": 0.0035174982622265816,
      "learning_rate": 6.344746666666667e-06,
      "loss": 0.0004,
      "step": 64010
    },
    {
      "epoch": 2.04864,
      "grad_norm": 0.006769195664674044,
      "learning_rate": 6.342613333333334e-06,
      "loss": 0.0134,
      "step": 64020
    },
    {
      "epoch": 2.04896,
      "grad_norm": 6.036489963531494,
      "learning_rate": 6.34048e-06,
      "loss": 0.0106,
      "step": 64030
    },
    {
      "epoch": 2.04928,
      "grad_norm": 0.010039130225777626,
      "learning_rate": 6.338346666666667e-06,
      "loss": 0.0167,
      "step": 64040
    },
    {
      "epoch": 2.0496,
      "grad_norm": 0.009489580057561398,
      "learning_rate": 6.336213333333334e-06,
      "loss": 0.0002,
      "step": 64050
    },
    {
      "epoch": 2.04992,
      "grad_norm": 0.005382637958973646,
      "learning_rate": 6.33408e-06,
      "loss": 0.0223,
      "step": 64060
    },
    {
      "epoch": 2.05024,
      "grad_norm": 0.004053936805576086,
      "learning_rate": 6.331946666666667e-06,
      "loss": 0.0002,
      "step": 64070
    },
    {
      "epoch": 2.05056,
      "grad_norm": 0.005773024633526802,
      "learning_rate": 6.329813333333334e-06,
      "loss": 0.0002,
      "step": 64080
    },
    {
      "epoch": 2.05088,
      "grad_norm": 0.008302298374474049,
      "learning_rate": 6.327680000000001e-06,
      "loss": 0.0002,
      "step": 64090
    },
    {
      "epoch": 2.0512,
      "grad_norm": 0.007392441853880882,
      "learning_rate": 6.325546666666668e-06,
      "loss": 0.0002,
      "step": 64100
    },
    {
      "epoch": 2.05152,
      "grad_norm": 0.009884360246360302,
      "learning_rate": 6.323413333333334e-06,
      "loss": 0.0003,
      "step": 64110
    },
    {
      "epoch": 2.05184,
      "grad_norm": 0.00427201297134161,
      "learning_rate": 6.321280000000001e-06,
      "loss": 0.0001,
      "step": 64120
    },
    {
      "epoch": 2.05216,
      "grad_norm": 0.004045276902616024,
      "learning_rate": 6.3191466666666665e-06,
      "loss": 0.0002,
      "step": 64130
    },
    {
      "epoch": 2.05248,
      "grad_norm": 0.003225715598091483,
      "learning_rate": 6.3170133333333335e-06,
      "loss": 0.0003,
      "step": 64140
    },
    {
      "epoch": 2.0528,
      "grad_norm": 0.0044995639473199844,
      "learning_rate": 6.314880000000001e-06,
      "loss": 0.0003,
      "step": 64150
    },
    {
      "epoch": 2.05312,
      "grad_norm": 0.0035561698023229837,
      "learning_rate": 6.312746666666667e-06,
      "loss": 0.0003,
      "step": 64160
    },
    {
      "epoch": 2.05344,
      "grad_norm": 0.0029290267266333103,
      "learning_rate": 6.310613333333334e-06,
      "loss": 0.0889,
      "step": 64170
    },
    {
      "epoch": 2.05376,
      "grad_norm": 0.005677369423210621,
      "learning_rate": 6.30848e-06,
      "loss": 0.0003,
      "step": 64180
    },
    {
      "epoch": 2.05408,
      "grad_norm": 0.00909547321498394,
      "learning_rate": 6.306346666666667e-06,
      "loss": 0.0169,
      "step": 64190
    },
    {
      "epoch": 2.0544,
      "grad_norm": 0.0034642918035387993,
      "learning_rate": 6.304213333333333e-06,
      "loss": 0.0164,
      "step": 64200
    },
    {
      "epoch": 2.05472,
      "grad_norm": 0.007043382152915001,
      "learning_rate": 6.3020800000000005e-06,
      "loss": 0.0003,
      "step": 64210
    },
    {
      "epoch": 2.05504,
      "grad_norm": 0.002481801202520728,
      "learning_rate": 6.2999466666666676e-06,
      "loss": 0.0006,
      "step": 64220
    },
    {
      "epoch": 2.05536,
      "grad_norm": 0.010027156211435795,
      "learning_rate": 6.297813333333334e-06,
      "loss": 0.0467,
      "step": 64230
    },
    {
      "epoch": 2.05568,
      "grad_norm": 0.006796276196837425,
      "learning_rate": 6.295680000000001e-06,
      "loss": 0.0173,
      "step": 64240
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.00844796933233738,
      "learning_rate": 6.293546666666667e-06,
      "loss": 0.0009,
      "step": 64250
    },
    {
      "epoch": 2.05632,
      "grad_norm": 0.0014130142517387867,
      "learning_rate": 6.291413333333334e-06,
      "loss": 0.0003,
      "step": 64260
    },
    {
      "epoch": 2.05664,
      "grad_norm": 0.006385762244462967,
      "learning_rate": 6.289280000000001e-06,
      "loss": 0.0002,
      "step": 64270
    },
    {
      "epoch": 2.05696,
      "grad_norm": 0.007563308347016573,
      "learning_rate": 6.287146666666667e-06,
      "loss": 0.0002,
      "step": 64280
    },
    {
      "epoch": 2.05728,
      "grad_norm": 0.00946192815899849,
      "learning_rate": 6.285013333333334e-06,
      "loss": 0.0079,
      "step": 64290
    },
    {
      "epoch": 2.0576,
      "grad_norm": 0.0028849721420556307,
      "learning_rate": 6.28288e-06,
      "loss": 0.0002,
      "step": 64300
    },
    {
      "epoch": 2.05792,
      "grad_norm": 0.010369613766670227,
      "learning_rate": 6.280746666666667e-06,
      "loss": 0.0003,
      "step": 64310
    },
    {
      "epoch": 2.05824,
      "grad_norm": 0.006553351413458586,
      "learning_rate": 6.278613333333334e-06,
      "loss": 0.0002,
      "step": 64320
    },
    {
      "epoch": 2.05856,
      "grad_norm": 0.007521291729062796,
      "learning_rate": 6.27648e-06,
      "loss": 0.0005,
      "step": 64330
    },
    {
      "epoch": 2.05888,
      "grad_norm": 0.0036212459672242403,
      "learning_rate": 6.274346666666667e-06,
      "loss": 0.0002,
      "step": 64340
    },
    {
      "epoch": 2.0592,
      "grad_norm": 0.0023114357609301805,
      "learning_rate": 6.2722133333333336e-06,
      "loss": 0.0003,
      "step": 64350
    },
    {
      "epoch": 2.05952,
      "grad_norm": 0.07616536319255829,
      "learning_rate": 6.270080000000001e-06,
      "loss": 0.0009,
      "step": 64360
    },
    {
      "epoch": 2.05984,
      "grad_norm": 0.015793340280652046,
      "learning_rate": 6.267946666666668e-06,
      "loss": 0.0002,
      "step": 64370
    },
    {
      "epoch": 2.06016,
      "grad_norm": 0.007464859634637833,
      "learning_rate": 6.265813333333334e-06,
      "loss": 0.0003,
      "step": 64380
    },
    {
      "epoch": 2.06048,
      "grad_norm": 0.007695662789046764,
      "learning_rate": 6.263680000000001e-06,
      "loss": 0.0523,
      "step": 64390
    },
    {
      "epoch": 2.0608,
      "grad_norm": 0.0026892824098467827,
      "learning_rate": 6.261546666666667e-06,
      "loss": 0.0002,
      "step": 64400
    },
    {
      "epoch": 2.06112,
      "grad_norm": 0.007367108482867479,
      "learning_rate": 6.259413333333334e-06,
      "loss": 0.0002,
      "step": 64410
    },
    {
      "epoch": 2.06144,
      "grad_norm": 0.004187550861388445,
      "learning_rate": 6.25728e-06,
      "loss": 0.0002,
      "step": 64420
    },
    {
      "epoch": 2.06176,
      "grad_norm": 0.003676825203001499,
      "learning_rate": 6.255146666666667e-06,
      "loss": 0.002,
      "step": 64430
    },
    {
      "epoch": 2.06208,
      "grad_norm": 0.011716901324689388,
      "learning_rate": 6.253013333333335e-06,
      "loss": 0.0003,
      "step": 64440
    },
    {
      "epoch": 2.0624,
      "grad_norm": 0.004307327792048454,
      "learning_rate": 6.25088e-06,
      "loss": 0.0002,
      "step": 64450
    },
    {
      "epoch": 2.06272,
      "grad_norm": 0.010547579266130924,
      "learning_rate": 6.248746666666667e-06,
      "loss": 0.0002,
      "step": 64460
    },
    {
      "epoch": 2.06304,
      "grad_norm": 0.004853380843997002,
      "learning_rate": 6.246613333333333e-06,
      "loss": 0.0006,
      "step": 64470
    },
    {
      "epoch": 2.06336,
      "grad_norm": 0.004721065051853657,
      "learning_rate": 6.24448e-06,
      "loss": 0.0002,
      "step": 64480
    },
    {
      "epoch": 2.06368,
      "grad_norm": 0.004955309443175793,
      "learning_rate": 6.2423466666666675e-06,
      "loss": 0.0016,
      "step": 64490
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.004209925886243582,
      "learning_rate": 6.240213333333334e-06,
      "loss": 0.0002,
      "step": 64500
    },
    {
      "epoch": 2.06432,
      "grad_norm": 0.003801809623837471,
      "learning_rate": 6.238080000000001e-06,
      "loss": 0.0003,
      "step": 64510
    },
    {
      "epoch": 2.06464,
      "grad_norm": 0.004068717826157808,
      "learning_rate": 6.235946666666667e-06,
      "loss": 0.0002,
      "step": 64520
    },
    {
      "epoch": 2.06496,
      "grad_norm": 0.005098186433315277,
      "learning_rate": 6.233813333333334e-06,
      "loss": 0.0003,
      "step": 64530
    },
    {
      "epoch": 2.06528,
      "grad_norm": 0.005946272984147072,
      "learning_rate": 6.231680000000001e-06,
      "loss": 0.0003,
      "step": 64540
    },
    {
      "epoch": 2.0656,
      "grad_norm": 0.023656703531742096,
      "learning_rate": 6.229546666666667e-06,
      "loss": 0.0333,
      "step": 64550
    },
    {
      "epoch": 2.06592,
      "grad_norm": 0.004940882325172424,
      "learning_rate": 6.227413333333334e-06,
      "loss": 0.0003,
      "step": 64560
    },
    {
      "epoch": 2.06624,
      "grad_norm": 0.003953809384256601,
      "learning_rate": 6.22528e-06,
      "loss": 0.0168,
      "step": 64570
    },
    {
      "epoch": 2.06656,
      "grad_norm": 0.0034377044066786766,
      "learning_rate": 6.223146666666668e-06,
      "loss": 0.0002,
      "step": 64580
    },
    {
      "epoch": 2.06688,
      "grad_norm": 0.003461799817159772,
      "learning_rate": 6.221013333333333e-06,
      "loss": 0.0003,
      "step": 64590
    },
    {
      "epoch": 2.0672,
      "grad_norm": 0.002881755121052265,
      "learning_rate": 6.21888e-06,
      "loss": 0.0002,
      "step": 64600
    },
    {
      "epoch": 2.06752,
      "grad_norm": 0.006497903261333704,
      "learning_rate": 6.216746666666667e-06,
      "loss": 0.0002,
      "step": 64610
    },
    {
      "epoch": 2.06784,
      "grad_norm": 0.00968771893531084,
      "learning_rate": 6.2146133333333335e-06,
      "loss": 0.0951,
      "step": 64620
    },
    {
      "epoch": 2.0681599999999998,
      "grad_norm": 0.0029087720904499292,
      "learning_rate": 6.2124800000000005e-06,
      "loss": 0.0003,
      "step": 64630
    },
    {
      "epoch": 2.06848,
      "grad_norm": 0.014195958152413368,
      "learning_rate": 6.210346666666667e-06,
      "loss": 0.0003,
      "step": 64640
    },
    {
      "epoch": 2.0688,
      "grad_norm": 0.0019708755426108837,
      "learning_rate": 6.208213333333334e-06,
      "loss": 0.0002,
      "step": 64650
    },
    {
      "epoch": 2.06912,
      "grad_norm": 0.0025710570625960827,
      "learning_rate": 6.206080000000001e-06,
      "loss": 0.0002,
      "step": 64660
    },
    {
      "epoch": 2.06944,
      "grad_norm": 0.004293019883334637,
      "learning_rate": 6.203946666666667e-06,
      "loss": 0.0003,
      "step": 64670
    },
    {
      "epoch": 2.06976,
      "grad_norm": 0.006014864891767502,
      "learning_rate": 6.201813333333334e-06,
      "loss": 0.0003,
      "step": 64680
    },
    {
      "epoch": 2.07008,
      "grad_norm": 0.005401652306318283,
      "learning_rate": 6.19968e-06,
      "loss": 0.0003,
      "step": 64690
    },
    {
      "epoch": 2.0704,
      "grad_norm": 0.005891336593776941,
      "learning_rate": 6.1975466666666675e-06,
      "loss": 0.0003,
      "step": 64700
    },
    {
      "epoch": 2.07072,
      "grad_norm": 0.004881549626588821,
      "learning_rate": 6.1954133333333346e-06,
      "loss": 0.0002,
      "step": 64710
    },
    {
      "epoch": 2.07104,
      "grad_norm": 0.001745589543133974,
      "learning_rate": 6.193280000000001e-06,
      "loss": 0.0003,
      "step": 64720
    },
    {
      "epoch": 2.07136,
      "grad_norm": 0.008917352184653282,
      "learning_rate": 6.191146666666668e-06,
      "loss": 0.0002,
      "step": 64730
    },
    {
      "epoch": 2.07168,
      "grad_norm": 0.00280199246481061,
      "learning_rate": 6.189013333333333e-06,
      "loss": 0.0502,
      "step": 64740
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.0030497265979647636,
      "learning_rate": 6.18688e-06,
      "loss": 0.0002,
      "step": 64750
    },
    {
      "epoch": 2.07232,
      "grad_norm": 0.007798166014254093,
      "learning_rate": 6.1847466666666665e-06,
      "loss": 0.0002,
      "step": 64760
    },
    {
      "epoch": 2.07264,
      "grad_norm": 0.0036583116743713617,
      "learning_rate": 6.182613333333334e-06,
      "loss": 0.0002,
      "step": 64770
    },
    {
      "epoch": 2.07296,
      "grad_norm": 0.006102235987782478,
      "learning_rate": 6.180480000000001e-06,
      "loss": 0.0002,
      "step": 64780
    },
    {
      "epoch": 2.07328,
      "grad_norm": 0.01787557080388069,
      "learning_rate": 6.178346666666667e-06,
      "loss": 0.0004,
      "step": 64790
    },
    {
      "epoch": 2.0736,
      "grad_norm": 0.0071101561188697815,
      "learning_rate": 6.176213333333334e-06,
      "loss": 0.0003,
      "step": 64800
    },
    {
      "epoch": 2.07392,
      "grad_norm": 0.0027097042184323072,
      "learning_rate": 6.17408e-06,
      "loss": 0.0435,
      "step": 64810
    },
    {
      "epoch": 2.07424,
      "grad_norm": 0.002293764380738139,
      "learning_rate": 6.171946666666667e-06,
      "loss": 0.0002,
      "step": 64820
    },
    {
      "epoch": 2.07456,
      "grad_norm": 0.0019485031953081489,
      "learning_rate": 6.169813333333334e-06,
      "loss": 0.0003,
      "step": 64830
    },
    {
      "epoch": 2.07488,
      "grad_norm": 0.007323838770389557,
      "learning_rate": 6.1676800000000006e-06,
      "loss": 0.0251,
      "step": 64840
    },
    {
      "epoch": 2.0752,
      "grad_norm": 0.005078533198684454,
      "learning_rate": 6.165546666666668e-06,
      "loss": 0.0002,
      "step": 64850
    },
    {
      "epoch": 2.07552,
      "grad_norm": 0.007601199671626091,
      "learning_rate": 6.163413333333334e-06,
      "loss": 0.0003,
      "step": 64860
    },
    {
      "epoch": 2.07584,
      "grad_norm": 0.007290970534086227,
      "learning_rate": 6.161280000000001e-06,
      "loss": 0.0005,
      "step": 64870
    },
    {
      "epoch": 2.07616,
      "grad_norm": 0.00480252830311656,
      "learning_rate": 6.159146666666668e-06,
      "loss": 0.0004,
      "step": 64880
    },
    {
      "epoch": 2.07648,
      "grad_norm": 0.003198498860001564,
      "learning_rate": 6.157013333333333e-06,
      "loss": 0.0002,
      "step": 64890
    },
    {
      "epoch": 2.0768,
      "grad_norm": 0.06715868413448334,
      "learning_rate": 6.1548800000000004e-06,
      "loss": 0.0006,
      "step": 64900
    },
    {
      "epoch": 2.07712,
      "grad_norm": 0.006573032587766647,
      "learning_rate": 6.152746666666667e-06,
      "loss": 0.0003,
      "step": 64910
    },
    {
      "epoch": 2.07744,
      "grad_norm": 0.011140367016196251,
      "learning_rate": 6.150613333333334e-06,
      "loss": 0.0002,
      "step": 64920
    },
    {
      "epoch": 2.07776,
      "grad_norm": 0.010283797979354858,
      "learning_rate": 6.14848e-06,
      "loss": 0.0003,
      "step": 64930
    },
    {
      "epoch": 2.07808,
      "grad_norm": 0.011791921220719814,
      "learning_rate": 6.146346666666667e-06,
      "loss": 0.0003,
      "step": 64940
    },
    {
      "epoch": 2.0784,
      "grad_norm": 0.002420391421765089,
      "learning_rate": 6.144213333333334e-06,
      "loss": 0.0002,
      "step": 64950
    },
    {
      "epoch": 2.07872,
      "grad_norm": 0.005657167173922062,
      "learning_rate": 6.14208e-06,
      "loss": 0.043,
      "step": 64960
    },
    {
      "epoch": 2.07904,
      "grad_norm": 0.007522441912442446,
      "learning_rate": 6.139946666666667e-06,
      "loss": 0.0002,
      "step": 64970
    },
    {
      "epoch": 2.07936,
      "grad_norm": 0.003782521700486541,
      "learning_rate": 6.137813333333334e-06,
      "loss": 0.0003,
      "step": 64980
    },
    {
      "epoch": 2.07968,
      "grad_norm": 0.010904612019658089,
      "learning_rate": 6.135680000000001e-06,
      "loss": 0.0002,
      "step": 64990
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.00471891462802887,
      "learning_rate": 6.133546666666668e-06,
      "loss": 0.0004,
      "step": 65000
    },
    {
      "epoch": 2.08032,
      "grad_norm": 0.006815745960921049,
      "learning_rate": 6.131413333333334e-06,
      "loss": 0.0003,
      "step": 65010
    },
    {
      "epoch": 2.08064,
      "grad_norm": 0.007380157243460417,
      "learning_rate": 6.129280000000001e-06,
      "loss": 0.004,
      "step": 65020
    },
    {
      "epoch": 2.08096,
      "grad_norm": 0.0029116522055119276,
      "learning_rate": 6.1271466666666664e-06,
      "loss": 0.0003,
      "step": 65030
    },
    {
      "epoch": 2.08128,
      "grad_norm": 0.0060094851069152355,
      "learning_rate": 6.1250133333333335e-06,
      "loss": 0.0016,
      "step": 65040
    },
    {
      "epoch": 2.0816,
      "grad_norm": 0.006058827508240938,
      "learning_rate": 6.122880000000001e-06,
      "loss": 0.0002,
      "step": 65050
    },
    {
      "epoch": 2.08192,
      "grad_norm": 0.005609377287328243,
      "learning_rate": 6.120746666666667e-06,
      "loss": 0.0003,
      "step": 65060
    },
    {
      "epoch": 2.08224,
      "grad_norm": 0.0038276170380413532,
      "learning_rate": 6.118613333333334e-06,
      "loss": 0.0003,
      "step": 65070
    },
    {
      "epoch": 2.08256,
      "grad_norm": 0.021905062720179558,
      "learning_rate": 6.11648e-06,
      "loss": 0.0002,
      "step": 65080
    },
    {
      "epoch": 2.08288,
      "grad_norm": 0.004334378521889448,
      "learning_rate": 6.114346666666667e-06,
      "loss": 0.0002,
      "step": 65090
    },
    {
      "epoch": 2.0832,
      "grad_norm": 0.003198162652552128,
      "learning_rate": 6.112213333333333e-06,
      "loss": 0.0001,
      "step": 65100
    },
    {
      "epoch": 2.08352,
      "grad_norm": 3.3103437423706055,
      "learning_rate": 6.1100800000000005e-06,
      "loss": 0.0044,
      "step": 65110
    },
    {
      "epoch": 2.08384,
      "grad_norm": 0.0021959503646939993,
      "learning_rate": 6.1079466666666675e-06,
      "loss": 0.0002,
      "step": 65120
    },
    {
      "epoch": 2.08416,
      "grad_norm": 0.00573901878669858,
      "learning_rate": 6.105813333333334e-06,
      "loss": 0.0002,
      "step": 65130
    },
    {
      "epoch": 2.08448,
      "grad_norm": 0.005025715567171574,
      "learning_rate": 6.103680000000001e-06,
      "loss": 0.0488,
      "step": 65140
    },
    {
      "epoch": 2.0848,
      "grad_norm": 0.004403941333293915,
      "learning_rate": 6.101546666666667e-06,
      "loss": 0.0032,
      "step": 65150
    },
    {
      "epoch": 2.08512,
      "grad_norm": 0.0033437302336096764,
      "learning_rate": 6.099413333333334e-06,
      "loss": 0.0001,
      "step": 65160
    },
    {
      "epoch": 2.08544,
      "grad_norm": 0.0060769072733819485,
      "learning_rate": 6.097280000000001e-06,
      "loss": 0.0368,
      "step": 65170
    },
    {
      "epoch": 2.08576,
      "grad_norm": 0.05121735855937004,
      "learning_rate": 6.0951466666666666e-06,
      "loss": 0.0003,
      "step": 65180
    },
    {
      "epoch": 2.08608,
      "grad_norm": 0.038819652050733566,
      "learning_rate": 6.0930133333333345e-06,
      "loss": 0.0003,
      "step": 65190
    },
    {
      "epoch": 2.0864,
      "grad_norm": 0.03740779310464859,
      "learning_rate": 6.09088e-06,
      "loss": 0.0003,
      "step": 65200
    },
    {
      "epoch": 2.08672,
      "grad_norm": 0.009433336555957794,
      "learning_rate": 6.088746666666667e-06,
      "loss": 0.0026,
      "step": 65210
    },
    {
      "epoch": 2.08704,
      "grad_norm": 0.0034792085643857718,
      "learning_rate": 6.086613333333334e-06,
      "loss": 0.0583,
      "step": 65220
    },
    {
      "epoch": 2.08736,
      "grad_norm": 0.008109594695270061,
      "learning_rate": 6.08448e-06,
      "loss": 0.0004,
      "step": 65230
    },
    {
      "epoch": 2.08768,
      "grad_norm": 0.0051489584147930145,
      "learning_rate": 6.082346666666667e-06,
      "loss": 0.0002,
      "step": 65240
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.005805423483252525,
      "learning_rate": 6.0802133333333335e-06,
      "loss": 0.052,
      "step": 65250
    },
    {
      "epoch": 2.08832,
      "grad_norm": 0.0038091547321528196,
      "learning_rate": 6.078080000000001e-06,
      "loss": 0.0072,
      "step": 65260
    },
    {
      "epoch": 2.08864,
      "grad_norm": 0.018139729276299477,
      "learning_rate": 6.075946666666667e-06,
      "loss": 0.0005,
      "step": 65270
    },
    {
      "epoch": 2.08896,
      "grad_norm": 0.0050657643005251884,
      "learning_rate": 6.073813333333334e-06,
      "loss": 0.0048,
      "step": 65280
    },
    {
      "epoch": 2.08928,
      "grad_norm": 0.002457484370097518,
      "learning_rate": 6.071680000000001e-06,
      "loss": 0.0002,
      "step": 65290
    },
    {
      "epoch": 2.0896,
      "grad_norm": 0.0034607157576829195,
      "learning_rate": 6.069546666666667e-06,
      "loss": 0.0219,
      "step": 65300
    },
    {
      "epoch": 2.08992,
      "grad_norm": 0.277785062789917,
      "learning_rate": 6.067413333333334e-06,
      "loss": 0.0006,
      "step": 65310
    },
    {
      "epoch": 2.09024,
      "grad_norm": 0.006571945268660784,
      "learning_rate": 6.06528e-06,
      "loss": 0.0002,
      "step": 65320
    },
    {
      "epoch": 2.09056,
      "grad_norm": 0.003223318373784423,
      "learning_rate": 6.0631466666666676e-06,
      "loss": 0.0002,
      "step": 65330
    },
    {
      "epoch": 2.09088,
      "grad_norm": 0.022221211344003677,
      "learning_rate": 6.061013333333335e-06,
      "loss": 0.0007,
      "step": 65340
    },
    {
      "epoch": 2.0912,
      "grad_norm": 0.015235364437103271,
      "learning_rate": 6.05888e-06,
      "loss": 0.0005,
      "step": 65350
    },
    {
      "epoch": 2.09152,
      "grad_norm": 0.004981370642781258,
      "learning_rate": 6.056746666666667e-06,
      "loss": 0.0028,
      "step": 65360
    },
    {
      "epoch": 2.09184,
      "grad_norm": 0.003894444089382887,
      "learning_rate": 6.054613333333333e-06,
      "loss": 0.0003,
      "step": 65370
    },
    {
      "epoch": 2.09216,
      "grad_norm": 0.004919915925711393,
      "learning_rate": 6.05248e-06,
      "loss": 0.0002,
      "step": 65380
    },
    {
      "epoch": 2.09248,
      "grad_norm": 0.010162226855754852,
      "learning_rate": 6.0503466666666674e-06,
      "loss": 0.0535,
      "step": 65390
    },
    {
      "epoch": 2.0928,
      "grad_norm": 0.007999000139534473,
      "learning_rate": 6.048213333333334e-06,
      "loss": 0.0013,
      "step": 65400
    },
    {
      "epoch": 2.09312,
      "grad_norm": 0.0017382260411977768,
      "learning_rate": 6.046080000000001e-06,
      "loss": 0.0003,
      "step": 65410
    },
    {
      "epoch": 2.09344,
      "grad_norm": 0.007868311367928982,
      "learning_rate": 6.043946666666667e-06,
      "loss": 0.0002,
      "step": 65420
    },
    {
      "epoch": 2.09376,
      "grad_norm": 0.006297532469034195,
      "learning_rate": 6.041813333333334e-06,
      "loss": 0.0012,
      "step": 65430
    },
    {
      "epoch": 2.09408,
      "grad_norm": 0.004870409611612558,
      "learning_rate": 6.03968e-06,
      "loss": 0.0007,
      "step": 65440
    },
    {
      "epoch": 2.0944,
      "grad_norm": 0.34115952253341675,
      "learning_rate": 6.037546666666667e-06,
      "loss": 0.0494,
      "step": 65450
    },
    {
      "epoch": 2.09472,
      "grad_norm": 0.0068291970528662205,
      "learning_rate": 6.035413333333334e-06,
      "loss": 0.0006,
      "step": 65460
    },
    {
      "epoch": 2.09504,
      "grad_norm": 0.006632366217672825,
      "learning_rate": 6.033280000000001e-06,
      "loss": 0.0003,
      "step": 65470
    },
    {
      "epoch": 2.09536,
      "grad_norm": 0.006227809004485607,
      "learning_rate": 6.031146666666668e-06,
      "loss": 0.0002,
      "step": 65480
    },
    {
      "epoch": 2.09568,
      "grad_norm": 0.006858031265437603,
      "learning_rate": 6.029013333333333e-06,
      "loss": 0.0002,
      "step": 65490
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.005883360747247934,
      "learning_rate": 6.02688e-06,
      "loss": 0.0013,
      "step": 65500
    },
    {
      "epoch": 2.09632,
      "grad_norm": 0.009272804483771324,
      "learning_rate": 6.024746666666667e-06,
      "loss": 0.0003,
      "step": 65510
    },
    {
      "epoch": 2.09664,
      "grad_norm": 0.00718486774712801,
      "learning_rate": 6.0226133333333334e-06,
      "loss": 0.0054,
      "step": 65520
    },
    {
      "epoch": 2.09696,
      "grad_norm": 0.007364417426288128,
      "learning_rate": 6.0204800000000005e-06,
      "loss": 0.0002,
      "step": 65530
    },
    {
      "epoch": 2.09728,
      "grad_norm": 0.004885506816208363,
      "learning_rate": 6.018346666666667e-06,
      "loss": 0.006,
      "step": 65540
    },
    {
      "epoch": 2.0976,
      "grad_norm": 0.003795529017224908,
      "learning_rate": 6.016213333333334e-06,
      "loss": 0.0019,
      "step": 65550
    },
    {
      "epoch": 2.09792,
      "grad_norm": 0.003001065691933036,
      "learning_rate": 6.014080000000001e-06,
      "loss": 0.0003,
      "step": 65560
    },
    {
      "epoch": 2.09824,
      "grad_norm": 0.0036035366356372833,
      "learning_rate": 6.011946666666667e-06,
      "loss": 0.0002,
      "step": 65570
    },
    {
      "epoch": 2.09856,
      "grad_norm": 0.0034825948532670736,
      "learning_rate": 6.009813333333334e-06,
      "loss": 0.0593,
      "step": 65580
    },
    {
      "epoch": 2.09888,
      "grad_norm": 0.0074646128341555595,
      "learning_rate": 6.00768e-06,
      "loss": 0.0003,
      "step": 65590
    },
    {
      "epoch": 2.0992,
      "grad_norm": 0.0019004797795787454,
      "learning_rate": 6.0055466666666675e-06,
      "loss": 0.0006,
      "step": 65600
    },
    {
      "epoch": 2.09952,
      "grad_norm": 0.003711115336045623,
      "learning_rate": 6.003413333333334e-06,
      "loss": 0.0002,
      "step": 65610
    },
    {
      "epoch": 2.09984,
      "grad_norm": 0.00258412747643888,
      "learning_rate": 6.001280000000001e-06,
      "loss": 0.0002,
      "step": 65620
    },
    {
      "epoch": 2.10016,
      "grad_norm": 0.005629018414765596,
      "learning_rate": 5.999146666666668e-06,
      "loss": 0.0002,
      "step": 65630
    },
    {
      "epoch": 2.10048,
      "grad_norm": 0.01256162952631712,
      "learning_rate": 5.997013333333333e-06,
      "loss": 0.0002,
      "step": 65640
    },
    {
      "epoch": 2.1008,
      "grad_norm": 0.009182695299386978,
      "learning_rate": 5.99488e-06,
      "loss": 0.0003,
      "step": 65650
    },
    {
      "epoch": 2.10112,
      "grad_norm": 7.513900279998779,
      "learning_rate": 5.9927466666666665e-06,
      "loss": 0.0157,
      "step": 65660
    },
    {
      "epoch": 2.10144,
      "grad_norm": 0.005938282702118158,
      "learning_rate": 5.9906133333333336e-06,
      "loss": 0.0006,
      "step": 65670
    },
    {
      "epoch": 2.10176,
      "grad_norm": 0.002118512988090515,
      "learning_rate": 5.988480000000001e-06,
      "loss": 0.0002,
      "step": 65680
    },
    {
      "epoch": 2.10208,
      "grad_norm": 0.0035187772009521723,
      "learning_rate": 5.986346666666667e-06,
      "loss": 0.0003,
      "step": 65690
    },
    {
      "epoch": 2.1024,
      "grad_norm": 0.0027346289716660976,
      "learning_rate": 5.984213333333334e-06,
      "loss": 0.0002,
      "step": 65700
    },
    {
      "epoch": 2.10272,
      "grad_norm": 0.004262709990143776,
      "learning_rate": 5.98208e-06,
      "loss": 0.0002,
      "step": 65710
    },
    {
      "epoch": 2.10304,
      "grad_norm": 0.1907760053873062,
      "learning_rate": 5.979946666666667e-06,
      "loss": 0.0004,
      "step": 65720
    },
    {
      "epoch": 2.10336,
      "grad_norm": 0.11665885150432587,
      "learning_rate": 5.977813333333334e-06,
      "loss": 0.0003,
      "step": 65730
    },
    {
      "epoch": 2.1036799999999998,
      "grad_norm": 0.16478680074214935,
      "learning_rate": 5.9756800000000005e-06,
      "loss": 0.0004,
      "step": 65740
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.0013399588642641902,
      "learning_rate": 5.973546666666668e-06,
      "loss": 0.0319,
      "step": 65750
    },
    {
      "epoch": 2.10432,
      "grad_norm": 0.004215880297124386,
      "learning_rate": 5.971413333333334e-06,
      "loss": 0.0472,
      "step": 65760
    },
    {
      "epoch": 2.10464,
      "grad_norm": 0.012769555673003197,
      "learning_rate": 5.969280000000001e-06,
      "loss": 0.0513,
      "step": 65770
    },
    {
      "epoch": 2.10496,
      "grad_norm": 0.003562662983313203,
      "learning_rate": 5.967146666666666e-06,
      "loss": 0.0002,
      "step": 65780
    },
    {
      "epoch": 2.10528,
      "grad_norm": 0.015773499384522438,
      "learning_rate": 5.965013333333334e-06,
      "loss": 0.0003,
      "step": 65790
    },
    {
      "epoch": 2.1056,
      "grad_norm": 0.004494489636272192,
      "learning_rate": 5.962880000000001e-06,
      "loss": 0.0009,
      "step": 65800
    },
    {
      "epoch": 2.10592,
      "grad_norm": 0.0027051817160099745,
      "learning_rate": 5.960746666666667e-06,
      "loss": 0.0002,
      "step": 65810
    },
    {
      "epoch": 2.10624,
      "grad_norm": 0.003829105757176876,
      "learning_rate": 5.958613333333334e-06,
      "loss": 0.0003,
      "step": 65820
    },
    {
      "epoch": 2.10656,
      "grad_norm": 0.008335850201547146,
      "learning_rate": 5.95648e-06,
      "loss": 0.0003,
      "step": 65830
    },
    {
      "epoch": 2.10688,
      "grad_norm": 0.01461654994636774,
      "learning_rate": 5.954346666666667e-06,
      "loss": 0.0344,
      "step": 65840
    },
    {
      "epoch": 2.1072,
      "grad_norm": 0.011844444088637829,
      "learning_rate": 5.952213333333334e-06,
      "loss": 0.0009,
      "step": 65850
    },
    {
      "epoch": 2.10752,
      "grad_norm": 0.014640936627984047,
      "learning_rate": 5.95008e-06,
      "loss": 0.0002,
      "step": 65860
    },
    {
      "epoch": 2.10784,
      "grad_norm": 8.291714668273926,
      "learning_rate": 5.947946666666667e-06,
      "loss": 0.0085,
      "step": 65870
    },
    {
      "epoch": 2.10816,
      "grad_norm": 0.0020274606067687273,
      "learning_rate": 5.945813333333334e-06,
      "loss": 0.0002,
      "step": 65880
    },
    {
      "epoch": 2.10848,
      "grad_norm": 0.006268978118896484,
      "learning_rate": 5.943680000000001e-06,
      "loss": 0.001,
      "step": 65890
    },
    {
      "epoch": 2.1088,
      "grad_norm": 0.005240880884230137,
      "learning_rate": 5.941546666666668e-06,
      "loss": 0.0898,
      "step": 65900
    },
    {
      "epoch": 2.10912,
      "grad_norm": 0.007432753220200539,
      "learning_rate": 5.939413333333334e-06,
      "loss": 0.0003,
      "step": 65910
    },
    {
      "epoch": 2.10944,
      "grad_norm": 0.005686613265424967,
      "learning_rate": 5.937280000000001e-06,
      "loss": 0.0046,
      "step": 65920
    },
    {
      "epoch": 2.10976,
      "grad_norm": 0.003987665753811598,
      "learning_rate": 5.935146666666667e-06,
      "loss": 0.0003,
      "step": 65930
    },
    {
      "epoch": 2.11008,
      "grad_norm": 0.006093057803809643,
      "learning_rate": 5.933013333333334e-06,
      "loss": 0.0003,
      "step": 65940
    },
    {
      "epoch": 2.1104,
      "grad_norm": 0.005381570663303137,
      "learning_rate": 5.93088e-06,
      "loss": 0.0003,
      "step": 65950
    },
    {
      "epoch": 2.11072,
      "grad_norm": 0.006022756919264793,
      "learning_rate": 5.928746666666667e-06,
      "loss": 0.0005,
      "step": 65960
    },
    {
      "epoch": 2.11104,
      "grad_norm": 0.005965903867036104,
      "learning_rate": 5.926613333333334e-06,
      "loss": 0.0003,
      "step": 65970
    },
    {
      "epoch": 2.11136,
      "grad_norm": 0.00492471270263195,
      "learning_rate": 5.92448e-06,
      "loss": 0.0002,
      "step": 65980
    },
    {
      "epoch": 2.11168,
      "grad_norm": 0.011563930660486221,
      "learning_rate": 5.922346666666667e-06,
      "loss": 0.0016,
      "step": 65990
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.005912708584219217,
      "learning_rate": 5.920213333333333e-06,
      "loss": 0.0002,
      "step": 66000
    },
    {
      "epoch": 2.11232,
      "grad_norm": 0.0028507912065833807,
      "learning_rate": 5.9180800000000004e-06,
      "loss": 0.0002,
      "step": 66010
    },
    {
      "epoch": 2.11264,
      "grad_norm": 0.0022297000978142023,
      "learning_rate": 5.9159466666666675e-06,
      "loss": 0.0142,
      "step": 66020
    },
    {
      "epoch": 2.11296,
      "grad_norm": 0.00799835380166769,
      "learning_rate": 5.913813333333334e-06,
      "loss": 0.0004,
      "step": 66030
    },
    {
      "epoch": 2.11328,
      "grad_norm": 0.01361574325710535,
      "learning_rate": 5.911680000000001e-06,
      "loss": 0.0445,
      "step": 66040
    },
    {
      "epoch": 2.1136,
      "grad_norm": 0.005629184190183878,
      "learning_rate": 5.909546666666667e-06,
      "loss": 0.0003,
      "step": 66050
    },
    {
      "epoch": 2.11392,
      "grad_norm": 0.008872243575751781,
      "learning_rate": 5.907413333333334e-06,
      "loss": 0.0008,
      "step": 66060
    },
    {
      "epoch": 2.11424,
      "grad_norm": 0.004502881783992052,
      "learning_rate": 5.905280000000001e-06,
      "loss": 0.0135,
      "step": 66070
    },
    {
      "epoch": 2.11456,
      "grad_norm": 0.006651152856647968,
      "learning_rate": 5.903146666666667e-06,
      "loss": 0.0003,
      "step": 66080
    },
    {
      "epoch": 2.11488,
      "grad_norm": 0.008725611492991447,
      "learning_rate": 5.9010133333333345e-06,
      "loss": 0.0002,
      "step": 66090
    },
    {
      "epoch": 2.1152,
      "grad_norm": 0.012779180891811848,
      "learning_rate": 5.89888e-06,
      "loss": 0.0309,
      "step": 66100
    },
    {
      "epoch": 2.11552,
      "grad_norm": 0.00486223166808486,
      "learning_rate": 5.896746666666667e-06,
      "loss": 0.0003,
      "step": 66110
    },
    {
      "epoch": 2.11584,
      "grad_norm": 0.006927699316293001,
      "learning_rate": 5.894613333333333e-06,
      "loss": 0.0005,
      "step": 66120
    },
    {
      "epoch": 2.11616,
      "grad_norm": 0.0051280115731060505,
      "learning_rate": 5.89248e-06,
      "loss": 0.0003,
      "step": 66130
    },
    {
      "epoch": 2.11648,
      "grad_norm": 0.0036860520485788584,
      "learning_rate": 5.890346666666667e-06,
      "loss": 0.0002,
      "step": 66140
    },
    {
      "epoch": 2.1168,
      "grad_norm": 0.0032851931173354387,
      "learning_rate": 5.8882133333333335e-06,
      "loss": 0.0002,
      "step": 66150
    },
    {
      "epoch": 2.11712,
      "grad_norm": 0.0064774625934660435,
      "learning_rate": 5.8860800000000006e-06,
      "loss": 0.0003,
      "step": 66160
    },
    {
      "epoch": 2.11744,
      "grad_norm": 0.012368487194180489,
      "learning_rate": 5.883946666666667e-06,
      "loss": 0.0003,
      "step": 66170
    },
    {
      "epoch": 2.11776,
      "grad_norm": 0.00849903468042612,
      "learning_rate": 5.881813333333334e-06,
      "loss": 0.0003,
      "step": 66180
    },
    {
      "epoch": 2.11808,
      "grad_norm": 0.007985788397490978,
      "learning_rate": 5.879680000000001e-06,
      "loss": 0.0002,
      "step": 66190
    },
    {
      "epoch": 2.1184,
      "grad_norm": 0.004750755149871111,
      "learning_rate": 5.877546666666667e-06,
      "loss": 0.0005,
      "step": 66200
    },
    {
      "epoch": 2.11872,
      "grad_norm": 0.017193712294101715,
      "learning_rate": 5.875413333333334e-06,
      "loss": 0.0003,
      "step": 66210
    },
    {
      "epoch": 2.11904,
      "grad_norm": 0.002149534411728382,
      "learning_rate": 5.8732800000000005e-06,
      "loss": 0.0002,
      "step": 66220
    },
    {
      "epoch": 2.11936,
      "grad_norm": 0.9218596816062927,
      "learning_rate": 5.8711466666666675e-06,
      "loss": 0.0452,
      "step": 66230
    },
    {
      "epoch": 2.11968,
      "grad_norm": 0.00933141354471445,
      "learning_rate": 5.869013333333335e-06,
      "loss": 0.0003,
      "step": 66240
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.012532932683825493,
      "learning_rate": 5.86688e-06,
      "loss": 0.0003,
      "step": 66250
    },
    {
      "epoch": 2.12032,
      "grad_norm": 0.015423830598592758,
      "learning_rate": 5.864746666666668e-06,
      "loss": 0.0003,
      "step": 66260
    },
    {
      "epoch": 2.12064,
      "grad_norm": 0.0037658954970538616,
      "learning_rate": 5.862613333333333e-06,
      "loss": 0.0103,
      "step": 66270
    },
    {
      "epoch": 2.12096,
      "grad_norm": 0.007766600698232651,
      "learning_rate": 5.86048e-06,
      "loss": 0.0003,
      "step": 66280
    },
    {
      "epoch": 2.12128,
      "grad_norm": 0.004167595878243446,
      "learning_rate": 5.8583466666666666e-06,
      "loss": 0.0002,
      "step": 66290
    },
    {
      "epoch": 2.1216,
      "grad_norm": 0.007892646826803684,
      "learning_rate": 5.856213333333334e-06,
      "loss": 0.0003,
      "step": 66300
    },
    {
      "epoch": 2.12192,
      "grad_norm": 0.010020777583122253,
      "learning_rate": 5.854080000000001e-06,
      "loss": 0.0002,
      "step": 66310
    },
    {
      "epoch": 2.12224,
      "grad_norm": 0.005072188563644886,
      "learning_rate": 5.851946666666667e-06,
      "loss": 0.0003,
      "step": 66320
    },
    {
      "epoch": 2.12256,
      "grad_norm": 0.02850695326924324,
      "learning_rate": 5.849813333333334e-06,
      "loss": 0.0004,
      "step": 66330
    },
    {
      "epoch": 2.12288,
      "grad_norm": 0.00550403818488121,
      "learning_rate": 5.84768e-06,
      "loss": 0.0003,
      "step": 66340
    },
    {
      "epoch": 2.1232,
      "grad_norm": 0.0062527405098080635,
      "learning_rate": 5.845546666666667e-06,
      "loss": 0.0084,
      "step": 66350
    },
    {
      "epoch": 2.12352,
      "grad_norm": 0.0066712829284369946,
      "learning_rate": 5.843413333333334e-06,
      "loss": 0.0013,
      "step": 66360
    },
    {
      "epoch": 2.12384,
      "grad_norm": 0.006265594623982906,
      "learning_rate": 5.841280000000001e-06,
      "loss": 0.0003,
      "step": 66370
    },
    {
      "epoch": 2.12416,
      "grad_norm": 0.003541982499882579,
      "learning_rate": 5.839146666666668e-06,
      "loss": 0.0003,
      "step": 66380
    },
    {
      "epoch": 2.12448,
      "grad_norm": 0.005812349263578653,
      "learning_rate": 5.837013333333333e-06,
      "loss": 0.0004,
      "step": 66390
    },
    {
      "epoch": 2.1248,
      "grad_norm": 0.002000360284000635,
      "learning_rate": 5.834880000000001e-06,
      "loss": 0.0003,
      "step": 66400
    },
    {
      "epoch": 2.12512,
      "grad_norm": 0.11573249101638794,
      "learning_rate": 5.832746666666668e-06,
      "loss": 0.0004,
      "step": 66410
    },
    {
      "epoch": 2.12544,
      "grad_norm": 0.004992322996258736,
      "learning_rate": 5.830613333333333e-06,
      "loss": 0.0095,
      "step": 66420
    },
    {
      "epoch": 2.12576,
      "grad_norm": 0.0013788569485768676,
      "learning_rate": 5.8284800000000005e-06,
      "loss": 0.0547,
      "step": 66430
    },
    {
      "epoch": 2.12608,
      "grad_norm": 0.005111331585794687,
      "learning_rate": 5.826346666666667e-06,
      "loss": 0.0166,
      "step": 66440
    },
    {
      "epoch": 2.1264,
      "grad_norm": 0.009716035798192024,
      "learning_rate": 5.824213333333334e-06,
      "loss": 0.0013,
      "step": 66450
    },
    {
      "epoch": 2.12672,
      "grad_norm": 0.007621862459927797,
      "learning_rate": 5.82208e-06,
      "loss": 0.0246,
      "step": 66460
    },
    {
      "epoch": 2.12704,
      "grad_norm": 0.004696794785559177,
      "learning_rate": 5.819946666666667e-06,
      "loss": 0.0003,
      "step": 66470
    },
    {
      "epoch": 2.12736,
      "grad_norm": 0.006941229570657015,
      "learning_rate": 5.817813333333334e-06,
      "loss": 0.0002,
      "step": 66480
    },
    {
      "epoch": 2.12768,
      "grad_norm": 0.0027259732596576214,
      "learning_rate": 5.81568e-06,
      "loss": 0.038,
      "step": 66490
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.008717346005141735,
      "learning_rate": 5.8135466666666674e-06,
      "loss": 0.0003,
      "step": 66500
    },
    {
      "epoch": 2.12832,
      "grad_norm": 0.024235719814896584,
      "learning_rate": 5.811413333333334e-06,
      "loss": 0.0003,
      "step": 66510
    },
    {
      "epoch": 2.12864,
      "grad_norm": 0.012082178145647049,
      "learning_rate": 5.809280000000001e-06,
      "loss": 0.0004,
      "step": 66520
    },
    {
      "epoch": 2.12896,
      "grad_norm": 0.01193904783576727,
      "learning_rate": 5.807146666666668e-06,
      "loss": 0.0005,
      "step": 66530
    },
    {
      "epoch": 2.12928,
      "grad_norm": 0.01021341048181057,
      "learning_rate": 5.805013333333334e-06,
      "loss": 0.0003,
      "step": 66540
    },
    {
      "epoch": 2.1296,
      "grad_norm": 1.2733975648880005,
      "learning_rate": 5.802880000000001e-06,
      "loss": 0.0532,
      "step": 66550
    },
    {
      "epoch": 2.12992,
      "grad_norm": 3.406115770339966,
      "learning_rate": 5.8007466666666665e-06,
      "loss": 0.0062,
      "step": 66560
    },
    {
      "epoch": 2.13024,
      "grad_norm": 0.005261795595288277,
      "learning_rate": 5.7986133333333335e-06,
      "loss": 0.0283,
      "step": 66570
    },
    {
      "epoch": 2.13056,
      "grad_norm": 0.007784556597471237,
      "learning_rate": 5.796480000000001e-06,
      "loss": 0.0003,
      "step": 66580
    },
    {
      "epoch": 2.13088,
      "grad_norm": 0.004223580937832594,
      "learning_rate": 5.794346666666667e-06,
      "loss": 0.0015,
      "step": 66590
    },
    {
      "epoch": 2.1312,
      "grad_norm": 0.005391422659158707,
      "learning_rate": 5.792213333333334e-06,
      "loss": 0.0003,
      "step": 66600
    },
    {
      "epoch": 2.13152,
      "grad_norm": 0.005171530414372683,
      "learning_rate": 5.79008e-06,
      "loss": 0.0004,
      "step": 66610
    },
    {
      "epoch": 2.13184,
      "grad_norm": 0.005829231813549995,
      "learning_rate": 5.787946666666667e-06,
      "loss": 0.0003,
      "step": 66620
    },
    {
      "epoch": 2.13216,
      "grad_norm": 0.004437834490090609,
      "learning_rate": 5.7858133333333334e-06,
      "loss": 0.0002,
      "step": 66630
    },
    {
      "epoch": 2.13248,
      "grad_norm": 0.0036732591688632965,
      "learning_rate": 5.7836800000000005e-06,
      "loss": 0.0002,
      "step": 66640
    },
    {
      "epoch": 2.1328,
      "grad_norm": 0.005456431768834591,
      "learning_rate": 5.7815466666666676e-06,
      "loss": 0.0002,
      "step": 66650
    },
    {
      "epoch": 2.13312,
      "grad_norm": 0.006533915176987648,
      "learning_rate": 5.779413333333334e-06,
      "loss": 0.0086,
      "step": 66660
    },
    {
      "epoch": 2.1334400000000002,
      "grad_norm": 0.013892032206058502,
      "learning_rate": 5.777280000000001e-06,
      "loss": 0.0073,
      "step": 66670
    },
    {
      "epoch": 2.13376,
      "grad_norm": 0.004472232423722744,
      "learning_rate": 5.775146666666667e-06,
      "loss": 0.0004,
      "step": 66680
    },
    {
      "epoch": 2.13408,
      "grad_norm": 0.011403551325201988,
      "learning_rate": 5.773013333333334e-06,
      "loss": 0.064,
      "step": 66690
    },
    {
      "epoch": 2.1344,
      "grad_norm": 0.012557013891637325,
      "learning_rate": 5.770880000000001e-06,
      "loss": 0.0003,
      "step": 66700
    },
    {
      "epoch": 2.13472,
      "grad_norm": 0.007432436570525169,
      "learning_rate": 5.768746666666667e-06,
      "loss": 0.0003,
      "step": 66710
    },
    {
      "epoch": 2.13504,
      "grad_norm": 0.007525663822889328,
      "learning_rate": 5.766613333333334e-06,
      "loss": 0.001,
      "step": 66720
    },
    {
      "epoch": 2.13536,
      "grad_norm": 0.00341998809017241,
      "learning_rate": 5.76448e-06,
      "loss": 0.0002,
      "step": 66730
    },
    {
      "epoch": 2.13568,
      "grad_norm": 0.020195430144667625,
      "learning_rate": 5.762346666666667e-06,
      "loss": 0.0002,
      "step": 66740
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.017244715243577957,
      "learning_rate": 5.760213333333334e-06,
      "loss": 0.0176,
      "step": 66750
    },
    {
      "epoch": 2.13632,
      "grad_norm": 0.007810145616531372,
      "learning_rate": 5.75808e-06,
      "loss": 0.0129,
      "step": 66760
    },
    {
      "epoch": 2.13664,
      "grad_norm": 0.002244452014565468,
      "learning_rate": 5.755946666666667e-06,
      "loss": 0.0708,
      "step": 66770
    },
    {
      "epoch": 2.13696,
      "grad_norm": 0.00534094450995326,
      "learning_rate": 5.7538133333333336e-06,
      "loss": 0.0003,
      "step": 66780
    },
    {
      "epoch": 2.13728,
      "grad_norm": 0.004425432067364454,
      "learning_rate": 5.751680000000001e-06,
      "loss": 0.0361,
      "step": 66790
    },
    {
      "epoch": 2.1376,
      "grad_norm": 0.0038571644108742476,
      "learning_rate": 5.749546666666667e-06,
      "loss": 0.0004,
      "step": 66800
    },
    {
      "epoch": 2.13792,
      "grad_norm": 0.003389128251001239,
      "learning_rate": 5.747413333333334e-06,
      "loss": 0.0003,
      "step": 66810
    },
    {
      "epoch": 2.13824,
      "grad_norm": 0.01426383201032877,
      "learning_rate": 5.745280000000001e-06,
      "loss": 0.0085,
      "step": 66820
    },
    {
      "epoch": 2.13856,
      "grad_norm": 0.0047904071398079395,
      "learning_rate": 5.743146666666667e-06,
      "loss": 0.0053,
      "step": 66830
    },
    {
      "epoch": 2.13888,
      "grad_norm": 0.006598615553230047,
      "learning_rate": 5.741013333333334e-06,
      "loss": 0.0003,
      "step": 66840
    },
    {
      "epoch": 2.1391999999999998,
      "grad_norm": 0.005513783544301987,
      "learning_rate": 5.73888e-06,
      "loss": 0.0004,
      "step": 66850
    },
    {
      "epoch": 2.13952,
      "grad_norm": 0.004301750101149082,
      "learning_rate": 5.736746666666667e-06,
      "loss": 0.0004,
      "step": 66860
    },
    {
      "epoch": 2.13984,
      "grad_norm": 0.003393283812329173,
      "learning_rate": 5.734613333333335e-06,
      "loss": 0.0003,
      "step": 66870
    },
    {
      "epoch": 2.14016,
      "grad_norm": 0.004839574918150902,
      "learning_rate": 5.73248e-06,
      "loss": 0.0003,
      "step": 66880
    },
    {
      "epoch": 2.14048,
      "grad_norm": 0.008718312717974186,
      "learning_rate": 5.730346666666667e-06,
      "loss": 0.0003,
      "step": 66890
    },
    {
      "epoch": 2.1408,
      "grad_norm": 0.006138879340142012,
      "learning_rate": 5.728213333333333e-06,
      "loss": 0.0004,
      "step": 66900
    },
    {
      "epoch": 2.14112,
      "grad_norm": 0.005392064340412617,
      "learning_rate": 5.72608e-06,
      "loss": 0.0003,
      "step": 66910
    },
    {
      "epoch": 2.1414400000000002,
      "grad_norm": 0.0031703284475952387,
      "learning_rate": 5.7239466666666675e-06,
      "loss": 0.001,
      "step": 66920
    },
    {
      "epoch": 2.14176,
      "grad_norm": 0.004181146156042814,
      "learning_rate": 5.721813333333334e-06,
      "loss": 0.0002,
      "step": 66930
    },
    {
      "epoch": 2.14208,
      "grad_norm": 0.003891382832080126,
      "learning_rate": 5.719680000000001e-06,
      "loss": 0.0003,
      "step": 66940
    },
    {
      "epoch": 2.1424,
      "grad_norm": 0.0033329222351312637,
      "learning_rate": 5.717546666666667e-06,
      "loss": 0.0003,
      "step": 66950
    },
    {
      "epoch": 2.14272,
      "grad_norm": 0.022203505039215088,
      "learning_rate": 5.715413333333334e-06,
      "loss": 0.041,
      "step": 66960
    },
    {
      "epoch": 2.14304,
      "grad_norm": 0.005262608639895916,
      "learning_rate": 5.71328e-06,
      "loss": 0.0003,
      "step": 66970
    },
    {
      "epoch": 2.14336,
      "grad_norm": 0.002695890609174967,
      "learning_rate": 5.711146666666667e-06,
      "loss": 0.0609,
      "step": 66980
    },
    {
      "epoch": 2.14368,
      "grad_norm": 0.0036070095375180244,
      "learning_rate": 5.7090133333333344e-06,
      "loss": 0.0003,
      "step": 66990
    },
    {
      "epoch": 2.144,
      "grad_norm": 4.769912242889404,
      "learning_rate": 5.70688e-06,
      "loss": 0.009,
      "step": 67000
    },
    {
      "epoch": 2.14432,
      "grad_norm": 0.006821428891271353,
      "learning_rate": 5.704746666666668e-06,
      "loss": 0.0003,
      "step": 67010
    },
    {
      "epoch": 2.14464,
      "grad_norm": 0.007799285929650068,
      "learning_rate": 5.702613333333333e-06,
      "loss": 0.0028,
      "step": 67020
    },
    {
      "epoch": 2.14496,
      "grad_norm": 0.003522912971675396,
      "learning_rate": 5.70048e-06,
      "loss": 0.0002,
      "step": 67030
    },
    {
      "epoch": 2.14528,
      "grad_norm": 0.007274087052792311,
      "learning_rate": 5.698346666666667e-06,
      "loss": 0.0031,
      "step": 67040
    },
    {
      "epoch": 2.1456,
      "grad_norm": 0.005738416220992804,
      "learning_rate": 5.6962133333333335e-06,
      "loss": 0.0002,
      "step": 67050
    },
    {
      "epoch": 2.14592,
      "grad_norm": 0.009688052348792553,
      "learning_rate": 5.6940800000000005e-06,
      "loss": 0.0002,
      "step": 67060
    },
    {
      "epoch": 2.14624,
      "grad_norm": 0.015842512249946594,
      "learning_rate": 5.691946666666667e-06,
      "loss": 0.0003,
      "step": 67070
    },
    {
      "epoch": 2.14656,
      "grad_norm": 0.007266057655215263,
      "learning_rate": 5.689813333333334e-06,
      "loss": 0.0126,
      "step": 67080
    },
    {
      "epoch": 2.14688,
      "grad_norm": 0.006438931450247765,
      "learning_rate": 5.687680000000001e-06,
      "loss": 0.0069,
      "step": 67090
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 0.0049491263926029205,
      "learning_rate": 5.685546666666667e-06,
      "loss": 0.0003,
      "step": 67100
    },
    {
      "epoch": 2.14752,
      "grad_norm": 0.0027636243030428886,
      "learning_rate": 5.683413333333334e-06,
      "loss": 0.0003,
      "step": 67110
    },
    {
      "epoch": 2.14784,
      "grad_norm": 0.0083246985450387,
      "learning_rate": 5.6812800000000004e-06,
      "loss": 0.0005,
      "step": 67120
    },
    {
      "epoch": 2.14816,
      "grad_norm": 0.0041795698925852776,
      "learning_rate": 5.6791466666666675e-06,
      "loss": 0.0002,
      "step": 67130
    },
    {
      "epoch": 2.14848,
      "grad_norm": 0.0020732481498271227,
      "learning_rate": 5.677013333333333e-06,
      "loss": 0.0191,
      "step": 67140
    },
    {
      "epoch": 2.1488,
      "grad_norm": 0.005156531929969788,
      "learning_rate": 5.674880000000001e-06,
      "loss": 0.0003,
      "step": 67150
    },
    {
      "epoch": 2.14912,
      "grad_norm": 0.0039449227042496204,
      "learning_rate": 5.672746666666668e-06,
      "loss": 0.0509,
      "step": 67160
    },
    {
      "epoch": 2.14944,
      "grad_norm": 0.0043465630151331425,
      "learning_rate": 5.670613333333333e-06,
      "loss": 0.0003,
      "step": 67170
    },
    {
      "epoch": 2.14976,
      "grad_norm": 0.007820154540240765,
      "learning_rate": 5.66848e-06,
      "loss": 0.0327,
      "step": 67180
    },
    {
      "epoch": 2.15008,
      "grad_norm": 0.007212040480226278,
      "learning_rate": 5.6663466666666665e-06,
      "loss": 0.0002,
      "step": 67190
    },
    {
      "epoch": 2.1504,
      "grad_norm": 0.0043639144860208035,
      "learning_rate": 5.664213333333334e-06,
      "loss": 0.0002,
      "step": 67200
    },
    {
      "epoch": 2.15072,
      "grad_norm": 0.0028566489927470684,
      "learning_rate": 5.662080000000001e-06,
      "loss": 0.0176,
      "step": 67210
    },
    {
      "epoch": 2.15104,
      "grad_norm": 0.005925940815359354,
      "learning_rate": 5.659946666666667e-06,
      "loss": 0.0002,
      "step": 67220
    },
    {
      "epoch": 2.15136,
      "grad_norm": 0.005033241119235754,
      "learning_rate": 5.657813333333334e-06,
      "loss": 0.0003,
      "step": 67230
    },
    {
      "epoch": 2.15168,
      "grad_norm": 0.01948426477611065,
      "learning_rate": 5.65568e-06,
      "loss": 0.0002,
      "step": 67240
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.002205285243690014,
      "learning_rate": 5.653546666666667e-06,
      "loss": 0.0239,
      "step": 67250
    },
    {
      "epoch": 2.15232,
      "grad_norm": 0.0043634409084916115,
      "learning_rate": 5.651413333333334e-06,
      "loss": 0.045,
      "step": 67260
    },
    {
      "epoch": 2.15264,
      "grad_norm": 0.006359381601214409,
      "learning_rate": 5.6492800000000006e-06,
      "loss": 0.0002,
      "step": 67270
    },
    {
      "epoch": 2.15296,
      "grad_norm": 0.006619871594011784,
      "learning_rate": 5.647146666666668e-06,
      "loss": 0.0003,
      "step": 67280
    },
    {
      "epoch": 2.15328,
      "grad_norm": 0.008979142643511295,
      "learning_rate": 5.645013333333334e-06,
      "loss": 0.0003,
      "step": 67290
    },
    {
      "epoch": 2.1536,
      "grad_norm": 0.009527063928544521,
      "learning_rate": 5.642880000000001e-06,
      "loss": 0.0451,
      "step": 67300
    },
    {
      "epoch": 2.15392,
      "grad_norm": 0.007101090159267187,
      "learning_rate": 5.640746666666666e-06,
      "loss": 0.0003,
      "step": 67310
    },
    {
      "epoch": 2.15424,
      "grad_norm": 0.002779727801680565,
      "learning_rate": 5.638613333333333e-06,
      "loss": 0.0002,
      "step": 67320
    },
    {
      "epoch": 2.15456,
      "grad_norm": 0.0040542734786868095,
      "learning_rate": 5.6364800000000005e-06,
      "loss": 0.0003,
      "step": 67330
    },
    {
      "epoch": 2.15488,
      "grad_norm": 0.0051636346615850925,
      "learning_rate": 5.634346666666667e-06,
      "loss": 0.0127,
      "step": 67340
    },
    {
      "epoch": 2.1552,
      "grad_norm": 0.0056446450762450695,
      "learning_rate": 5.632213333333334e-06,
      "loss": 0.0494,
      "step": 67350
    },
    {
      "epoch": 2.15552,
      "grad_norm": 0.005483803804963827,
      "learning_rate": 5.63008e-06,
      "loss": 0.0004,
      "step": 67360
    },
    {
      "epoch": 2.15584,
      "grad_norm": 0.0033640002366155386,
      "learning_rate": 5.627946666666667e-06,
      "loss": 0.0003,
      "step": 67370
    },
    {
      "epoch": 2.15616,
      "grad_norm": 0.00863604061305523,
      "learning_rate": 5.625813333333334e-06,
      "loss": 0.0003,
      "step": 67380
    },
    {
      "epoch": 2.15648,
      "grad_norm": 0.00827618595212698,
      "learning_rate": 5.62368e-06,
      "loss": 0.0012,
      "step": 67390
    },
    {
      "epoch": 2.1568,
      "grad_norm": 0.007656779605895281,
      "learning_rate": 5.621546666666667e-06,
      "loss": 0.0003,
      "step": 67400
    },
    {
      "epoch": 2.15712,
      "grad_norm": 0.005238571669906378,
      "learning_rate": 5.619413333333334e-06,
      "loss": 0.0026,
      "step": 67410
    },
    {
      "epoch": 2.15744,
      "grad_norm": 0.005897144321352243,
      "learning_rate": 5.617280000000001e-06,
      "loss": 0.0055,
      "step": 67420
    },
    {
      "epoch": 2.15776,
      "grad_norm": 0.0031219646334648132,
      "learning_rate": 5.615146666666668e-06,
      "loss": 0.0072,
      "step": 67430
    },
    {
      "epoch": 2.15808,
      "grad_norm": 0.00416907761245966,
      "learning_rate": 5.613013333333334e-06,
      "loss": 0.0003,
      "step": 67440
    },
    {
      "epoch": 2.1584,
      "grad_norm": 0.007022504229098558,
      "learning_rate": 5.610880000000001e-06,
      "loss": 0.0006,
      "step": 67450
    },
    {
      "epoch": 2.15872,
      "grad_norm": 6.970623016357422,
      "learning_rate": 5.6087466666666664e-06,
      "loss": 0.0432,
      "step": 67460
    },
    {
      "epoch": 2.15904,
      "grad_norm": 0.0155099555850029,
      "learning_rate": 5.6066133333333335e-06,
      "loss": 0.0004,
      "step": 67470
    },
    {
      "epoch": 2.15936,
      "grad_norm": 0.005531050264835358,
      "learning_rate": 5.60448e-06,
      "loss": 0.0006,
      "step": 67480
    },
    {
      "epoch": 2.15968,
      "grad_norm": 0.004854773636907339,
      "learning_rate": 5.602346666666667e-06,
      "loss": 0.0013,
      "step": 67490
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.003224588232114911,
      "learning_rate": 5.600213333333334e-06,
      "loss": 0.0003,
      "step": 67500
    },
    {
      "epoch": 2.16032,
      "grad_norm": 0.004261251073330641,
      "learning_rate": 5.59808e-06,
      "loss": 0.0022,
      "step": 67510
    },
    {
      "epoch": 2.16064,
      "grad_norm": 0.0029122554697096348,
      "learning_rate": 5.595946666666667e-06,
      "loss": 0.0003,
      "step": 67520
    },
    {
      "epoch": 2.16096,
      "grad_norm": 0.004625704139471054,
      "learning_rate": 5.593813333333333e-06,
      "loss": 0.0002,
      "step": 67530
    },
    {
      "epoch": 2.16128,
      "grad_norm": 0.01478271372616291,
      "learning_rate": 5.5916800000000005e-06,
      "loss": 0.0002,
      "step": 67540
    },
    {
      "epoch": 2.1616,
      "grad_norm": 0.018958911299705505,
      "learning_rate": 5.5895466666666675e-06,
      "loss": 0.0005,
      "step": 67550
    },
    {
      "epoch": 2.16192,
      "grad_norm": 0.0059104193933308125,
      "learning_rate": 5.587413333333334e-06,
      "loss": 0.0002,
      "step": 67560
    },
    {
      "epoch": 2.16224,
      "grad_norm": 0.00441400520503521,
      "learning_rate": 5.585280000000001e-06,
      "loss": 0.0005,
      "step": 67570
    },
    {
      "epoch": 2.16256,
      "grad_norm": 1.6721158027648926,
      "learning_rate": 5.583146666666667e-06,
      "loss": 0.0503,
      "step": 67580
    },
    {
      "epoch": 2.16288,
      "grad_norm": 0.014568249695003033,
      "learning_rate": 5.581013333333334e-06,
      "loss": 0.0229,
      "step": 67590
    },
    {
      "epoch": 2.1632,
      "grad_norm": 0.007282246369868517,
      "learning_rate": 5.578880000000001e-06,
      "loss": 0.0011,
      "step": 67600
    },
    {
      "epoch": 2.16352,
      "grad_norm": 0.01209005806595087,
      "learning_rate": 5.576746666666667e-06,
      "loss": 0.0406,
      "step": 67610
    },
    {
      "epoch": 2.16384,
      "grad_norm": 0.009908748790621758,
      "learning_rate": 5.5746133333333345e-06,
      "loss": 0.0003,
      "step": 67620
    },
    {
      "epoch": 2.16416,
      "grad_norm": 0.0075664701871573925,
      "learning_rate": 5.57248e-06,
      "loss": 0.0138,
      "step": 67630
    },
    {
      "epoch": 2.16448,
      "grad_norm": 0.0030677802860736847,
      "learning_rate": 5.570346666666667e-06,
      "loss": 0.0005,
      "step": 67640
    },
    {
      "epoch": 2.1648,
      "grad_norm": 0.010605559684336185,
      "learning_rate": 5.568213333333333e-06,
      "loss": 0.0084,
      "step": 67650
    },
    {
      "epoch": 2.16512,
      "grad_norm": 0.005227779038250446,
      "learning_rate": 5.56608e-06,
      "loss": 0.0002,
      "step": 67660
    },
    {
      "epoch": 2.16544,
      "grad_norm": 0.11201219260692596,
      "learning_rate": 5.563946666666667e-06,
      "loss": 0.0005,
      "step": 67670
    },
    {
      "epoch": 2.16576,
      "grad_norm": 0.03946959227323532,
      "learning_rate": 5.5618133333333335e-06,
      "loss": 0.0003,
      "step": 67680
    },
    {
      "epoch": 2.16608,
      "grad_norm": 0.026214895769953728,
      "learning_rate": 5.559680000000001e-06,
      "loss": 0.0023,
      "step": 67690
    },
    {
      "epoch": 2.1664,
      "grad_norm": 0.004179942421615124,
      "learning_rate": 5.557546666666667e-06,
      "loss": 0.0026,
      "step": 67700
    },
    {
      "epoch": 2.16672,
      "grad_norm": 0.007977114990353584,
      "learning_rate": 5.555413333333334e-06,
      "loss": 0.0551,
      "step": 67710
    },
    {
      "epoch": 2.16704,
      "grad_norm": 0.009462974965572357,
      "learning_rate": 5.553280000000001e-06,
      "loss": 0.0003,
      "step": 67720
    },
    {
      "epoch": 2.16736,
      "grad_norm": 0.007603360805660486,
      "learning_rate": 5.551146666666667e-06,
      "loss": 0.0339,
      "step": 67730
    },
    {
      "epoch": 2.16768,
      "grad_norm": 0.0038597227539867163,
      "learning_rate": 5.549013333333334e-06,
      "loss": 0.0003,
      "step": 67740
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.012799383141100407,
      "learning_rate": 5.54688e-06,
      "loss": 0.009,
      "step": 67750
    },
    {
      "epoch": 2.16832,
      "grad_norm": 0.005475121550261974,
      "learning_rate": 5.5447466666666676e-06,
      "loss": 0.0403,
      "step": 67760
    },
    {
      "epoch": 2.16864,
      "grad_norm": 0.008606545627117157,
      "learning_rate": 5.542613333333335e-06,
      "loss": 0.0003,
      "step": 67770
    },
    {
      "epoch": 2.16896,
      "grad_norm": 0.013338778167963028,
      "learning_rate": 5.54048e-06,
      "loss": 0.0006,
      "step": 67780
    },
    {
      "epoch": 2.16928,
      "grad_norm": 0.009595156647264957,
      "learning_rate": 5.538346666666667e-06,
      "loss": 0.0117,
      "step": 67790
    },
    {
      "epoch": 2.1696,
      "grad_norm": 0.004751531407237053,
      "learning_rate": 5.536213333333333e-06,
      "loss": 0.0003,
      "step": 67800
    },
    {
      "epoch": 2.16992,
      "grad_norm": 0.003001004923135042,
      "learning_rate": 5.53408e-06,
      "loss": 0.0006,
      "step": 67810
    },
    {
      "epoch": 2.17024,
      "grad_norm": 0.010760565288364887,
      "learning_rate": 5.531946666666667e-06,
      "loss": 0.0131,
      "step": 67820
    },
    {
      "epoch": 2.17056,
      "grad_norm": 0.005727648269385099,
      "learning_rate": 5.529813333333334e-06,
      "loss": 0.0002,
      "step": 67830
    },
    {
      "epoch": 2.17088,
      "grad_norm": 0.14974400401115417,
      "learning_rate": 5.527680000000001e-06,
      "loss": 0.0006,
      "step": 67840
    },
    {
      "epoch": 2.1712,
      "grad_norm": 0.06982410699129105,
      "learning_rate": 5.525546666666667e-06,
      "loss": 0.0035,
      "step": 67850
    },
    {
      "epoch": 2.17152,
      "grad_norm": 0.012091828510165215,
      "learning_rate": 5.523413333333334e-06,
      "loss": 0.0004,
      "step": 67860
    },
    {
      "epoch": 2.17184,
      "grad_norm": 0.013563413172960281,
      "learning_rate": 5.52128e-06,
      "loss": 0.0015,
      "step": 67870
    },
    {
      "epoch": 2.17216,
      "grad_norm": 0.0030656547751277685,
      "learning_rate": 5.519146666666667e-06,
      "loss": 0.0006,
      "step": 67880
    },
    {
      "epoch": 2.17248,
      "grad_norm": 0.006149702705442905,
      "learning_rate": 5.517013333333334e-06,
      "loss": 0.0003,
      "step": 67890
    },
    {
      "epoch": 2.1728,
      "grad_norm": 0.003807397559285164,
      "learning_rate": 5.514880000000001e-06,
      "loss": 0.0011,
      "step": 67900
    },
    {
      "epoch": 2.17312,
      "grad_norm": 0.004952369257807732,
      "learning_rate": 5.512746666666668e-06,
      "loss": 0.0002,
      "step": 67910
    },
    {
      "epoch": 2.17344,
      "grad_norm": 0.008882801979780197,
      "learning_rate": 5.510613333333333e-06,
      "loss": 0.0781,
      "step": 67920
    },
    {
      "epoch": 2.17376,
      "grad_norm": 0.032528068870306015,
      "learning_rate": 5.50848e-06,
      "loss": 0.0003,
      "step": 67930
    },
    {
      "epoch": 2.17408,
      "grad_norm": 0.004593385383486748,
      "learning_rate": 5.506346666666667e-06,
      "loss": 0.0002,
      "step": 67940
    },
    {
      "epoch": 2.1744,
      "grad_norm": 0.007019517011940479,
      "learning_rate": 5.5042133333333335e-06,
      "loss": 0.0542,
      "step": 67950
    },
    {
      "epoch": 2.1747199999999998,
      "grad_norm": 0.01175371278077364,
      "learning_rate": 5.5020800000000005e-06,
      "loss": 0.0002,
      "step": 67960
    },
    {
      "epoch": 2.17504,
      "grad_norm": 0.003936593886464834,
      "learning_rate": 5.499946666666667e-06,
      "loss": 0.0003,
      "step": 67970
    },
    {
      "epoch": 2.17536,
      "grad_norm": 0.006271454505622387,
      "learning_rate": 5.497813333333334e-06,
      "loss": 0.0019,
      "step": 67980
    },
    {
      "epoch": 2.17568,
      "grad_norm": 0.002374659525230527,
      "learning_rate": 5.49568e-06,
      "loss": 0.0334,
      "step": 67990
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.00440647779032588,
      "learning_rate": 5.493546666666667e-06,
      "loss": 0.0016,
      "step": 68000
    },
    {
      "epoch": 2.17632,
      "grad_norm": 0.12633559107780457,
      "learning_rate": 5.491413333333334e-06,
      "loss": 0.0003,
      "step": 68010
    },
    {
      "epoch": 2.17664,
      "grad_norm": 0.0059020062908530235,
      "learning_rate": 5.48928e-06,
      "loss": 0.0007,
      "step": 68020
    },
    {
      "epoch": 2.1769600000000002,
      "grad_norm": 0.004958231467753649,
      "learning_rate": 5.4871466666666675e-06,
      "loss": 0.0021,
      "step": 68030
    },
    {
      "epoch": 2.17728,
      "grad_norm": 0.006125146523118019,
      "learning_rate": 5.485013333333334e-06,
      "loss": 0.0009,
      "step": 68040
    },
    {
      "epoch": 2.1776,
      "grad_norm": 0.004313791636377573,
      "learning_rate": 5.482880000000001e-06,
      "loss": 0.0005,
      "step": 68050
    },
    {
      "epoch": 2.17792,
      "grad_norm": 0.01390205230563879,
      "learning_rate": 5.480746666666668e-06,
      "loss": 0.0013,
      "step": 68060
    },
    {
      "epoch": 2.17824,
      "grad_norm": 0.003732497338205576,
      "learning_rate": 5.478613333333333e-06,
      "loss": 0.0222,
      "step": 68070
    },
    {
      "epoch": 2.17856,
      "grad_norm": 0.01001717895269394,
      "learning_rate": 5.47648e-06,
      "loss": 0.0378,
      "step": 68080
    },
    {
      "epoch": 2.17888,
      "grad_norm": 0.0102741913869977,
      "learning_rate": 5.4743466666666665e-06,
      "loss": 0.0005,
      "step": 68090
    },
    {
      "epoch": 2.1792,
      "grad_norm": 0.00437568174675107,
      "learning_rate": 5.472213333333334e-06,
      "loss": 0.0003,
      "step": 68100
    },
    {
      "epoch": 2.17952,
      "grad_norm": 0.045651309192180634,
      "learning_rate": 5.470080000000001e-06,
      "loss": 0.0005,
      "step": 68110
    },
    {
      "epoch": 2.17984,
      "grad_norm": 0.016927698627114296,
      "learning_rate": 5.467946666666667e-06,
      "loss": 0.0006,
      "step": 68120
    },
    {
      "epoch": 2.18016,
      "grad_norm": 3.477324962615967,
      "learning_rate": 5.465813333333334e-06,
      "loss": 0.0045,
      "step": 68130
    },
    {
      "epoch": 2.18048,
      "grad_norm": 0.005521996878087521,
      "learning_rate": 5.46368e-06,
      "loss": 0.0035,
      "step": 68140
    },
    {
      "epoch": 2.1808,
      "grad_norm": 0.005892146844416857,
      "learning_rate": 5.461546666666667e-06,
      "loss": 0.0004,
      "step": 68150
    },
    {
      "epoch": 2.18112,
      "grad_norm": 0.006462425924837589,
      "learning_rate": 5.4594133333333335e-06,
      "loss": 0.0003,
      "step": 68160
    },
    {
      "epoch": 2.18144,
      "grad_norm": 0.0027635248843580484,
      "learning_rate": 5.4572800000000005e-06,
      "loss": 0.0433,
      "step": 68170
    },
    {
      "epoch": 2.18176,
      "grad_norm": 1.4599107503890991,
      "learning_rate": 5.455146666666668e-06,
      "loss": 0.0038,
      "step": 68180
    },
    {
      "epoch": 2.18208,
      "grad_norm": 0.028371799737215042,
      "learning_rate": 5.453013333333334e-06,
      "loss": 0.033,
      "step": 68190
    },
    {
      "epoch": 2.1824,
      "grad_norm": 0.007265565916895866,
      "learning_rate": 5.450880000000001e-06,
      "loss": 0.0006,
      "step": 68200
    },
    {
      "epoch": 2.1827199999999998,
      "grad_norm": 0.006530158221721649,
      "learning_rate": 5.448746666666666e-06,
      "loss": 0.0003,
      "step": 68210
    },
    {
      "epoch": 2.18304,
      "grad_norm": 1.0091898441314697,
      "learning_rate": 5.446613333333333e-06,
      "loss": 0.0011,
      "step": 68220
    },
    {
      "epoch": 2.18336,
      "grad_norm": 0.004484871402382851,
      "learning_rate": 5.444480000000001e-06,
      "loss": 0.0006,
      "step": 68230
    },
    {
      "epoch": 2.18368,
      "grad_norm": 0.00372361671179533,
      "learning_rate": 5.442346666666667e-06,
      "loss": 0.0005,
      "step": 68240
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.003068589372560382,
      "learning_rate": 5.440213333333334e-06,
      "loss": 0.0004,
      "step": 68250
    },
    {
      "epoch": 2.18432,
      "grad_norm": 0.007089813705533743,
      "learning_rate": 5.43808e-06,
      "loss": 0.0025,
      "step": 68260
    },
    {
      "epoch": 2.18464,
      "grad_norm": 0.03837357833981514,
      "learning_rate": 5.435946666666667e-06,
      "loss": 0.0004,
      "step": 68270
    },
    {
      "epoch": 2.1849600000000002,
      "grad_norm": 0.009984361007809639,
      "learning_rate": 5.433813333333334e-06,
      "loss": 0.0003,
      "step": 68280
    },
    {
      "epoch": 2.18528,
      "grad_norm": 0.016044173389673233,
      "learning_rate": 5.43168e-06,
      "loss": 0.0496,
      "step": 68290
    },
    {
      "epoch": 2.1856,
      "grad_norm": 0.0094038937240839,
      "learning_rate": 5.429546666666667e-06,
      "loss": 0.0003,
      "step": 68300
    },
    {
      "epoch": 2.18592,
      "grad_norm": 0.01580185629427433,
      "learning_rate": 5.427413333333334e-06,
      "loss": 0.0009,
      "step": 68310
    },
    {
      "epoch": 2.18624,
      "grad_norm": 0.0054817721247673035,
      "learning_rate": 5.425280000000001e-06,
      "loss": 0.0003,
      "step": 68320
    },
    {
      "epoch": 2.18656,
      "grad_norm": 0.013384053483605385,
      "learning_rate": 5.423146666666667e-06,
      "loss": 0.0009,
      "step": 68330
    },
    {
      "epoch": 2.18688,
      "grad_norm": 0.005043488927185535,
      "learning_rate": 5.421013333333334e-06,
      "loss": 0.0004,
      "step": 68340
    },
    {
      "epoch": 2.1872,
      "grad_norm": 0.0018067600904032588,
      "learning_rate": 5.418880000000001e-06,
      "loss": 0.0002,
      "step": 68350
    },
    {
      "epoch": 2.18752,
      "grad_norm": 0.013882007449865341,
      "learning_rate": 5.4167466666666664e-06,
      "loss": 0.0003,
      "step": 68360
    },
    {
      "epoch": 2.18784,
      "grad_norm": 0.01428545918315649,
      "learning_rate": 5.414613333333334e-06,
      "loss": 0.0442,
      "step": 68370
    },
    {
      "epoch": 2.18816,
      "grad_norm": 0.010049409233033657,
      "learning_rate": 5.41248e-06,
      "loss": 0.0005,
      "step": 68380
    },
    {
      "epoch": 2.18848,
      "grad_norm": 0.003218942554667592,
      "learning_rate": 5.410346666666667e-06,
      "loss": 0.0004,
      "step": 68390
    },
    {
      "epoch": 2.1888,
      "grad_norm": 0.01979639194905758,
      "learning_rate": 5.408213333333334e-06,
      "loss": 0.0003,
      "step": 68400
    },
    {
      "epoch": 2.18912,
      "grad_norm": 0.00499576423317194,
      "learning_rate": 5.40608e-06,
      "loss": 0.0003,
      "step": 68410
    },
    {
      "epoch": 2.18944,
      "grad_norm": 0.010632861405611038,
      "learning_rate": 5.403946666666667e-06,
      "loss": 0.0003,
      "step": 68420
    },
    {
      "epoch": 2.18976,
      "grad_norm": 0.156809002161026,
      "learning_rate": 5.401813333333333e-06,
      "loss": 0.0004,
      "step": 68430
    },
    {
      "epoch": 2.19008,
      "grad_norm": 0.0232711061835289,
      "learning_rate": 5.3996800000000005e-06,
      "loss": 0.0005,
      "step": 68440
    },
    {
      "epoch": 2.1904,
      "grad_norm": 0.07684354484081268,
      "learning_rate": 5.3975466666666675e-06,
      "loss": 0.0005,
      "step": 68450
    },
    {
      "epoch": 2.19072,
      "grad_norm": 0.008695913478732109,
      "learning_rate": 5.395413333333334e-06,
      "loss": 0.0003,
      "step": 68460
    },
    {
      "epoch": 2.19104,
      "grad_norm": 0.008207916282117367,
      "learning_rate": 5.393280000000001e-06,
      "loss": 0.06,
      "step": 68470
    },
    {
      "epoch": 2.19136,
      "grad_norm": 0.038108911365270615,
      "learning_rate": 5.391146666666667e-06,
      "loss": 0.0003,
      "step": 68480
    },
    {
      "epoch": 2.19168,
      "grad_norm": 0.004635742399841547,
      "learning_rate": 5.389013333333334e-06,
      "loss": 0.0006,
      "step": 68490
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.0059118811041116714,
      "learning_rate": 5.3868799999999995e-06,
      "loss": 0.002,
      "step": 68500
    },
    {
      "epoch": 2.19232,
      "grad_norm": 0.006105240434408188,
      "learning_rate": 5.384746666666667e-06,
      "loss": 0.0003,
      "step": 68510
    },
    {
      "epoch": 2.19264,
      "grad_norm": 0.004255709238350391,
      "learning_rate": 5.3826133333333345e-06,
      "loss": 0.0002,
      "step": 68520
    },
    {
      "epoch": 2.19296,
      "grad_norm": 0.008146173320710659,
      "learning_rate": 5.38048e-06,
      "loss": 0.0003,
      "step": 68530
    },
    {
      "epoch": 2.19328,
      "grad_norm": 0.005032611079514027,
      "learning_rate": 5.378346666666667e-06,
      "loss": 0.0003,
      "step": 68540
    },
    {
      "epoch": 2.1936,
      "grad_norm": 0.010289676487445831,
      "learning_rate": 5.376213333333333e-06,
      "loss": 0.0003,
      "step": 68550
    },
    {
      "epoch": 2.19392,
      "grad_norm": 0.00591595284640789,
      "learning_rate": 5.37408e-06,
      "loss": 0.0002,
      "step": 68560
    },
    {
      "epoch": 2.19424,
      "grad_norm": 0.002125528873875737,
      "learning_rate": 5.371946666666667e-06,
      "loss": 0.0048,
      "step": 68570
    },
    {
      "epoch": 2.19456,
      "grad_norm": 0.007313466165214777,
      "learning_rate": 5.3698133333333335e-06,
      "loss": 0.0276,
      "step": 68580
    },
    {
      "epoch": 2.19488,
      "grad_norm": 0.005872504319995642,
      "learning_rate": 5.367680000000001e-06,
      "loss": 0.0002,
      "step": 68590
    },
    {
      "epoch": 2.1952,
      "grad_norm": 0.004715680610388517,
      "learning_rate": 5.365546666666667e-06,
      "loss": 0.0002,
      "step": 68600
    },
    {
      "epoch": 2.19552,
      "grad_norm": 0.020345080643892288,
      "learning_rate": 5.363413333333334e-06,
      "loss": 0.0003,
      "step": 68610
    },
    {
      "epoch": 2.19584,
      "grad_norm": 0.0036895149387419224,
      "learning_rate": 5.361280000000001e-06,
      "loss": 0.0002,
      "step": 68620
    },
    {
      "epoch": 2.19616,
      "grad_norm": 0.007653652690351009,
      "learning_rate": 5.359146666666667e-06,
      "loss": 0.0005,
      "step": 68630
    },
    {
      "epoch": 2.19648,
      "grad_norm": 0.0052077374421060085,
      "learning_rate": 5.357013333333334e-06,
      "loss": 0.0004,
      "step": 68640
    },
    {
      "epoch": 2.1968,
      "grad_norm": 0.0035007174592465162,
      "learning_rate": 5.3548800000000005e-06,
      "loss": 0.0003,
      "step": 68650
    },
    {
      "epoch": 2.19712,
      "grad_norm": 0.007414211519062519,
      "learning_rate": 5.3527466666666675e-06,
      "loss": 0.0002,
      "step": 68660
    },
    {
      "epoch": 2.19744,
      "grad_norm": 0.0031442490871995687,
      "learning_rate": 5.350613333333335e-06,
      "loss": 0.0003,
      "step": 68670
    },
    {
      "epoch": 2.19776,
      "grad_norm": 0.004134992603212595,
      "learning_rate": 5.34848e-06,
      "loss": 0.0003,
      "step": 68680
    },
    {
      "epoch": 2.19808,
      "grad_norm": 0.0053929551504552364,
      "learning_rate": 5.346346666666667e-06,
      "loss": 0.0002,
      "step": 68690
    },
    {
      "epoch": 2.1984,
      "grad_norm": 0.00469405809417367,
      "learning_rate": 5.344213333333333e-06,
      "loss": 0.0002,
      "step": 68700
    },
    {
      "epoch": 2.19872,
      "grad_norm": 0.00456111179664731,
      "learning_rate": 5.34208e-06,
      "loss": 0.0004,
      "step": 68710
    },
    {
      "epoch": 2.19904,
      "grad_norm": 0.008346951566636562,
      "learning_rate": 5.339946666666667e-06,
      "loss": 0.0002,
      "step": 68720
    },
    {
      "epoch": 2.19936,
      "grad_norm": 0.006796937435865402,
      "learning_rate": 5.337813333333334e-06,
      "loss": 0.0002,
      "step": 68730
    },
    {
      "epoch": 2.19968,
      "grad_norm": 0.0029075713828206062,
      "learning_rate": 5.335680000000001e-06,
      "loss": 0.0004,
      "step": 68740
    },
    {
      "epoch": 2.2,
      "grad_norm": 3.305325746536255,
      "learning_rate": 5.333546666666667e-06,
      "loss": 0.0195,
      "step": 68750
    },
    {
      "epoch": 2.20032,
      "grad_norm": 0.0066034528426826,
      "learning_rate": 5.331413333333334e-06,
      "loss": 0.0002,
      "step": 68760
    },
    {
      "epoch": 2.20064,
      "grad_norm": 0.005031368229538202,
      "learning_rate": 5.32928e-06,
      "loss": 0.0005,
      "step": 68770
    },
    {
      "epoch": 2.20096,
      "grad_norm": 0.004796119406819344,
      "learning_rate": 5.327146666666667e-06,
      "loss": 0.0002,
      "step": 68780
    },
    {
      "epoch": 2.20128,
      "grad_norm": 0.005812385585159063,
      "learning_rate": 5.325013333333334e-06,
      "loss": 0.0002,
      "step": 68790
    },
    {
      "epoch": 2.2016,
      "grad_norm": 0.011720266193151474,
      "learning_rate": 5.322880000000001e-06,
      "loss": 0.0002,
      "step": 68800
    },
    {
      "epoch": 2.20192,
      "grad_norm": 0.006958652753382921,
      "learning_rate": 5.320746666666668e-06,
      "loss": 0.0005,
      "step": 68810
    },
    {
      "epoch": 2.20224,
      "grad_norm": 0.012155394069850445,
      "learning_rate": 5.318613333333333e-06,
      "loss": 0.0003,
      "step": 68820
    },
    {
      "epoch": 2.20256,
      "grad_norm": 0.0013982473174110055,
      "learning_rate": 5.31648e-06,
      "loss": 0.0002,
      "step": 68830
    },
    {
      "epoch": 2.20288,
      "grad_norm": 0.004862927831709385,
      "learning_rate": 5.314346666666668e-06,
      "loss": 0.0002,
      "step": 68840
    },
    {
      "epoch": 2.2032,
      "grad_norm": 0.004462988581508398,
      "learning_rate": 5.3122133333333334e-06,
      "loss": 0.0003,
      "step": 68850
    },
    {
      "epoch": 2.20352,
      "grad_norm": 0.006137993186712265,
      "learning_rate": 5.3100800000000005e-06,
      "loss": 0.0003,
      "step": 68860
    },
    {
      "epoch": 2.20384,
      "grad_norm": 0.0039017668459564447,
      "learning_rate": 5.307946666666667e-06,
      "loss": 0.0802,
      "step": 68870
    },
    {
      "epoch": 2.20416,
      "grad_norm": 0.0022265128791332245,
      "learning_rate": 5.305813333333334e-06,
      "loss": 0.0002,
      "step": 68880
    },
    {
      "epoch": 2.20448,
      "grad_norm": 0.004814520478248596,
      "learning_rate": 5.30368e-06,
      "loss": 0.0443,
      "step": 68890
    },
    {
      "epoch": 2.2048,
      "grad_norm": 0.00580290611833334,
      "learning_rate": 5.301546666666667e-06,
      "loss": 0.0004,
      "step": 68900
    },
    {
      "epoch": 2.20512,
      "grad_norm": 0.00598991010338068,
      "learning_rate": 5.299413333333334e-06,
      "loss": 0.0475,
      "step": 68910
    },
    {
      "epoch": 2.20544,
      "grad_norm": 0.006233226042240858,
      "learning_rate": 5.29728e-06,
      "loss": 0.0521,
      "step": 68920
    },
    {
      "epoch": 2.20576,
      "grad_norm": 0.005660554859787226,
      "learning_rate": 5.2951466666666675e-06,
      "loss": 0.0003,
      "step": 68930
    },
    {
      "epoch": 2.20608,
      "grad_norm": 0.004913238808512688,
      "learning_rate": 5.293013333333334e-06,
      "loss": 0.0003,
      "step": 68940
    },
    {
      "epoch": 2.2064,
      "grad_norm": 0.00960199162364006,
      "learning_rate": 5.290880000000001e-06,
      "loss": 0.0003,
      "step": 68950
    },
    {
      "epoch": 2.20672,
      "grad_norm": 0.011688555590808392,
      "learning_rate": 5.288746666666668e-06,
      "loss": 0.0003,
      "step": 68960
    },
    {
      "epoch": 2.20704,
      "grad_norm": 0.010105674155056477,
      "learning_rate": 5.286613333333333e-06,
      "loss": 0.0002,
      "step": 68970
    },
    {
      "epoch": 2.20736,
      "grad_norm": 0.01671936735510826,
      "learning_rate": 5.284480000000001e-06,
      "loss": 0.0008,
      "step": 68980
    },
    {
      "epoch": 2.20768,
      "grad_norm": 0.00622874079272151,
      "learning_rate": 5.2823466666666665e-06,
      "loss": 0.0457,
      "step": 68990
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.011319560930132866,
      "learning_rate": 5.2802133333333336e-06,
      "loss": 0.0005,
      "step": 69000
    },
    {
      "epoch": 2.20832,
      "grad_norm": 0.003154544625431299,
      "learning_rate": 5.278080000000001e-06,
      "loss": 0.0002,
      "step": 69010
    },
    {
      "epoch": 2.20864,
      "grad_norm": 0.005942113697528839,
      "learning_rate": 5.275946666666667e-06,
      "loss": 0.0003,
      "step": 69020
    },
    {
      "epoch": 2.20896,
      "grad_norm": 0.002256169216707349,
      "learning_rate": 5.273813333333334e-06,
      "loss": 0.0002,
      "step": 69030
    },
    {
      "epoch": 2.20928,
      "grad_norm": 0.004597589839249849,
      "learning_rate": 5.27168e-06,
      "loss": 0.0008,
      "step": 69040
    },
    {
      "epoch": 2.2096,
      "grad_norm": 0.0037077646702528,
      "learning_rate": 5.269546666666667e-06,
      "loss": 0.0006,
      "step": 69050
    },
    {
      "epoch": 2.20992,
      "grad_norm": 0.0021876150276511908,
      "learning_rate": 5.2674133333333334e-06,
      "loss": 0.0005,
      "step": 69060
    },
    {
      "epoch": 2.21024,
      "grad_norm": 0.007733521517366171,
      "learning_rate": 5.2652800000000005e-06,
      "loss": 0.0003,
      "step": 69070
    },
    {
      "epoch": 2.21056,
      "grad_norm": 0.0046966299414634705,
      "learning_rate": 5.263146666666668e-06,
      "loss": 0.0003,
      "step": 69080
    },
    {
      "epoch": 2.21088,
      "grad_norm": 0.04038020223379135,
      "learning_rate": 5.261013333333334e-06,
      "loss": 0.0003,
      "step": 69090
    },
    {
      "epoch": 2.2112,
      "grad_norm": 0.010770756751298904,
      "learning_rate": 5.258880000000001e-06,
      "loss": 0.0003,
      "step": 69100
    },
    {
      "epoch": 2.21152,
      "grad_norm": 0.008917237631976604,
      "learning_rate": 5.256746666666666e-06,
      "loss": 0.0003,
      "step": 69110
    },
    {
      "epoch": 2.21184,
      "grad_norm": 0.003884583478793502,
      "learning_rate": 5.254613333333334e-06,
      "loss": 0.0628,
      "step": 69120
    },
    {
      "epoch": 2.21216,
      "grad_norm": 0.004349424038082361,
      "learning_rate": 5.252480000000001e-06,
      "loss": 0.0002,
      "step": 69130
    },
    {
      "epoch": 2.2124800000000002,
      "grad_norm": 0.003218892263248563,
      "learning_rate": 5.250346666666667e-06,
      "loss": 0.0002,
      "step": 69140
    },
    {
      "epoch": 2.2128,
      "grad_norm": 7.210740566253662,
      "learning_rate": 5.248213333333334e-06,
      "loss": 0.0352,
      "step": 69150
    },
    {
      "epoch": 2.21312,
      "grad_norm": 0.006076459307223558,
      "learning_rate": 5.24608e-06,
      "loss": 0.0013,
      "step": 69160
    },
    {
      "epoch": 2.21344,
      "grad_norm": 0.0037077157758176327,
      "learning_rate": 5.243946666666667e-06,
      "loss": 0.0003,
      "step": 69170
    },
    {
      "epoch": 2.21376,
      "grad_norm": 0.00391323771327734,
      "learning_rate": 5.241813333333334e-06,
      "loss": 0.0003,
      "step": 69180
    },
    {
      "epoch": 2.21408,
      "grad_norm": 0.006307666655629873,
      "learning_rate": 5.23968e-06,
      "loss": 0.0002,
      "step": 69190
    },
    {
      "epoch": 2.2144,
      "grad_norm": 0.0037428683135658503,
      "learning_rate": 5.237546666666667e-06,
      "loss": 0.029,
      "step": 69200
    },
    {
      "epoch": 2.21472,
      "grad_norm": 0.0042419168166816235,
      "learning_rate": 5.235413333333334e-06,
      "loss": 0.0083,
      "step": 69210
    },
    {
      "epoch": 2.21504,
      "grad_norm": 0.015257068909704685,
      "learning_rate": 5.233280000000001e-06,
      "loss": 0.0003,
      "step": 69220
    },
    {
      "epoch": 2.21536,
      "grad_norm": 0.0101780341938138,
      "learning_rate": 5.231146666666667e-06,
      "loss": 0.0002,
      "step": 69230
    },
    {
      "epoch": 2.21568,
      "grad_norm": 0.005354031454771757,
      "learning_rate": 5.229013333333334e-06,
      "loss": 0.0002,
      "step": 69240
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.020744917914271355,
      "learning_rate": 5.226880000000001e-06,
      "loss": 0.004,
      "step": 69250
    },
    {
      "epoch": 2.21632,
      "grad_norm": 0.004145442042499781,
      "learning_rate": 5.224746666666667e-06,
      "loss": 0.0318,
      "step": 69260
    },
    {
      "epoch": 2.21664,
      "grad_norm": 0.007523816078901291,
      "learning_rate": 5.222613333333334e-06,
      "loss": 0.0003,
      "step": 69270
    },
    {
      "epoch": 2.21696,
      "grad_norm": 0.004441868048161268,
      "learning_rate": 5.22048e-06,
      "loss": 0.0061,
      "step": 69280
    },
    {
      "epoch": 2.21728,
      "grad_norm": 0.0066008660942316055,
      "learning_rate": 5.218346666666667e-06,
      "loss": 0.0233,
      "step": 69290
    },
    {
      "epoch": 2.2176,
      "grad_norm": 0.0060379416681826115,
      "learning_rate": 5.216213333333334e-06,
      "loss": 0.0002,
      "step": 69300
    },
    {
      "epoch": 2.21792,
      "grad_norm": 0.08911662548780441,
      "learning_rate": 5.21408e-06,
      "loss": 0.0005,
      "step": 69310
    },
    {
      "epoch": 2.2182399999999998,
      "grad_norm": 0.021390466019511223,
      "learning_rate": 5.211946666666667e-06,
      "loss": 0.0002,
      "step": 69320
    },
    {
      "epoch": 2.21856,
      "grad_norm": 0.002497230190783739,
      "learning_rate": 5.209813333333333e-06,
      "loss": 0.0004,
      "step": 69330
    },
    {
      "epoch": 2.21888,
      "grad_norm": 0.005655390676110983,
      "learning_rate": 5.2076800000000004e-06,
      "loss": 0.0002,
      "step": 69340
    },
    {
      "epoch": 2.2192,
      "grad_norm": 0.0036305857356637716,
      "learning_rate": 5.2055466666666675e-06,
      "loss": 0.0003,
      "step": 69350
    },
    {
      "epoch": 2.21952,
      "grad_norm": 0.008494886569678783,
      "learning_rate": 5.203413333333334e-06,
      "loss": 0.0007,
      "step": 69360
    },
    {
      "epoch": 2.21984,
      "grad_norm": 0.006265970412641764,
      "learning_rate": 5.201280000000001e-06,
      "loss": 0.0109,
      "step": 69370
    },
    {
      "epoch": 2.22016,
      "grad_norm": 0.012313595041632652,
      "learning_rate": 5.199146666666667e-06,
      "loss": 0.0002,
      "step": 69380
    },
    {
      "epoch": 2.2204800000000002,
      "grad_norm": 0.0111306831240654,
      "learning_rate": 5.197013333333334e-06,
      "loss": 0.0003,
      "step": 69390
    },
    {
      "epoch": 2.2208,
      "grad_norm": 0.006023023277521133,
      "learning_rate": 5.19488e-06,
      "loss": 0.057,
      "step": 69400
    },
    {
      "epoch": 2.22112,
      "grad_norm": 0.010478278622031212,
      "learning_rate": 5.192746666666667e-06,
      "loss": 0.0079,
      "step": 69410
    },
    {
      "epoch": 2.22144,
      "grad_norm": 0.049890559166669846,
      "learning_rate": 5.1906133333333345e-06,
      "loss": 0.0004,
      "step": 69420
    },
    {
      "epoch": 2.22176,
      "grad_norm": 0.0026552744675427675,
      "learning_rate": 5.18848e-06,
      "loss": 0.0024,
      "step": 69430
    },
    {
      "epoch": 2.22208,
      "grad_norm": 0.0060441442765295506,
      "learning_rate": 5.186346666666667e-06,
      "loss": 0.0003,
      "step": 69440
    },
    {
      "epoch": 2.2224,
      "grad_norm": 0.0025677685625851154,
      "learning_rate": 5.184213333333333e-06,
      "loss": 0.001,
      "step": 69450
    },
    {
      "epoch": 2.22272,
      "grad_norm": 0.00911932997405529,
      "learning_rate": 5.18208e-06,
      "loss": 0.0003,
      "step": 69460
    },
    {
      "epoch": 2.22304,
      "grad_norm": 0.0036600548774003983,
      "learning_rate": 5.179946666666667e-06,
      "loss": 0.0002,
      "step": 69470
    },
    {
      "epoch": 2.22336,
      "grad_norm": 0.0041501144878566265,
      "learning_rate": 5.1778133333333335e-06,
      "loss": 0.0003,
      "step": 69480
    },
    {
      "epoch": 2.22368,
      "grad_norm": 0.03745976835489273,
      "learning_rate": 5.1756800000000006e-06,
      "loss": 0.0003,
      "step": 69490
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.00428300304338336,
      "learning_rate": 5.173546666666667e-06,
      "loss": 0.0006,
      "step": 69500
    },
    {
      "epoch": 2.22432,
      "grad_norm": 0.004179348703473806,
      "learning_rate": 5.171413333333334e-06,
      "loss": 0.0003,
      "step": 69510
    },
    {
      "epoch": 2.22464,
      "grad_norm": 0.012835554778575897,
      "learning_rate": 5.169280000000001e-06,
      "loss": 0.0008,
      "step": 69520
    },
    {
      "epoch": 2.22496,
      "grad_norm": 0.00538114458322525,
      "learning_rate": 5.167146666666667e-06,
      "loss": 0.0002,
      "step": 69530
    },
    {
      "epoch": 2.22528,
      "grad_norm": 0.10953966528177261,
      "learning_rate": 5.165013333333334e-06,
      "loss": 0.0422,
      "step": 69540
    },
    {
      "epoch": 2.2256,
      "grad_norm": 0.024523435160517693,
      "learning_rate": 5.1628800000000004e-06,
      "loss": 0.0002,
      "step": 69550
    },
    {
      "epoch": 2.22592,
      "grad_norm": 0.002083472441881895,
      "learning_rate": 5.1607466666666675e-06,
      "loss": 0.0002,
      "step": 69560
    },
    {
      "epoch": 2.2262399999999998,
      "grad_norm": 0.0030240104533731937,
      "learning_rate": 5.158613333333333e-06,
      "loss": 0.0002,
      "step": 69570
    },
    {
      "epoch": 2.22656,
      "grad_norm": 0.010519590228796005,
      "learning_rate": 5.15648e-06,
      "loss": 0.0002,
      "step": 69580
    },
    {
      "epoch": 2.22688,
      "grad_norm": 0.016334494575858116,
      "learning_rate": 5.154346666666668e-06,
      "loss": 0.0488,
      "step": 69590
    },
    {
      "epoch": 2.2272,
      "grad_norm": 0.006067998707294464,
      "learning_rate": 5.152213333333333e-06,
      "loss": 0.0002,
      "step": 69600
    },
    {
      "epoch": 2.22752,
      "grad_norm": 0.006587446667253971,
      "learning_rate": 5.15008e-06,
      "loss": 0.0002,
      "step": 69610
    },
    {
      "epoch": 2.22784,
      "grad_norm": 0.0047128950245678425,
      "learning_rate": 5.1479466666666666e-06,
      "loss": 0.0093,
      "step": 69620
    },
    {
      "epoch": 2.22816,
      "grad_norm": 0.003584589809179306,
      "learning_rate": 5.145813333333334e-06,
      "loss": 0.0016,
      "step": 69630
    },
    {
      "epoch": 2.22848,
      "grad_norm": 0.005353058688342571,
      "learning_rate": 5.143680000000001e-06,
      "loss": 0.0003,
      "step": 69640
    },
    {
      "epoch": 2.2288,
      "grad_norm": 0.004912142176181078,
      "learning_rate": 5.141546666666667e-06,
      "loss": 0.0006,
      "step": 69650
    },
    {
      "epoch": 2.22912,
      "grad_norm": 0.01604347676038742,
      "learning_rate": 5.139413333333334e-06,
      "loss": 0.0468,
      "step": 69660
    },
    {
      "epoch": 2.22944,
      "grad_norm": 0.010101137682795525,
      "learning_rate": 5.13728e-06,
      "loss": 0.0003,
      "step": 69670
    },
    {
      "epoch": 2.22976,
      "grad_norm": 0.004063645843416452,
      "learning_rate": 5.135146666666667e-06,
      "loss": 0.0009,
      "step": 69680
    },
    {
      "epoch": 2.23008,
      "grad_norm": 0.0036381855607032776,
      "learning_rate": 5.133013333333334e-06,
      "loss": 0.0002,
      "step": 69690
    },
    {
      "epoch": 2.2304,
      "grad_norm": 0.008772671222686768,
      "learning_rate": 5.130880000000001e-06,
      "loss": 0.0031,
      "step": 69700
    },
    {
      "epoch": 2.23072,
      "grad_norm": 0.01336652971804142,
      "learning_rate": 5.128746666666668e-06,
      "loss": 0.0192,
      "step": 69710
    },
    {
      "epoch": 2.23104,
      "grad_norm": 0.010155834257602692,
      "learning_rate": 5.126613333333333e-06,
      "loss": 0.0003,
      "step": 69720
    },
    {
      "epoch": 2.23136,
      "grad_norm": 0.0035345046781003475,
      "learning_rate": 5.124480000000001e-06,
      "loss": 0.0429,
      "step": 69730
    },
    {
      "epoch": 2.23168,
      "grad_norm": 0.015917133539915085,
      "learning_rate": 5.122346666666666e-06,
      "loss": 0.0006,
      "step": 69740
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.007648391183465719,
      "learning_rate": 5.120213333333333e-06,
      "loss": 0.0002,
      "step": 69750
    },
    {
      "epoch": 2.23232,
      "grad_norm": 0.002904047491028905,
      "learning_rate": 5.1180800000000005e-06,
      "loss": 0.0003,
      "step": 69760
    },
    {
      "epoch": 2.23264,
      "grad_norm": 0.003170906798914075,
      "learning_rate": 5.115946666666667e-06,
      "loss": 0.0002,
      "step": 69770
    },
    {
      "epoch": 2.23296,
      "grad_norm": 0.016751376911997795,
      "learning_rate": 5.113813333333334e-06,
      "loss": 0.0012,
      "step": 69780
    },
    {
      "epoch": 2.23328,
      "grad_norm": 0.012215018272399902,
      "learning_rate": 5.11168e-06,
      "loss": 0.0003,
      "step": 69790
    },
    {
      "epoch": 2.2336,
      "grad_norm": 0.0029374687001109123,
      "learning_rate": 5.109546666666667e-06,
      "loss": 0.0002,
      "step": 69800
    },
    {
      "epoch": 2.23392,
      "grad_norm": 0.0038820402696728706,
      "learning_rate": 5.107413333333334e-06,
      "loss": 0.0002,
      "step": 69810
    },
    {
      "epoch": 2.23424,
      "grad_norm": 0.002152472734451294,
      "learning_rate": 5.10528e-06,
      "loss": 0.0002,
      "step": 69820
    },
    {
      "epoch": 2.23456,
      "grad_norm": 0.00849160272628069,
      "learning_rate": 5.1031466666666674e-06,
      "loss": 0.0003,
      "step": 69830
    },
    {
      "epoch": 2.23488,
      "grad_norm": 0.005943465977907181,
      "learning_rate": 5.101013333333334e-06,
      "loss": 0.0179,
      "step": 69840
    },
    {
      "epoch": 2.2352,
      "grad_norm": 1.7215909957885742,
      "learning_rate": 5.098880000000001e-06,
      "loss": 0.0864,
      "step": 69850
    },
    {
      "epoch": 2.23552,
      "grad_norm": 0.0044660321436822414,
      "learning_rate": 5.096746666666668e-06,
      "loss": 0.0002,
      "step": 69860
    },
    {
      "epoch": 2.23584,
      "grad_norm": 0.00868031196296215,
      "learning_rate": 5.094613333333334e-06,
      "loss": 0.0004,
      "step": 69870
    },
    {
      "epoch": 2.23616,
      "grad_norm": 0.0042119394056499004,
      "learning_rate": 5.092480000000001e-06,
      "loss": 0.0315,
      "step": 69880
    },
    {
      "epoch": 2.23648,
      "grad_norm": 0.007646298501640558,
      "learning_rate": 5.0903466666666665e-06,
      "loss": 0.0005,
      "step": 69890
    },
    {
      "epoch": 2.2368,
      "grad_norm": 0.06585901975631714,
      "learning_rate": 5.0882133333333335e-06,
      "loss": 0.0005,
      "step": 69900
    },
    {
      "epoch": 2.23712,
      "grad_norm": 0.006619460415095091,
      "learning_rate": 5.08608e-06,
      "loss": 0.0003,
      "step": 69910
    },
    {
      "epoch": 2.23744,
      "grad_norm": 0.00547107495367527,
      "learning_rate": 5.083946666666667e-06,
      "loss": 0.0007,
      "step": 69920
    },
    {
      "epoch": 2.23776,
      "grad_norm": 0.01356741227209568,
      "learning_rate": 5.081813333333334e-06,
      "loss": 0.0004,
      "step": 69930
    },
    {
      "epoch": 2.23808,
      "grad_norm": 0.005821090657263994,
      "learning_rate": 5.07968e-06,
      "loss": 0.0425,
      "step": 69940
    },
    {
      "epoch": 2.2384,
      "grad_norm": 0.009899991564452648,
      "learning_rate": 5.077546666666667e-06,
      "loss": 0.0003,
      "step": 69950
    },
    {
      "epoch": 2.23872,
      "grad_norm": 0.005474373698234558,
      "learning_rate": 5.075413333333333e-06,
      "loss": 0.0002,
      "step": 69960
    },
    {
      "epoch": 2.23904,
      "grad_norm": 0.004669185262173414,
      "learning_rate": 5.0732800000000005e-06,
      "loss": 0.0003,
      "step": 69970
    },
    {
      "epoch": 2.23936,
      "grad_norm": 0.004715428687632084,
      "learning_rate": 5.0711466666666676e-06,
      "loss": 0.0004,
      "step": 69980
    },
    {
      "epoch": 2.23968,
      "grad_norm": 0.03590291365981102,
      "learning_rate": 5.069013333333334e-06,
      "loss": 0.0002,
      "step": 69990
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.009270066395401955,
      "learning_rate": 5.066880000000001e-06,
      "loss": 0.0004,
      "step": 70000
    },
    {
      "epoch": 2.24032,
      "grad_norm": 0.006450729910284281,
      "learning_rate": 5.064746666666667e-06,
      "loss": 0.0003,
      "step": 70010
    },
    {
      "epoch": 2.24064,
      "grad_norm": 0.00391435855999589,
      "learning_rate": 5.062613333333334e-06,
      "loss": 0.0016,
      "step": 70020
    },
    {
      "epoch": 2.24096,
      "grad_norm": 0.0025404307525604963,
      "learning_rate": 5.060480000000001e-06,
      "loss": 0.0006,
      "step": 70030
    },
    {
      "epoch": 2.24128,
      "grad_norm": 0.005318583454936743,
      "learning_rate": 5.058346666666667e-06,
      "loss": 0.0028,
      "step": 70040
    },
    {
      "epoch": 2.2416,
      "grad_norm": 0.009657720103859901,
      "learning_rate": 5.056213333333334e-06,
      "loss": 0.0006,
      "step": 70050
    },
    {
      "epoch": 2.24192,
      "grad_norm": 0.00520124938338995,
      "learning_rate": 5.05408e-06,
      "loss": 0.0002,
      "step": 70060
    },
    {
      "epoch": 2.24224,
      "grad_norm": 0.004583290312439203,
      "learning_rate": 5.051946666666667e-06,
      "loss": 0.0003,
      "step": 70070
    },
    {
      "epoch": 2.24256,
      "grad_norm": 0.003558145835995674,
      "learning_rate": 5.049813333333333e-06,
      "loss": 0.0003,
      "step": 70080
    },
    {
      "epoch": 2.24288,
      "grad_norm": 0.00259231380186975,
      "learning_rate": 5.04768e-06,
      "loss": 0.0008,
      "step": 70090
    },
    {
      "epoch": 2.2432,
      "grad_norm": 0.002691303612664342,
      "learning_rate": 5.045546666666667e-06,
      "loss": 0.0004,
      "step": 70100
    },
    {
      "epoch": 2.24352,
      "grad_norm": 0.005820396356284618,
      "learning_rate": 5.0434133333333336e-06,
      "loss": 0.0003,
      "step": 70110
    },
    {
      "epoch": 2.24384,
      "grad_norm": 0.009418978355824947,
      "learning_rate": 5.041280000000001e-06,
      "loss": 0.0003,
      "step": 70120
    },
    {
      "epoch": 2.24416,
      "grad_norm": 0.008251892402768135,
      "learning_rate": 5.039146666666667e-06,
      "loss": 0.0002,
      "step": 70130
    },
    {
      "epoch": 2.24448,
      "grad_norm": 0.010500440374016762,
      "learning_rate": 5.037013333333334e-06,
      "loss": 0.0004,
      "step": 70140
    },
    {
      "epoch": 2.2448,
      "grad_norm": 0.0046956464648246765,
      "learning_rate": 5.034880000000001e-06,
      "loss": 0.0003,
      "step": 70150
    },
    {
      "epoch": 2.24512,
      "grad_norm": 0.003500532591715455,
      "learning_rate": 5.032746666666667e-06,
      "loss": 0.0002,
      "step": 70160
    },
    {
      "epoch": 2.24544,
      "grad_norm": 0.0014245593920350075,
      "learning_rate": 5.030613333333334e-06,
      "loss": 0.0002,
      "step": 70170
    },
    {
      "epoch": 2.24576,
      "grad_norm": 0.004355971701443195,
      "learning_rate": 5.02848e-06,
      "loss": 0.0003,
      "step": 70180
    },
    {
      "epoch": 2.24608,
      "grad_norm": 0.009986543096601963,
      "learning_rate": 5.026346666666667e-06,
      "loss": 0.0003,
      "step": 70190
    },
    {
      "epoch": 2.2464,
      "grad_norm": 0.004332329146564007,
      "learning_rate": 5.024213333333335e-06,
      "loss": 0.0516,
      "step": 70200
    },
    {
      "epoch": 2.24672,
      "grad_norm": 0.005233114119619131,
      "learning_rate": 5.02208e-06,
      "loss": 0.0016,
      "step": 70210
    },
    {
      "epoch": 2.24704,
      "grad_norm": 0.0028490968979895115,
      "learning_rate": 5.019946666666667e-06,
      "loss": 0.0018,
      "step": 70220
    },
    {
      "epoch": 2.24736,
      "grad_norm": 3.6685891151428223,
      "learning_rate": 5.017813333333333e-06,
      "loss": 0.0458,
      "step": 70230
    },
    {
      "epoch": 2.24768,
      "grad_norm": 0.010594550520181656,
      "learning_rate": 5.01568e-06,
      "loss": 0.0002,
      "step": 70240
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.005979074165225029,
      "learning_rate": 5.013546666666667e-06,
      "loss": 0.0002,
      "step": 70250
    },
    {
      "epoch": 2.24832,
      "grad_norm": 0.021847395226359367,
      "learning_rate": 5.011413333333334e-06,
      "loss": 0.0003,
      "step": 70260
    },
    {
      "epoch": 2.24864,
      "grad_norm": 0.005757624749094248,
      "learning_rate": 5.009280000000001e-06,
      "loss": 0.0002,
      "step": 70270
    },
    {
      "epoch": 2.24896,
      "grad_norm": 0.005627806764096022,
      "learning_rate": 5.007146666666667e-06,
      "loss": 0.0002,
      "step": 70280
    },
    {
      "epoch": 2.24928,
      "grad_norm": 0.0038588568568229675,
      "learning_rate": 5.005013333333334e-06,
      "loss": 0.0155,
      "step": 70290
    },
    {
      "epoch": 2.2496,
      "grad_norm": 0.0025115683674812317,
      "learning_rate": 5.00288e-06,
      "loss": 0.0002,
      "step": 70300
    },
    {
      "epoch": 2.24992,
      "grad_norm": 0.03047466278076172,
      "learning_rate": 5.000746666666667e-06,
      "loss": 0.0003,
      "step": 70310
    },
    {
      "epoch": 2.25024,
      "grad_norm": 0.002614381490275264,
      "learning_rate": 4.998613333333334e-06,
      "loss": 0.0007,
      "step": 70320
    },
    {
      "epoch": 2.25056,
      "grad_norm": 0.011219971813261509,
      "learning_rate": 4.99648e-06,
      "loss": 0.0003,
      "step": 70330
    },
    {
      "epoch": 2.25088,
      "grad_norm": 0.047288570553064346,
      "learning_rate": 4.994346666666668e-06,
      "loss": 0.0005,
      "step": 70340
    },
    {
      "epoch": 2.2512,
      "grad_norm": 0.0032782829366624355,
      "learning_rate": 4.992213333333334e-06,
      "loss": 0.0008,
      "step": 70350
    },
    {
      "epoch": 2.25152,
      "grad_norm": 0.0028303719591349363,
      "learning_rate": 4.99008e-06,
      "loss": 0.0003,
      "step": 70360
    },
    {
      "epoch": 2.25184,
      "grad_norm": 0.005503231659531593,
      "learning_rate": 4.987946666666667e-06,
      "loss": 0.0003,
      "step": 70370
    },
    {
      "epoch": 2.25216,
      "grad_norm": 0.007182859815657139,
      "learning_rate": 4.9858133333333335e-06,
      "loss": 0.0004,
      "step": 70380
    },
    {
      "epoch": 2.25248,
      "grad_norm": 0.00444289855659008,
      "learning_rate": 4.9836800000000005e-06,
      "loss": 0.0002,
      "step": 70390
    },
    {
      "epoch": 2.2528,
      "grad_norm": 0.004454256501048803,
      "learning_rate": 4.981546666666668e-06,
      "loss": 0.0003,
      "step": 70400
    },
    {
      "epoch": 2.25312,
      "grad_norm": 0.007595141418278217,
      "learning_rate": 4.979413333333334e-06,
      "loss": 0.0095,
      "step": 70410
    },
    {
      "epoch": 2.25344,
      "grad_norm": 0.003575881477445364,
      "learning_rate": 4.97728e-06,
      "loss": 0.0002,
      "step": 70420
    },
    {
      "epoch": 2.2537599999999998,
      "grad_norm": 0.001552644302137196,
      "learning_rate": 4.975146666666667e-06,
      "loss": 0.0002,
      "step": 70430
    },
    {
      "epoch": 2.25408,
      "grad_norm": 0.003725294955074787,
      "learning_rate": 4.973013333333333e-06,
      "loss": 0.0003,
      "step": 70440
    },
    {
      "epoch": 2.2544,
      "grad_norm": 0.002529534511268139,
      "learning_rate": 4.9708800000000004e-06,
      "loss": 0.0005,
      "step": 70450
    },
    {
      "epoch": 2.25472,
      "grad_norm": 0.0051322923973202705,
      "learning_rate": 4.9687466666666675e-06,
      "loss": 0.0002,
      "step": 70460
    },
    {
      "epoch": 2.25504,
      "grad_norm": 0.0066739823669195175,
      "learning_rate": 4.966613333333334e-06,
      "loss": 0.0003,
      "step": 70470
    },
    {
      "epoch": 2.25536,
      "grad_norm": 0.0071087796241045,
      "learning_rate": 4.964480000000001e-06,
      "loss": 0.0003,
      "step": 70480
    },
    {
      "epoch": 2.25568,
      "grad_norm": 0.0051147895865142345,
      "learning_rate": 4.962346666666667e-06,
      "loss": 0.0003,
      "step": 70490
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.007521375082433224,
      "learning_rate": 4.960213333333333e-06,
      "loss": 0.0003,
      "step": 70500
    },
    {
      "epoch": 2.25632,
      "grad_norm": 0.0046079992316663265,
      "learning_rate": 4.95808e-06,
      "loss": 0.0002,
      "step": 70510
    },
    {
      "epoch": 2.25664,
      "grad_norm": 0.0048642694018781185,
      "learning_rate": 4.955946666666667e-06,
      "loss": 0.0002,
      "step": 70520
    },
    {
      "epoch": 2.25696,
      "grad_norm": 0.01880190335214138,
      "learning_rate": 4.953813333333334e-06,
      "loss": 0.0005,
      "step": 70530
    },
    {
      "epoch": 2.25728,
      "grad_norm": 0.0038964212872087955,
      "learning_rate": 4.951680000000001e-06,
      "loss": 0.0777,
      "step": 70540
    },
    {
      "epoch": 2.2576,
      "grad_norm": 0.005470398347824812,
      "learning_rate": 4.949546666666667e-06,
      "loss": 0.0138,
      "step": 70550
    },
    {
      "epoch": 2.25792,
      "grad_norm": 0.002480414230376482,
      "learning_rate": 4.947413333333333e-06,
      "loss": 0.0003,
      "step": 70560
    },
    {
      "epoch": 2.25824,
      "grad_norm": 1.4435027837753296,
      "learning_rate": 4.945280000000001e-06,
      "loss": 0.0198,
      "step": 70570
    },
    {
      "epoch": 2.25856,
      "grad_norm": 0.0017834858736023307,
      "learning_rate": 4.943146666666667e-06,
      "loss": 0.0004,
      "step": 70580
    },
    {
      "epoch": 2.25888,
      "grad_norm": 0.005000109318643808,
      "learning_rate": 4.9410133333333335e-06,
      "loss": 0.0002,
      "step": 70590
    },
    {
      "epoch": 2.2592,
      "grad_norm": 0.0034076787997037172,
      "learning_rate": 4.9388800000000006e-06,
      "loss": 0.0002,
      "step": 70600
    },
    {
      "epoch": 2.25952,
      "grad_norm": 0.7661815285682678,
      "learning_rate": 4.936746666666667e-06,
      "loss": 0.0006,
      "step": 70610
    },
    {
      "epoch": 2.25984,
      "grad_norm": 0.3328183591365814,
      "learning_rate": 4.934613333333334e-06,
      "loss": 0.0006,
      "step": 70620
    },
    {
      "epoch": 2.26016,
      "grad_norm": 0.004965764936059713,
      "learning_rate": 4.932480000000001e-06,
      "loss": 0.0002,
      "step": 70630
    },
    {
      "epoch": 2.26048,
      "grad_norm": 0.003071695566177368,
      "learning_rate": 4.930346666666667e-06,
      "loss": 0.0002,
      "step": 70640
    },
    {
      "epoch": 2.2608,
      "grad_norm": 0.008107087574899197,
      "learning_rate": 4.928213333333333e-06,
      "loss": 0.0002,
      "step": 70650
    },
    {
      "epoch": 2.26112,
      "grad_norm": 0.006048424169421196,
      "learning_rate": 4.9260800000000004e-06,
      "loss": 0.0023,
      "step": 70660
    },
    {
      "epoch": 2.26144,
      "grad_norm": 0.004717414267361164,
      "learning_rate": 4.923946666666667e-06,
      "loss": 0.0002,
      "step": 70670
    },
    {
      "epoch": 2.2617599999999998,
      "grad_norm": 0.0029800974298268557,
      "learning_rate": 4.921813333333334e-06,
      "loss": 0.0002,
      "step": 70680
    },
    {
      "epoch": 2.26208,
      "grad_norm": 0.07795679569244385,
      "learning_rate": 4.919680000000001e-06,
      "loss": 0.0972,
      "step": 70690
    },
    {
      "epoch": 2.2624,
      "grad_norm": 0.0033848388120532036,
      "learning_rate": 4.917546666666667e-06,
      "loss": 0.0005,
      "step": 70700
    },
    {
      "epoch": 2.26272,
      "grad_norm": 0.011912846937775612,
      "learning_rate": 4.915413333333334e-06,
      "loss": 0.0005,
      "step": 70710
    },
    {
      "epoch": 2.26304,
      "grad_norm": 0.005332152359187603,
      "learning_rate": 4.91328e-06,
      "loss": 0.0003,
      "step": 70720
    },
    {
      "epoch": 2.26336,
      "grad_norm": 0.0024690076243132353,
      "learning_rate": 4.9111466666666666e-06,
      "loss": 0.0013,
      "step": 70730
    },
    {
      "epoch": 2.26368,
      "grad_norm": 0.0036836769431829453,
      "learning_rate": 4.909013333333334e-06,
      "loss": 0.0005,
      "step": 70740
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.008300727233290672,
      "learning_rate": 4.906880000000001e-06,
      "loss": 0.0011,
      "step": 70750
    },
    {
      "epoch": 2.26432,
      "grad_norm": 0.0032498673535883427,
      "learning_rate": 4.904746666666667e-06,
      "loss": 0.0003,
      "step": 70760
    },
    {
      "epoch": 2.26464,
      "grad_norm": 0.011467481032013893,
      "learning_rate": 4.902613333333334e-06,
      "loss": 0.0562,
      "step": 70770
    },
    {
      "epoch": 2.26496,
      "grad_norm": 0.0043894462287425995,
      "learning_rate": 4.90048e-06,
      "loss": 0.0042,
      "step": 70780
    },
    {
      "epoch": 2.26528,
      "grad_norm": 0.013452610932290554,
      "learning_rate": 4.8983466666666664e-06,
      "loss": 0.0017,
      "step": 70790
    },
    {
      "epoch": 2.2656,
      "grad_norm": 0.0031819541472941637,
      "learning_rate": 4.8962133333333335e-06,
      "loss": 0.0642,
      "step": 70800
    },
    {
      "epoch": 2.26592,
      "grad_norm": 0.0027648869436234236,
      "learning_rate": 4.894080000000001e-06,
      "loss": 0.0021,
      "step": 70810
    },
    {
      "epoch": 2.26624,
      "grad_norm": 0.004945659544318914,
      "learning_rate": 4.891946666666667e-06,
      "loss": 0.0011,
      "step": 70820
    },
    {
      "epoch": 2.26656,
      "grad_norm": 0.05067363381385803,
      "learning_rate": 4.889813333333334e-06,
      "loss": 0.0004,
      "step": 70830
    },
    {
      "epoch": 2.26688,
      "grad_norm": 0.004366371780633926,
      "learning_rate": 4.88768e-06,
      "loss": 0.0002,
      "step": 70840
    },
    {
      "epoch": 2.2672,
      "grad_norm": 0.005303539335727692,
      "learning_rate": 4.885546666666667e-06,
      "loss": 0.0006,
      "step": 70850
    },
    {
      "epoch": 2.26752,
      "grad_norm": 0.012013999745249748,
      "learning_rate": 4.883413333333334e-06,
      "loss": 0.0012,
      "step": 70860
    },
    {
      "epoch": 2.26784,
      "grad_norm": 0.012179218232631683,
      "learning_rate": 4.8812800000000005e-06,
      "loss": 0.0002,
      "step": 70870
    },
    {
      "epoch": 2.26816,
      "grad_norm": 0.01821928098797798,
      "learning_rate": 4.879146666666667e-06,
      "loss": 0.0009,
      "step": 70880
    },
    {
      "epoch": 2.26848,
      "grad_norm": 0.01027651783078909,
      "learning_rate": 4.877013333333334e-06,
      "loss": 0.0002,
      "step": 70890
    },
    {
      "epoch": 2.2688,
      "grad_norm": 0.002514147898182273,
      "learning_rate": 4.87488e-06,
      "loss": 0.0002,
      "step": 70900
    },
    {
      "epoch": 2.26912,
      "grad_norm": 0.0044419229961931705,
      "learning_rate": 4.872746666666667e-06,
      "loss": 0.0002,
      "step": 70910
    },
    {
      "epoch": 2.26944,
      "grad_norm": 0.003189639886841178,
      "learning_rate": 4.870613333333334e-06,
      "loss": 0.0002,
      "step": 70920
    },
    {
      "epoch": 2.2697599999999998,
      "grad_norm": 0.00903992634266615,
      "learning_rate": 4.86848e-06,
      "loss": 0.0017,
      "step": 70930
    },
    {
      "epoch": 2.27008,
      "grad_norm": 0.005256510805338621,
      "learning_rate": 4.866346666666667e-06,
      "loss": 0.0002,
      "step": 70940
    },
    {
      "epoch": 2.2704,
      "grad_norm": 0.007012153510004282,
      "learning_rate": 4.864213333333334e-06,
      "loss": 0.0003,
      "step": 70950
    },
    {
      "epoch": 2.27072,
      "grad_norm": 0.0019542768131941557,
      "learning_rate": 4.86208e-06,
      "loss": 0.0002,
      "step": 70960
    },
    {
      "epoch": 2.27104,
      "grad_norm": 0.00721995672211051,
      "learning_rate": 4.859946666666667e-06,
      "loss": 0.0349,
      "step": 70970
    },
    {
      "epoch": 2.27136,
      "grad_norm": 0.009376002475619316,
      "learning_rate": 4.857813333333334e-06,
      "loss": 0.0002,
      "step": 70980
    },
    {
      "epoch": 2.27168,
      "grad_norm": 0.006089976988732815,
      "learning_rate": 4.85568e-06,
      "loss": 0.0003,
      "step": 70990
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.0024613949935883284,
      "learning_rate": 4.853546666666667e-06,
      "loss": 0.0002,
      "step": 71000
    },
    {
      "epoch": 2.27232,
      "grad_norm": 0.013182376511394978,
      "learning_rate": 4.8514133333333335e-06,
      "loss": 0.0075,
      "step": 71010
    },
    {
      "epoch": 2.27264,
      "grad_norm": 0.0026043783873319626,
      "learning_rate": 4.849280000000001e-06,
      "loss": 0.0002,
      "step": 71020
    },
    {
      "epoch": 2.27296,
      "grad_norm": 0.004054451826959848,
      "learning_rate": 4.847146666666667e-06,
      "loss": 0.0016,
      "step": 71030
    },
    {
      "epoch": 2.27328,
      "grad_norm": 0.00156773638445884,
      "learning_rate": 4.845013333333334e-06,
      "loss": 0.0293,
      "step": 71040
    },
    {
      "epoch": 2.2736,
      "grad_norm": 0.002434374066069722,
      "learning_rate": 4.84288e-06,
      "loss": 0.0033,
      "step": 71050
    },
    {
      "epoch": 2.27392,
      "grad_norm": 0.00514741986989975,
      "learning_rate": 4.840746666666667e-06,
      "loss": 0.0002,
      "step": 71060
    },
    {
      "epoch": 2.27424,
      "grad_norm": 0.003935212269425392,
      "learning_rate": 4.838613333333333e-06,
      "loss": 0.0001,
      "step": 71070
    },
    {
      "epoch": 2.27456,
      "grad_norm": 0.013099226169288158,
      "learning_rate": 4.8364800000000005e-06,
      "loss": 0.0013,
      "step": 71080
    },
    {
      "epoch": 2.27488,
      "grad_norm": 0.02714734710752964,
      "learning_rate": 4.8343466666666676e-06,
      "loss": 0.0002,
      "step": 71090
    },
    {
      "epoch": 2.2752,
      "grad_norm": 0.0035279474686831236,
      "learning_rate": 4.832213333333334e-06,
      "loss": 0.0402,
      "step": 71100
    },
    {
      "epoch": 2.27552,
      "grad_norm": 0.13319970667362213,
      "learning_rate": 4.83008e-06,
      "loss": 0.0005,
      "step": 71110
    },
    {
      "epoch": 2.27584,
      "grad_norm": 0.004321876913309097,
      "learning_rate": 4.827946666666667e-06,
      "loss": 0.0008,
      "step": 71120
    },
    {
      "epoch": 2.27616,
      "grad_norm": 0.024725191295146942,
      "learning_rate": 4.825813333333333e-06,
      "loss": 0.0002,
      "step": 71130
    },
    {
      "epoch": 2.27648,
      "grad_norm": 0.005099180620163679,
      "learning_rate": 4.82368e-06,
      "loss": 0.0002,
      "step": 71140
    },
    {
      "epoch": 2.2768,
      "grad_norm": 0.019622700288891792,
      "learning_rate": 4.8215466666666674e-06,
      "loss": 0.0003,
      "step": 71150
    },
    {
      "epoch": 2.27712,
      "grad_norm": 0.005487094633281231,
      "learning_rate": 4.819413333333334e-06,
      "loss": 0.0002,
      "step": 71160
    },
    {
      "epoch": 2.27744,
      "grad_norm": 0.007223339751362801,
      "learning_rate": 4.81728e-06,
      "loss": 0.0002,
      "step": 71170
    },
    {
      "epoch": 2.27776,
      "grad_norm": 0.0023754453286528587,
      "learning_rate": 4.815146666666667e-06,
      "loss": 0.0002,
      "step": 71180
    },
    {
      "epoch": 2.27808,
      "grad_norm": 0.004459352698177099,
      "learning_rate": 4.813013333333334e-06,
      "loss": 0.0002,
      "step": 71190
    },
    {
      "epoch": 2.2784,
      "grad_norm": 0.001691951765678823,
      "learning_rate": 4.81088e-06,
      "loss": 0.0002,
      "step": 71200
    },
    {
      "epoch": 2.27872,
      "grad_norm": 0.008376283571124077,
      "learning_rate": 4.808746666666667e-06,
      "loss": 0.0002,
      "step": 71210
    },
    {
      "epoch": 2.27904,
      "grad_norm": 0.0029319908935576677,
      "learning_rate": 4.8066133333333336e-06,
      "loss": 0.0002,
      "step": 71220
    },
    {
      "epoch": 2.27936,
      "grad_norm": 0.004826892167329788,
      "learning_rate": 4.804480000000001e-06,
      "loss": 0.051,
      "step": 71230
    },
    {
      "epoch": 2.27968,
      "grad_norm": 0.004856905899941921,
      "learning_rate": 4.802346666666667e-06,
      "loss": 0.0002,
      "step": 71240
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.002914652694016695,
      "learning_rate": 4.800213333333334e-06,
      "loss": 0.0002,
      "step": 71250
    },
    {
      "epoch": 2.28032,
      "grad_norm": 0.00380329554900527,
      "learning_rate": 4.79808e-06,
      "loss": 0.0002,
      "step": 71260
    },
    {
      "epoch": 2.28064,
      "grad_norm": 0.004189637489616871,
      "learning_rate": 4.795946666666667e-06,
      "loss": 0.0002,
      "step": 71270
    },
    {
      "epoch": 2.28096,
      "grad_norm": 0.00842924602329731,
      "learning_rate": 4.7938133333333334e-06,
      "loss": 0.0003,
      "step": 71280
    },
    {
      "epoch": 2.2812799999999998,
      "grad_norm": 0.0057020848616957664,
      "learning_rate": 4.7916800000000005e-06,
      "loss": 0.0027,
      "step": 71290
    },
    {
      "epoch": 2.2816,
      "grad_norm": 0.010175374336540699,
      "learning_rate": 4.789546666666667e-06,
      "loss": 0.0002,
      "step": 71300
    },
    {
      "epoch": 2.28192,
      "grad_norm": 0.003833333496004343,
      "learning_rate": 4.787413333333334e-06,
      "loss": 0.0002,
      "step": 71310
    },
    {
      "epoch": 2.28224,
      "grad_norm": 0.001799528836272657,
      "learning_rate": 4.785280000000001e-06,
      "loss": 0.0004,
      "step": 71320
    },
    {
      "epoch": 2.28256,
      "grad_norm": 0.006404327228665352,
      "learning_rate": 4.783146666666667e-06,
      "loss": 0.0086,
      "step": 71330
    },
    {
      "epoch": 2.28288,
      "grad_norm": 0.0028077068272978067,
      "learning_rate": 4.781013333333333e-06,
      "loss": 0.001,
      "step": 71340
    },
    {
      "epoch": 2.2832,
      "grad_norm": 0.004332630895078182,
      "learning_rate": 4.77888e-06,
      "loss": 0.0002,
      "step": 71350
    },
    {
      "epoch": 2.28352,
      "grad_norm": 0.0028083587531000376,
      "learning_rate": 4.7767466666666675e-06,
      "loss": 0.0002,
      "step": 71360
    },
    {
      "epoch": 2.28384,
      "grad_norm": 0.005305233411490917,
      "learning_rate": 4.774613333333334e-06,
      "loss": 0.0002,
      "step": 71370
    },
    {
      "epoch": 2.28416,
      "grad_norm": 0.003046971745789051,
      "learning_rate": 4.772480000000001e-06,
      "loss": 0.0002,
      "step": 71380
    },
    {
      "epoch": 2.28448,
      "grad_norm": 0.0033782203681766987,
      "learning_rate": 4.770346666666667e-06,
      "loss": 0.0003,
      "step": 71390
    },
    {
      "epoch": 2.2848,
      "grad_norm": 0.02869539149105549,
      "learning_rate": 4.768213333333333e-06,
      "loss": 0.0002,
      "step": 71400
    },
    {
      "epoch": 2.28512,
      "grad_norm": 0.0039712730795145035,
      "learning_rate": 4.76608e-06,
      "loss": 0.0003,
      "step": 71410
    },
    {
      "epoch": 2.28544,
      "grad_norm": 0.0033820942044258118,
      "learning_rate": 4.763946666666667e-06,
      "loss": 0.0002,
      "step": 71420
    },
    {
      "epoch": 2.28576,
      "grad_norm": 0.00433625653386116,
      "learning_rate": 4.761813333333334e-06,
      "loss": 0.0015,
      "step": 71430
    },
    {
      "epoch": 2.28608,
      "grad_norm": 0.0027876298408955336,
      "learning_rate": 4.759680000000001e-06,
      "loss": 0.0008,
      "step": 71440
    },
    {
      "epoch": 2.2864,
      "grad_norm": 0.0060144332237541676,
      "learning_rate": 4.757546666666667e-06,
      "loss": 0.0002,
      "step": 71450
    },
    {
      "epoch": 2.28672,
      "grad_norm": 0.03172667697072029,
      "learning_rate": 4.755413333333334e-06,
      "loss": 0.0002,
      "step": 71460
    },
    {
      "epoch": 2.28704,
      "grad_norm": 0.0020010482985526323,
      "learning_rate": 4.75328e-06,
      "loss": 0.0179,
      "step": 71470
    },
    {
      "epoch": 2.28736,
      "grad_norm": 0.004136180970817804,
      "learning_rate": 4.751146666666667e-06,
      "loss": 0.0002,
      "step": 71480
    },
    {
      "epoch": 2.28768,
      "grad_norm": 0.0022534639574587345,
      "learning_rate": 4.7490133333333335e-06,
      "loss": 0.0002,
      "step": 71490
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.004542534705251455,
      "learning_rate": 4.7468800000000005e-06,
      "loss": 0.0005,
      "step": 71500
    },
    {
      "epoch": 2.28832,
      "grad_norm": 0.004163408651947975,
      "learning_rate": 4.744746666666667e-06,
      "loss": 0.0004,
      "step": 71510
    },
    {
      "epoch": 2.28864,
      "grad_norm": 0.005979543551802635,
      "learning_rate": 4.742613333333334e-06,
      "loss": 0.0496,
      "step": 71520
    },
    {
      "epoch": 2.28896,
      "grad_norm": 0.00471210852265358,
      "learning_rate": 4.740480000000001e-06,
      "loss": 0.0003,
      "step": 71530
    },
    {
      "epoch": 2.2892799999999998,
      "grad_norm": 0.0277284886687994,
      "learning_rate": 4.738346666666667e-06,
      "loss": 0.0003,
      "step": 71540
    },
    {
      "epoch": 2.2896,
      "grad_norm": 0.00307123432867229,
      "learning_rate": 4.736213333333334e-06,
      "loss": 0.0002,
      "step": 71550
    },
    {
      "epoch": 2.28992,
      "grad_norm": 0.0041984450072050095,
      "learning_rate": 4.73408e-06,
      "loss": 0.0002,
      "step": 71560
    },
    {
      "epoch": 2.29024,
      "grad_norm": 0.0018984022317454219,
      "learning_rate": 4.731946666666667e-06,
      "loss": 0.0004,
      "step": 71570
    },
    {
      "epoch": 2.29056,
      "grad_norm": 0.004912351258099079,
      "learning_rate": 4.729813333333334e-06,
      "loss": 0.0485,
      "step": 71580
    },
    {
      "epoch": 2.29088,
      "grad_norm": 2.516146659851074,
      "learning_rate": 4.727680000000001e-06,
      "loss": 0.0459,
      "step": 71590
    },
    {
      "epoch": 2.2912,
      "grad_norm": 0.04111585021018982,
      "learning_rate": 4.725546666666667e-06,
      "loss": 0.0003,
      "step": 71600
    },
    {
      "epoch": 2.2915200000000002,
      "grad_norm": 0.00570000009611249,
      "learning_rate": 4.723413333333334e-06,
      "loss": 0.0002,
      "step": 71610
    },
    {
      "epoch": 2.29184,
      "grad_norm": 0.0022748459596186876,
      "learning_rate": 4.72128e-06,
      "loss": 0.0009,
      "step": 71620
    },
    {
      "epoch": 2.29216,
      "grad_norm": 0.005318313837051392,
      "learning_rate": 4.7191466666666665e-06,
      "loss": 0.0002,
      "step": 71630
    },
    {
      "epoch": 2.29248,
      "grad_norm": 0.004607454873621464,
      "learning_rate": 4.717013333333334e-06,
      "loss": 0.0389,
      "step": 71640
    },
    {
      "epoch": 2.2928,
      "grad_norm": 0.0063255550339818,
      "learning_rate": 4.714880000000001e-06,
      "loss": 0.0003,
      "step": 71650
    },
    {
      "epoch": 2.29312,
      "grad_norm": 0.0045049190521240234,
      "learning_rate": 4.712746666666667e-06,
      "loss": 0.0004,
      "step": 71660
    },
    {
      "epoch": 2.29344,
      "grad_norm": 0.0033102212473750114,
      "learning_rate": 4.710613333333334e-06,
      "loss": 0.0015,
      "step": 71670
    },
    {
      "epoch": 2.29376,
      "grad_norm": 0.0038975654169917107,
      "learning_rate": 4.70848e-06,
      "loss": 0.0171,
      "step": 71680
    },
    {
      "epoch": 2.29408,
      "grad_norm": 2.339991807937622,
      "learning_rate": 4.706346666666667e-06,
      "loss": 0.0017,
      "step": 71690
    },
    {
      "epoch": 2.2944,
      "grad_norm": 0.0036527514457702637,
      "learning_rate": 4.704213333333334e-06,
      "loss": 0.0006,
      "step": 71700
    },
    {
      "epoch": 2.29472,
      "grad_norm": 0.003811108646914363,
      "learning_rate": 4.7020800000000006e-06,
      "loss": 0.0002,
      "step": 71710
    },
    {
      "epoch": 2.29504,
      "grad_norm": 0.0015361674595624208,
      "learning_rate": 4.699946666666667e-06,
      "loss": 0.0003,
      "step": 71720
    },
    {
      "epoch": 2.29536,
      "grad_norm": 0.0037315760273486376,
      "learning_rate": 4.697813333333334e-06,
      "loss": 0.0002,
      "step": 71730
    },
    {
      "epoch": 2.29568,
      "grad_norm": 0.005752861499786377,
      "learning_rate": 4.69568e-06,
      "loss": 0.0004,
      "step": 71740
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.005682720337063074,
      "learning_rate": 4.693546666666667e-06,
      "loss": 0.0002,
      "step": 71750
    },
    {
      "epoch": 2.29632,
      "grad_norm": 0.002638972131535411,
      "learning_rate": 4.691413333333334e-06,
      "loss": 0.0002,
      "step": 71760
    },
    {
      "epoch": 2.29664,
      "grad_norm": 0.006902894005179405,
      "learning_rate": 4.6892800000000004e-06,
      "loss": 0.0003,
      "step": 71770
    },
    {
      "epoch": 2.29696,
      "grad_norm": 0.004352133721113205,
      "learning_rate": 4.687146666666667e-06,
      "loss": 0.0002,
      "step": 71780
    },
    {
      "epoch": 2.2972799999999998,
      "grad_norm": 0.003131604753434658,
      "learning_rate": 4.685013333333334e-06,
      "loss": 0.0003,
      "step": 71790
    },
    {
      "epoch": 2.2976,
      "grad_norm": 0.0061945985071361065,
      "learning_rate": 4.68288e-06,
      "loss": 0.0001,
      "step": 71800
    },
    {
      "epoch": 2.29792,
      "grad_norm": 0.02844410017132759,
      "learning_rate": 4.680746666666667e-06,
      "loss": 0.0126,
      "step": 71810
    },
    {
      "epoch": 2.29824,
      "grad_norm": 0.0069062840193510056,
      "learning_rate": 4.678613333333334e-06,
      "loss": 0.0002,
      "step": 71820
    },
    {
      "epoch": 2.29856,
      "grad_norm": 0.007168380543589592,
      "learning_rate": 4.67648e-06,
      "loss": 0.0002,
      "step": 71830
    },
    {
      "epoch": 2.29888,
      "grad_norm": 0.0009043986210599542,
      "learning_rate": 4.674346666666667e-06,
      "loss": 0.0013,
      "step": 71840
    },
    {
      "epoch": 2.2992,
      "grad_norm": 0.0021195465233176947,
      "learning_rate": 4.672213333333334e-06,
      "loss": 0.0002,
      "step": 71850
    },
    {
      "epoch": 2.2995200000000002,
      "grad_norm": 0.00294526107609272,
      "learning_rate": 4.67008e-06,
      "loss": 0.0397,
      "step": 71860
    },
    {
      "epoch": 2.29984,
      "grad_norm": 0.00910125207155943,
      "learning_rate": 4.667946666666667e-06,
      "loss": 0.0471,
      "step": 71870
    },
    {
      "epoch": 2.30016,
      "grad_norm": 0.0037382144946604967,
      "learning_rate": 4.665813333333334e-06,
      "loss": 0.0002,
      "step": 71880
    },
    {
      "epoch": 2.30048,
      "grad_norm": 0.006899992935359478,
      "learning_rate": 4.66368e-06,
      "loss": 0.0002,
      "step": 71890
    },
    {
      "epoch": 2.3008,
      "grad_norm": 0.001174774020910263,
      "learning_rate": 4.661546666666667e-06,
      "loss": 0.0002,
      "step": 71900
    },
    {
      "epoch": 2.30112,
      "grad_norm": 6.935823917388916,
      "learning_rate": 4.6594133333333335e-06,
      "loss": 0.0144,
      "step": 71910
    },
    {
      "epoch": 2.30144,
      "grad_norm": 0.00574077432975173,
      "learning_rate": 4.65728e-06,
      "loss": 0.0005,
      "step": 71920
    },
    {
      "epoch": 2.30176,
      "grad_norm": 0.005528932902961969,
      "learning_rate": 4.655146666666668e-06,
      "loss": 0.0002,
      "step": 71930
    },
    {
      "epoch": 2.30208,
      "grad_norm": 0.005988579709082842,
      "learning_rate": 4.653013333333334e-06,
      "loss": 0.0002,
      "step": 71940
    },
    {
      "epoch": 2.3024,
      "grad_norm": 0.0025883193593472242,
      "learning_rate": 4.65088e-06,
      "loss": 0.0009,
      "step": 71950
    },
    {
      "epoch": 2.30272,
      "grad_norm": 0.006225630175322294,
      "learning_rate": 4.648746666666667e-06,
      "loss": 0.0274,
      "step": 71960
    },
    {
      "epoch": 2.30304,
      "grad_norm": 0.004298275336623192,
      "learning_rate": 4.646613333333333e-06,
      "loss": 0.0007,
      "step": 71970
    },
    {
      "epoch": 2.30336,
      "grad_norm": 0.004529841244220734,
      "learning_rate": 4.6444800000000005e-06,
      "loss": 0.0002,
      "step": 71980
    },
    {
      "epoch": 2.30368,
      "grad_norm": 0.09711962193250656,
      "learning_rate": 4.6423466666666675e-06,
      "loss": 0.0003,
      "step": 71990
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.001973633421584964,
      "learning_rate": 4.640213333333334e-06,
      "loss": 0.0002,
      "step": 72000
    },
    {
      "epoch": 2.30432,
      "grad_norm": 0.004505884367972612,
      "learning_rate": 4.63808e-06,
      "loss": 0.0002,
      "step": 72010
    },
    {
      "epoch": 2.30464,
      "grad_norm": 0.0033680456690490246,
      "learning_rate": 4.635946666666667e-06,
      "loss": 0.0002,
      "step": 72020
    },
    {
      "epoch": 2.30496,
      "grad_norm": 0.026068056002259254,
      "learning_rate": 4.633813333333333e-06,
      "loss": 0.0003,
      "step": 72030
    },
    {
      "epoch": 2.3052799999999998,
      "grad_norm": 0.012243115343153477,
      "learning_rate": 4.63168e-06,
      "loss": 0.0268,
      "step": 72040
    },
    {
      "epoch": 2.3056,
      "grad_norm": 0.009080963209271431,
      "learning_rate": 4.629546666666667e-06,
      "loss": 0.0215,
      "step": 72050
    },
    {
      "epoch": 2.30592,
      "grad_norm": 0.003484484041109681,
      "learning_rate": 4.627413333333334e-06,
      "loss": 0.0004,
      "step": 72060
    },
    {
      "epoch": 2.30624,
      "grad_norm": 0.006310968194156885,
      "learning_rate": 4.625280000000001e-06,
      "loss": 0.0003,
      "step": 72070
    },
    {
      "epoch": 2.30656,
      "grad_norm": 0.008992623537778854,
      "learning_rate": 4.623146666666667e-06,
      "loss": 0.0004,
      "step": 72080
    },
    {
      "epoch": 2.30688,
      "grad_norm": 0.004814580082893372,
      "learning_rate": 4.621013333333333e-06,
      "loss": 0.0002,
      "step": 72090
    },
    {
      "epoch": 2.3072,
      "grad_norm": 0.01696995459496975,
      "learning_rate": 4.61888e-06,
      "loss": 0.0002,
      "step": 72100
    },
    {
      "epoch": 2.3075200000000002,
      "grad_norm": 0.00862791482359171,
      "learning_rate": 4.616746666666667e-06,
      "loss": 0.0002,
      "step": 72110
    },
    {
      "epoch": 2.30784,
      "grad_norm": 0.0020201040897518396,
      "learning_rate": 4.6146133333333335e-06,
      "loss": 0.0002,
      "step": 72120
    },
    {
      "epoch": 2.30816,
      "grad_norm": 0.006353497505187988,
      "learning_rate": 4.612480000000001e-06,
      "loss": 0.0002,
      "step": 72130
    },
    {
      "epoch": 2.30848,
      "grad_norm": 0.024171145632863045,
      "learning_rate": 4.610346666666667e-06,
      "loss": 0.0214,
      "step": 72140
    },
    {
      "epoch": 2.3088,
      "grad_norm": 0.009201431646943092,
      "learning_rate": 4.608213333333334e-06,
      "loss": 0.0002,
      "step": 72150
    },
    {
      "epoch": 2.30912,
      "grad_norm": 0.004828052595257759,
      "learning_rate": 4.606080000000001e-06,
      "loss": 0.0002,
      "step": 72160
    },
    {
      "epoch": 2.30944,
      "grad_norm": 0.11442185193300247,
      "learning_rate": 4.603946666666667e-06,
      "loss": 0.0003,
      "step": 72170
    },
    {
      "epoch": 2.30976,
      "grad_norm": 0.011161926202476025,
      "learning_rate": 4.601813333333333e-06,
      "loss": 0.0002,
      "step": 72180
    },
    {
      "epoch": 2.31008,
      "grad_norm": 0.0054624611511826515,
      "learning_rate": 4.5996800000000005e-06,
      "loss": 0.0002,
      "step": 72190
    },
    {
      "epoch": 2.3104,
      "grad_norm": 0.0034628885332494974,
      "learning_rate": 4.597546666666667e-06,
      "loss": 0.0002,
      "step": 72200
    },
    {
      "epoch": 2.31072,
      "grad_norm": 0.00737645011395216,
      "learning_rate": 4.595413333333334e-06,
      "loss": 0.0002,
      "step": 72210
    },
    {
      "epoch": 2.31104,
      "grad_norm": 0.004063310567289591,
      "learning_rate": 4.593280000000001e-06,
      "loss": 0.0002,
      "step": 72220
    },
    {
      "epoch": 2.31136,
      "grad_norm": 0.014365916140377522,
      "learning_rate": 4.591146666666667e-06,
      "loss": 0.0002,
      "step": 72230
    },
    {
      "epoch": 2.31168,
      "grad_norm": 0.003270613495260477,
      "learning_rate": 4.589013333333333e-06,
      "loss": 0.0002,
      "step": 72240
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.0024009323678910732,
      "learning_rate": 4.58688e-06,
      "loss": 0.0003,
      "step": 72250
    },
    {
      "epoch": 2.31232,
      "grad_norm": 0.008808590471744537,
      "learning_rate": 4.584746666666667e-06,
      "loss": 0.0015,
      "step": 72260
    },
    {
      "epoch": 2.31264,
      "grad_norm": 0.00391103932633996,
      "learning_rate": 4.582613333333334e-06,
      "loss": 0.0002,
      "step": 72270
    },
    {
      "epoch": 2.31296,
      "grad_norm": 0.01230529323220253,
      "learning_rate": 4.580480000000001e-06,
      "loss": 0.0002,
      "step": 72280
    },
    {
      "epoch": 2.31328,
      "grad_norm": 0.004365683998912573,
      "learning_rate": 4.578346666666667e-06,
      "loss": 0.0002,
      "step": 72290
    },
    {
      "epoch": 2.3136,
      "grad_norm": 0.017525464296340942,
      "learning_rate": 4.576213333333334e-06,
      "loss": 0.0002,
      "step": 72300
    },
    {
      "epoch": 2.31392,
      "grad_norm": 0.0033300018403679132,
      "learning_rate": 4.57408e-06,
      "loss": 0.0488,
      "step": 72310
    },
    {
      "epoch": 2.31424,
      "grad_norm": 0.0031290764454752207,
      "learning_rate": 4.571946666666667e-06,
      "loss": 0.0002,
      "step": 72320
    },
    {
      "epoch": 2.31456,
      "grad_norm": 0.003080165944993496,
      "learning_rate": 4.5698133333333336e-06,
      "loss": 0.0172,
      "step": 72330
    },
    {
      "epoch": 2.31488,
      "grad_norm": 0.019691860303282738,
      "learning_rate": 4.567680000000001e-06,
      "loss": 0.0002,
      "step": 72340
    },
    {
      "epoch": 2.3152,
      "grad_norm": 0.004901683423668146,
      "learning_rate": 4.565546666666667e-06,
      "loss": 0.0003,
      "step": 72350
    },
    {
      "epoch": 2.3155200000000002,
      "grad_norm": 0.004494587890803814,
      "learning_rate": 4.563413333333334e-06,
      "loss": 0.0003,
      "step": 72360
    },
    {
      "epoch": 2.31584,
      "grad_norm": 0.0030018738470971584,
      "learning_rate": 4.56128e-06,
      "loss": 0.0485,
      "step": 72370
    },
    {
      "epoch": 2.31616,
      "grad_norm": 0.003947013057768345,
      "learning_rate": 4.559146666666667e-06,
      "loss": 0.0002,
      "step": 72380
    },
    {
      "epoch": 2.31648,
      "grad_norm": 0.0040430533699691296,
      "learning_rate": 4.5570133333333334e-06,
      "loss": 0.0003,
      "step": 72390
    },
    {
      "epoch": 2.3168,
      "grad_norm": 0.004878065083175898,
      "learning_rate": 4.5548800000000005e-06,
      "loss": 0.0211,
      "step": 72400
    },
    {
      "epoch": 2.31712,
      "grad_norm": 0.002385512227192521,
      "learning_rate": 4.552746666666667e-06,
      "loss": 0.0097,
      "step": 72410
    },
    {
      "epoch": 2.31744,
      "grad_norm": 0.002847554162144661,
      "learning_rate": 4.550613333333334e-06,
      "loss": 0.0011,
      "step": 72420
    },
    {
      "epoch": 2.31776,
      "grad_norm": 0.038695674389600754,
      "learning_rate": 4.54848e-06,
      "loss": 0.0003,
      "step": 72430
    },
    {
      "epoch": 2.31808,
      "grad_norm": 0.0042995307594537735,
      "learning_rate": 4.546346666666667e-06,
      "loss": 0.0002,
      "step": 72440
    },
    {
      "epoch": 2.3184,
      "grad_norm": 0.0030076010152697563,
      "learning_rate": 4.544213333333334e-06,
      "loss": 0.0002,
      "step": 72450
    },
    {
      "epoch": 2.31872,
      "grad_norm": 0.005131799262017012,
      "learning_rate": 4.54208e-06,
      "loss": 0.0105,
      "step": 72460
    },
    {
      "epoch": 2.31904,
      "grad_norm": 0.006676985416561365,
      "learning_rate": 4.539946666666667e-06,
      "loss": 0.0007,
      "step": 72470
    },
    {
      "epoch": 2.31936,
      "grad_norm": 0.0029173744842410088,
      "learning_rate": 4.537813333333334e-06,
      "loss": 0.0002,
      "step": 72480
    },
    {
      "epoch": 2.31968,
      "grad_norm": 0.00381901441141963,
      "learning_rate": 4.535680000000001e-06,
      "loss": 0.0002,
      "step": 72490
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.0036271032877266407,
      "learning_rate": 4.533546666666667e-06,
      "loss": 0.0007,
      "step": 72500
    },
    {
      "epoch": 2.32032,
      "grad_norm": 0.007245960179716349,
      "learning_rate": 4.531413333333334e-06,
      "loss": 0.0002,
      "step": 72510
    },
    {
      "epoch": 2.32064,
      "grad_norm": 0.006789421197026968,
      "learning_rate": 4.52928e-06,
      "loss": 0.0002,
      "step": 72520
    },
    {
      "epoch": 2.32096,
      "grad_norm": 0.007369539700448513,
      "learning_rate": 4.5271466666666665e-06,
      "loss": 0.0002,
      "step": 72530
    },
    {
      "epoch": 2.32128,
      "grad_norm": 0.006373705808073282,
      "learning_rate": 4.5250133333333336e-06,
      "loss": 0.0003,
      "step": 72540
    },
    {
      "epoch": 2.3216,
      "grad_norm": 0.0026725458446890116,
      "learning_rate": 4.522880000000001e-06,
      "loss": 0.0002,
      "step": 72550
    },
    {
      "epoch": 2.32192,
      "grad_norm": 0.0016876119188964367,
      "learning_rate": 4.520746666666667e-06,
      "loss": 0.0002,
      "step": 72560
    },
    {
      "epoch": 2.32224,
      "grad_norm": 0.007487953174859285,
      "learning_rate": 4.518613333333334e-06,
      "loss": 0.0002,
      "step": 72570
    },
    {
      "epoch": 2.32256,
      "grad_norm": 0.0036646253429353237,
      "learning_rate": 4.51648e-06,
      "loss": 0.0002,
      "step": 72580
    },
    {
      "epoch": 2.32288,
      "grad_norm": 5.565598487854004,
      "learning_rate": 4.514346666666667e-06,
      "loss": 0.0364,
      "step": 72590
    },
    {
      "epoch": 2.3232,
      "grad_norm": 0.011451798491179943,
      "learning_rate": 4.5122133333333335e-06,
      "loss": 0.0316,
      "step": 72600
    },
    {
      "epoch": 2.32352,
      "grad_norm": 0.0025291224010288715,
      "learning_rate": 4.5100800000000005e-06,
      "loss": 0.0002,
      "step": 72610
    },
    {
      "epoch": 2.32384,
      "grad_norm": 0.005960026755928993,
      "learning_rate": 4.507946666666667e-06,
      "loss": 0.0003,
      "step": 72620
    },
    {
      "epoch": 2.32416,
      "grad_norm": 0.0034441493917256594,
      "learning_rate": 4.505813333333334e-06,
      "loss": 0.0005,
      "step": 72630
    },
    {
      "epoch": 2.32448,
      "grad_norm": 0.0050794524140655994,
      "learning_rate": 4.50368e-06,
      "loss": 0.0429,
      "step": 72640
    },
    {
      "epoch": 2.3247999999999998,
      "grad_norm": 0.00430894922465086,
      "learning_rate": 4.501546666666667e-06,
      "loss": 0.0003,
      "step": 72650
    },
    {
      "epoch": 2.32512,
      "grad_norm": 0.029407227411866188,
      "learning_rate": 4.499413333333334e-06,
      "loss": 0.0003,
      "step": 72660
    },
    {
      "epoch": 2.32544,
      "grad_norm": 0.0030455482192337513,
      "learning_rate": 4.49728e-06,
      "loss": 0.0002,
      "step": 72670
    },
    {
      "epoch": 2.32576,
      "grad_norm": 0.010462327860295773,
      "learning_rate": 4.4951466666666675e-06,
      "loss": 0.0003,
      "step": 72680
    },
    {
      "epoch": 2.32608,
      "grad_norm": 0.002997901290655136,
      "learning_rate": 4.493013333333334e-06,
      "loss": 0.0447,
      "step": 72690
    },
    {
      "epoch": 2.3264,
      "grad_norm": 0.004378346726298332,
      "learning_rate": 4.49088e-06,
      "loss": 0.0161,
      "step": 72700
    },
    {
      "epoch": 2.32672,
      "grad_norm": 0.0058484370820224285,
      "learning_rate": 4.488746666666667e-06,
      "loss": 0.0003,
      "step": 72710
    },
    {
      "epoch": 2.32704,
      "grad_norm": 0.010937435552477837,
      "learning_rate": 4.486613333333334e-06,
      "loss": 0.0003,
      "step": 72720
    },
    {
      "epoch": 2.32736,
      "grad_norm": 0.006960459053516388,
      "learning_rate": 4.48448e-06,
      "loss": 0.0507,
      "step": 72730
    },
    {
      "epoch": 2.32768,
      "grad_norm": 0.00849337037652731,
      "learning_rate": 4.482346666666667e-06,
      "loss": 0.0633,
      "step": 72740
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.005916784983128309,
      "learning_rate": 4.480213333333334e-06,
      "loss": 0.0003,
      "step": 72750
    },
    {
      "epoch": 2.32832,
      "grad_norm": 0.002490810351446271,
      "learning_rate": 4.47808e-06,
      "loss": 0.0002,
      "step": 72760
    },
    {
      "epoch": 2.32864,
      "grad_norm": 0.004469531588256359,
      "learning_rate": 4.475946666666667e-06,
      "loss": 0.0002,
      "step": 72770
    },
    {
      "epoch": 2.32896,
      "grad_norm": 0.023244597017765045,
      "learning_rate": 4.473813333333334e-06,
      "loss": 0.0003,
      "step": 72780
    },
    {
      "epoch": 2.32928,
      "grad_norm": 0.004819914698600769,
      "learning_rate": 4.47168e-06,
      "loss": 0.0002,
      "step": 72790
    },
    {
      "epoch": 2.3296,
      "grad_norm": 0.016736768186092377,
      "learning_rate": 4.469546666666667e-06,
      "loss": 0.001,
      "step": 72800
    },
    {
      "epoch": 2.32992,
      "grad_norm": 0.011850535869598389,
      "learning_rate": 4.4674133333333335e-06,
      "loss": 0.0002,
      "step": 72810
    },
    {
      "epoch": 2.33024,
      "grad_norm": 0.008002884685993195,
      "learning_rate": 4.4652800000000006e-06,
      "loss": 0.0316,
      "step": 72820
    },
    {
      "epoch": 2.33056,
      "grad_norm": 0.00781707651913166,
      "learning_rate": 4.463146666666668e-06,
      "loss": 0.0078,
      "step": 72830
    },
    {
      "epoch": 2.33088,
      "grad_norm": 0.006380601320415735,
      "learning_rate": 4.461013333333334e-06,
      "loss": 0.0002,
      "step": 72840
    },
    {
      "epoch": 2.3312,
      "grad_norm": 0.003926882054656744,
      "learning_rate": 4.45888e-06,
      "loss": 0.0002,
      "step": 72850
    },
    {
      "epoch": 2.33152,
      "grad_norm": 0.003546744352206588,
      "learning_rate": 4.456746666666667e-06,
      "loss": 0.0325,
      "step": 72860
    },
    {
      "epoch": 2.33184,
      "grad_norm": 7.312349796295166,
      "learning_rate": 4.454613333333333e-06,
      "loss": 0.0381,
      "step": 72870
    },
    {
      "epoch": 2.33216,
      "grad_norm": 0.010853954590857029,
      "learning_rate": 4.4524800000000004e-06,
      "loss": 0.0002,
      "step": 72880
    },
    {
      "epoch": 2.33248,
      "grad_norm": 0.00789896585047245,
      "learning_rate": 4.4503466666666675e-06,
      "loss": 0.0328,
      "step": 72890
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 0.002949727699160576,
      "learning_rate": 4.448213333333334e-06,
      "loss": 0.0228,
      "step": 72900
    },
    {
      "epoch": 2.33312,
      "grad_norm": 0.010288191959261894,
      "learning_rate": 4.446080000000001e-06,
      "loss": 0.0004,
      "step": 72910
    },
    {
      "epoch": 2.33344,
      "grad_norm": 0.004238293971866369,
      "learning_rate": 4.443946666666667e-06,
      "loss": 0.0002,
      "step": 72920
    },
    {
      "epoch": 2.33376,
      "grad_norm": 0.0018793867202475667,
      "learning_rate": 4.441813333333333e-06,
      "loss": 0.0011,
      "step": 72930
    },
    {
      "epoch": 2.33408,
      "grad_norm": 0.014678350649774075,
      "learning_rate": 4.43968e-06,
      "loss": 0.0008,
      "step": 72940
    },
    {
      "epoch": 2.3344,
      "grad_norm": 0.006953058764338493,
      "learning_rate": 4.437546666666667e-06,
      "loss": 0.0022,
      "step": 72950
    },
    {
      "epoch": 2.33472,
      "grad_norm": 0.012508598156273365,
      "learning_rate": 4.435413333333334e-06,
      "loss": 0.0011,
      "step": 72960
    },
    {
      "epoch": 2.3350400000000002,
      "grad_norm": 0.008039084263145924,
      "learning_rate": 4.433280000000001e-06,
      "loss": 0.0003,
      "step": 72970
    },
    {
      "epoch": 2.33536,
      "grad_norm": 0.007537401746958494,
      "learning_rate": 4.431146666666667e-06,
      "loss": 0.0002,
      "step": 72980
    },
    {
      "epoch": 2.33568,
      "grad_norm": 0.003906506113708019,
      "learning_rate": 4.429013333333333e-06,
      "loss": 0.0002,
      "step": 72990
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.01468002051115036,
      "learning_rate": 4.42688e-06,
      "loss": 0.0005,
      "step": 73000
    },
    {
      "epoch": 2.33632,
      "grad_norm": 0.003835987765341997,
      "learning_rate": 4.424746666666667e-06,
      "loss": 0.0649,
      "step": 73010
    },
    {
      "epoch": 2.33664,
      "grad_norm": 0.003542229300364852,
      "learning_rate": 4.4226133333333335e-06,
      "loss": 0.0002,
      "step": 73020
    },
    {
      "epoch": 2.33696,
      "grad_norm": 0.016212550923228264,
      "learning_rate": 4.420480000000001e-06,
      "loss": 0.0003,
      "step": 73030
    },
    {
      "epoch": 2.33728,
      "grad_norm": 0.0018441108986735344,
      "learning_rate": 4.418346666666667e-06,
      "loss": 0.0446,
      "step": 73040
    },
    {
      "epoch": 2.3376,
      "grad_norm": 0.004566890187561512,
      "learning_rate": 4.416213333333334e-06,
      "loss": 0.0003,
      "step": 73050
    },
    {
      "epoch": 2.33792,
      "grad_norm": 0.006491457112133503,
      "learning_rate": 4.414080000000001e-06,
      "loss": 0.0004,
      "step": 73060
    },
    {
      "epoch": 2.33824,
      "grad_norm": 0.00555872218683362,
      "learning_rate": 4.411946666666667e-06,
      "loss": 0.0002,
      "step": 73070
    },
    {
      "epoch": 2.33856,
      "grad_norm": 0.0068979267962276936,
      "learning_rate": 4.409813333333333e-06,
      "loss": 0.0004,
      "step": 73080
    },
    {
      "epoch": 2.33888,
      "grad_norm": 0.0061232950538396835,
      "learning_rate": 4.4076800000000005e-06,
      "loss": 0.0498,
      "step": 73090
    },
    {
      "epoch": 2.3392,
      "grad_norm": 0.0054709031246602535,
      "learning_rate": 4.405546666666667e-06,
      "loss": 0.0032,
      "step": 73100
    },
    {
      "epoch": 2.33952,
      "grad_norm": 0.03860551863908768,
      "learning_rate": 4.403413333333334e-06,
      "loss": 0.0003,
      "step": 73110
    },
    {
      "epoch": 2.33984,
      "grad_norm": 0.09593037515878677,
      "learning_rate": 4.401280000000001e-06,
      "loss": 0.0005,
      "step": 73120
    },
    {
      "epoch": 2.34016,
      "grad_norm": 0.03508433327078819,
      "learning_rate": 4.399146666666667e-06,
      "loss": 0.0007,
      "step": 73130
    },
    {
      "epoch": 2.34048,
      "grad_norm": 0.004876478109508753,
      "learning_rate": 4.397013333333333e-06,
      "loss": 0.0002,
      "step": 73140
    },
    {
      "epoch": 2.3407999999999998,
      "grad_norm": 0.005552999675273895,
      "learning_rate": 4.39488e-06,
      "loss": 0.0002,
      "step": 73150
    },
    {
      "epoch": 2.34112,
      "grad_norm": 0.009603550657629967,
      "learning_rate": 4.3927466666666666e-06,
      "loss": 0.0002,
      "step": 73160
    },
    {
      "epoch": 2.34144,
      "grad_norm": 5.402574062347412,
      "learning_rate": 4.390613333333334e-06,
      "loss": 0.0071,
      "step": 73170
    },
    {
      "epoch": 2.34176,
      "grad_norm": 0.010468807071447372,
      "learning_rate": 4.388480000000001e-06,
      "loss": 0.0299,
      "step": 73180
    },
    {
      "epoch": 2.34208,
      "grad_norm": 0.008805393241345882,
      "learning_rate": 4.386346666666667e-06,
      "loss": 0.0004,
      "step": 73190
    },
    {
      "epoch": 2.3424,
      "grad_norm": 0.0057362657971680164,
      "learning_rate": 4.384213333333334e-06,
      "loss": 0.0003,
      "step": 73200
    },
    {
      "epoch": 2.34272,
      "grad_norm": 0.21285757422447205,
      "learning_rate": 4.38208e-06,
      "loss": 0.0008,
      "step": 73210
    },
    {
      "epoch": 2.3430400000000002,
      "grad_norm": 0.004042138811200857,
      "learning_rate": 4.3799466666666665e-06,
      "loss": 0.0137,
      "step": 73220
    },
    {
      "epoch": 2.34336,
      "grad_norm": 0.003866806859150529,
      "learning_rate": 4.3778133333333335e-06,
      "loss": 0.0004,
      "step": 73230
    },
    {
      "epoch": 2.34368,
      "grad_norm": 0.00945739634335041,
      "learning_rate": 4.375680000000001e-06,
      "loss": 0.0316,
      "step": 73240
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.008660550229251385,
      "learning_rate": 4.373546666666667e-06,
      "loss": 0.0523,
      "step": 73250
    },
    {
      "epoch": 2.34432,
      "grad_norm": 0.010927212424576283,
      "learning_rate": 4.371413333333334e-06,
      "loss": 0.0003,
      "step": 73260
    },
    {
      "epoch": 2.34464,
      "grad_norm": 0.008460274897515774,
      "learning_rate": 4.36928e-06,
      "loss": 0.0536,
      "step": 73270
    },
    {
      "epoch": 2.34496,
      "grad_norm": 0.004746591672301292,
      "learning_rate": 4.367146666666666e-06,
      "loss": 0.0288,
      "step": 73280
    },
    {
      "epoch": 2.34528,
      "grad_norm": 0.009640509262681007,
      "learning_rate": 4.365013333333334e-06,
      "loss": 0.0005,
      "step": 73290
    },
    {
      "epoch": 2.3456,
      "grad_norm": 0.012166931293904781,
      "learning_rate": 4.3628800000000005e-06,
      "loss": 0.0004,
      "step": 73300
    },
    {
      "epoch": 2.34592,
      "grad_norm": 0.0036746396217495203,
      "learning_rate": 4.360746666666667e-06,
      "loss": 0.0675,
      "step": 73310
    },
    {
      "epoch": 2.34624,
      "grad_norm": 0.014519866555929184,
      "learning_rate": 4.358613333333334e-06,
      "loss": 0.003,
      "step": 73320
    },
    {
      "epoch": 2.34656,
      "grad_norm": 0.007479404099285603,
      "learning_rate": 4.35648e-06,
      "loss": 0.031,
      "step": 73330
    },
    {
      "epoch": 2.34688,
      "grad_norm": 0.0077846297062933445,
      "learning_rate": 4.354346666666667e-06,
      "loss": 0.0004,
      "step": 73340
    },
    {
      "epoch": 2.3472,
      "grad_norm": 0.031135233119130135,
      "learning_rate": 4.352213333333334e-06,
      "loss": 0.0004,
      "step": 73350
    },
    {
      "epoch": 2.34752,
      "grad_norm": 0.0071984538808465,
      "learning_rate": 4.35008e-06,
      "loss": 0.0005,
      "step": 73360
    },
    {
      "epoch": 2.34784,
      "grad_norm": 0.004342762287706137,
      "learning_rate": 4.347946666666667e-06,
      "loss": 0.1138,
      "step": 73370
    },
    {
      "epoch": 2.34816,
      "grad_norm": 0.03456076607108116,
      "learning_rate": 4.345813333333334e-06,
      "loss": 0.0003,
      "step": 73380
    },
    {
      "epoch": 2.34848,
      "grad_norm": 0.0038371197879314423,
      "learning_rate": 4.34368e-06,
      "loss": 0.0004,
      "step": 73390
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 0.004645871929824352,
      "learning_rate": 4.341546666666667e-06,
      "loss": 0.0003,
      "step": 73400
    },
    {
      "epoch": 2.34912,
      "grad_norm": 0.006450477056205273,
      "learning_rate": 4.339413333333334e-06,
      "loss": 0.0003,
      "step": 73410
    },
    {
      "epoch": 2.34944,
      "grad_norm": 0.007241011597216129,
      "learning_rate": 4.33728e-06,
      "loss": 0.0003,
      "step": 73420
    },
    {
      "epoch": 2.34976,
      "grad_norm": 0.010798643343150616,
      "learning_rate": 4.335146666666667e-06,
      "loss": 0.0004,
      "step": 73430
    },
    {
      "epoch": 2.35008,
      "grad_norm": 0.02317996323108673,
      "learning_rate": 4.3330133333333336e-06,
      "loss": 0.0003,
      "step": 73440
    },
    {
      "epoch": 2.3504,
      "grad_norm": 0.005732486955821514,
      "learning_rate": 4.33088e-06,
      "loss": 0.0007,
      "step": 73450
    },
    {
      "epoch": 2.35072,
      "grad_norm": 0.007881097495555878,
      "learning_rate": 4.328746666666667e-06,
      "loss": 0.0003,
      "step": 73460
    },
    {
      "epoch": 2.3510400000000002,
      "grad_norm": 0.11564663797616959,
      "learning_rate": 4.326613333333334e-06,
      "loss": 0.0005,
      "step": 73470
    },
    {
      "epoch": 2.35136,
      "grad_norm": 0.014551748521625996,
      "learning_rate": 4.32448e-06,
      "loss": 0.0002,
      "step": 73480
    },
    {
      "epoch": 2.35168,
      "grad_norm": 0.0032926315907388926,
      "learning_rate": 4.322346666666667e-06,
      "loss": 0.0005,
      "step": 73490
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.006887282244861126,
      "learning_rate": 4.3202133333333334e-06,
      "loss": 0.0004,
      "step": 73500
    },
    {
      "epoch": 2.35232,
      "grad_norm": 0.018516039475798607,
      "learning_rate": 4.3180800000000005e-06,
      "loss": 0.0004,
      "step": 73510
    },
    {
      "epoch": 2.35264,
      "grad_norm": 0.37122994661331177,
      "learning_rate": 4.315946666666668e-06,
      "loss": 0.0006,
      "step": 73520
    },
    {
      "epoch": 2.35296,
      "grad_norm": 0.005337984301149845,
      "learning_rate": 4.313813333333334e-06,
      "loss": 0.0241,
      "step": 73530
    },
    {
      "epoch": 2.35328,
      "grad_norm": 0.016083503141999245,
      "learning_rate": 4.31168e-06,
      "loss": 0.0004,
      "step": 73540
    },
    {
      "epoch": 2.3536,
      "grad_norm": 0.007017130497843027,
      "learning_rate": 4.309546666666667e-06,
      "loss": 0.0006,
      "step": 73550
    },
    {
      "epoch": 2.35392,
      "grad_norm": 0.012256563641130924,
      "learning_rate": 4.307413333333333e-06,
      "loss": 0.0004,
      "step": 73560
    },
    {
      "epoch": 2.35424,
      "grad_norm": 0.010183083824813366,
      "learning_rate": 4.30528e-06,
      "loss": 0.0325,
      "step": 73570
    },
    {
      "epoch": 2.35456,
      "grad_norm": 0.11137942969799042,
      "learning_rate": 4.3031466666666675e-06,
      "loss": 0.0003,
      "step": 73580
    },
    {
      "epoch": 2.35488,
      "grad_norm": 0.007679955568164587,
      "learning_rate": 4.301013333333334e-06,
      "loss": 0.0003,
      "step": 73590
    },
    {
      "epoch": 2.3552,
      "grad_norm": 0.009244482964277267,
      "learning_rate": 4.29888e-06,
      "loss": 0.0003,
      "step": 73600
    },
    {
      "epoch": 2.35552,
      "grad_norm": 0.0019412157125771046,
      "learning_rate": 4.296746666666667e-06,
      "loss": 0.0278,
      "step": 73610
    },
    {
      "epoch": 2.35584,
      "grad_norm": 0.03353855013847351,
      "learning_rate": 4.294613333333333e-06,
      "loss": 0.0003,
      "step": 73620
    },
    {
      "epoch": 2.35616,
      "grad_norm": 0.0039823963306844234,
      "learning_rate": 4.29248e-06,
      "loss": 0.0006,
      "step": 73630
    },
    {
      "epoch": 2.35648,
      "grad_norm": 0.005231222603470087,
      "learning_rate": 4.290346666666667e-06,
      "loss": 0.0514,
      "step": 73640
    },
    {
      "epoch": 2.3568,
      "grad_norm": 0.012690686620771885,
      "learning_rate": 4.2882133333333336e-06,
      "loss": 0.0003,
      "step": 73650
    },
    {
      "epoch": 2.35712,
      "grad_norm": 0.0037425963673740625,
      "learning_rate": 4.286080000000001e-06,
      "loss": 0.0095,
      "step": 73660
    },
    {
      "epoch": 2.35744,
      "grad_norm": 0.0036556760314852,
      "learning_rate": 4.283946666666667e-06,
      "loss": 0.0003,
      "step": 73670
    },
    {
      "epoch": 2.35776,
      "grad_norm": 0.007106867153197527,
      "learning_rate": 4.281813333333334e-06,
      "loss": 0.0003,
      "step": 73680
    },
    {
      "epoch": 2.35808,
      "grad_norm": 0.004098284989595413,
      "learning_rate": 4.27968e-06,
      "loss": 0.0464,
      "step": 73690
    },
    {
      "epoch": 2.3584,
      "grad_norm": 0.006241008173674345,
      "learning_rate": 4.277546666666667e-06,
      "loss": 0.0004,
      "step": 73700
    },
    {
      "epoch": 2.35872,
      "grad_norm": 0.011048583313822746,
      "learning_rate": 4.2754133333333335e-06,
      "loss": 0.0107,
      "step": 73710
    },
    {
      "epoch": 2.3590400000000002,
      "grad_norm": 0.010311488062143326,
      "learning_rate": 4.2732800000000005e-06,
      "loss": 0.0487,
      "step": 73720
    },
    {
      "epoch": 2.35936,
      "grad_norm": 0.009405961260199547,
      "learning_rate": 4.271146666666667e-06,
      "loss": 0.0005,
      "step": 73730
    },
    {
      "epoch": 2.35968,
      "grad_norm": 7.831635475158691,
      "learning_rate": 4.269013333333334e-06,
      "loss": 0.0656,
      "step": 73740
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.016914045438170433,
      "learning_rate": 4.26688e-06,
      "loss": 0.0005,
      "step": 73750
    },
    {
      "epoch": 2.3603199999999998,
      "grad_norm": 0.004810314159840345,
      "learning_rate": 4.264746666666667e-06,
      "loss": 0.0003,
      "step": 73760
    },
    {
      "epoch": 2.36064,
      "grad_norm": 0.002699717180803418,
      "learning_rate": 4.262613333333333e-06,
      "loss": 0.0003,
      "step": 73770
    },
    {
      "epoch": 2.36096,
      "grad_norm": 4.451337814331055,
      "learning_rate": 4.26048e-06,
      "loss": 0.0594,
      "step": 73780
    },
    {
      "epoch": 2.36128,
      "grad_norm": 0.07987769693136215,
      "learning_rate": 4.258346666666667e-06,
      "loss": 0.0006,
      "step": 73790
    },
    {
      "epoch": 2.3616,
      "grad_norm": 0.005885755643248558,
      "learning_rate": 4.256213333333334e-06,
      "loss": 0.0003,
      "step": 73800
    },
    {
      "epoch": 2.36192,
      "grad_norm": 0.007092724554240704,
      "learning_rate": 4.254080000000001e-06,
      "loss": 0.0203,
      "step": 73810
    },
    {
      "epoch": 2.36224,
      "grad_norm": 0.012250151485204697,
      "learning_rate": 4.251946666666667e-06,
      "loss": 0.0003,
      "step": 73820
    },
    {
      "epoch": 2.36256,
      "grad_norm": 0.01053079403936863,
      "learning_rate": 4.249813333333333e-06,
      "loss": 0.0006,
      "step": 73830
    },
    {
      "epoch": 2.36288,
      "grad_norm": 0.003377765417098999,
      "learning_rate": 4.24768e-06,
      "loss": 0.0032,
      "step": 73840
    },
    {
      "epoch": 2.3632,
      "grad_norm": 0.005912153050303459,
      "learning_rate": 4.245546666666667e-06,
      "loss": 0.0394,
      "step": 73850
    },
    {
      "epoch": 2.36352,
      "grad_norm": 0.005392560735344887,
      "learning_rate": 4.243413333333334e-06,
      "loss": 0.0004,
      "step": 73860
    },
    {
      "epoch": 2.36384,
      "grad_norm": 0.006019633263349533,
      "learning_rate": 4.241280000000001e-06,
      "loss": 0.0005,
      "step": 73870
    },
    {
      "epoch": 2.36416,
      "grad_norm": 0.004722535144537687,
      "learning_rate": 4.239146666666667e-06,
      "loss": 0.0019,
      "step": 73880
    },
    {
      "epoch": 2.36448,
      "grad_norm": 0.03785402700304985,
      "learning_rate": 4.237013333333333e-06,
      "loss": 0.0157,
      "step": 73890
    },
    {
      "epoch": 2.3648,
      "grad_norm": 0.00322426063939929,
      "learning_rate": 4.23488e-06,
      "loss": 0.0447,
      "step": 73900
    },
    {
      "epoch": 2.36512,
      "grad_norm": 0.02952832542359829,
      "learning_rate": 4.232746666666667e-06,
      "loss": 0.0003,
      "step": 73910
    },
    {
      "epoch": 2.36544,
      "grad_norm": 0.011893442831933498,
      "learning_rate": 4.2306133333333335e-06,
      "loss": 0.0003,
      "step": 73920
    },
    {
      "epoch": 2.36576,
      "grad_norm": 0.005853462032973766,
      "learning_rate": 4.2284800000000006e-06,
      "loss": 0.0003,
      "step": 73930
    },
    {
      "epoch": 2.36608,
      "grad_norm": 0.005940428469330072,
      "learning_rate": 4.226346666666667e-06,
      "loss": 0.0023,
      "step": 73940
    },
    {
      "epoch": 2.3664,
      "grad_norm": 0.004979540128260851,
      "learning_rate": 4.224213333333334e-06,
      "loss": 0.0003,
      "step": 73950
    },
    {
      "epoch": 2.36672,
      "grad_norm": 0.007328106090426445,
      "learning_rate": 4.222080000000001e-06,
      "loss": 0.0003,
      "step": 73960
    },
    {
      "epoch": 2.36704,
      "grad_norm": 0.00378288677893579,
      "learning_rate": 4.219946666666667e-06,
      "loss": 0.0003,
      "step": 73970
    },
    {
      "epoch": 2.36736,
      "grad_norm": 0.008296413347125053,
      "learning_rate": 4.217813333333333e-06,
      "loss": 0.0277,
      "step": 73980
    },
    {
      "epoch": 2.36768,
      "grad_norm": 0.008026895113289356,
      "learning_rate": 4.2156800000000004e-06,
      "loss": 0.0003,
      "step": 73990
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.008465057238936424,
      "learning_rate": 4.213546666666667e-06,
      "loss": 0.0003,
      "step": 74000
    },
    {
      "epoch": 2.3683199999999998,
      "grad_norm": 0.04883255809545517,
      "learning_rate": 4.211413333333334e-06,
      "loss": 0.0003,
      "step": 74010
    },
    {
      "epoch": 2.36864,
      "grad_norm": 0.0070087602362036705,
      "learning_rate": 4.209280000000001e-06,
      "loss": 0.0232,
      "step": 74020
    },
    {
      "epoch": 2.36896,
      "grad_norm": 0.009286719374358654,
      "learning_rate": 4.207146666666667e-06,
      "loss": 0.0003,
      "step": 74030
    },
    {
      "epoch": 2.36928,
      "grad_norm": 0.004599316976964474,
      "learning_rate": 4.205013333333334e-06,
      "loss": 0.0003,
      "step": 74040
    },
    {
      "epoch": 2.3696,
      "grad_norm": 0.2323453575372696,
      "learning_rate": 4.20288e-06,
      "loss": 0.0377,
      "step": 74050
    },
    {
      "epoch": 2.36992,
      "grad_norm": 0.008776342496275902,
      "learning_rate": 4.2007466666666665e-06,
      "loss": 0.0003,
      "step": 74060
    },
    {
      "epoch": 2.37024,
      "grad_norm": 0.0036986330524086952,
      "learning_rate": 4.198613333333334e-06,
      "loss": 0.0004,
      "step": 74070
    },
    {
      "epoch": 2.3705600000000002,
      "grad_norm": 0.5167164206504822,
      "learning_rate": 4.196480000000001e-06,
      "loss": 0.0013,
      "step": 74080
    },
    {
      "epoch": 2.37088,
      "grad_norm": 0.004475962836295366,
      "learning_rate": 4.194346666666667e-06,
      "loss": 0.0008,
      "step": 74090
    },
    {
      "epoch": 2.3712,
      "grad_norm": 0.008921314030885696,
      "learning_rate": 4.192213333333334e-06,
      "loss": 0.0003,
      "step": 74100
    },
    {
      "epoch": 2.37152,
      "grad_norm": 0.0036994782276451588,
      "learning_rate": 4.19008e-06,
      "loss": 0.0002,
      "step": 74110
    },
    {
      "epoch": 2.37184,
      "grad_norm": 5.234808921813965,
      "learning_rate": 4.1879466666666664e-06,
      "loss": 0.0142,
      "step": 74120
    },
    {
      "epoch": 2.37216,
      "grad_norm": 0.004053820855915546,
      "learning_rate": 4.185813333333334e-06,
      "loss": 0.0003,
      "step": 74130
    },
    {
      "epoch": 2.37248,
      "grad_norm": 1.7760095596313477,
      "learning_rate": 4.1836800000000006e-06,
      "loss": 0.0492,
      "step": 74140
    },
    {
      "epoch": 2.3728,
      "grad_norm": 0.010015260428190231,
      "learning_rate": 4.181546666666667e-06,
      "loss": 0.006,
      "step": 74150
    },
    {
      "epoch": 2.37312,
      "grad_norm": 0.02693330869078636,
      "learning_rate": 4.179413333333334e-06,
      "loss": 0.0003,
      "step": 74160
    },
    {
      "epoch": 2.37344,
      "grad_norm": 0.005134911276400089,
      "learning_rate": 4.17728e-06,
      "loss": 0.0003,
      "step": 74170
    },
    {
      "epoch": 2.37376,
      "grad_norm": 0.008264678530395031,
      "learning_rate": 4.175146666666667e-06,
      "loss": 0.0003,
      "step": 74180
    },
    {
      "epoch": 2.37408,
      "grad_norm": 0.006915755104273558,
      "learning_rate": 4.173013333333334e-06,
      "loss": 0.0003,
      "step": 74190
    },
    {
      "epoch": 2.3744,
      "grad_norm": 0.00726292235776782,
      "learning_rate": 4.1708800000000005e-06,
      "loss": 0.0003,
      "step": 74200
    },
    {
      "epoch": 2.37472,
      "grad_norm": 0.004194587003439665,
      "learning_rate": 4.168746666666667e-06,
      "loss": 0.0004,
      "step": 74210
    },
    {
      "epoch": 2.37504,
      "grad_norm": 0.0069493986666202545,
      "learning_rate": 4.166613333333334e-06,
      "loss": 0.0012,
      "step": 74220
    },
    {
      "epoch": 2.37536,
      "grad_norm": 0.004889575764536858,
      "learning_rate": 4.16448e-06,
      "loss": 0.0002,
      "step": 74230
    },
    {
      "epoch": 2.37568,
      "grad_norm": 0.007554629817605019,
      "learning_rate": 4.162346666666667e-06,
      "loss": 0.0003,
      "step": 74240
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.017347674816846848,
      "learning_rate": 4.160213333333334e-06,
      "loss": 0.0005,
      "step": 74250
    },
    {
      "epoch": 2.3763199999999998,
      "grad_norm": 0.009231096133589745,
      "learning_rate": 4.15808e-06,
      "loss": 0.0004,
      "step": 74260
    },
    {
      "epoch": 2.37664,
      "grad_norm": 0.013398058712482452,
      "learning_rate": 4.155946666666667e-06,
      "loss": 0.0004,
      "step": 74270
    },
    {
      "epoch": 2.37696,
      "grad_norm": 0.010911339893937111,
      "learning_rate": 4.153813333333334e-06,
      "loss": 0.0004,
      "step": 74280
    },
    {
      "epoch": 2.37728,
      "grad_norm": 0.003270863089710474,
      "learning_rate": 4.15168e-06,
      "loss": 0.0003,
      "step": 74290
    },
    {
      "epoch": 2.3776,
      "grad_norm": 0.0041115861386060715,
      "learning_rate": 4.149546666666667e-06,
      "loss": 0.0012,
      "step": 74300
    },
    {
      "epoch": 2.37792,
      "grad_norm": 0.011619293130934238,
      "learning_rate": 4.147413333333334e-06,
      "loss": 0.0005,
      "step": 74310
    },
    {
      "epoch": 2.37824,
      "grad_norm": 0.0030746490228921175,
      "learning_rate": 4.14528e-06,
      "loss": 0.0002,
      "step": 74320
    },
    {
      "epoch": 2.3785600000000002,
      "grad_norm": 0.004275219980627298,
      "learning_rate": 4.143146666666667e-06,
      "loss": 0.0002,
      "step": 74330
    },
    {
      "epoch": 2.37888,
      "grad_norm": 0.007073089014738798,
      "learning_rate": 4.1410133333333335e-06,
      "loss": 0.0005,
      "step": 74340
    },
    {
      "epoch": 2.3792,
      "grad_norm": 0.0058466773480176926,
      "learning_rate": 4.13888e-06,
      "loss": 0.0002,
      "step": 74350
    },
    {
      "epoch": 2.37952,
      "grad_norm": 0.0030366890132427216,
      "learning_rate": 4.136746666666667e-06,
      "loss": 0.0003,
      "step": 74360
    },
    {
      "epoch": 2.37984,
      "grad_norm": 0.021424785256385803,
      "learning_rate": 4.134613333333334e-06,
      "loss": 0.0102,
      "step": 74370
    },
    {
      "epoch": 2.38016,
      "grad_norm": 0.0034647921565920115,
      "learning_rate": 4.13248e-06,
      "loss": 0.0007,
      "step": 74380
    },
    {
      "epoch": 2.38048,
      "grad_norm": 0.0032119471579790115,
      "learning_rate": 4.130346666666667e-06,
      "loss": 0.0003,
      "step": 74390
    },
    {
      "epoch": 2.3808,
      "grad_norm": 0.004757605958729982,
      "learning_rate": 4.128213333333333e-06,
      "loss": 0.0042,
      "step": 74400
    },
    {
      "epoch": 2.38112,
      "grad_norm": 0.010878372937440872,
      "learning_rate": 4.1260800000000005e-06,
      "loss": 0.0262,
      "step": 74410
    },
    {
      "epoch": 2.38144,
      "grad_norm": 0.003730383701622486,
      "learning_rate": 4.1239466666666676e-06,
      "loss": 0.0003,
      "step": 74420
    },
    {
      "epoch": 2.38176,
      "grad_norm": 0.0027604198548942804,
      "learning_rate": 4.121813333333334e-06,
      "loss": 0.0002,
      "step": 74430
    },
    {
      "epoch": 2.38208,
      "grad_norm": 1.6847598552703857,
      "learning_rate": 4.11968e-06,
      "loss": 0.0458,
      "step": 74440
    },
    {
      "epoch": 2.3824,
      "grad_norm": 0.003728096606209874,
      "learning_rate": 4.117546666666667e-06,
      "loss": 0.0003,
      "step": 74450
    },
    {
      "epoch": 2.38272,
      "grad_norm": 0.006909852847456932,
      "learning_rate": 4.115413333333333e-06,
      "loss": 0.0012,
      "step": 74460
    },
    {
      "epoch": 2.38304,
      "grad_norm": 0.007066171616315842,
      "learning_rate": 4.11328e-06,
      "loss": 0.0002,
      "step": 74470
    },
    {
      "epoch": 2.38336,
      "grad_norm": 0.006189970299601555,
      "learning_rate": 4.1111466666666674e-06,
      "loss": 0.0007,
      "step": 74480
    },
    {
      "epoch": 2.38368,
      "grad_norm": 0.0027923386078327894,
      "learning_rate": 4.109013333333334e-06,
      "loss": 0.0007,
      "step": 74490
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.005415195599198341,
      "learning_rate": 4.106880000000001e-06,
      "loss": 0.0002,
      "step": 74500
    },
    {
      "epoch": 2.3843199999999998,
      "grad_norm": 0.0032418908085674047,
      "learning_rate": 4.104746666666667e-06,
      "loss": 0.0005,
      "step": 74510
    },
    {
      "epoch": 2.38464,
      "grad_norm": 0.008109590969979763,
      "learning_rate": 4.102613333333333e-06,
      "loss": 0.0004,
      "step": 74520
    },
    {
      "epoch": 2.38496,
      "grad_norm": 0.00394871411845088,
      "learning_rate": 4.10048e-06,
      "loss": 0.0003,
      "step": 74530
    },
    {
      "epoch": 2.38528,
      "grad_norm": 0.006350843235850334,
      "learning_rate": 4.098346666666667e-06,
      "loss": 0.0002,
      "step": 74540
    },
    {
      "epoch": 2.3856,
      "grad_norm": 0.00522499019280076,
      "learning_rate": 4.0962133333333335e-06,
      "loss": 0.0002,
      "step": 74550
    },
    {
      "epoch": 2.38592,
      "grad_norm": 0.012634414248168468,
      "learning_rate": 4.094080000000001e-06,
      "loss": 0.0003,
      "step": 74560
    },
    {
      "epoch": 2.38624,
      "grad_norm": 0.0070610251277685165,
      "learning_rate": 4.091946666666667e-06,
      "loss": 0.0007,
      "step": 74570
    },
    {
      "epoch": 2.3865600000000002,
      "grad_norm": 0.007477655541151762,
      "learning_rate": 4.089813333333333e-06,
      "loss": 0.0002,
      "step": 74580
    },
    {
      "epoch": 2.38688,
      "grad_norm": 0.005750823300331831,
      "learning_rate": 4.08768e-06,
      "loss": 0.0004,
      "step": 74590
    },
    {
      "epoch": 2.3872,
      "grad_norm": 0.002431666012853384,
      "learning_rate": 4.085546666666667e-06,
      "loss": 0.0002,
      "step": 74600
    },
    {
      "epoch": 2.38752,
      "grad_norm": 0.00410945201292634,
      "learning_rate": 4.0834133333333334e-06,
      "loss": 0.0003,
      "step": 74610
    },
    {
      "epoch": 2.38784,
      "grad_norm": 0.03228297829627991,
      "learning_rate": 4.0812800000000005e-06,
      "loss": 0.0005,
      "step": 74620
    },
    {
      "epoch": 2.38816,
      "grad_norm": 0.0071622771210968494,
      "learning_rate": 4.079146666666667e-06,
      "loss": 0.0003,
      "step": 74630
    },
    {
      "epoch": 2.38848,
      "grad_norm": 0.008459295146167278,
      "learning_rate": 4.077013333333334e-06,
      "loss": 0.0236,
      "step": 74640
    },
    {
      "epoch": 2.3888,
      "grad_norm": 0.01743891090154648,
      "learning_rate": 4.074880000000001e-06,
      "loss": 0.0271,
      "step": 74650
    },
    {
      "epoch": 2.38912,
      "grad_norm": 0.004959397949278355,
      "learning_rate": 4.072746666666667e-06,
      "loss": 0.0003,
      "step": 74660
    },
    {
      "epoch": 2.38944,
      "grad_norm": 0.002356340643018484,
      "learning_rate": 4.070613333333333e-06,
      "loss": 0.0002,
      "step": 74670
    },
    {
      "epoch": 2.38976,
      "grad_norm": 0.011477484367787838,
      "learning_rate": 4.06848e-06,
      "loss": 0.0002,
      "step": 74680
    },
    {
      "epoch": 2.39008,
      "grad_norm": 4.616028785705566,
      "learning_rate": 4.066346666666667e-06,
      "loss": 0.0049,
      "step": 74690
    },
    {
      "epoch": 2.3904,
      "grad_norm": 0.0025049911346286535,
      "learning_rate": 4.064213333333334e-06,
      "loss": 0.0003,
      "step": 74700
    },
    {
      "epoch": 2.39072,
      "grad_norm": 0.0053759412840008736,
      "learning_rate": 4.062080000000001e-06,
      "loss": 0.0004,
      "step": 74710
    },
    {
      "epoch": 2.39104,
      "grad_norm": 0.009747280739247799,
      "learning_rate": 4.059946666666667e-06,
      "loss": 0.0002,
      "step": 74720
    },
    {
      "epoch": 2.39136,
      "grad_norm": 0.002603270346298814,
      "learning_rate": 4.057813333333333e-06,
      "loss": 0.0003,
      "step": 74730
    },
    {
      "epoch": 2.39168,
      "grad_norm": 0.002985788742080331,
      "learning_rate": 4.05568e-06,
      "loss": 0.0006,
      "step": 74740
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.0021909712813794613,
      "learning_rate": 4.0535466666666665e-06,
      "loss": 0.0002,
      "step": 74750
    },
    {
      "epoch": 2.39232,
      "grad_norm": 0.004972050432115793,
      "learning_rate": 4.0514133333333336e-06,
      "loss": 0.0003,
      "step": 74760
    },
    {
      "epoch": 2.39264,
      "grad_norm": 0.0017656880663707852,
      "learning_rate": 4.049280000000001e-06,
      "loss": 0.0484,
      "step": 74770
    },
    {
      "epoch": 2.39296,
      "grad_norm": 0.006239047273993492,
      "learning_rate": 4.047146666666667e-06,
      "loss": 0.0002,
      "step": 74780
    },
    {
      "epoch": 2.39328,
      "grad_norm": 0.00716619286686182,
      "learning_rate": 4.045013333333334e-06,
      "loss": 0.0002,
      "step": 74790
    },
    {
      "epoch": 2.3936,
      "grad_norm": 0.027613583952188492,
      "learning_rate": 4.04288e-06,
      "loss": 0.029,
      "step": 74800
    },
    {
      "epoch": 2.39392,
      "grad_norm": 0.03799160197377205,
      "learning_rate": 4.040746666666667e-06,
      "loss": 0.0003,
      "step": 74810
    },
    {
      "epoch": 2.39424,
      "grad_norm": 0.009025306440889835,
      "learning_rate": 4.0386133333333335e-06,
      "loss": 0.0158,
      "step": 74820
    },
    {
      "epoch": 2.3945600000000002,
      "grad_norm": 0.008015863597393036,
      "learning_rate": 4.0364800000000005e-06,
      "loss": 0.0012,
      "step": 74830
    },
    {
      "epoch": 2.39488,
      "grad_norm": 0.006986377760767937,
      "learning_rate": 4.034346666666667e-06,
      "loss": 0.0003,
      "step": 74840
    },
    {
      "epoch": 2.3952,
      "grad_norm": 0.012048250995576382,
      "learning_rate": 4.032213333333334e-06,
      "loss": 0.0087,
      "step": 74850
    },
    {
      "epoch": 2.39552,
      "grad_norm": 0.008774238638579845,
      "learning_rate": 4.03008e-06,
      "loss": 0.0013,
      "step": 74860
    },
    {
      "epoch": 2.39584,
      "grad_norm": 0.003391238860785961,
      "learning_rate": 4.027946666666667e-06,
      "loss": 0.0005,
      "step": 74870
    },
    {
      "epoch": 2.39616,
      "grad_norm": 0.034654829651117325,
      "learning_rate": 4.025813333333334e-06,
      "loss": 0.0004,
      "step": 74880
    },
    {
      "epoch": 2.39648,
      "grad_norm": 0.004264393821358681,
      "learning_rate": 4.02368e-06,
      "loss": 0.0005,
      "step": 74890
    },
    {
      "epoch": 2.3968,
      "grad_norm": 0.0021203202195465565,
      "learning_rate": 4.021546666666667e-06,
      "loss": 0.0004,
      "step": 74900
    },
    {
      "epoch": 2.39712,
      "grad_norm": 0.004849450662732124,
      "learning_rate": 4.019413333333334e-06,
      "loss": 0.0238,
      "step": 74910
    },
    {
      "epoch": 2.39744,
      "grad_norm": 0.0017752853455021977,
      "learning_rate": 4.01728e-06,
      "loss": 0.0003,
      "step": 74920
    },
    {
      "epoch": 2.39776,
      "grad_norm": 0.00919840857386589,
      "learning_rate": 4.015146666666667e-06,
      "loss": 0.0002,
      "step": 74930
    },
    {
      "epoch": 2.39808,
      "grad_norm": 0.0067415907979011536,
      "learning_rate": 4.013013333333334e-06,
      "loss": 0.0003,
      "step": 74940
    },
    {
      "epoch": 2.3984,
      "grad_norm": 0.006322757340967655,
      "learning_rate": 4.01088e-06,
      "loss": 0.0404,
      "step": 74950
    },
    {
      "epoch": 2.39872,
      "grad_norm": 0.003954519517719746,
      "learning_rate": 4.0087466666666665e-06,
      "loss": 0.0004,
      "step": 74960
    },
    {
      "epoch": 2.39904,
      "grad_norm": 0.0025263684801757336,
      "learning_rate": 4.006613333333334e-06,
      "loss": 0.0002,
      "step": 74970
    },
    {
      "epoch": 2.39936,
      "grad_norm": 0.002992813941091299,
      "learning_rate": 4.004480000000001e-06,
      "loss": 0.0004,
      "step": 74980
    },
    {
      "epoch": 2.39968,
      "grad_norm": 0.010301143862307072,
      "learning_rate": 4.002346666666667e-06,
      "loss": 0.0017,
      "step": 74990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.006203904282301664,
      "learning_rate": 4.000213333333334e-06,
      "loss": 0.0002,
      "step": 75000
    },
    {
      "epoch": 2.40032,
      "grad_norm": 0.012262471951544285,
      "learning_rate": 3.99808e-06,
      "loss": 0.0002,
      "step": 75010
    },
    {
      "epoch": 2.40064,
      "grad_norm": 0.006382251624017954,
      "learning_rate": 3.995946666666667e-06,
      "loss": 0.0392,
      "step": 75020
    },
    {
      "epoch": 2.40096,
      "grad_norm": 0.002945777028799057,
      "learning_rate": 3.9938133333333335e-06,
      "loss": 0.0003,
      "step": 75030
    },
    {
      "epoch": 2.40128,
      "grad_norm": 0.009133795276284218,
      "learning_rate": 3.9916800000000005e-06,
      "loss": 0.0003,
      "step": 75040
    },
    {
      "epoch": 2.4016,
      "grad_norm": 0.007100480608642101,
      "learning_rate": 3.989546666666667e-06,
      "loss": 0.0003,
      "step": 75050
    },
    {
      "epoch": 2.40192,
      "grad_norm": 0.009402005933225155,
      "learning_rate": 3.987413333333334e-06,
      "loss": 0.0002,
      "step": 75060
    },
    {
      "epoch": 2.40224,
      "grad_norm": 0.002206691075116396,
      "learning_rate": 3.98528e-06,
      "loss": 0.0474,
      "step": 75070
    },
    {
      "epoch": 2.40256,
      "grad_norm": 0.004693089053034782,
      "learning_rate": 3.983146666666667e-06,
      "loss": 0.0003,
      "step": 75080
    },
    {
      "epoch": 2.40288,
      "grad_norm": 0.010017813183367252,
      "learning_rate": 3.981013333333333e-06,
      "loss": 0.0005,
      "step": 75090
    },
    {
      "epoch": 2.4032,
      "grad_norm": 0.005292774643748999,
      "learning_rate": 3.9788800000000004e-06,
      "loss": 0.0004,
      "step": 75100
    },
    {
      "epoch": 2.40352,
      "grad_norm": 0.003180111525580287,
      "learning_rate": 3.9767466666666675e-06,
      "loss": 0.0002,
      "step": 75110
    },
    {
      "epoch": 2.4038399999999998,
      "grad_norm": 0.02880186401307583,
      "learning_rate": 3.974613333333334e-06,
      "loss": 0.0003,
      "step": 75120
    },
    {
      "epoch": 2.40416,
      "grad_norm": 0.0036797772627323866,
      "learning_rate": 3.97248e-06,
      "loss": 0.0003,
      "step": 75130
    },
    {
      "epoch": 2.40448,
      "grad_norm": 0.003026205115020275,
      "learning_rate": 3.970346666666667e-06,
      "loss": 0.0003,
      "step": 75140
    },
    {
      "epoch": 2.4048,
      "grad_norm": 0.007413746789097786,
      "learning_rate": 3.968213333333334e-06,
      "loss": 0.0003,
      "step": 75150
    },
    {
      "epoch": 2.40512,
      "grad_norm": 0.007710760924965143,
      "learning_rate": 3.96608e-06,
      "loss": 0.0002,
      "step": 75160
    },
    {
      "epoch": 2.40544,
      "grad_norm": 0.0034455389250069857,
      "learning_rate": 3.963946666666667e-06,
      "loss": 0.0005,
      "step": 75170
    },
    {
      "epoch": 2.40576,
      "grad_norm": 0.00243315938860178,
      "learning_rate": 3.961813333333334e-06,
      "loss": 0.0002,
      "step": 75180
    },
    {
      "epoch": 2.40608,
      "grad_norm": 0.0033907683100551367,
      "learning_rate": 3.95968e-06,
      "loss": 0.0003,
      "step": 75190
    },
    {
      "epoch": 2.4064,
      "grad_norm": 0.0034235732164233923,
      "learning_rate": 3.957546666666667e-06,
      "loss": 0.053,
      "step": 75200
    },
    {
      "epoch": 2.40672,
      "grad_norm": 0.002633246360346675,
      "learning_rate": 3.955413333333334e-06,
      "loss": 0.0003,
      "step": 75210
    },
    {
      "epoch": 2.40704,
      "grad_norm": 0.00400821166113019,
      "learning_rate": 3.95328e-06,
      "loss": 0.0018,
      "step": 75220
    },
    {
      "epoch": 2.40736,
      "grad_norm": 0.002902113599702716,
      "learning_rate": 3.951146666666667e-06,
      "loss": 0.0003,
      "step": 75230
    },
    {
      "epoch": 2.40768,
      "grad_norm": 0.005507461726665497,
      "learning_rate": 3.9490133333333335e-06,
      "loss": 0.0173,
      "step": 75240
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.008563973009586334,
      "learning_rate": 3.9468800000000006e-06,
      "loss": 0.0245,
      "step": 75250
    },
    {
      "epoch": 2.40832,
      "grad_norm": 0.005809510126709938,
      "learning_rate": 3.944746666666667e-06,
      "loss": 0.0003,
      "step": 75260
    },
    {
      "epoch": 2.40864,
      "grad_norm": 0.00915730930864811,
      "learning_rate": 3.942613333333334e-06,
      "loss": 0.0003,
      "step": 75270
    },
    {
      "epoch": 2.40896,
      "grad_norm": 0.005825182422995567,
      "learning_rate": 3.94048e-06,
      "loss": 0.0011,
      "step": 75280
    },
    {
      "epoch": 2.40928,
      "grad_norm": 0.0225419569760561,
      "learning_rate": 3.938346666666667e-06,
      "loss": 0.0003,
      "step": 75290
    },
    {
      "epoch": 2.4096,
      "grad_norm": 0.004156820941716433,
      "learning_rate": 3.936213333333333e-06,
      "loss": 0.0002,
      "step": 75300
    },
    {
      "epoch": 2.40992,
      "grad_norm": 0.00808609090745449,
      "learning_rate": 3.9340800000000005e-06,
      "loss": 0.0003,
      "step": 75310
    },
    {
      "epoch": 2.41024,
      "grad_norm": 0.027601251378655434,
      "learning_rate": 3.9319466666666675e-06,
      "loss": 0.0003,
      "step": 75320
    },
    {
      "epoch": 2.41056,
      "grad_norm": 0.0041243284940719604,
      "learning_rate": 3.929813333333334e-06,
      "loss": 0.0003,
      "step": 75330
    },
    {
      "epoch": 2.41088,
      "grad_norm": 0.002530387369915843,
      "learning_rate": 3.92768e-06,
      "loss": 0.0002,
      "step": 75340
    },
    {
      "epoch": 2.4112,
      "grad_norm": 0.0027737817727029324,
      "learning_rate": 3.925546666666667e-06,
      "loss": 0.0003,
      "step": 75350
    },
    {
      "epoch": 2.41152,
      "grad_norm": 0.002786667086184025,
      "learning_rate": 3.923413333333333e-06,
      "loss": 0.0004,
      "step": 75360
    },
    {
      "epoch": 2.4118399999999998,
      "grad_norm": 0.009800070896744728,
      "learning_rate": 3.92128e-06,
      "loss": 0.0005,
      "step": 75370
    },
    {
      "epoch": 2.41216,
      "grad_norm": 0.004557948559522629,
      "learning_rate": 3.919146666666667e-06,
      "loss": 0.0002,
      "step": 75380
    },
    {
      "epoch": 2.41248,
      "grad_norm": 0.026771824806928635,
      "learning_rate": 3.917013333333334e-06,
      "loss": 0.0003,
      "step": 75390
    },
    {
      "epoch": 2.4128,
      "grad_norm": 0.0042508854530751705,
      "learning_rate": 3.914880000000001e-06,
      "loss": 0.0004,
      "step": 75400
    },
    {
      "epoch": 2.41312,
      "grad_norm": 0.023771386593580246,
      "learning_rate": 3.912746666666667e-06,
      "loss": 0.0003,
      "step": 75410
    },
    {
      "epoch": 2.41344,
      "grad_norm": 0.016456831246614456,
      "learning_rate": 3.910613333333333e-06,
      "loss": 0.0005,
      "step": 75420
    },
    {
      "epoch": 2.41376,
      "grad_norm": 0.005569624714553356,
      "learning_rate": 3.90848e-06,
      "loss": 0.0003,
      "step": 75430
    },
    {
      "epoch": 2.4140800000000002,
      "grad_norm": 0.005751088261604309,
      "learning_rate": 3.906346666666667e-06,
      "loss": 0.0003,
      "step": 75440
    },
    {
      "epoch": 2.4144,
      "grad_norm": 0.008334137499332428,
      "learning_rate": 3.9042133333333335e-06,
      "loss": 0.0003,
      "step": 75450
    },
    {
      "epoch": 2.41472,
      "grad_norm": 0.005896246992051601,
      "learning_rate": 3.902080000000001e-06,
      "loss": 0.0061,
      "step": 75460
    },
    {
      "epoch": 2.41504,
      "grad_norm": 0.020224813371896744,
      "learning_rate": 3.899946666666667e-06,
      "loss": 0.0009,
      "step": 75470
    },
    {
      "epoch": 2.41536,
      "grad_norm": 0.004358783829957247,
      "learning_rate": 3.897813333333333e-06,
      "loss": 0.0005,
      "step": 75480
    },
    {
      "epoch": 2.41568,
      "grad_norm": 0.001655751490034163,
      "learning_rate": 3.895680000000001e-06,
      "loss": 0.0027,
      "step": 75490
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.004036969505250454,
      "learning_rate": 3.893546666666667e-06,
      "loss": 0.032,
      "step": 75500
    },
    {
      "epoch": 2.41632,
      "grad_norm": 0.0033172513358294964,
      "learning_rate": 3.891413333333333e-06,
      "loss": 0.0002,
      "step": 75510
    },
    {
      "epoch": 2.41664,
      "grad_norm": 0.003434597048908472,
      "learning_rate": 3.8892800000000005e-06,
      "loss": 0.0002,
      "step": 75520
    },
    {
      "epoch": 2.41696,
      "grad_norm": 0.004005275201052427,
      "learning_rate": 3.887146666666667e-06,
      "loss": 0.0003,
      "step": 75530
    },
    {
      "epoch": 2.41728,
      "grad_norm": 0.008286651223897934,
      "learning_rate": 3.885013333333334e-06,
      "loss": 0.0001,
      "step": 75540
    },
    {
      "epoch": 2.4176,
      "grad_norm": 0.005839637015014887,
      "learning_rate": 3.882880000000001e-06,
      "loss": 0.0398,
      "step": 75550
    },
    {
      "epoch": 2.41792,
      "grad_norm": 0.007678416091948748,
      "learning_rate": 3.880746666666667e-06,
      "loss": 0.0002,
      "step": 75560
    },
    {
      "epoch": 2.41824,
      "grad_norm": 0.006195047404617071,
      "learning_rate": 3.878613333333333e-06,
      "loss": 0.0002,
      "step": 75570
    },
    {
      "epoch": 2.41856,
      "grad_norm": 0.003838867647573352,
      "learning_rate": 3.87648e-06,
      "loss": 0.0002,
      "step": 75580
    },
    {
      "epoch": 2.41888,
      "grad_norm": 0.004903046879917383,
      "learning_rate": 3.874346666666667e-06,
      "loss": 0.0004,
      "step": 75590
    },
    {
      "epoch": 2.4192,
      "grad_norm": 0.004221892915666103,
      "learning_rate": 3.872213333333334e-06,
      "loss": 0.0527,
      "step": 75600
    },
    {
      "epoch": 2.41952,
      "grad_norm": 0.18852950632572174,
      "learning_rate": 3.870080000000001e-06,
      "loss": 0.0005,
      "step": 75610
    },
    {
      "epoch": 2.4198399999999998,
      "grad_norm": 0.008124499581754208,
      "learning_rate": 3.867946666666667e-06,
      "loss": 0.0002,
      "step": 75620
    },
    {
      "epoch": 2.42016,
      "grad_norm": 0.006430344190448523,
      "learning_rate": 3.865813333333334e-06,
      "loss": 0.0003,
      "step": 75630
    },
    {
      "epoch": 2.42048,
      "grad_norm": 0.02384762093424797,
      "learning_rate": 3.86368e-06,
      "loss": 0.0002,
      "step": 75640
    },
    {
      "epoch": 2.4208,
      "grad_norm": 0.005663626827299595,
      "learning_rate": 3.8615466666666665e-06,
      "loss": 0.0002,
      "step": 75650
    },
    {
      "epoch": 2.42112,
      "grad_norm": 0.007404323201626539,
      "learning_rate": 3.8594133333333335e-06,
      "loss": 0.0002,
      "step": 75660
    },
    {
      "epoch": 2.42144,
      "grad_norm": 0.008262885734438896,
      "learning_rate": 3.857280000000001e-06,
      "loss": 0.0091,
      "step": 75670
    },
    {
      "epoch": 2.42176,
      "grad_norm": 0.003439391264691949,
      "learning_rate": 3.855146666666667e-06,
      "loss": 0.0008,
      "step": 75680
    },
    {
      "epoch": 2.4220800000000002,
      "grad_norm": 0.007124210242182016,
      "learning_rate": 3.853013333333334e-06,
      "loss": 0.0467,
      "step": 75690
    },
    {
      "epoch": 2.4224,
      "grad_norm": 0.0036191027611494064,
      "learning_rate": 3.85088e-06,
      "loss": 0.0019,
      "step": 75700
    },
    {
      "epoch": 2.42272,
      "grad_norm": 0.010140991769731045,
      "learning_rate": 3.848746666666666e-06,
      "loss": 0.0141,
      "step": 75710
    },
    {
      "epoch": 2.42304,
      "grad_norm": 0.011465063318610191,
      "learning_rate": 3.846613333333334e-06,
      "loss": 0.0003,
      "step": 75720
    },
    {
      "epoch": 2.42336,
      "grad_norm": 0.004084554500877857,
      "learning_rate": 3.8444800000000005e-06,
      "loss": 0.0002,
      "step": 75730
    },
    {
      "epoch": 2.42368,
      "grad_norm": 0.00847402960062027,
      "learning_rate": 3.842346666666667e-06,
      "loss": 0.0004,
      "step": 75740
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.17234140634536743,
      "learning_rate": 3.840213333333334e-06,
      "loss": 0.0004,
      "step": 75750
    },
    {
      "epoch": 2.42432,
      "grad_norm": 0.00823025219142437,
      "learning_rate": 3.83808e-06,
      "loss": 0.0003,
      "step": 75760
    },
    {
      "epoch": 2.42464,
      "grad_norm": 0.004033352248370647,
      "learning_rate": 3.835946666666667e-06,
      "loss": 0.0002,
      "step": 75770
    },
    {
      "epoch": 2.42496,
      "grad_norm": 0.0018739593215286732,
      "learning_rate": 3.833813333333334e-06,
      "loss": 0.0075,
      "step": 75780
    },
    {
      "epoch": 2.42528,
      "grad_norm": 0.0038849283009767532,
      "learning_rate": 3.83168e-06,
      "loss": 0.0007,
      "step": 75790
    },
    {
      "epoch": 2.4256,
      "grad_norm": 0.0024538699071854353,
      "learning_rate": 3.829546666666667e-06,
      "loss": 0.0002,
      "step": 75800
    },
    {
      "epoch": 2.42592,
      "grad_norm": 0.01198822632431984,
      "learning_rate": 3.827413333333334e-06,
      "loss": 0.0002,
      "step": 75810
    },
    {
      "epoch": 2.42624,
      "grad_norm": 0.008719438686966896,
      "learning_rate": 3.82528e-06,
      "loss": 0.0004,
      "step": 75820
    },
    {
      "epoch": 2.42656,
      "grad_norm": 0.012898845598101616,
      "learning_rate": 3.823146666666667e-06,
      "loss": 0.0003,
      "step": 75830
    },
    {
      "epoch": 2.42688,
      "grad_norm": 0.005624372977763414,
      "learning_rate": 3.821013333333334e-06,
      "loss": 0.0003,
      "step": 75840
    },
    {
      "epoch": 2.4272,
      "grad_norm": 0.005326096434146166,
      "learning_rate": 3.81888e-06,
      "loss": 0.0003,
      "step": 75850
    },
    {
      "epoch": 2.42752,
      "grad_norm": 0.018713146448135376,
      "learning_rate": 3.816746666666667e-06,
      "loss": 0.0119,
      "step": 75860
    },
    {
      "epoch": 2.4278399999999998,
      "grad_norm": 0.0053383102640509605,
      "learning_rate": 3.8146133333333336e-06,
      "loss": 0.0183,
      "step": 75870
    },
    {
      "epoch": 2.42816,
      "grad_norm": 0.0043259067460894585,
      "learning_rate": 3.8124800000000002e-06,
      "loss": 0.0002,
      "step": 75880
    },
    {
      "epoch": 2.42848,
      "grad_norm": 0.005301937460899353,
      "learning_rate": 3.8103466666666673e-06,
      "loss": 0.0005,
      "step": 75890
    },
    {
      "epoch": 2.4288,
      "grad_norm": 0.007790707983076572,
      "learning_rate": 3.808213333333334e-06,
      "loss": 0.0002,
      "step": 75900
    },
    {
      "epoch": 2.42912,
      "grad_norm": 0.002081788843497634,
      "learning_rate": 3.80608e-06,
      "loss": 0.0003,
      "step": 75910
    },
    {
      "epoch": 2.42944,
      "grad_norm": 0.0017196210101246834,
      "learning_rate": 3.803946666666667e-06,
      "loss": 0.0002,
      "step": 75920
    },
    {
      "epoch": 2.42976,
      "grad_norm": 0.0035288690123707056,
      "learning_rate": 3.8018133333333335e-06,
      "loss": 0.0004,
      "step": 75930
    },
    {
      "epoch": 2.4300800000000002,
      "grad_norm": 0.00595921091735363,
      "learning_rate": 3.79968e-06,
      "loss": 0.053,
      "step": 75940
    },
    {
      "epoch": 2.4304,
      "grad_norm": 0.005164169240742922,
      "learning_rate": 3.797546666666667e-06,
      "loss": 0.0163,
      "step": 75950
    },
    {
      "epoch": 2.43072,
      "grad_norm": 0.003218540921807289,
      "learning_rate": 3.795413333333334e-06,
      "loss": 0.0002,
      "step": 75960
    },
    {
      "epoch": 2.43104,
      "grad_norm": 0.017478767782449722,
      "learning_rate": 3.7932800000000005e-06,
      "loss": 0.0004,
      "step": 75970
    },
    {
      "epoch": 2.43136,
      "grad_norm": 0.008486010134220123,
      "learning_rate": 3.7911466666666667e-06,
      "loss": 0.0002,
      "step": 75980
    },
    {
      "epoch": 2.43168,
      "grad_norm": 0.004523155745118856,
      "learning_rate": 3.7890133333333333e-06,
      "loss": 0.0159,
      "step": 75990
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.016289861872792244,
      "learning_rate": 3.7868800000000004e-06,
      "loss": 0.0002,
      "step": 76000
    },
    {
      "epoch": 2.43232,
      "grad_norm": 0.004768203943967819,
      "learning_rate": 3.784746666666667e-06,
      "loss": 0.0002,
      "step": 76010
    },
    {
      "epoch": 2.43264,
      "grad_norm": 0.004977127071470022,
      "learning_rate": 3.7826133333333337e-06,
      "loss": 0.0002,
      "step": 76020
    },
    {
      "epoch": 2.43296,
      "grad_norm": 0.016131384298205376,
      "learning_rate": 3.7804800000000004e-06,
      "loss": 0.0002,
      "step": 76030
    },
    {
      "epoch": 2.43328,
      "grad_norm": 0.009977822192013264,
      "learning_rate": 3.778346666666667e-06,
      "loss": 0.0003,
      "step": 76040
    },
    {
      "epoch": 2.4336,
      "grad_norm": 0.005621408578008413,
      "learning_rate": 3.7762133333333332e-06,
      "loss": 0.0005,
      "step": 76050
    },
    {
      "epoch": 2.43392,
      "grad_norm": 0.005238063633441925,
      "learning_rate": 3.7740800000000007e-06,
      "loss": 0.0002,
      "step": 76060
    },
    {
      "epoch": 2.43424,
      "grad_norm": 0.0026535135693848133,
      "learning_rate": 3.771946666666667e-06,
      "loss": 0.0001,
      "step": 76070
    },
    {
      "epoch": 2.43456,
      "grad_norm": 0.008818821050226688,
      "learning_rate": 3.7698133333333336e-06,
      "loss": 0.0486,
      "step": 76080
    },
    {
      "epoch": 2.43488,
      "grad_norm": 0.003948190715163946,
      "learning_rate": 3.7676800000000002e-06,
      "loss": 0.0002,
      "step": 76090
    },
    {
      "epoch": 2.4352,
      "grad_norm": 0.0046337442472577095,
      "learning_rate": 3.765546666666667e-06,
      "loss": 0.0064,
      "step": 76100
    },
    {
      "epoch": 2.43552,
      "grad_norm": 0.0028931940905749798,
      "learning_rate": 3.763413333333334e-06,
      "loss": 0.0002,
      "step": 76110
    },
    {
      "epoch": 2.43584,
      "grad_norm": 0.0021953084506094456,
      "learning_rate": 3.7612800000000006e-06,
      "loss": 0.001,
      "step": 76120
    },
    {
      "epoch": 2.43616,
      "grad_norm": 0.0034144532401114702,
      "learning_rate": 3.7591466666666673e-06,
      "loss": 0.0004,
      "step": 76130
    },
    {
      "epoch": 2.43648,
      "grad_norm": 0.006624673958867788,
      "learning_rate": 3.7570133333333335e-06,
      "loss": 0.0002,
      "step": 76140
    },
    {
      "epoch": 2.4368,
      "grad_norm": 0.04386759176850319,
      "learning_rate": 3.75488e-06,
      "loss": 0.0056,
      "step": 76150
    },
    {
      "epoch": 2.43712,
      "grad_norm": 0.004096743185073137,
      "learning_rate": 3.7527466666666668e-06,
      "loss": 0.002,
      "step": 76160
    },
    {
      "epoch": 2.43744,
      "grad_norm": 0.0029514790512621403,
      "learning_rate": 3.750613333333334e-06,
      "loss": 0.0385,
      "step": 76170
    },
    {
      "epoch": 2.43776,
      "grad_norm": 0.0060076117515563965,
      "learning_rate": 3.7484800000000005e-06,
      "loss": 0.0017,
      "step": 76180
    },
    {
      "epoch": 2.4380800000000002,
      "grad_norm": 0.0027255909517407417,
      "learning_rate": 3.746346666666667e-06,
      "loss": 0.0306,
      "step": 76190
    },
    {
      "epoch": 2.4384,
      "grad_norm": 0.0026499333325773478,
      "learning_rate": 3.7442133333333338e-06,
      "loss": 0.001,
      "step": 76200
    },
    {
      "epoch": 2.43872,
      "grad_norm": 0.0040048896335065365,
      "learning_rate": 3.74208e-06,
      "loss": 0.0003,
      "step": 76210
    },
    {
      "epoch": 2.43904,
      "grad_norm": 0.002901709172874689,
      "learning_rate": 3.7399466666666667e-06,
      "loss": 0.0002,
      "step": 76220
    },
    {
      "epoch": 2.4393599999999998,
      "grad_norm": 0.003857261734083295,
      "learning_rate": 3.7378133333333337e-06,
      "loss": 0.0002,
      "step": 76230
    },
    {
      "epoch": 2.43968,
      "grad_norm": 0.008614471182227135,
      "learning_rate": 3.7356800000000004e-06,
      "loss": 0.0031,
      "step": 76240
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.07960331439971924,
      "learning_rate": 3.733546666666667e-06,
      "loss": 0.0003,
      "step": 76250
    },
    {
      "epoch": 2.44032,
      "grad_norm": 0.0070893410593271255,
      "learning_rate": 3.7314133333333337e-06,
      "loss": 0.0015,
      "step": 76260
    },
    {
      "epoch": 2.44064,
      "grad_norm": 0.12628857791423798,
      "learning_rate": 3.7292800000000003e-06,
      "loss": 0.0003,
      "step": 76270
    },
    {
      "epoch": 2.44096,
      "grad_norm": 0.01691526733338833,
      "learning_rate": 3.7271466666666674e-06,
      "loss": 0.0914,
      "step": 76280
    },
    {
      "epoch": 2.44128,
      "grad_norm": 0.031042689457535744,
      "learning_rate": 3.7250133333333336e-06,
      "loss": 0.0002,
      "step": 76290
    },
    {
      "epoch": 2.4416,
      "grad_norm": 0.0017899534432217479,
      "learning_rate": 3.7228800000000003e-06,
      "loss": 0.0002,
      "step": 76300
    },
    {
      "epoch": 2.44192,
      "grad_norm": 0.008386638015508652,
      "learning_rate": 3.720746666666667e-06,
      "loss": 0.0002,
      "step": 76310
    },
    {
      "epoch": 2.44224,
      "grad_norm": 0.007487531751394272,
      "learning_rate": 3.7186133333333336e-06,
      "loss": 0.0005,
      "step": 76320
    },
    {
      "epoch": 2.44256,
      "grad_norm": 0.0028133790474385023,
      "learning_rate": 3.71648e-06,
      "loss": 0.0011,
      "step": 76330
    },
    {
      "epoch": 2.44288,
      "grad_norm": 0.004096623510122299,
      "learning_rate": 3.7143466666666673e-06,
      "loss": 0.0004,
      "step": 76340
    },
    {
      "epoch": 2.4432,
      "grad_norm": 0.005015508271753788,
      "learning_rate": 3.712213333333334e-06,
      "loss": 0.0006,
      "step": 76350
    },
    {
      "epoch": 2.44352,
      "grad_norm": 0.003365057986229658,
      "learning_rate": 3.71008e-06,
      "loss": 0.0002,
      "step": 76360
    },
    {
      "epoch": 2.44384,
      "grad_norm": 0.0038267062045633793,
      "learning_rate": 3.707946666666667e-06,
      "loss": 0.0126,
      "step": 76370
    },
    {
      "epoch": 2.44416,
      "grad_norm": 0.008660552091896534,
      "learning_rate": 3.7058133333333334e-06,
      "loss": 0.0327,
      "step": 76380
    },
    {
      "epoch": 2.44448,
      "grad_norm": 1.9648597240447998,
      "learning_rate": 3.70368e-06,
      "loss": 0.0523,
      "step": 76390
    },
    {
      "epoch": 2.4448,
      "grad_norm": 0.004696968477219343,
      "learning_rate": 3.701546666666667e-06,
      "loss": 0.0002,
      "step": 76400
    },
    {
      "epoch": 2.44512,
      "grad_norm": 0.00569998100399971,
      "learning_rate": 3.699413333333334e-06,
      "loss": 0.0017,
      "step": 76410
    },
    {
      "epoch": 2.44544,
      "grad_norm": 0.010719616897404194,
      "learning_rate": 3.6972800000000005e-06,
      "loss": 0.0097,
      "step": 76420
    },
    {
      "epoch": 2.44576,
      "grad_norm": 0.013898245058953762,
      "learning_rate": 3.6951466666666667e-06,
      "loss": 0.0002,
      "step": 76430
    },
    {
      "epoch": 2.44608,
      "grad_norm": 0.005806554574519396,
      "learning_rate": 3.6930133333333333e-06,
      "loss": 0.0003,
      "step": 76440
    },
    {
      "epoch": 2.4464,
      "grad_norm": 0.0028775313403457403,
      "learning_rate": 3.6908800000000004e-06,
      "loss": 0.0005,
      "step": 76450
    },
    {
      "epoch": 2.44672,
      "grad_norm": 0.004590136464685202,
      "learning_rate": 3.688746666666667e-06,
      "loss": 0.0002,
      "step": 76460
    },
    {
      "epoch": 2.44704,
      "grad_norm": 0.00719613628461957,
      "learning_rate": 3.6866133333333337e-06,
      "loss": 0.0002,
      "step": 76470
    },
    {
      "epoch": 2.4473599999999998,
      "grad_norm": 0.007029682397842407,
      "learning_rate": 3.6844800000000003e-06,
      "loss": 0.0003,
      "step": 76480
    },
    {
      "epoch": 2.44768,
      "grad_norm": 0.0172769445925951,
      "learning_rate": 3.682346666666667e-06,
      "loss": 0.0003,
      "step": 76490
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.0053599257953464985,
      "learning_rate": 3.680213333333333e-06,
      "loss": 0.0002,
      "step": 76500
    },
    {
      "epoch": 2.44832,
      "grad_norm": 0.020835435017943382,
      "learning_rate": 3.6780800000000007e-06,
      "loss": 0.0004,
      "step": 76510
    },
    {
      "epoch": 2.44864,
      "grad_norm": 0.0016773814568296075,
      "learning_rate": 3.675946666666667e-06,
      "loss": 0.0006,
      "step": 76520
    },
    {
      "epoch": 2.44896,
      "grad_norm": 0.011153225786983967,
      "learning_rate": 3.6738133333333336e-06,
      "loss": 0.0002,
      "step": 76530
    },
    {
      "epoch": 2.44928,
      "grad_norm": 0.004803755786269903,
      "learning_rate": 3.6716800000000002e-06,
      "loss": 0.0002,
      "step": 76540
    },
    {
      "epoch": 2.4496,
      "grad_norm": 0.0016251016641035676,
      "learning_rate": 3.669546666666667e-06,
      "loss": 0.0014,
      "step": 76550
    },
    {
      "epoch": 2.44992,
      "grad_norm": 0.005556012038141489,
      "learning_rate": 3.6674133333333335e-06,
      "loss": 0.0002,
      "step": 76560
    },
    {
      "epoch": 2.45024,
      "grad_norm": 0.00725538469851017,
      "learning_rate": 3.6652800000000006e-06,
      "loss": 0.0002,
      "step": 76570
    },
    {
      "epoch": 2.45056,
      "grad_norm": 0.0036719930358231068,
      "learning_rate": 3.6631466666666672e-06,
      "loss": 0.0221,
      "step": 76580
    },
    {
      "epoch": 2.45088,
      "grad_norm": 0.011195129714906216,
      "learning_rate": 3.6610133333333335e-06,
      "loss": 0.0006,
      "step": 76590
    },
    {
      "epoch": 2.4512,
      "grad_norm": 0.0033192597329616547,
      "learning_rate": 3.65888e-06,
      "loss": 0.0002,
      "step": 76600
    },
    {
      "epoch": 2.45152,
      "grad_norm": 0.003102047834545374,
      "learning_rate": 3.6567466666666668e-06,
      "loss": 0.0002,
      "step": 76610
    },
    {
      "epoch": 2.45184,
      "grad_norm": 0.00395926320925355,
      "learning_rate": 3.654613333333334e-06,
      "loss": 0.0003,
      "step": 76620
    },
    {
      "epoch": 2.45216,
      "grad_norm": 0.0026313047856092453,
      "learning_rate": 3.6524800000000005e-06,
      "loss": 0.0021,
      "step": 76630
    },
    {
      "epoch": 2.45248,
      "grad_norm": 0.0038716758135706186,
      "learning_rate": 3.650346666666667e-06,
      "loss": 0.0002,
      "step": 76640
    },
    {
      "epoch": 2.4528,
      "grad_norm": 0.003948743920773268,
      "learning_rate": 3.6482133333333338e-06,
      "loss": 0.0002,
      "step": 76650
    },
    {
      "epoch": 2.45312,
      "grad_norm": 0.038208890706300735,
      "learning_rate": 3.64608e-06,
      "loss": 0.0046,
      "step": 76660
    },
    {
      "epoch": 2.45344,
      "grad_norm": 0.008751550689339638,
      "learning_rate": 3.6439466666666666e-06,
      "loss": 0.0151,
      "step": 76670
    },
    {
      "epoch": 2.45376,
      "grad_norm": 0.002151138847693801,
      "learning_rate": 3.6418133333333337e-06,
      "loss": 0.0523,
      "step": 76680
    },
    {
      "epoch": 2.45408,
      "grad_norm": 0.005499996244907379,
      "learning_rate": 3.6396800000000004e-06,
      "loss": 0.0003,
      "step": 76690
    },
    {
      "epoch": 2.4544,
      "grad_norm": 0.007407621946185827,
      "learning_rate": 3.637546666666667e-06,
      "loss": 0.0002,
      "step": 76700
    },
    {
      "epoch": 2.45472,
      "grad_norm": 0.007272685877978802,
      "learning_rate": 3.6354133333333337e-06,
      "loss": 0.0002,
      "step": 76710
    },
    {
      "epoch": 2.45504,
      "grad_norm": 7.542830467224121,
      "learning_rate": 3.6332800000000003e-06,
      "loss": 0.0114,
      "step": 76720
    },
    {
      "epoch": 2.4553599999999998,
      "grad_norm": 0.004135377239435911,
      "learning_rate": 3.6311466666666665e-06,
      "loss": 0.0005,
      "step": 76730
    },
    {
      "epoch": 2.45568,
      "grad_norm": 0.13857080042362213,
      "learning_rate": 3.629013333333334e-06,
      "loss": 0.0004,
      "step": 76740
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.014840646646916866,
      "learning_rate": 3.6268800000000002e-06,
      "loss": 0.0002,
      "step": 76750
    },
    {
      "epoch": 2.45632,
      "grad_norm": 0.0023519631940871477,
      "learning_rate": 3.624746666666667e-06,
      "loss": 0.0001,
      "step": 76760
    },
    {
      "epoch": 2.45664,
      "grad_norm": 0.001587592181749642,
      "learning_rate": 3.6226133333333335e-06,
      "loss": 0.0164,
      "step": 76770
    },
    {
      "epoch": 2.45696,
      "grad_norm": 0.0033273238223046064,
      "learning_rate": 3.62048e-06,
      "loss": 0.0002,
      "step": 76780
    },
    {
      "epoch": 2.45728,
      "grad_norm": 0.0038842540234327316,
      "learning_rate": 3.6183466666666673e-06,
      "loss": 0.0002,
      "step": 76790
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 0.003610244020819664,
      "learning_rate": 3.616213333333334e-06,
      "loss": 0.0002,
      "step": 76800
    },
    {
      "epoch": 2.45792,
      "grad_norm": 0.004283049143850803,
      "learning_rate": 3.6140800000000006e-06,
      "loss": 0.0001,
      "step": 76810
    },
    {
      "epoch": 2.45824,
      "grad_norm": 0.007328082341700792,
      "learning_rate": 3.6119466666666668e-06,
      "loss": 0.0002,
      "step": 76820
    },
    {
      "epoch": 2.45856,
      "grad_norm": 0.0026192362420260906,
      "learning_rate": 3.6098133333333334e-06,
      "loss": 0.0002,
      "step": 76830
    },
    {
      "epoch": 2.45888,
      "grad_norm": 0.0029865961987525225,
      "learning_rate": 3.60768e-06,
      "loss": 0.0002,
      "step": 76840
    },
    {
      "epoch": 2.4592,
      "grad_norm": 0.0016171762254089117,
      "learning_rate": 3.605546666666667e-06,
      "loss": 0.0006,
      "step": 76850
    },
    {
      "epoch": 2.45952,
      "grad_norm": 0.0033374621998518705,
      "learning_rate": 3.603413333333334e-06,
      "loss": 0.0002,
      "step": 76860
    },
    {
      "epoch": 2.45984,
      "grad_norm": 0.00236489181406796,
      "learning_rate": 3.6012800000000004e-06,
      "loss": 0.0002,
      "step": 76870
    },
    {
      "epoch": 2.46016,
      "grad_norm": 0.004418232478201389,
      "learning_rate": 3.599146666666667e-06,
      "loss": 0.0019,
      "step": 76880
    },
    {
      "epoch": 2.46048,
      "grad_norm": 0.055052705109119415,
      "learning_rate": 3.5970133333333333e-06,
      "loss": 0.0003,
      "step": 76890
    },
    {
      "epoch": 2.4608,
      "grad_norm": 0.004467884078621864,
      "learning_rate": 3.59488e-06,
      "loss": 0.0005,
      "step": 76900
    },
    {
      "epoch": 2.46112,
      "grad_norm": 0.004060306586325169,
      "learning_rate": 3.592746666666667e-06,
      "loss": 0.0003,
      "step": 76910
    },
    {
      "epoch": 2.46144,
      "grad_norm": 0.010605556890368462,
      "learning_rate": 3.5906133333333337e-06,
      "loss": 0.0323,
      "step": 76920
    },
    {
      "epoch": 2.46176,
      "grad_norm": 0.004765716847032309,
      "learning_rate": 3.5884800000000003e-06,
      "loss": 0.0005,
      "step": 76930
    },
    {
      "epoch": 2.46208,
      "grad_norm": 0.012191626243293285,
      "learning_rate": 3.586346666666667e-06,
      "loss": 0.0002,
      "step": 76940
    },
    {
      "epoch": 2.4624,
      "grad_norm": 0.0038914382457733154,
      "learning_rate": 3.5842133333333336e-06,
      "loss": 0.0001,
      "step": 76950
    },
    {
      "epoch": 2.46272,
      "grad_norm": 0.003965263720601797,
      "learning_rate": 3.5820800000000007e-06,
      "loss": 0.0002,
      "step": 76960
    },
    {
      "epoch": 2.46304,
      "grad_norm": 0.002292412333190441,
      "learning_rate": 3.579946666666667e-06,
      "loss": 0.0002,
      "step": 76970
    },
    {
      "epoch": 2.4633599999999998,
      "grad_norm": 0.003229995258152485,
      "learning_rate": 3.5778133333333336e-06,
      "loss": 0.0028,
      "step": 76980
    },
    {
      "epoch": 2.46368,
      "grad_norm": 0.0035661899019032717,
      "learning_rate": 3.5756800000000002e-06,
      "loss": 0.0014,
      "step": 76990
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.009180386550724506,
      "learning_rate": 3.573546666666667e-06,
      "loss": 0.0002,
      "step": 77000
    },
    {
      "epoch": 2.46432,
      "grad_norm": 0.0037092678248882294,
      "learning_rate": 3.5714133333333335e-06,
      "loss": 0.0001,
      "step": 77010
    },
    {
      "epoch": 2.46464,
      "grad_norm": 0.003155994229018688,
      "learning_rate": 3.5692800000000006e-06,
      "loss": 0.011,
      "step": 77020
    },
    {
      "epoch": 2.46496,
      "grad_norm": 0.0038102588150650263,
      "learning_rate": 3.5671466666666672e-06,
      "loss": 0.0004,
      "step": 77030
    },
    {
      "epoch": 2.46528,
      "grad_norm": 0.052006009966135025,
      "learning_rate": 3.5650133333333335e-06,
      "loss": 0.0003,
      "step": 77040
    },
    {
      "epoch": 2.4656000000000002,
      "grad_norm": 0.0057491399347782135,
      "learning_rate": 3.56288e-06,
      "loss": 0.0002,
      "step": 77050
    },
    {
      "epoch": 2.46592,
      "grad_norm": 0.005321996286511421,
      "learning_rate": 3.5607466666666667e-06,
      "loss": 0.0004,
      "step": 77060
    },
    {
      "epoch": 2.46624,
      "grad_norm": 0.020876964554190636,
      "learning_rate": 3.5586133333333334e-06,
      "loss": 0.0055,
      "step": 77070
    },
    {
      "epoch": 2.46656,
      "grad_norm": 3.206854820251465,
      "learning_rate": 3.5564800000000005e-06,
      "loss": 0.0025,
      "step": 77080
    },
    {
      "epoch": 2.46688,
      "grad_norm": 2.7133629322052,
      "learning_rate": 3.554346666666667e-06,
      "loss": 0.0027,
      "step": 77090
    },
    {
      "epoch": 2.4672,
      "grad_norm": 0.0012961853062734008,
      "learning_rate": 3.5522133333333338e-06,
      "loss": 0.0002,
      "step": 77100
    },
    {
      "epoch": 2.46752,
      "grad_norm": 0.0024666718672960997,
      "learning_rate": 3.55008e-06,
      "loss": 0.041,
      "step": 77110
    },
    {
      "epoch": 2.46784,
      "grad_norm": 2.2706820964813232,
      "learning_rate": 3.5479466666666666e-06,
      "loss": 0.0545,
      "step": 77120
    },
    {
      "epoch": 2.46816,
      "grad_norm": 0.0053645032458007336,
      "learning_rate": 3.5458133333333337e-06,
      "loss": 0.0002,
      "step": 77130
    },
    {
      "epoch": 2.46848,
      "grad_norm": 0.005340603180229664,
      "learning_rate": 3.5436800000000003e-06,
      "loss": 0.0002,
      "step": 77140
    },
    {
      "epoch": 2.4688,
      "grad_norm": 0.06287547945976257,
      "learning_rate": 3.541546666666667e-06,
      "loss": 0.0003,
      "step": 77150
    },
    {
      "epoch": 2.46912,
      "grad_norm": 0.008689174428582191,
      "learning_rate": 3.5394133333333336e-06,
      "loss": 0.0002,
      "step": 77160
    },
    {
      "epoch": 2.46944,
      "grad_norm": 0.003666489152237773,
      "learning_rate": 3.5372800000000003e-06,
      "loss": 0.0002,
      "step": 77170
    },
    {
      "epoch": 2.46976,
      "grad_norm": 0.016864445060491562,
      "learning_rate": 3.5351466666666665e-06,
      "loss": 0.0002,
      "step": 77180
    },
    {
      "epoch": 2.47008,
      "grad_norm": 0.003736303886398673,
      "learning_rate": 3.533013333333334e-06,
      "loss": 0.0002,
      "step": 77190
    },
    {
      "epoch": 2.4704,
      "grad_norm": 0.002533980179578066,
      "learning_rate": 3.5308800000000002e-06,
      "loss": 0.0002,
      "step": 77200
    },
    {
      "epoch": 2.47072,
      "grad_norm": 0.002313697710633278,
      "learning_rate": 3.528746666666667e-06,
      "loss": 0.0002,
      "step": 77210
    },
    {
      "epoch": 2.47104,
      "grad_norm": 0.005180502310395241,
      "learning_rate": 3.5266133333333335e-06,
      "loss": 0.0011,
      "step": 77220
    },
    {
      "epoch": 2.47136,
      "grad_norm": 0.005228261463344097,
      "learning_rate": 3.52448e-06,
      "loss": 0.0003,
      "step": 77230
    },
    {
      "epoch": 2.47168,
      "grad_norm": 0.0029450533911585808,
      "learning_rate": 3.522346666666667e-06,
      "loss": 0.0006,
      "step": 77240
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.004406592808663845,
      "learning_rate": 3.520213333333334e-06,
      "loss": 0.0002,
      "step": 77250
    },
    {
      "epoch": 2.47232,
      "grad_norm": 0.7692288160324097,
      "learning_rate": 3.5180800000000005e-06,
      "loss": 0.0007,
      "step": 77260
    },
    {
      "epoch": 2.47264,
      "grad_norm": 0.0034871904645115137,
      "learning_rate": 3.5159466666666668e-06,
      "loss": 0.0002,
      "step": 77270
    },
    {
      "epoch": 2.47296,
      "grad_norm": 0.0028021750040352345,
      "learning_rate": 3.5138133333333334e-06,
      "loss": 0.0036,
      "step": 77280
    },
    {
      "epoch": 2.47328,
      "grad_norm": 0.006141628138720989,
      "learning_rate": 3.51168e-06,
      "loss": 0.0002,
      "step": 77290
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 0.0030445547308772802,
      "learning_rate": 3.509546666666667e-06,
      "loss": 0.0002,
      "step": 77300
    },
    {
      "epoch": 2.47392,
      "grad_norm": 0.00210057501681149,
      "learning_rate": 3.5074133333333338e-06,
      "loss": 0.0002,
      "step": 77310
    },
    {
      "epoch": 2.47424,
      "grad_norm": 0.007370965089648962,
      "learning_rate": 3.5052800000000004e-06,
      "loss": 0.0002,
      "step": 77320
    },
    {
      "epoch": 2.47456,
      "grad_norm": 0.005256102420389652,
      "learning_rate": 3.503146666666667e-06,
      "loss": 0.0002,
      "step": 77330
    },
    {
      "epoch": 2.47488,
      "grad_norm": 0.0020657109562307596,
      "learning_rate": 3.5010133333333333e-06,
      "loss": 0.0004,
      "step": 77340
    },
    {
      "epoch": 2.4752,
      "grad_norm": 0.01076427660882473,
      "learning_rate": 3.49888e-06,
      "loss": 0.0003,
      "step": 77350
    },
    {
      "epoch": 2.47552,
      "grad_norm": 0.004530349280685186,
      "learning_rate": 3.496746666666667e-06,
      "loss": 0.0002,
      "step": 77360
    },
    {
      "epoch": 2.47584,
      "grad_norm": 0.0017498417291790247,
      "learning_rate": 3.4946133333333337e-06,
      "loss": 0.0001,
      "step": 77370
    },
    {
      "epoch": 2.47616,
      "grad_norm": 0.0059038372710347176,
      "learning_rate": 3.4924800000000003e-06,
      "loss": 0.0003,
      "step": 77380
    },
    {
      "epoch": 2.47648,
      "grad_norm": 0.0025063720531761646,
      "learning_rate": 3.490346666666667e-06,
      "loss": 0.0004,
      "step": 77390
    },
    {
      "epoch": 2.4768,
      "grad_norm": 0.0026052615139633417,
      "learning_rate": 3.4882133333333336e-06,
      "loss": 0.0003,
      "step": 77400
    },
    {
      "epoch": 2.47712,
      "grad_norm": 0.020916743203997612,
      "learning_rate": 3.48608e-06,
      "loss": 0.0003,
      "step": 77410
    },
    {
      "epoch": 2.47744,
      "grad_norm": 0.002840046538040042,
      "learning_rate": 3.4839466666666673e-06,
      "loss": 0.0002,
      "step": 77420
    },
    {
      "epoch": 2.47776,
      "grad_norm": 0.003866518149152398,
      "learning_rate": 3.4818133333333336e-06,
      "loss": 0.0002,
      "step": 77430
    },
    {
      "epoch": 2.47808,
      "grad_norm": 0.001760738785378635,
      "learning_rate": 3.47968e-06,
      "loss": 0.0001,
      "step": 77440
    },
    {
      "epoch": 2.4784,
      "grad_norm": 0.00700948853045702,
      "learning_rate": 3.477546666666667e-06,
      "loss": 0.0002,
      "step": 77450
    },
    {
      "epoch": 2.47872,
      "grad_norm": 0.0036124344915151596,
      "learning_rate": 3.4754133333333335e-06,
      "loss": 0.0002,
      "step": 77460
    },
    {
      "epoch": 2.47904,
      "grad_norm": 0.011120829731225967,
      "learning_rate": 3.4732800000000006e-06,
      "loss": 0.0014,
      "step": 77470
    },
    {
      "epoch": 2.47936,
      "grad_norm": 0.0013556096237152815,
      "learning_rate": 3.4711466666666672e-06,
      "loss": 0.0294,
      "step": 77480
    },
    {
      "epoch": 2.47968,
      "grad_norm": 0.004235445521771908,
      "learning_rate": 3.469013333333334e-06,
      "loss": 0.0004,
      "step": 77490
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.002032314660027623,
      "learning_rate": 3.46688e-06,
      "loss": 0.0001,
      "step": 77500
    },
    {
      "epoch": 2.48032,
      "grad_norm": 0.0036871088668704033,
      "learning_rate": 3.4647466666666667e-06,
      "loss": 0.0086,
      "step": 77510
    },
    {
      "epoch": 2.48064,
      "grad_norm": 0.0018863111035898328,
      "learning_rate": 3.4626133333333334e-06,
      "loss": 0.0003,
      "step": 77520
    },
    {
      "epoch": 2.48096,
      "grad_norm": 0.003178843529894948,
      "learning_rate": 3.4604800000000005e-06,
      "loss": 0.0002,
      "step": 77530
    },
    {
      "epoch": 2.48128,
      "grad_norm": 0.003590172156691551,
      "learning_rate": 3.458346666666667e-06,
      "loss": 0.0002,
      "step": 77540
    },
    {
      "epoch": 2.4816,
      "grad_norm": 0.003888819832354784,
      "learning_rate": 3.4562133333333337e-06,
      "loss": 0.0084,
      "step": 77550
    },
    {
      "epoch": 2.48192,
      "grad_norm": 0.004945996217429638,
      "learning_rate": 3.4540800000000004e-06,
      "loss": 0.0003,
      "step": 77560
    },
    {
      "epoch": 2.48224,
      "grad_norm": 0.0037863722536712885,
      "learning_rate": 3.4519466666666666e-06,
      "loss": 0.0002,
      "step": 77570
    },
    {
      "epoch": 2.48256,
      "grad_norm": 0.004297095816582441,
      "learning_rate": 3.4498133333333333e-06,
      "loss": 0.0001,
      "step": 77580
    },
    {
      "epoch": 2.4828799999999998,
      "grad_norm": 3.789006233215332,
      "learning_rate": 3.4476800000000003e-06,
      "loss": 0.0075,
      "step": 77590
    },
    {
      "epoch": 2.4832,
      "grad_norm": 0.00607307581230998,
      "learning_rate": 3.445546666666667e-06,
      "loss": 0.0004,
      "step": 77600
    },
    {
      "epoch": 2.48352,
      "grad_norm": 0.00579351931810379,
      "learning_rate": 3.4434133333333336e-06,
      "loss": 0.0002,
      "step": 77610
    },
    {
      "epoch": 2.48384,
      "grad_norm": 0.00653113704174757,
      "learning_rate": 3.4412800000000003e-06,
      "loss": 0.0002,
      "step": 77620
    },
    {
      "epoch": 2.48416,
      "grad_norm": 0.07849310338497162,
      "learning_rate": 3.439146666666667e-06,
      "loss": 0.0003,
      "step": 77630
    },
    {
      "epoch": 2.48448,
      "grad_norm": 0.01770784892141819,
      "learning_rate": 3.437013333333334e-06,
      "loss": 0.0002,
      "step": 77640
    },
    {
      "epoch": 2.4848,
      "grad_norm": 0.003944265190511942,
      "learning_rate": 3.4348800000000006e-06,
      "loss": 0.0002,
      "step": 77650
    },
    {
      "epoch": 2.48512,
      "grad_norm": 0.0027069535572081804,
      "learning_rate": 3.432746666666667e-06,
      "loss": 0.0098,
      "step": 77660
    },
    {
      "epoch": 2.48544,
      "grad_norm": 0.013353029265999794,
      "learning_rate": 3.4306133333333335e-06,
      "loss": 0.0002,
      "step": 77670
    },
    {
      "epoch": 2.48576,
      "grad_norm": 0.0045312317088246346,
      "learning_rate": 3.42848e-06,
      "loss": 0.0029,
      "step": 77680
    },
    {
      "epoch": 2.48608,
      "grad_norm": 0.001923464355058968,
      "learning_rate": 3.426346666666667e-06,
      "loss": 0.0008,
      "step": 77690
    },
    {
      "epoch": 2.4864,
      "grad_norm": 0.003548369975760579,
      "learning_rate": 3.424213333333334e-06,
      "loss": 0.0002,
      "step": 77700
    },
    {
      "epoch": 2.48672,
      "grad_norm": 0.007362366653978825,
      "learning_rate": 3.4220800000000005e-06,
      "loss": 0.011,
      "step": 77710
    },
    {
      "epoch": 2.48704,
      "grad_norm": 0.007343434728682041,
      "learning_rate": 3.419946666666667e-06,
      "loss": 0.0075,
      "step": 77720
    },
    {
      "epoch": 2.48736,
      "grad_norm": 0.0038420376367866993,
      "learning_rate": 3.4178133333333334e-06,
      "loss": 0.0002,
      "step": 77730
    },
    {
      "epoch": 2.48768,
      "grad_norm": 0.0030208753887563944,
      "learning_rate": 3.41568e-06,
      "loss": 0.0001,
      "step": 77740
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.004546849988400936,
      "learning_rate": 3.4135466666666667e-06,
      "loss": 0.0001,
      "step": 77750
    },
    {
      "epoch": 2.48832,
      "grad_norm": 0.0031298098620027304,
      "learning_rate": 3.4114133333333338e-06,
      "loss": 0.0002,
      "step": 77760
    },
    {
      "epoch": 2.48864,
      "grad_norm": 0.005599123891443014,
      "learning_rate": 3.4092800000000004e-06,
      "loss": 0.0002,
      "step": 77770
    },
    {
      "epoch": 2.48896,
      "grad_norm": 0.004915148951113224,
      "learning_rate": 3.407146666666667e-06,
      "loss": 0.0001,
      "step": 77780
    },
    {
      "epoch": 2.48928,
      "grad_norm": 0.003885551355779171,
      "learning_rate": 3.4050133333333337e-06,
      "loss": 0.007,
      "step": 77790
    },
    {
      "epoch": 2.4896,
      "grad_norm": 0.00473427539691329,
      "learning_rate": 3.40288e-06,
      "loss": 0.0083,
      "step": 77800
    },
    {
      "epoch": 2.48992,
      "grad_norm": 0.009055678732693195,
      "learning_rate": 3.400746666666667e-06,
      "loss": 0.0002,
      "step": 77810
    },
    {
      "epoch": 2.49024,
      "grad_norm": 0.002954938216134906,
      "learning_rate": 3.3986133333333337e-06,
      "loss": 0.0067,
      "step": 77820
    },
    {
      "epoch": 2.49056,
      "grad_norm": 0.001397838699631393,
      "learning_rate": 3.3964800000000003e-06,
      "loss": 0.0002,
      "step": 77830
    },
    {
      "epoch": 2.4908799999999998,
      "grad_norm": 0.0018450379138812423,
      "learning_rate": 3.394346666666667e-06,
      "loss": 0.0001,
      "step": 77840
    },
    {
      "epoch": 2.4912,
      "grad_norm": 0.006114938296377659,
      "learning_rate": 3.3922133333333336e-06,
      "loss": 0.0001,
      "step": 77850
    },
    {
      "epoch": 2.49152,
      "grad_norm": 0.007747999858111143,
      "learning_rate": 3.3900800000000002e-06,
      "loss": 0.0002,
      "step": 77860
    },
    {
      "epoch": 2.49184,
      "grad_norm": 0.003859234508126974,
      "learning_rate": 3.3879466666666673e-06,
      "loss": 0.0436,
      "step": 77870
    },
    {
      "epoch": 2.49216,
      "grad_norm": 0.00391005352139473,
      "learning_rate": 3.3858133333333335e-06,
      "loss": 0.0002,
      "step": 77880
    },
    {
      "epoch": 2.49248,
      "grad_norm": 0.00801260955631733,
      "learning_rate": 3.38368e-06,
      "loss": 0.0002,
      "step": 77890
    },
    {
      "epoch": 2.4928,
      "grad_norm": 0.0009679319337010384,
      "learning_rate": 3.381546666666667e-06,
      "loss": 0.0002,
      "step": 77900
    },
    {
      "epoch": 2.4931200000000002,
      "grad_norm": 0.010126114822924137,
      "learning_rate": 3.3794133333333335e-06,
      "loss": 0.0001,
      "step": 77910
    },
    {
      "epoch": 2.49344,
      "grad_norm": 0.0040134540759027,
      "learning_rate": 3.37728e-06,
      "loss": 0.0002,
      "step": 77920
    },
    {
      "epoch": 2.49376,
      "grad_norm": 0.005576719529926777,
      "learning_rate": 3.375146666666667e-06,
      "loss": 0.0003,
      "step": 77930
    },
    {
      "epoch": 2.49408,
      "grad_norm": 0.0020989093463867903,
      "learning_rate": 3.373013333333334e-06,
      "loss": 0.0002,
      "step": 77940
    },
    {
      "epoch": 2.4944,
      "grad_norm": 0.004139785189181566,
      "learning_rate": 3.37088e-06,
      "loss": 0.0002,
      "step": 77950
    },
    {
      "epoch": 2.49472,
      "grad_norm": 0.001920137321576476,
      "learning_rate": 3.3687466666666667e-06,
      "loss": 0.0014,
      "step": 77960
    },
    {
      "epoch": 2.49504,
      "grad_norm": 0.0023487659636884928,
      "learning_rate": 3.3666133333333334e-06,
      "loss": 0.0002,
      "step": 77970
    },
    {
      "epoch": 2.49536,
      "grad_norm": 0.004674701951444149,
      "learning_rate": 3.3644800000000004e-06,
      "loss": 0.0473,
      "step": 77980
    },
    {
      "epoch": 2.49568,
      "grad_norm": 0.0033044160809367895,
      "learning_rate": 3.362346666666667e-06,
      "loss": 0.0061,
      "step": 77990
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.012516715563833714,
      "learning_rate": 3.3602133333333337e-06,
      "loss": 0.0002,
      "step": 78000
    },
    {
      "epoch": 2.49632,
      "grad_norm": 0.0034472248516976833,
      "learning_rate": 3.3580800000000004e-06,
      "loss": 0.0006,
      "step": 78010
    },
    {
      "epoch": 2.49664,
      "grad_norm": 0.01754099875688553,
      "learning_rate": 3.3559466666666666e-06,
      "loss": 0.0021,
      "step": 78020
    },
    {
      "epoch": 2.49696,
      "grad_norm": 0.002039501676335931,
      "learning_rate": 3.3538133333333333e-06,
      "loss": 0.0002,
      "step": 78030
    },
    {
      "epoch": 2.49728,
      "grad_norm": 0.007761715445667505,
      "learning_rate": 3.3516800000000003e-06,
      "loss": 0.0002,
      "step": 78040
    },
    {
      "epoch": 2.4976,
      "grad_norm": 0.00554232532158494,
      "learning_rate": 3.349546666666667e-06,
      "loss": 0.0507,
      "step": 78050
    },
    {
      "epoch": 2.49792,
      "grad_norm": 0.002979811280965805,
      "learning_rate": 3.3474133333333336e-06,
      "loss": 0.0002,
      "step": 78060
    },
    {
      "epoch": 2.49824,
      "grad_norm": 0.005604620091617107,
      "learning_rate": 3.3452800000000003e-06,
      "loss": 0.0002,
      "step": 78070
    },
    {
      "epoch": 2.49856,
      "grad_norm": 0.006315913982689381,
      "learning_rate": 3.343146666666667e-06,
      "loss": 0.0404,
      "step": 78080
    },
    {
      "epoch": 2.4988799999999998,
      "grad_norm": 0.006892456207424402,
      "learning_rate": 3.341013333333333e-06,
      "loss": 0.0013,
      "step": 78090
    },
    {
      "epoch": 2.4992,
      "grad_norm": 0.0012428112095221877,
      "learning_rate": 3.3388800000000006e-06,
      "loss": 0.0024,
      "step": 78100
    },
    {
      "epoch": 2.49952,
      "grad_norm": 0.004416300915181637,
      "learning_rate": 3.336746666666667e-06,
      "loss": 0.0002,
      "step": 78110
    },
    {
      "epoch": 2.49984,
      "grad_norm": 0.0066499896347522736,
      "learning_rate": 3.3346133333333335e-06,
      "loss": 0.0003,
      "step": 78120
    },
    {
      "epoch": 2.50016,
      "grad_norm": 0.01409261766821146,
      "learning_rate": 3.33248e-06,
      "loss": 0.0002,
      "step": 78130
    },
    {
      "epoch": 2.50048,
      "grad_norm": 0.0032699834555387497,
      "learning_rate": 3.330346666666667e-06,
      "loss": 0.0001,
      "step": 78140
    },
    {
      "epoch": 2.5008,
      "grad_norm": 0.0017114657675847411,
      "learning_rate": 3.328213333333334e-06,
      "loss": 0.0001,
      "step": 78150
    },
    {
      "epoch": 2.5011200000000002,
      "grad_norm": 0.002293273573741317,
      "learning_rate": 3.3260800000000005e-06,
      "loss": 0.0002,
      "step": 78160
    },
    {
      "epoch": 2.50144,
      "grad_norm": 0.0012878854759037495,
      "learning_rate": 3.323946666666667e-06,
      "loss": 0.0002,
      "step": 78170
    },
    {
      "epoch": 2.50176,
      "grad_norm": 0.02384013868868351,
      "learning_rate": 3.3218133333333334e-06,
      "loss": 0.0002,
      "step": 78180
    },
    {
      "epoch": 2.50208,
      "grad_norm": 0.002913564210757613,
      "learning_rate": 3.31968e-06,
      "loss": 0.0002,
      "step": 78190
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 0.004789234139025211,
      "learning_rate": 3.3175466666666667e-06,
      "loss": 0.0001,
      "step": 78200
    },
    {
      "epoch": 2.50272,
      "grad_norm": 0.0032498298678547144,
      "learning_rate": 3.3154133333333338e-06,
      "loss": 0.0002,
      "step": 78210
    },
    {
      "epoch": 2.50304,
      "grad_norm": 0.002405290026217699,
      "learning_rate": 3.3132800000000004e-06,
      "loss": 0.0002,
      "step": 78220
    },
    {
      "epoch": 2.50336,
      "grad_norm": 0.0036563281901180744,
      "learning_rate": 3.311146666666667e-06,
      "loss": 0.0001,
      "step": 78230
    },
    {
      "epoch": 2.50368,
      "grad_norm": 0.054837316274642944,
      "learning_rate": 3.3090133333333337e-06,
      "loss": 0.0008,
      "step": 78240
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.0020928499288856983,
      "learning_rate": 3.30688e-06,
      "loss": 0.0002,
      "step": 78250
    },
    {
      "epoch": 2.50432,
      "grad_norm": 0.07273349165916443,
      "learning_rate": 3.3047466666666674e-06,
      "loss": 0.0003,
      "step": 78260
    },
    {
      "epoch": 2.50464,
      "grad_norm": 0.002812638645991683,
      "learning_rate": 3.3026133333333336e-06,
      "loss": 0.0002,
      "step": 78270
    },
    {
      "epoch": 2.50496,
      "grad_norm": 0.007293877191841602,
      "learning_rate": 3.3004800000000003e-06,
      "loss": 0.0003,
      "step": 78280
    },
    {
      "epoch": 2.50528,
      "grad_norm": 0.008341138251125813,
      "learning_rate": 3.298346666666667e-06,
      "loss": 0.0002,
      "step": 78290
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 0.03655874356627464,
      "learning_rate": 3.2962133333333336e-06,
      "loss": 0.0447,
      "step": 78300
    },
    {
      "epoch": 2.50592,
      "grad_norm": 0.0038827788084745407,
      "learning_rate": 3.2940800000000002e-06,
      "loss": 0.0002,
      "step": 78310
    },
    {
      "epoch": 2.50624,
      "grad_norm": 0.002237088978290558,
      "learning_rate": 3.2919466666666673e-06,
      "loss": 0.0003,
      "step": 78320
    },
    {
      "epoch": 2.50656,
      "grad_norm": 0.003443164052441716,
      "learning_rate": 3.289813333333334e-06,
      "loss": 0.0002,
      "step": 78330
    },
    {
      "epoch": 2.5068799999999998,
      "grad_norm": 0.002124861581251025,
      "learning_rate": 3.28768e-06,
      "loss": 0.0002,
      "step": 78340
    },
    {
      "epoch": 2.5072,
      "grad_norm": 0.0038086867425590754,
      "learning_rate": 3.285546666666667e-06,
      "loss": 0.0001,
      "step": 78350
    },
    {
      "epoch": 2.50752,
      "grad_norm": 0.0023157994728535414,
      "learning_rate": 3.2834133333333335e-06,
      "loss": 0.0002,
      "step": 78360
    },
    {
      "epoch": 2.50784,
      "grad_norm": 0.0034091442357748747,
      "learning_rate": 3.28128e-06,
      "loss": 0.0002,
      "step": 78370
    },
    {
      "epoch": 2.50816,
      "grad_norm": 0.008110200986266136,
      "learning_rate": 3.279146666666667e-06,
      "loss": 0.0002,
      "step": 78380
    },
    {
      "epoch": 2.50848,
      "grad_norm": 0.001441852655261755,
      "learning_rate": 3.277013333333334e-06,
      "loss": 0.0005,
      "step": 78390
    },
    {
      "epoch": 2.5088,
      "grad_norm": 0.0034185380209237337,
      "learning_rate": 3.2748800000000005e-06,
      "loss": 0.0001,
      "step": 78400
    },
    {
      "epoch": 2.5091200000000002,
      "grad_norm": 0.004747017752379179,
      "learning_rate": 3.2727466666666667e-06,
      "loss": 0.0007,
      "step": 78410
    },
    {
      "epoch": 2.50944,
      "grad_norm": 0.0052972096018493176,
      "learning_rate": 3.2706133333333334e-06,
      "loss": 0.0211,
      "step": 78420
    },
    {
      "epoch": 2.50976,
      "grad_norm": 0.0033389385789632797,
      "learning_rate": 3.2684800000000004e-06,
      "loss": 0.0463,
      "step": 78430
    },
    {
      "epoch": 2.51008,
      "grad_norm": 0.02469220571219921,
      "learning_rate": 3.266346666666667e-06,
      "loss": 0.0002,
      "step": 78440
    },
    {
      "epoch": 2.5103999999999997,
      "grad_norm": 0.0506322868168354,
      "learning_rate": 3.2642133333333337e-06,
      "loss": 0.0003,
      "step": 78450
    },
    {
      "epoch": 2.51072,
      "grad_norm": 0.07767762988805771,
      "learning_rate": 3.2620800000000004e-06,
      "loss": 0.0003,
      "step": 78460
    },
    {
      "epoch": 2.51104,
      "grad_norm": 0.004201671574264765,
      "learning_rate": 3.259946666666667e-06,
      "loss": 0.0002,
      "step": 78470
    },
    {
      "epoch": 2.51136,
      "grad_norm": 0.008638434112071991,
      "learning_rate": 3.2578133333333332e-06,
      "loss": 0.0003,
      "step": 78480
    },
    {
      "epoch": 2.51168,
      "grad_norm": 0.0028523283544927835,
      "learning_rate": 3.2556800000000003e-06,
      "loss": 0.0002,
      "step": 78490
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.01779135875403881,
      "learning_rate": 3.253546666666667e-06,
      "loss": 0.0002,
      "step": 78500
    },
    {
      "epoch": 2.51232,
      "grad_norm": 0.003239533631131053,
      "learning_rate": 3.2514133333333336e-06,
      "loss": 0.0436,
      "step": 78510
    },
    {
      "epoch": 2.51264,
      "grad_norm": 0.004052924923598766,
      "learning_rate": 3.2492800000000003e-06,
      "loss": 0.0153,
      "step": 78520
    },
    {
      "epoch": 2.51296,
      "grad_norm": 0.002258982975035906,
      "learning_rate": 3.247146666666667e-06,
      "loss": 0.0005,
      "step": 78530
    },
    {
      "epoch": 2.51328,
      "grad_norm": 0.0015599820762872696,
      "learning_rate": 3.2450133333333335e-06,
      "loss": 0.0002,
      "step": 78540
    },
    {
      "epoch": 2.5136,
      "grad_norm": 0.003553826827555895,
      "learning_rate": 3.2428800000000006e-06,
      "loss": 0.0002,
      "step": 78550
    },
    {
      "epoch": 2.51392,
      "grad_norm": 0.051161255687475204,
      "learning_rate": 3.240746666666667e-06,
      "loss": 0.0012,
      "step": 78560
    },
    {
      "epoch": 2.51424,
      "grad_norm": 0.6784212589263916,
      "learning_rate": 3.2386133333333335e-06,
      "loss": 0.0041,
      "step": 78570
    },
    {
      "epoch": 2.51456,
      "grad_norm": 0.016651444137096405,
      "learning_rate": 3.23648e-06,
      "loss": 0.0002,
      "step": 78580
    },
    {
      "epoch": 2.51488,
      "grad_norm": 0.004231791943311691,
      "learning_rate": 3.2343466666666668e-06,
      "loss": 0.0068,
      "step": 78590
    },
    {
      "epoch": 2.5152,
      "grad_norm": 0.005410376936197281,
      "learning_rate": 3.232213333333334e-06,
      "loss": 0.0002,
      "step": 78600
    },
    {
      "epoch": 2.51552,
      "grad_norm": 0.001701529137790203,
      "learning_rate": 3.2300800000000005e-06,
      "loss": 0.0002,
      "step": 78610
    },
    {
      "epoch": 2.51584,
      "grad_norm": 0.0009322675759904087,
      "learning_rate": 3.227946666666667e-06,
      "loss": 0.0002,
      "step": 78620
    },
    {
      "epoch": 2.51616,
      "grad_norm": 0.0036494669038802385,
      "learning_rate": 3.2258133333333334e-06,
      "loss": 0.0001,
      "step": 78630
    },
    {
      "epoch": 2.51648,
      "grad_norm": 0.006319382693618536,
      "learning_rate": 3.22368e-06,
      "loss": 0.0496,
      "step": 78640
    },
    {
      "epoch": 2.5168,
      "grad_norm": 0.002895851619541645,
      "learning_rate": 3.2215466666666667e-06,
      "loss": 0.0002,
      "step": 78650
    },
    {
      "epoch": 2.5171200000000002,
      "grad_norm": 0.0051748622208833694,
      "learning_rate": 3.2194133333333337e-06,
      "loss": 0.0309,
      "step": 78660
    },
    {
      "epoch": 2.51744,
      "grad_norm": 0.006946988869458437,
      "learning_rate": 3.2172800000000004e-06,
      "loss": 0.0001,
      "step": 78670
    },
    {
      "epoch": 2.51776,
      "grad_norm": 0.0037419593427330256,
      "learning_rate": 3.215146666666667e-06,
      "loss": 0.0002,
      "step": 78680
    },
    {
      "epoch": 2.51808,
      "grad_norm": 0.01978445053100586,
      "learning_rate": 3.2130133333333337e-06,
      "loss": 0.0002,
      "step": 78690
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 0.0018020992865785956,
      "learning_rate": 3.21088e-06,
      "loss": 0.0004,
      "step": 78700
    },
    {
      "epoch": 2.51872,
      "grad_norm": 0.007243097759783268,
      "learning_rate": 3.2087466666666666e-06,
      "loss": 0.0002,
      "step": 78710
    },
    {
      "epoch": 2.51904,
      "grad_norm": 0.011283665895462036,
      "learning_rate": 3.2066133333333336e-06,
      "loss": 0.0548,
      "step": 78720
    },
    {
      "epoch": 2.51936,
      "grad_norm": 0.014181016013026237,
      "learning_rate": 3.2044800000000003e-06,
      "loss": 0.0002,
      "step": 78730
    },
    {
      "epoch": 2.51968,
      "grad_norm": 0.0077164312824606895,
      "learning_rate": 3.202346666666667e-06,
      "loss": 0.0072,
      "step": 78740
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.002840661210939288,
      "learning_rate": 3.2002133333333336e-06,
      "loss": 0.0003,
      "step": 78750
    },
    {
      "epoch": 2.52032,
      "grad_norm": 0.004057286307215691,
      "learning_rate": 3.1980800000000002e-06,
      "loss": 0.0002,
      "step": 78760
    },
    {
      "epoch": 2.52064,
      "grad_norm": 0.00391351105645299,
      "learning_rate": 3.1959466666666673e-06,
      "loss": 0.0001,
      "step": 78770
    },
    {
      "epoch": 2.52096,
      "grad_norm": 0.004407880362123251,
      "learning_rate": 3.193813333333334e-06,
      "loss": 0.0002,
      "step": 78780
    },
    {
      "epoch": 2.52128,
      "grad_norm": 0.0031503294594585896,
      "learning_rate": 3.19168e-06,
      "loss": 0.0002,
      "step": 78790
    },
    {
      "epoch": 2.5216,
      "grad_norm": 0.008271676488220692,
      "learning_rate": 3.189546666666667e-06,
      "loss": 0.0346,
      "step": 78800
    },
    {
      "epoch": 2.52192,
      "grad_norm": 0.002624697284772992,
      "learning_rate": 3.1874133333333335e-06,
      "loss": 0.0001,
      "step": 78810
    },
    {
      "epoch": 2.52224,
      "grad_norm": 0.006905372254550457,
      "learning_rate": 3.18528e-06,
      "loss": 0.0001,
      "step": 78820
    },
    {
      "epoch": 2.52256,
      "grad_norm": 0.0026114308275282383,
      "learning_rate": 3.183146666666667e-06,
      "loss": 0.0002,
      "step": 78830
    },
    {
      "epoch": 2.52288,
      "grad_norm": 0.0023406795226037502,
      "learning_rate": 3.181013333333334e-06,
      "loss": 0.0001,
      "step": 78840
    },
    {
      "epoch": 2.5232,
      "grad_norm": 0.003634435124695301,
      "learning_rate": 3.1788800000000005e-06,
      "loss": 0.0004,
      "step": 78850
    },
    {
      "epoch": 2.52352,
      "grad_norm": 0.002162617165595293,
      "learning_rate": 3.1767466666666667e-06,
      "loss": 0.0011,
      "step": 78860
    },
    {
      "epoch": 2.52384,
      "grad_norm": 0.005783634725958109,
      "learning_rate": 3.1746133333333333e-06,
      "loss": 0.0029,
      "step": 78870
    },
    {
      "epoch": 2.52416,
      "grad_norm": 0.019756484776735306,
      "learning_rate": 3.17248e-06,
      "loss": 0.0003,
      "step": 78880
    },
    {
      "epoch": 2.52448,
      "grad_norm": 0.007225462701171637,
      "learning_rate": 3.170346666666667e-06,
      "loss": 0.0004,
      "step": 78890
    },
    {
      "epoch": 2.5248,
      "grad_norm": 0.01709386520087719,
      "learning_rate": 3.1682133333333337e-06,
      "loss": 0.0002,
      "step": 78900
    },
    {
      "epoch": 2.5251200000000003,
      "grad_norm": 0.0037336936220526695,
      "learning_rate": 3.1660800000000004e-06,
      "loss": 0.0003,
      "step": 78910
    },
    {
      "epoch": 2.52544,
      "grad_norm": 0.006504887714982033,
      "learning_rate": 3.163946666666667e-06,
      "loss": 0.0001,
      "step": 78920
    },
    {
      "epoch": 2.52576,
      "grad_norm": 0.01569449156522751,
      "learning_rate": 3.1618133333333332e-06,
      "loss": 0.0003,
      "step": 78930
    },
    {
      "epoch": 2.52608,
      "grad_norm": 0.0064275749027729034,
      "learning_rate": 3.1596800000000007e-06,
      "loss": 0.0571,
      "step": 78940
    },
    {
      "epoch": 2.5263999999999998,
      "grad_norm": 0.0027935446705669165,
      "learning_rate": 3.157546666666667e-06,
      "loss": 0.0002,
      "step": 78950
    },
    {
      "epoch": 2.52672,
      "grad_norm": 0.0033173388801515102,
      "learning_rate": 3.1554133333333336e-06,
      "loss": 0.0002,
      "step": 78960
    },
    {
      "epoch": 2.52704,
      "grad_norm": 0.004208933096379042,
      "learning_rate": 3.1532800000000002e-06,
      "loss": 0.0001,
      "step": 78970
    },
    {
      "epoch": 2.52736,
      "grad_norm": 0.008574658073484898,
      "learning_rate": 3.151146666666667e-06,
      "loss": 0.0002,
      "step": 78980
    },
    {
      "epoch": 2.52768,
      "grad_norm": 0.0021586271468549967,
      "learning_rate": 3.1490133333333335e-06,
      "loss": 0.0042,
      "step": 78990
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.0027616124134510756,
      "learning_rate": 3.1468800000000006e-06,
      "loss": 0.0002,
      "step": 79000
    },
    {
      "epoch": 2.52832,
      "grad_norm": 0.0024739564396440983,
      "learning_rate": 3.1447466666666673e-06,
      "loss": 0.0126,
      "step": 79010
    },
    {
      "epoch": 2.52864,
      "grad_norm": 0.0033168247900903225,
      "learning_rate": 3.1426133333333335e-06,
      "loss": 0.0001,
      "step": 79020
    },
    {
      "epoch": 2.52896,
      "grad_norm": 0.002812341321259737,
      "learning_rate": 3.14048e-06,
      "loss": 0.0002,
      "step": 79030
    },
    {
      "epoch": 2.52928,
      "grad_norm": 0.0069426000118255615,
      "learning_rate": 3.1383466666666668e-06,
      "loss": 0.0002,
      "step": 79040
    },
    {
      "epoch": 2.5296,
      "grad_norm": 0.008239259012043476,
      "learning_rate": 3.1362133333333334e-06,
      "loss": 0.0003,
      "step": 79050
    },
    {
      "epoch": 2.5299199999999997,
      "grad_norm": 0.003995866049081087,
      "learning_rate": 3.1340800000000005e-06,
      "loss": 0.0001,
      "step": 79060
    },
    {
      "epoch": 2.53024,
      "grad_norm": 0.0022152797318995,
      "learning_rate": 3.131946666666667e-06,
      "loss": 0.0002,
      "step": 79070
    },
    {
      "epoch": 2.53056,
      "grad_norm": 0.014284918084740639,
      "learning_rate": 3.1298133333333338e-06,
      "loss": 0.0002,
      "step": 79080
    },
    {
      "epoch": 2.53088,
      "grad_norm": 0.0037258060183376074,
      "learning_rate": 3.12768e-06,
      "loss": 0.0001,
      "step": 79090
    },
    {
      "epoch": 2.5312,
      "grad_norm": 0.003717985935509205,
      "learning_rate": 3.1255466666666667e-06,
      "loss": 0.0002,
      "step": 79100
    },
    {
      "epoch": 2.53152,
      "grad_norm": 0.01300668716430664,
      "learning_rate": 3.1234133333333337e-06,
      "loss": 0.0002,
      "step": 79110
    },
    {
      "epoch": 2.53184,
      "grad_norm": 0.0036749953869730234,
      "learning_rate": 3.1212800000000004e-06,
      "loss": 0.0047,
      "step": 79120
    },
    {
      "epoch": 2.53216,
      "grad_norm": 0.001905360841192305,
      "learning_rate": 3.119146666666667e-06,
      "loss": 0.0002,
      "step": 79130
    },
    {
      "epoch": 2.53248,
      "grad_norm": 0.0021512173116207123,
      "learning_rate": 3.1170133333333337e-06,
      "loss": 0.0039,
      "step": 79140
    },
    {
      "epoch": 2.5328,
      "grad_norm": 0.0036766398698091507,
      "learning_rate": 3.1148800000000003e-06,
      "loss": 0.0042,
      "step": 79150
    },
    {
      "epoch": 2.5331200000000003,
      "grad_norm": 0.0038074154872447252,
      "learning_rate": 3.1127466666666665e-06,
      "loss": 0.0002,
      "step": 79160
    },
    {
      "epoch": 2.53344,
      "grad_norm": 0.002151086227968335,
      "learning_rate": 3.1106133333333336e-06,
      "loss": 0.0389,
      "step": 79170
    },
    {
      "epoch": 2.53376,
      "grad_norm": 0.002382708480581641,
      "learning_rate": 3.1084800000000003e-06,
      "loss": 0.0001,
      "step": 79180
    },
    {
      "epoch": 2.53408,
      "grad_norm": 0.00347761157900095,
      "learning_rate": 3.106346666666667e-06,
      "loss": 0.0008,
      "step": 79190
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 0.0032735501881688833,
      "learning_rate": 3.1042133333333336e-06,
      "loss": 0.0227,
      "step": 79200
    },
    {
      "epoch": 2.53472,
      "grad_norm": 0.0017990396590903401,
      "learning_rate": 3.10208e-06,
      "loss": 0.0728,
      "step": 79210
    },
    {
      "epoch": 2.53504,
      "grad_norm": 0.02515372447669506,
      "learning_rate": 3.099946666666667e-06,
      "loss": 0.0009,
      "step": 79220
    },
    {
      "epoch": 2.53536,
      "grad_norm": 0.003943574149161577,
      "learning_rate": 3.097813333333334e-06,
      "loss": 0.0002,
      "step": 79230
    },
    {
      "epoch": 2.53568,
      "grad_norm": 0.00446619326248765,
      "learning_rate": 3.09568e-06,
      "loss": 0.0002,
      "step": 79240
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.0023328629322350025,
      "learning_rate": 3.093546666666667e-06,
      "loss": 0.0005,
      "step": 79250
    },
    {
      "epoch": 2.53632,
      "grad_norm": 0.0016157933278009295,
      "learning_rate": 3.0914133333333334e-06,
      "loss": 0.0002,
      "step": 79260
    },
    {
      "epoch": 2.5366400000000002,
      "grad_norm": 0.008835794404149055,
      "learning_rate": 3.08928e-06,
      "loss": 0.0268,
      "step": 79270
    },
    {
      "epoch": 2.53696,
      "grad_norm": 0.0030129053629934788,
      "learning_rate": 3.087146666666667e-06,
      "loss": 0.0002,
      "step": 79280
    },
    {
      "epoch": 2.53728,
      "grad_norm": 0.002443209057673812,
      "learning_rate": 3.085013333333334e-06,
      "loss": 0.0002,
      "step": 79290
    },
    {
      "epoch": 2.5376,
      "grad_norm": 0.005413140170276165,
      "learning_rate": 3.0828800000000005e-06,
      "loss": 0.0003,
      "step": 79300
    },
    {
      "epoch": 2.5379199999999997,
      "grad_norm": 0.004744845442473888,
      "learning_rate": 3.0807466666666667e-06,
      "loss": 0.0002,
      "step": 79310
    },
    {
      "epoch": 2.53824,
      "grad_norm": 0.019419897347688675,
      "learning_rate": 3.0786133333333333e-06,
      "loss": 0.0002,
      "step": 79320
    },
    {
      "epoch": 2.53856,
      "grad_norm": 0.0018421505810692906,
      "learning_rate": 3.07648e-06,
      "loss": 0.0007,
      "step": 79330
    },
    {
      "epoch": 2.53888,
      "grad_norm": 0.009441385976970196,
      "learning_rate": 3.074346666666667e-06,
      "loss": 0.0003,
      "step": 79340
    },
    {
      "epoch": 2.5392,
      "grad_norm": 0.008145870640873909,
      "learning_rate": 3.0722133333333337e-06,
      "loss": 0.0004,
      "step": 79350
    },
    {
      "epoch": 2.53952,
      "grad_norm": 0.005290399771183729,
      "learning_rate": 3.0700800000000003e-06,
      "loss": 0.0002,
      "step": 79360
    },
    {
      "epoch": 2.53984,
      "grad_norm": 0.0015929769724607468,
      "learning_rate": 3.067946666666667e-06,
      "loss": 0.0001,
      "step": 79370
    },
    {
      "epoch": 2.54016,
      "grad_norm": 0.0024867954198271036,
      "learning_rate": 3.0658133333333332e-06,
      "loss": 0.0001,
      "step": 79380
    },
    {
      "epoch": 2.54048,
      "grad_norm": 0.003444184549152851,
      "learning_rate": 3.06368e-06,
      "loss": 0.0002,
      "step": 79390
    },
    {
      "epoch": 2.5408,
      "grad_norm": 0.002237448701635003,
      "learning_rate": 3.061546666666667e-06,
      "loss": 0.0002,
      "step": 79400
    },
    {
      "epoch": 2.5411200000000003,
      "grad_norm": 0.08361825346946716,
      "learning_rate": 3.0594133333333336e-06,
      "loss": 0.0003,
      "step": 79410
    },
    {
      "epoch": 2.54144,
      "grad_norm": 0.04807069152593613,
      "learning_rate": 3.0572800000000002e-06,
      "loss": 0.0002,
      "step": 79420
    },
    {
      "epoch": 2.54176,
      "grad_norm": 0.0023754420690238476,
      "learning_rate": 3.055146666666667e-06,
      "loss": 0.0003,
      "step": 79430
    },
    {
      "epoch": 2.54208,
      "grad_norm": 0.00582968071103096,
      "learning_rate": 3.0530133333333335e-06,
      "loss": 0.0001,
      "step": 79440
    },
    {
      "epoch": 2.5423999999999998,
      "grad_norm": 0.003177695209160447,
      "learning_rate": 3.0508800000000006e-06,
      "loss": 0.0008,
      "step": 79450
    },
    {
      "epoch": 2.54272,
      "grad_norm": 0.008239535614848137,
      "learning_rate": 3.0487466666666672e-06,
      "loss": 0.0003,
      "step": 79460
    },
    {
      "epoch": 2.54304,
      "grad_norm": 0.0020753892604261637,
      "learning_rate": 3.0466133333333335e-06,
      "loss": 0.0001,
      "step": 79470
    },
    {
      "epoch": 2.54336,
      "grad_norm": 0.0041457051411271095,
      "learning_rate": 3.04448e-06,
      "loss": 0.0495,
      "step": 79480
    },
    {
      "epoch": 2.54368,
      "grad_norm": 0.009136168286204338,
      "learning_rate": 3.0423466666666668e-06,
      "loss": 0.0101,
      "step": 79490
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.002237029606476426,
      "learning_rate": 3.0402133333333334e-06,
      "loss": 0.0029,
      "step": 79500
    },
    {
      "epoch": 2.54432,
      "grad_norm": 0.00442085973918438,
      "learning_rate": 3.0380800000000005e-06,
      "loss": 0.0007,
      "step": 79510
    },
    {
      "epoch": 2.5446400000000002,
      "grad_norm": 0.0013402689946815372,
      "learning_rate": 3.035946666666667e-06,
      "loss": 0.0002,
      "step": 79520
    },
    {
      "epoch": 2.54496,
      "grad_norm": 0.23022258281707764,
      "learning_rate": 3.0338133333333338e-06,
      "loss": 0.0004,
      "step": 79530
    },
    {
      "epoch": 2.54528,
      "grad_norm": 0.011113638058304787,
      "learning_rate": 3.03168e-06,
      "loss": 0.0002,
      "step": 79540
    },
    {
      "epoch": 2.5456,
      "grad_norm": 0.003493433352559805,
      "learning_rate": 3.0295466666666666e-06,
      "loss": 0.0002,
      "step": 79550
    },
    {
      "epoch": 2.5459199999999997,
      "grad_norm": 0.0034411493688821793,
      "learning_rate": 3.0274133333333333e-06,
      "loss": 0.0001,
      "step": 79560
    },
    {
      "epoch": 2.54624,
      "grad_norm": 0.00427319947630167,
      "learning_rate": 3.0252800000000004e-06,
      "loss": 0.0005,
      "step": 79570
    },
    {
      "epoch": 2.54656,
      "grad_norm": 0.002923000371083617,
      "learning_rate": 3.023146666666667e-06,
      "loss": 0.0004,
      "step": 79580
    },
    {
      "epoch": 2.54688,
      "grad_norm": 0.0027342468965798616,
      "learning_rate": 3.0210133333333337e-06,
      "loss": 0.045,
      "step": 79590
    },
    {
      "epoch": 2.5472,
      "grad_norm": 0.0027686101384460926,
      "learning_rate": 3.0188800000000003e-06,
      "loss": 0.0002,
      "step": 79600
    },
    {
      "epoch": 2.54752,
      "grad_norm": 0.006518922746181488,
      "learning_rate": 3.0167466666666665e-06,
      "loss": 0.0003,
      "step": 79610
    },
    {
      "epoch": 2.54784,
      "grad_norm": 0.011943851597607136,
      "learning_rate": 3.014613333333334e-06,
      "loss": 0.0002,
      "step": 79620
    },
    {
      "epoch": 2.54816,
      "grad_norm": 0.005309258121997118,
      "learning_rate": 3.0124800000000003e-06,
      "loss": 0.0005,
      "step": 79630
    },
    {
      "epoch": 2.54848,
      "grad_norm": 0.004796819295734167,
      "learning_rate": 3.010346666666667e-06,
      "loss": 0.0001,
      "step": 79640
    },
    {
      "epoch": 2.5488,
      "grad_norm": 0.04015003517270088,
      "learning_rate": 3.0082133333333335e-06,
      "loss": 0.041,
      "step": 79650
    },
    {
      "epoch": 2.5491200000000003,
      "grad_norm": 0.002612491836771369,
      "learning_rate": 3.00608e-06,
      "loss": 0.0003,
      "step": 79660
    },
    {
      "epoch": 2.54944,
      "grad_norm": 0.013181467540562153,
      "learning_rate": 3.003946666666667e-06,
      "loss": 0.0002,
      "step": 79670
    },
    {
      "epoch": 2.54976,
      "grad_norm": 0.004630563780665398,
      "learning_rate": 3.001813333333334e-06,
      "loss": 0.0002,
      "step": 79680
    },
    {
      "epoch": 2.55008,
      "grad_norm": 0.0036292332224547863,
      "learning_rate": 2.9996800000000006e-06,
      "loss": 0.0005,
      "step": 79690
    },
    {
      "epoch": 2.5504,
      "grad_norm": 0.005495238117873669,
      "learning_rate": 2.9975466666666668e-06,
      "loss": 0.0003,
      "step": 79700
    },
    {
      "epoch": 2.55072,
      "grad_norm": 0.0024780817329883575,
      "learning_rate": 2.9954133333333334e-06,
      "loss": 0.0002,
      "step": 79710
    },
    {
      "epoch": 2.55104,
      "grad_norm": 0.00863663386553526,
      "learning_rate": 2.99328e-06,
      "loss": 0.0002,
      "step": 79720
    },
    {
      "epoch": 2.55136,
      "grad_norm": 0.3156876862049103,
      "learning_rate": 2.9911466666666667e-06,
      "loss": 0.0004,
      "step": 79730
    },
    {
      "epoch": 2.55168,
      "grad_norm": 0.004113984294235706,
      "learning_rate": 2.989013333333334e-06,
      "loss": 0.0001,
      "step": 79740
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.0022654468193650246,
      "learning_rate": 2.9868800000000004e-06,
      "loss": 0.0005,
      "step": 79750
    },
    {
      "epoch": 2.55232,
      "grad_norm": 0.0025139895733445883,
      "learning_rate": 2.984746666666667e-06,
      "loss": 0.0363,
      "step": 79760
    },
    {
      "epoch": 2.5526400000000002,
      "grad_norm": 0.006826773751527071,
      "learning_rate": 2.9826133333333333e-06,
      "loss": 0.0002,
      "step": 79770
    },
    {
      "epoch": 2.55296,
      "grad_norm": 0.002260955749079585,
      "learning_rate": 2.98048e-06,
      "loss": 0.0002,
      "step": 79780
    },
    {
      "epoch": 2.55328,
      "grad_norm": 0.0014886228600516915,
      "learning_rate": 2.978346666666667e-06,
      "loss": 0.0001,
      "step": 79790
    },
    {
      "epoch": 2.5536,
      "grad_norm": 0.0036226147785782814,
      "learning_rate": 2.9762133333333337e-06,
      "loss": 0.0002,
      "step": 79800
    },
    {
      "epoch": 2.5539199999999997,
      "grad_norm": 0.005253743380308151,
      "learning_rate": 2.9740800000000003e-06,
      "loss": 0.0002,
      "step": 79810
    },
    {
      "epoch": 2.55424,
      "grad_norm": 0.009181048721075058,
      "learning_rate": 2.971946666666667e-06,
      "loss": 0.0084,
      "step": 79820
    },
    {
      "epoch": 2.55456,
      "grad_norm": 0.0025675653014332056,
      "learning_rate": 2.9698133333333336e-06,
      "loss": 0.0046,
      "step": 79830
    },
    {
      "epoch": 2.55488,
      "grad_norm": 0.0031590599101036787,
      "learning_rate": 2.96768e-06,
      "loss": 0.0002,
      "step": 79840
    },
    {
      "epoch": 2.5552,
      "grad_norm": 0.027481431141495705,
      "learning_rate": 2.965546666666667e-06,
      "loss": 0.0002,
      "step": 79850
    },
    {
      "epoch": 2.55552,
      "grad_norm": 0.0020560440607368946,
      "learning_rate": 2.9634133333333336e-06,
      "loss": 0.0024,
      "step": 79860
    },
    {
      "epoch": 2.55584,
      "grad_norm": 0.003920558374375105,
      "learning_rate": 2.9612800000000002e-06,
      "loss": 0.0136,
      "step": 79870
    },
    {
      "epoch": 2.55616,
      "grad_norm": 0.004017147701233625,
      "learning_rate": 2.959146666666667e-06,
      "loss": 0.0662,
      "step": 79880
    },
    {
      "epoch": 2.55648,
      "grad_norm": 0.01046527735888958,
      "learning_rate": 2.9570133333333335e-06,
      "loss": 0.0002,
      "step": 79890
    },
    {
      "epoch": 2.5568,
      "grad_norm": 0.004098073113709688,
      "learning_rate": 2.95488e-06,
      "loss": 0.0808,
      "step": 79900
    },
    {
      "epoch": 2.55712,
      "grad_norm": 0.007841896265745163,
      "learning_rate": 2.9527466666666672e-06,
      "loss": 0.0005,
      "step": 79910
    },
    {
      "epoch": 2.55744,
      "grad_norm": 0.0024954958353191614,
      "learning_rate": 2.9506133333333335e-06,
      "loss": 0.0001,
      "step": 79920
    },
    {
      "epoch": 2.55776,
      "grad_norm": 0.0032640795689076185,
      "learning_rate": 2.94848e-06,
      "loss": 0.0004,
      "step": 79930
    },
    {
      "epoch": 2.55808,
      "grad_norm": 0.004503125790506601,
      "learning_rate": 2.9463466666666667e-06,
      "loss": 0.0002,
      "step": 79940
    },
    {
      "epoch": 2.5584,
      "grad_norm": 0.00964736845344305,
      "learning_rate": 2.9442133333333334e-06,
      "loss": 0.0002,
      "step": 79950
    },
    {
      "epoch": 2.55872,
      "grad_norm": 0.005938237998634577,
      "learning_rate": 2.9420800000000005e-06,
      "loss": 0.0002,
      "step": 79960
    },
    {
      "epoch": 2.55904,
      "grad_norm": 0.01486416719853878,
      "learning_rate": 2.939946666666667e-06,
      "loss": 0.0003,
      "step": 79970
    },
    {
      "epoch": 2.55936,
      "grad_norm": 0.006233167834579945,
      "learning_rate": 2.9378133333333338e-06,
      "loss": 0.0005,
      "step": 79980
    },
    {
      "epoch": 2.55968,
      "grad_norm": 0.002261711284518242,
      "learning_rate": 2.93568e-06,
      "loss": 0.0003,
      "step": 79990
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.0029806906823068857,
      "learning_rate": 2.9335466666666666e-06,
      "loss": 0.0605,
      "step": 80000
    },
    {
      "epoch": 2.56032,
      "grad_norm": 0.021264292299747467,
      "learning_rate": 2.9314133333333333e-06,
      "loss": 0.0534,
      "step": 80010
    },
    {
      "epoch": 2.5606400000000002,
      "grad_norm": 0.0023210658691823483,
      "learning_rate": 2.9292800000000004e-06,
      "loss": 0.0003,
      "step": 80020
    },
    {
      "epoch": 2.56096,
      "grad_norm": 0.2903367578983307,
      "learning_rate": 2.927146666666667e-06,
      "loss": 0.0004,
      "step": 80030
    },
    {
      "epoch": 2.56128,
      "grad_norm": 0.0026495049241930246,
      "learning_rate": 2.9250133333333336e-06,
      "loss": 0.0002,
      "step": 80040
    },
    {
      "epoch": 2.5616,
      "grad_norm": 0.0037906640209257603,
      "learning_rate": 2.9228800000000003e-06,
      "loss": 0.0002,
      "step": 80050
    },
    {
      "epoch": 2.5619199999999998,
      "grad_norm": 0.0017986355815082788,
      "learning_rate": 2.9207466666666665e-06,
      "loss": 0.0003,
      "step": 80060
    },
    {
      "epoch": 2.56224,
      "grad_norm": 0.010332955978810787,
      "learning_rate": 2.918613333333333e-06,
      "loss": 0.0004,
      "step": 80070
    },
    {
      "epoch": 2.56256,
      "grad_norm": 0.0032320229802280664,
      "learning_rate": 2.9164800000000002e-06,
      "loss": 0.0023,
      "step": 80080
    },
    {
      "epoch": 2.56288,
      "grad_norm": 0.0023067512083798647,
      "learning_rate": 2.914346666666667e-06,
      "loss": 0.0002,
      "step": 80090
    },
    {
      "epoch": 2.5632,
      "grad_norm": 0.002034416887909174,
      "learning_rate": 2.9122133333333335e-06,
      "loss": 0.0002,
      "step": 80100
    },
    {
      "epoch": 2.56352,
      "grad_norm": 0.001885926933027804,
      "learning_rate": 2.91008e-06,
      "loss": 0.0002,
      "step": 80110
    },
    {
      "epoch": 2.56384,
      "grad_norm": 0.002973894588649273,
      "learning_rate": 2.907946666666667e-06,
      "loss": 0.0438,
      "step": 80120
    },
    {
      "epoch": 2.56416,
      "grad_norm": 0.02531181089580059,
      "learning_rate": 2.905813333333334e-06,
      "loss": 0.0002,
      "step": 80130
    },
    {
      "epoch": 2.56448,
      "grad_norm": 0.005536215845495462,
      "learning_rate": 2.9036800000000005e-06,
      "loss": 0.0002,
      "step": 80140
    },
    {
      "epoch": 2.5648,
      "grad_norm": 0.005847688764333725,
      "learning_rate": 2.9015466666666668e-06,
      "loss": 0.0002,
      "step": 80150
    },
    {
      "epoch": 2.56512,
      "grad_norm": 0.006954669486731291,
      "learning_rate": 2.8994133333333334e-06,
      "loss": 0.0003,
      "step": 80160
    },
    {
      "epoch": 2.56544,
      "grad_norm": 0.007297431584447622,
      "learning_rate": 2.89728e-06,
      "loss": 0.0499,
      "step": 80170
    },
    {
      "epoch": 2.56576,
      "grad_norm": 0.0045952340587973595,
      "learning_rate": 2.8951466666666667e-06,
      "loss": 0.0529,
      "step": 80180
    },
    {
      "epoch": 2.56608,
      "grad_norm": 0.008312620222568512,
      "learning_rate": 2.8930133333333338e-06,
      "loss": 0.0046,
      "step": 80190
    },
    {
      "epoch": 2.5664,
      "grad_norm": 0.002170924562960863,
      "learning_rate": 2.8908800000000004e-06,
      "loss": 0.0003,
      "step": 80200
    },
    {
      "epoch": 2.56672,
      "grad_norm": 0.0015048420755192637,
      "learning_rate": 2.888746666666667e-06,
      "loss": 0.0005,
      "step": 80210
    },
    {
      "epoch": 2.56704,
      "grad_norm": 0.003691716818138957,
      "learning_rate": 2.8866133333333333e-06,
      "loss": 0.0003,
      "step": 80220
    },
    {
      "epoch": 2.56736,
      "grad_norm": 0.004719398450106382,
      "learning_rate": 2.88448e-06,
      "loss": 0.0002,
      "step": 80230
    },
    {
      "epoch": 2.56768,
      "grad_norm": 0.007704581134021282,
      "learning_rate": 2.8823466666666666e-06,
      "loss": 0.0002,
      "step": 80240
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.006070744711905718,
      "learning_rate": 2.8802133333333337e-06,
      "loss": 0.0003,
      "step": 80250
    },
    {
      "epoch": 2.56832,
      "grad_norm": 0.005063486751168966,
      "learning_rate": 2.8780800000000003e-06,
      "loss": 0.0002,
      "step": 80260
    },
    {
      "epoch": 2.5686400000000003,
      "grad_norm": 0.0025903270579874516,
      "learning_rate": 2.875946666666667e-06,
      "loss": 0.0455,
      "step": 80270
    },
    {
      "epoch": 2.56896,
      "grad_norm": 0.008402259089052677,
      "learning_rate": 2.8738133333333336e-06,
      "loss": 0.0002,
      "step": 80280
    },
    {
      "epoch": 2.56928,
      "grad_norm": 0.00290254526771605,
      "learning_rate": 2.87168e-06,
      "loss": 0.0311,
      "step": 80290
    },
    {
      "epoch": 2.5696,
      "grad_norm": 0.01600312627851963,
      "learning_rate": 2.8695466666666673e-06,
      "loss": 0.0002,
      "step": 80300
    },
    {
      "epoch": 2.5699199999999998,
      "grad_norm": 0.009609134867787361,
      "learning_rate": 2.8674133333333336e-06,
      "loss": 0.0006,
      "step": 80310
    },
    {
      "epoch": 2.57024,
      "grad_norm": 0.0038974119815975428,
      "learning_rate": 2.86528e-06,
      "loss": 0.0001,
      "step": 80320
    },
    {
      "epoch": 2.57056,
      "grad_norm": 0.04580435901880264,
      "learning_rate": 2.863146666666667e-06,
      "loss": 0.0005,
      "step": 80330
    },
    {
      "epoch": 2.57088,
      "grad_norm": 0.008933615870773792,
      "learning_rate": 2.8610133333333335e-06,
      "loss": 0.0003,
      "step": 80340
    },
    {
      "epoch": 2.5712,
      "grad_norm": 0.0013810383388772607,
      "learning_rate": 2.85888e-06,
      "loss": 0.0002,
      "step": 80350
    },
    {
      "epoch": 2.57152,
      "grad_norm": 0.004957393277436495,
      "learning_rate": 2.8567466666666672e-06,
      "loss": 0.0171,
      "step": 80360
    },
    {
      "epoch": 2.57184,
      "grad_norm": 0.005512646399438381,
      "learning_rate": 2.854613333333334e-06,
      "loss": 0.0002,
      "step": 80370
    },
    {
      "epoch": 2.5721600000000002,
      "grad_norm": 0.0162013228982687,
      "learning_rate": 2.85248e-06,
      "loss": 0.0002,
      "step": 80380
    },
    {
      "epoch": 2.57248,
      "grad_norm": 0.003042626893147826,
      "learning_rate": 2.8503466666666667e-06,
      "loss": 0.0002,
      "step": 80390
    },
    {
      "epoch": 2.5728,
      "grad_norm": 0.0027247415855526924,
      "learning_rate": 2.8482133333333334e-06,
      "loss": 0.0002,
      "step": 80400
    },
    {
      "epoch": 2.57312,
      "grad_norm": 0.005276234820485115,
      "learning_rate": 2.8460800000000005e-06,
      "loss": 0.0002,
      "step": 80410
    },
    {
      "epoch": 2.5734399999999997,
      "grad_norm": 0.005468645133078098,
      "learning_rate": 2.843946666666667e-06,
      "loss": 0.0002,
      "step": 80420
    },
    {
      "epoch": 2.57376,
      "grad_norm": 0.002251555910333991,
      "learning_rate": 2.8418133333333338e-06,
      "loss": 0.0004,
      "step": 80430
    },
    {
      "epoch": 2.57408,
      "grad_norm": 0.009146589785814285,
      "learning_rate": 2.8396800000000004e-06,
      "loss": 0.0002,
      "step": 80440
    },
    {
      "epoch": 2.5744,
      "grad_norm": 0.004108717665076256,
      "learning_rate": 2.8375466666666666e-06,
      "loss": 0.0002,
      "step": 80450
    },
    {
      "epoch": 2.57472,
      "grad_norm": 0.004055858589708805,
      "learning_rate": 2.8354133333333333e-06,
      "loss": 0.0002,
      "step": 80460
    },
    {
      "epoch": 2.57504,
      "grad_norm": 0.004705031868070364,
      "learning_rate": 2.8332800000000003e-06,
      "loss": 0.0003,
      "step": 80470
    },
    {
      "epoch": 2.57536,
      "grad_norm": 0.006119263358414173,
      "learning_rate": 2.831146666666667e-06,
      "loss": 0.0034,
      "step": 80480
    },
    {
      "epoch": 2.57568,
      "grad_norm": 0.01768847554922104,
      "learning_rate": 2.8290133333333336e-06,
      "loss": 0.0007,
      "step": 80490
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.0030809124000370502,
      "learning_rate": 2.8268800000000003e-06,
      "loss": 0.0003,
      "step": 80500
    },
    {
      "epoch": 2.57632,
      "grad_norm": 0.005413259845227003,
      "learning_rate": 2.824746666666667e-06,
      "loss": 0.0008,
      "step": 80510
    },
    {
      "epoch": 2.5766400000000003,
      "grad_norm": 0.0041055078618228436,
      "learning_rate": 2.822613333333333e-06,
      "loss": 0.0002,
      "step": 80520
    },
    {
      "epoch": 2.57696,
      "grad_norm": 0.0015032264636829495,
      "learning_rate": 2.8204800000000006e-06,
      "loss": 0.0002,
      "step": 80530
    },
    {
      "epoch": 2.57728,
      "grad_norm": 0.0015999925090000033,
      "learning_rate": 2.818346666666667e-06,
      "loss": 0.045,
      "step": 80540
    },
    {
      "epoch": 2.5776,
      "grad_norm": 0.005917343311011791,
      "learning_rate": 2.8162133333333335e-06,
      "loss": 0.0002,
      "step": 80550
    },
    {
      "epoch": 2.5779199999999998,
      "grad_norm": 0.0077598788775503635,
      "learning_rate": 2.81408e-06,
      "loss": 0.0003,
      "step": 80560
    },
    {
      "epoch": 2.57824,
      "grad_norm": 0.002882571192458272,
      "learning_rate": 2.811946666666667e-06,
      "loss": 0.0092,
      "step": 80570
    },
    {
      "epoch": 2.57856,
      "grad_norm": 0.0018513597315177321,
      "learning_rate": 2.809813333333334e-06,
      "loss": 0.0004,
      "step": 80580
    },
    {
      "epoch": 2.57888,
      "grad_norm": 0.0021853053476661444,
      "learning_rate": 2.8076800000000005e-06,
      "loss": 0.0003,
      "step": 80590
    },
    {
      "epoch": 2.5792,
      "grad_norm": 0.0037666242569684982,
      "learning_rate": 2.805546666666667e-06,
      "loss": 0.0281,
      "step": 80600
    },
    {
      "epoch": 2.57952,
      "grad_norm": 0.0011782827787101269,
      "learning_rate": 2.8034133333333334e-06,
      "loss": 0.0004,
      "step": 80610
    },
    {
      "epoch": 2.57984,
      "grad_norm": 0.0062092444859445095,
      "learning_rate": 2.80128e-06,
      "loss": 0.0002,
      "step": 80620
    },
    {
      "epoch": 2.5801600000000002,
      "grad_norm": 0.008511479943990707,
      "learning_rate": 2.7991466666666667e-06,
      "loss": 0.0002,
      "step": 80630
    },
    {
      "epoch": 2.58048,
      "grad_norm": 0.004415492992848158,
      "learning_rate": 2.7970133333333338e-06,
      "loss": 0.0003,
      "step": 80640
    },
    {
      "epoch": 2.5808,
      "grad_norm": 0.07840053737163544,
      "learning_rate": 2.7948800000000004e-06,
      "loss": 0.0004,
      "step": 80650
    },
    {
      "epoch": 2.58112,
      "grad_norm": 0.00288392580114305,
      "learning_rate": 2.792746666666667e-06,
      "loss": 0.0003,
      "step": 80660
    },
    {
      "epoch": 2.5814399999999997,
      "grad_norm": 0.005587355699390173,
      "learning_rate": 2.7906133333333337e-06,
      "loss": 0.0002,
      "step": 80670
    },
    {
      "epoch": 2.58176,
      "grad_norm": 0.01273941807448864,
      "learning_rate": 2.78848e-06,
      "loss": 0.0019,
      "step": 80680
    },
    {
      "epoch": 2.58208,
      "grad_norm": 0.0018465898465365171,
      "learning_rate": 2.7863466666666666e-06,
      "loss": 0.0002,
      "step": 80690
    },
    {
      "epoch": 2.5824,
      "grad_norm": 0.004493140149861574,
      "learning_rate": 2.7842133333333337e-06,
      "loss": 0.0002,
      "step": 80700
    },
    {
      "epoch": 2.58272,
      "grad_norm": 0.004356154706329107,
      "learning_rate": 2.7820800000000003e-06,
      "loss": 0.0002,
      "step": 80710
    },
    {
      "epoch": 2.58304,
      "grad_norm": 0.0043593524023890495,
      "learning_rate": 2.779946666666667e-06,
      "loss": 0.0475,
      "step": 80720
    },
    {
      "epoch": 2.58336,
      "grad_norm": 0.018272610381245613,
      "learning_rate": 2.7778133333333336e-06,
      "loss": 0.0003,
      "step": 80730
    },
    {
      "epoch": 2.58368,
      "grad_norm": 0.006007114425301552,
      "learning_rate": 2.7756800000000002e-06,
      "loss": 0.0003,
      "step": 80740
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.0042251162230968475,
      "learning_rate": 2.7735466666666673e-06,
      "loss": 0.0002,
      "step": 80750
    },
    {
      "epoch": 2.58432,
      "grad_norm": 0.005433875136077404,
      "learning_rate": 2.7714133333333335e-06,
      "loss": 0.0002,
      "step": 80760
    },
    {
      "epoch": 2.5846400000000003,
      "grad_norm": 0.002505969488993287,
      "learning_rate": 2.76928e-06,
      "loss": 0.0002,
      "step": 80770
    },
    {
      "epoch": 2.58496,
      "grad_norm": 0.0011886149877682328,
      "learning_rate": 2.767146666666667e-06,
      "loss": 0.0002,
      "step": 80780
    },
    {
      "epoch": 2.58528,
      "grad_norm": 0.00272154388949275,
      "learning_rate": 2.7650133333333335e-06,
      "loss": 0.0006,
      "step": 80790
    },
    {
      "epoch": 2.5856,
      "grad_norm": 0.005263155791908503,
      "learning_rate": 2.76288e-06,
      "loss": 0.0005,
      "step": 80800
    },
    {
      "epoch": 2.5859199999999998,
      "grad_norm": 0.00449554855003953,
      "learning_rate": 2.760746666666667e-06,
      "loss": 0.0002,
      "step": 80810
    },
    {
      "epoch": 2.58624,
      "grad_norm": 0.00874012615531683,
      "learning_rate": 2.758613333333334e-06,
      "loss": 0.0003,
      "step": 80820
    },
    {
      "epoch": 2.58656,
      "grad_norm": 0.0017872200114652514,
      "learning_rate": 2.75648e-06,
      "loss": 0.0002,
      "step": 80830
    },
    {
      "epoch": 2.58688,
      "grad_norm": 0.004198579117655754,
      "learning_rate": 2.7543466666666667e-06,
      "loss": 0.0004,
      "step": 80840
    },
    {
      "epoch": 2.5872,
      "grad_norm": 0.004678970202803612,
      "learning_rate": 2.7522133333333334e-06,
      "loss": 0.0002,
      "step": 80850
    },
    {
      "epoch": 2.58752,
      "grad_norm": 0.6092216968536377,
      "learning_rate": 2.75008e-06,
      "loss": 0.0537,
      "step": 80860
    },
    {
      "epoch": 2.58784,
      "grad_norm": 0.019600851461291313,
      "learning_rate": 2.747946666666667e-06,
      "loss": 0.0002,
      "step": 80870
    },
    {
      "epoch": 2.5881600000000002,
      "grad_norm": 0.001488700625486672,
      "learning_rate": 2.7458133333333337e-06,
      "loss": 0.0002,
      "step": 80880
    },
    {
      "epoch": 2.58848,
      "grad_norm": 0.002969413297250867,
      "learning_rate": 2.7436800000000004e-06,
      "loss": 0.0002,
      "step": 80890
    },
    {
      "epoch": 2.5888,
      "grad_norm": 0.0014880712842568755,
      "learning_rate": 2.7415466666666666e-06,
      "loss": 0.0002,
      "step": 80900
    },
    {
      "epoch": 2.58912,
      "grad_norm": 0.016897844150662422,
      "learning_rate": 2.7394133333333333e-06,
      "loss": 0.0003,
      "step": 80910
    },
    {
      "epoch": 2.5894399999999997,
      "grad_norm": 0.004924560897052288,
      "learning_rate": 2.7372800000000003e-06,
      "loss": 0.0002,
      "step": 80920
    },
    {
      "epoch": 2.58976,
      "grad_norm": 0.016742121428251266,
      "learning_rate": 2.735146666666667e-06,
      "loss": 0.0004,
      "step": 80930
    },
    {
      "epoch": 2.59008,
      "grad_norm": 0.00413205660879612,
      "learning_rate": 2.7330133333333336e-06,
      "loss": 0.0002,
      "step": 80940
    },
    {
      "epoch": 2.5904,
      "grad_norm": 0.004599106032401323,
      "learning_rate": 2.7308800000000003e-06,
      "loss": 0.0003,
      "step": 80950
    },
    {
      "epoch": 2.59072,
      "grad_norm": 0.004637301899492741,
      "learning_rate": 2.728746666666667e-06,
      "loss": 0.0002,
      "step": 80960
    },
    {
      "epoch": 2.59104,
      "grad_norm": 0.003203338710591197,
      "learning_rate": 2.726613333333333e-06,
      "loss": 0.0002,
      "step": 80970
    },
    {
      "epoch": 2.59136,
      "grad_norm": 0.005753928795456886,
      "learning_rate": 2.7244800000000006e-06,
      "loss": 0.0002,
      "step": 80980
    },
    {
      "epoch": 2.59168,
      "grad_norm": 0.004941495135426521,
      "learning_rate": 2.722346666666667e-06,
      "loss": 0.0002,
      "step": 80990
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.0037730226758867502,
      "learning_rate": 2.7202133333333335e-06,
      "loss": 0.0504,
      "step": 81000
    },
    {
      "epoch": 2.59232,
      "grad_norm": 0.0024099270813167095,
      "learning_rate": 2.71808e-06,
      "loss": 0.0004,
      "step": 81010
    },
    {
      "epoch": 2.59264,
      "grad_norm": 0.0012820418924093246,
      "learning_rate": 2.715946666666667e-06,
      "loss": 0.0207,
      "step": 81020
    },
    {
      "epoch": 2.59296,
      "grad_norm": 0.0037599040661007166,
      "learning_rate": 2.7138133333333335e-06,
      "loss": 0.0002,
      "step": 81030
    },
    {
      "epoch": 2.59328,
      "grad_norm": 0.0013239221880212426,
      "learning_rate": 2.7116800000000005e-06,
      "loss": 0.0007,
      "step": 81040
    },
    {
      "epoch": 2.5936,
      "grad_norm": 0.0024781515821814537,
      "learning_rate": 2.709546666666667e-06,
      "loss": 0.0001,
      "step": 81050
    },
    {
      "epoch": 2.59392,
      "grad_norm": 1.8662675619125366,
      "learning_rate": 2.7074133333333334e-06,
      "loss": 0.044,
      "step": 81060
    },
    {
      "epoch": 2.59424,
      "grad_norm": 0.004707608371973038,
      "learning_rate": 2.70528e-06,
      "loss": 0.0002,
      "step": 81070
    },
    {
      "epoch": 2.59456,
      "grad_norm": 0.004279138520359993,
      "learning_rate": 2.7031466666666667e-06,
      "loss": 0.0003,
      "step": 81080
    },
    {
      "epoch": 2.59488,
      "grad_norm": 0.0035671605728566647,
      "learning_rate": 2.7010133333333338e-06,
      "loss": 0.0095,
      "step": 81090
    },
    {
      "epoch": 2.5952,
      "grad_norm": 0.004811495542526245,
      "learning_rate": 2.6988800000000004e-06,
      "loss": 0.0003,
      "step": 81100
    },
    {
      "epoch": 2.59552,
      "grad_norm": 0.01182167325168848,
      "learning_rate": 2.696746666666667e-06,
      "loss": 0.0002,
      "step": 81110
    },
    {
      "epoch": 2.59584,
      "grad_norm": 0.0059410640969872475,
      "learning_rate": 2.6946133333333337e-06,
      "loss": 0.0002,
      "step": 81120
    },
    {
      "epoch": 2.5961600000000002,
      "grad_norm": 0.003644622629508376,
      "learning_rate": 2.69248e-06,
      "loss": 0.0002,
      "step": 81130
    },
    {
      "epoch": 2.59648,
      "grad_norm": 0.015702497214078903,
      "learning_rate": 2.6903466666666666e-06,
      "loss": 0.0253,
      "step": 81140
    },
    {
      "epoch": 2.5968,
      "grad_norm": 0.1306999772787094,
      "learning_rate": 2.6882133333333336e-06,
      "loss": 0.0005,
      "step": 81150
    },
    {
      "epoch": 2.59712,
      "grad_norm": 0.006424706894904375,
      "learning_rate": 2.6860800000000003e-06,
      "loss": 0.0415,
      "step": 81160
    },
    {
      "epoch": 2.5974399999999997,
      "grad_norm": 0.024302320554852486,
      "learning_rate": 2.683946666666667e-06,
      "loss": 0.0004,
      "step": 81170
    },
    {
      "epoch": 2.59776,
      "grad_norm": 0.039379898458719254,
      "learning_rate": 2.6818133333333336e-06,
      "loss": 0.0002,
      "step": 81180
    },
    {
      "epoch": 2.59808,
      "grad_norm": 0.004310493357479572,
      "learning_rate": 2.6796800000000002e-06,
      "loss": 0.1097,
      "step": 81190
    },
    {
      "epoch": 2.5984,
      "grad_norm": 0.7714638710021973,
      "learning_rate": 2.6775466666666665e-06,
      "loss": 0.0008,
      "step": 81200
    },
    {
      "epoch": 2.59872,
      "grad_norm": 0.010581621900200844,
      "learning_rate": 2.675413333333334e-06,
      "loss": 0.0002,
      "step": 81210
    },
    {
      "epoch": 2.59904,
      "grad_norm": 0.008835937827825546,
      "learning_rate": 2.67328e-06,
      "loss": 0.0535,
      "step": 81220
    },
    {
      "epoch": 2.59936,
      "grad_norm": 0.004196731373667717,
      "learning_rate": 2.671146666666667e-06,
      "loss": 0.0396,
      "step": 81230
    },
    {
      "epoch": 2.59968,
      "grad_norm": 5.498220443725586,
      "learning_rate": 2.6690133333333335e-06,
      "loss": 0.0066,
      "step": 81240
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.007451196201145649,
      "learning_rate": 2.66688e-06,
      "loss": 0.0045,
      "step": 81250
    },
    {
      "epoch": 2.60032,
      "grad_norm": 0.006534680724143982,
      "learning_rate": 2.664746666666667e-06,
      "loss": 0.0002,
      "step": 81260
    },
    {
      "epoch": 2.60064,
      "grad_norm": 0.007938748225569725,
      "learning_rate": 2.662613333333334e-06,
      "loss": 0.0002,
      "step": 81270
    },
    {
      "epoch": 2.60096,
      "grad_norm": 0.03231630101799965,
      "learning_rate": 2.6604800000000005e-06,
      "loss": 0.0003,
      "step": 81280
    },
    {
      "epoch": 2.60128,
      "grad_norm": 0.004803301300853491,
      "learning_rate": 2.6583466666666667e-06,
      "loss": 0.0002,
      "step": 81290
    },
    {
      "epoch": 2.6016,
      "grad_norm": 0.018740272149443626,
      "learning_rate": 2.6562133333333334e-06,
      "loss": 0.0004,
      "step": 81300
    },
    {
      "epoch": 2.60192,
      "grad_norm": 0.005974687170237303,
      "learning_rate": 2.65408e-06,
      "loss": 0.0002,
      "step": 81310
    },
    {
      "epoch": 2.60224,
      "grad_norm": 0.004615317098796368,
      "learning_rate": 2.651946666666667e-06,
      "loss": 0.0342,
      "step": 81320
    },
    {
      "epoch": 2.60256,
      "grad_norm": 0.002645963802933693,
      "learning_rate": 2.6498133333333337e-06,
      "loss": 0.0003,
      "step": 81330
    },
    {
      "epoch": 2.60288,
      "grad_norm": 0.006671418435871601,
      "learning_rate": 2.6476800000000004e-06,
      "loss": 0.0003,
      "step": 81340
    },
    {
      "epoch": 2.6032,
      "grad_norm": 0.003961984999477863,
      "learning_rate": 2.645546666666667e-06,
      "loss": 0.0003,
      "step": 81350
    },
    {
      "epoch": 2.60352,
      "grad_norm": 0.0122673399746418,
      "learning_rate": 2.6434133333333332e-06,
      "loss": 0.0003,
      "step": 81360
    },
    {
      "epoch": 2.60384,
      "grad_norm": 0.00290311174467206,
      "learning_rate": 2.64128e-06,
      "loss": 0.0004,
      "step": 81370
    },
    {
      "epoch": 2.6041600000000003,
      "grad_norm": 0.010661615058779716,
      "learning_rate": 2.639146666666667e-06,
      "loss": 0.0002,
      "step": 81380
    },
    {
      "epoch": 2.60448,
      "grad_norm": 0.013962579891085625,
      "learning_rate": 2.6370133333333336e-06,
      "loss": 0.0002,
      "step": 81390
    },
    {
      "epoch": 2.6048,
      "grad_norm": 0.004614066798239946,
      "learning_rate": 2.6348800000000003e-06,
      "loss": 0.0002,
      "step": 81400
    },
    {
      "epoch": 2.60512,
      "grad_norm": 0.007744318805634975,
      "learning_rate": 2.632746666666667e-06,
      "loss": 0.0002,
      "step": 81410
    },
    {
      "epoch": 2.6054399999999998,
      "grad_norm": 0.007123674266040325,
      "learning_rate": 2.6306133333333336e-06,
      "loss": 0.0002,
      "step": 81420
    },
    {
      "epoch": 2.60576,
      "grad_norm": 0.006009146571159363,
      "learning_rate": 2.6284800000000006e-06,
      "loss": 0.0003,
      "step": 81430
    },
    {
      "epoch": 2.60608,
      "grad_norm": 0.06209158897399902,
      "learning_rate": 2.626346666666667e-06,
      "loss": 0.0004,
      "step": 81440
    },
    {
      "epoch": 2.6064,
      "grad_norm": 0.010099565610289574,
      "learning_rate": 2.6242133333333335e-06,
      "loss": 0.0003,
      "step": 81450
    },
    {
      "epoch": 2.60672,
      "grad_norm": 0.006568692158907652,
      "learning_rate": 2.62208e-06,
      "loss": 0.0012,
      "step": 81460
    },
    {
      "epoch": 2.60704,
      "grad_norm": 0.0066529735922813416,
      "learning_rate": 2.619946666666667e-06,
      "loss": 0.0002,
      "step": 81470
    },
    {
      "epoch": 2.60736,
      "grad_norm": 0.00362234003841877,
      "learning_rate": 2.6178133333333334e-06,
      "loss": 0.0209,
      "step": 81480
    },
    {
      "epoch": 2.60768,
      "grad_norm": 0.0020296729635447264,
      "learning_rate": 2.6156800000000005e-06,
      "loss": 0.0002,
      "step": 81490
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.007884220220148563,
      "learning_rate": 2.613546666666667e-06,
      "loss": 0.0002,
      "step": 81500
    },
    {
      "epoch": 2.60832,
      "grad_norm": 0.004833030514419079,
      "learning_rate": 2.6114133333333334e-06,
      "loss": 0.0431,
      "step": 81510
    },
    {
      "epoch": 2.60864,
      "grad_norm": 0.010290753096342087,
      "learning_rate": 2.60928e-06,
      "loss": 0.0002,
      "step": 81520
    },
    {
      "epoch": 2.6089599999999997,
      "grad_norm": 0.005277727730572224,
      "learning_rate": 2.6071466666666667e-06,
      "loss": 0.0325,
      "step": 81530
    },
    {
      "epoch": 2.60928,
      "grad_norm": 0.003024191129952669,
      "learning_rate": 2.6050133333333333e-06,
      "loss": 0.0006,
      "step": 81540
    },
    {
      "epoch": 2.6096,
      "grad_norm": 0.011408111080527306,
      "learning_rate": 2.6028800000000004e-06,
      "loss": 0.0017,
      "step": 81550
    },
    {
      "epoch": 2.60992,
      "grad_norm": 0.0025107774417847395,
      "learning_rate": 2.600746666666667e-06,
      "loss": 0.0002,
      "step": 81560
    },
    {
      "epoch": 2.61024,
      "grad_norm": 5.903695583343506,
      "learning_rate": 2.5986133333333337e-06,
      "loss": 0.0076,
      "step": 81570
    },
    {
      "epoch": 2.61056,
      "grad_norm": 0.004011685494333506,
      "learning_rate": 2.59648e-06,
      "loss": 0.0002,
      "step": 81580
    },
    {
      "epoch": 2.61088,
      "grad_norm": 0.5582175254821777,
      "learning_rate": 2.5943466666666666e-06,
      "loss": 0.0004,
      "step": 81590
    },
    {
      "epoch": 2.6112,
      "grad_norm": 0.011684617958962917,
      "learning_rate": 2.5922133333333336e-06,
      "loss": 0.0007,
      "step": 81600
    },
    {
      "epoch": 2.61152,
      "grad_norm": 0.005908040329813957,
      "learning_rate": 2.5900800000000003e-06,
      "loss": 0.0546,
      "step": 81610
    },
    {
      "epoch": 2.61184,
      "grad_norm": 0.0022062091156840324,
      "learning_rate": 2.587946666666667e-06,
      "loss": 0.0003,
      "step": 81620
    },
    {
      "epoch": 2.6121600000000003,
      "grad_norm": 0.04484334960579872,
      "learning_rate": 2.5858133333333336e-06,
      "loss": 0.0002,
      "step": 81630
    },
    {
      "epoch": 2.61248,
      "grad_norm": 0.0020675885025411844,
      "learning_rate": 2.5836800000000002e-06,
      "loss": 0.0004,
      "step": 81640
    },
    {
      "epoch": 2.6128,
      "grad_norm": 0.0491705983877182,
      "learning_rate": 2.5815466666666664e-06,
      "loss": 0.0002,
      "step": 81650
    },
    {
      "epoch": 2.61312,
      "grad_norm": 0.005063507705926895,
      "learning_rate": 2.579413333333334e-06,
      "loss": 0.0441,
      "step": 81660
    },
    {
      "epoch": 2.6134399999999998,
      "grad_norm": 0.004032859578728676,
      "learning_rate": 2.57728e-06,
      "loss": 0.0017,
      "step": 81670
    },
    {
      "epoch": 2.61376,
      "grad_norm": 0.005441125947982073,
      "learning_rate": 2.575146666666667e-06,
      "loss": 0.0002,
      "step": 81680
    },
    {
      "epoch": 2.61408,
      "grad_norm": 0.004388035275042057,
      "learning_rate": 2.5730133333333335e-06,
      "loss": 0.0003,
      "step": 81690
    },
    {
      "epoch": 2.6144,
      "grad_norm": 0.005838682875037193,
      "learning_rate": 2.57088e-06,
      "loss": 0.0031,
      "step": 81700
    },
    {
      "epoch": 2.61472,
      "grad_norm": 0.025616928935050964,
      "learning_rate": 2.5687466666666668e-06,
      "loss": 0.0002,
      "step": 81710
    },
    {
      "epoch": 2.61504,
      "grad_norm": 0.002348656766116619,
      "learning_rate": 2.566613333333334e-06,
      "loss": 0.0003,
      "step": 81720
    },
    {
      "epoch": 2.61536,
      "grad_norm": 0.0029182799626141787,
      "learning_rate": 2.5644800000000005e-06,
      "loss": 0.0004,
      "step": 81730
    },
    {
      "epoch": 2.6156800000000002,
      "grad_norm": 0.00894579291343689,
      "learning_rate": 2.5623466666666667e-06,
      "loss": 0.0003,
      "step": 81740
    },
    {
      "epoch": 2.616,
      "grad_norm": 2.352957248687744,
      "learning_rate": 2.5602133333333333e-06,
      "loss": 0.0172,
      "step": 81750
    },
    {
      "epoch": 2.61632,
      "grad_norm": 0.0035620923154056072,
      "learning_rate": 2.55808e-06,
      "loss": 0.0003,
      "step": 81760
    },
    {
      "epoch": 2.61664,
      "grad_norm": 0.003554299473762512,
      "learning_rate": 2.555946666666667e-06,
      "loss": 0.0002,
      "step": 81770
    },
    {
      "epoch": 2.6169599999999997,
      "grad_norm": 0.0014855609042569995,
      "learning_rate": 2.5538133333333337e-06,
      "loss": 0.0016,
      "step": 81780
    },
    {
      "epoch": 2.61728,
      "grad_norm": 0.0039728679694235325,
      "learning_rate": 2.5516800000000004e-06,
      "loss": 0.0422,
      "step": 81790
    },
    {
      "epoch": 2.6176,
      "grad_norm": 0.0029328439850360155,
      "learning_rate": 2.549546666666667e-06,
      "loss": 0.0644,
      "step": 81800
    },
    {
      "epoch": 2.61792,
      "grad_norm": 0.006560974754393101,
      "learning_rate": 2.5474133333333332e-06,
      "loss": 0.0002,
      "step": 81810
    },
    {
      "epoch": 2.61824,
      "grad_norm": 0.0039453208446502686,
      "learning_rate": 2.54528e-06,
      "loss": 0.0002,
      "step": 81820
    },
    {
      "epoch": 2.61856,
      "grad_norm": 0.003349600825458765,
      "learning_rate": 2.543146666666667e-06,
      "loss": 0.0002,
      "step": 81830
    },
    {
      "epoch": 2.61888,
      "grad_norm": 0.0016792700625956059,
      "learning_rate": 2.5410133333333336e-06,
      "loss": 0.0003,
      "step": 81840
    },
    {
      "epoch": 2.6192,
      "grad_norm": 0.007151921279728413,
      "learning_rate": 2.5388800000000002e-06,
      "loss": 0.0268,
      "step": 81850
    },
    {
      "epoch": 2.61952,
      "grad_norm": 0.03192264586687088,
      "learning_rate": 2.536746666666667e-06,
      "loss": 0.0111,
      "step": 81860
    },
    {
      "epoch": 2.61984,
      "grad_norm": 0.0032488282304257154,
      "learning_rate": 2.5346133333333335e-06,
      "loss": 0.0002,
      "step": 81870
    },
    {
      "epoch": 2.6201600000000003,
      "grad_norm": 10.58779239654541,
      "learning_rate": 2.5324799999999998e-06,
      "loss": 0.0565,
      "step": 81880
    },
    {
      "epoch": 2.62048,
      "grad_norm": 0.002472389955073595,
      "learning_rate": 2.5303466666666673e-06,
      "loss": 0.0003,
      "step": 81890
    },
    {
      "epoch": 2.6208,
      "grad_norm": 0.00655663525685668,
      "learning_rate": 2.5282133333333335e-06,
      "loss": 0.0052,
      "step": 81900
    },
    {
      "epoch": 2.62112,
      "grad_norm": 0.09047349542379379,
      "learning_rate": 2.52608e-06,
      "loss": 0.0004,
      "step": 81910
    },
    {
      "epoch": 2.6214399999999998,
      "grad_norm": 0.003294173860922456,
      "learning_rate": 2.5239466666666668e-06,
      "loss": 0.0001,
      "step": 81920
    },
    {
      "epoch": 2.62176,
      "grad_norm": 0.015656812116503716,
      "learning_rate": 2.5218133333333334e-06,
      "loss": 0.0003,
      "step": 81930
    },
    {
      "epoch": 2.62208,
      "grad_norm": 0.005636041518300772,
      "learning_rate": 2.5196800000000005e-06,
      "loss": 0.0002,
      "step": 81940
    },
    {
      "epoch": 2.6224,
      "grad_norm": 0.011079000309109688,
      "learning_rate": 2.517546666666667e-06,
      "loss": 0.0002,
      "step": 81950
    },
    {
      "epoch": 2.62272,
      "grad_norm": 0.004262923263013363,
      "learning_rate": 2.515413333333334e-06,
      "loss": 0.0002,
      "step": 81960
    },
    {
      "epoch": 2.62304,
      "grad_norm": 0.02128901518881321,
      "learning_rate": 2.51328e-06,
      "loss": 0.0002,
      "step": 81970
    },
    {
      "epoch": 2.62336,
      "grad_norm": 0.008055724203586578,
      "learning_rate": 2.5111466666666667e-06,
      "loss": 0.0003,
      "step": 81980
    },
    {
      "epoch": 2.6236800000000002,
      "grad_norm": 0.003791450522840023,
      "learning_rate": 2.5090133333333333e-06,
      "loss": 0.0003,
      "step": 81990
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.004281239118427038,
      "learning_rate": 2.5068800000000004e-06,
      "loss": 0.0002,
      "step": 82000
    },
    {
      "epoch": 2.62432,
      "grad_norm": 0.009529734961688519,
      "learning_rate": 2.504746666666667e-06,
      "loss": 0.0003,
      "step": 82010
    },
    {
      "epoch": 2.62464,
      "grad_norm": 0.010886458680033684,
      "learning_rate": 2.5026133333333337e-06,
      "loss": 0.0013,
      "step": 82020
    },
    {
      "epoch": 2.6249599999999997,
      "grad_norm": 0.040263984352350235,
      "learning_rate": 2.5004800000000003e-06,
      "loss": 0.0003,
      "step": 82030
    },
    {
      "epoch": 2.62528,
      "grad_norm": 0.0032748670782893896,
      "learning_rate": 2.498346666666667e-06,
      "loss": 0.0016,
      "step": 82040
    },
    {
      "epoch": 2.6256,
      "grad_norm": 0.0034953001886606216,
      "learning_rate": 2.4962133333333336e-06,
      "loss": 0.0002,
      "step": 82050
    },
    {
      "epoch": 2.62592,
      "grad_norm": 0.003910618834197521,
      "learning_rate": 2.4940800000000003e-06,
      "loss": 0.0002,
      "step": 82060
    },
    {
      "epoch": 2.62624,
      "grad_norm": 0.0035695626866072416,
      "learning_rate": 2.491946666666667e-06,
      "loss": 0.0003,
      "step": 82070
    },
    {
      "epoch": 2.62656,
      "grad_norm": 3.6028337478637695,
      "learning_rate": 2.4898133333333336e-06,
      "loss": 0.005,
      "step": 82080
    },
    {
      "epoch": 2.62688,
      "grad_norm": 0.002426212653517723,
      "learning_rate": 2.48768e-06,
      "loss": 0.0005,
      "step": 82090
    },
    {
      "epoch": 2.6272,
      "grad_norm": 0.0017372635193169117,
      "learning_rate": 2.485546666666667e-06,
      "loss": 0.0003,
      "step": 82100
    },
    {
      "epoch": 2.62752,
      "grad_norm": 0.011213463731110096,
      "learning_rate": 2.4834133333333335e-06,
      "loss": 0.0002,
      "step": 82110
    },
    {
      "epoch": 2.62784,
      "grad_norm": 0.0034237252548336983,
      "learning_rate": 2.48128e-06,
      "loss": 0.0003,
      "step": 82120
    },
    {
      "epoch": 2.6281600000000003,
      "grad_norm": 0.008009705692529678,
      "learning_rate": 2.479146666666667e-06,
      "loss": 0.0002,
      "step": 82130
    },
    {
      "epoch": 2.62848,
      "grad_norm": 0.0026684317272156477,
      "learning_rate": 2.4770133333333334e-06,
      "loss": 0.0002,
      "step": 82140
    },
    {
      "epoch": 2.6288,
      "grad_norm": 0.0054121240973472595,
      "learning_rate": 2.4748800000000005e-06,
      "loss": 0.0002,
      "step": 82150
    },
    {
      "epoch": 2.62912,
      "grad_norm": 0.014897572807967663,
      "learning_rate": 2.4727466666666667e-06,
      "loss": 0.0002,
      "step": 82160
    },
    {
      "epoch": 2.6294399999999998,
      "grad_norm": 4.70698356628418,
      "learning_rate": 2.4706133333333334e-06,
      "loss": 0.004,
      "step": 82170
    },
    {
      "epoch": 2.62976,
      "grad_norm": 0.0019733060616999865,
      "learning_rate": 2.4684800000000005e-06,
      "loss": 0.0093,
      "step": 82180
    },
    {
      "epoch": 2.63008,
      "grad_norm": 0.01852002739906311,
      "learning_rate": 2.4663466666666667e-06,
      "loss": 0.0002,
      "step": 82190
    },
    {
      "epoch": 2.6304,
      "grad_norm": 0.004577859304845333,
      "learning_rate": 2.4642133333333333e-06,
      "loss": 0.0006,
      "step": 82200
    },
    {
      "epoch": 2.63072,
      "grad_norm": 0.018064046278595924,
      "learning_rate": 2.4620800000000004e-06,
      "loss": 0.0004,
      "step": 82210
    },
    {
      "epoch": 2.63104,
      "grad_norm": 0.010970198549330235,
      "learning_rate": 2.459946666666667e-06,
      "loss": 0.0005,
      "step": 82220
    },
    {
      "epoch": 2.63136,
      "grad_norm": 0.0076888082548975945,
      "learning_rate": 2.4578133333333333e-06,
      "loss": 0.0003,
      "step": 82230
    },
    {
      "epoch": 2.6316800000000002,
      "grad_norm": 0.023992888629436493,
      "learning_rate": 2.4556800000000003e-06,
      "loss": 0.0341,
      "step": 82240
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.0031681787222623825,
      "learning_rate": 2.453546666666667e-06,
      "loss": 0.0002,
      "step": 82250
    },
    {
      "epoch": 2.63232,
      "grad_norm": 0.003867953550070524,
      "learning_rate": 2.4514133333333332e-06,
      "loss": 0.0039,
      "step": 82260
    },
    {
      "epoch": 2.63264,
      "grad_norm": 0.005403921473771334,
      "learning_rate": 2.4492800000000003e-06,
      "loss": 0.0005,
      "step": 82270
    },
    {
      "epoch": 2.6329599999999997,
      "grad_norm": 0.0016844288911670446,
      "learning_rate": 2.447146666666667e-06,
      "loss": 0.0007,
      "step": 82280
    },
    {
      "epoch": 2.63328,
      "grad_norm": 0.005366669036448002,
      "learning_rate": 2.4450133333333336e-06,
      "loss": 0.0002,
      "step": 82290
    },
    {
      "epoch": 2.6336,
      "grad_norm": 0.00775667279958725,
      "learning_rate": 2.4428800000000002e-06,
      "loss": 0.0003,
      "step": 82300
    },
    {
      "epoch": 2.63392,
      "grad_norm": 0.006761881522834301,
      "learning_rate": 2.440746666666667e-06,
      "loss": 0.0003,
      "step": 82310
    },
    {
      "epoch": 2.63424,
      "grad_norm": 0.002783148782327771,
      "learning_rate": 2.4386133333333335e-06,
      "loss": 0.0003,
      "step": 82320
    },
    {
      "epoch": 2.63456,
      "grad_norm": 0.002815213520079851,
      "learning_rate": 2.43648e-06,
      "loss": 0.0073,
      "step": 82330
    },
    {
      "epoch": 2.63488,
      "grad_norm": 0.007319767959415913,
      "learning_rate": 2.434346666666667e-06,
      "loss": 0.001,
      "step": 82340
    },
    {
      "epoch": 2.6352,
      "grad_norm": 0.004705127794295549,
      "learning_rate": 2.4322133333333335e-06,
      "loss": 0.004,
      "step": 82350
    },
    {
      "epoch": 2.63552,
      "grad_norm": 0.005922788288444281,
      "learning_rate": 2.43008e-06,
      "loss": 0.0002,
      "step": 82360
    },
    {
      "epoch": 2.63584,
      "grad_norm": 0.006743194069713354,
      "learning_rate": 2.4279466666666668e-06,
      "loss": 0.0018,
      "step": 82370
    },
    {
      "epoch": 2.63616,
      "grad_norm": 0.0032037391792982817,
      "learning_rate": 2.425813333333334e-06,
      "loss": 0.0002,
      "step": 82380
    },
    {
      "epoch": 2.63648,
      "grad_norm": 0.007535473443567753,
      "learning_rate": 2.42368e-06,
      "loss": 0.0498,
      "step": 82390
    },
    {
      "epoch": 2.6368,
      "grad_norm": 0.013001138344407082,
      "learning_rate": 2.4215466666666667e-06,
      "loss": 0.0002,
      "step": 82400
    },
    {
      "epoch": 2.63712,
      "grad_norm": 3.8075923919677734,
      "learning_rate": 2.4194133333333338e-06,
      "loss": 0.077,
      "step": 82410
    },
    {
      "epoch": 2.63744,
      "grad_norm": 0.004577151499688625,
      "learning_rate": 2.41728e-06,
      "loss": 0.0003,
      "step": 82420
    },
    {
      "epoch": 2.63776,
      "grad_norm": 0.004132688976824284,
      "learning_rate": 2.4151466666666667e-06,
      "loss": 0.0002,
      "step": 82430
    },
    {
      "epoch": 2.63808,
      "grad_norm": 0.006145171821117401,
      "learning_rate": 2.4130133333333337e-06,
      "loss": 0.0004,
      "step": 82440
    },
    {
      "epoch": 2.6384,
      "grad_norm": 0.016556331887841225,
      "learning_rate": 2.4108800000000004e-06,
      "loss": 0.0006,
      "step": 82450
    },
    {
      "epoch": 2.63872,
      "grad_norm": 0.0032115643844008446,
      "learning_rate": 2.4087466666666666e-06,
      "loss": 0.0025,
      "step": 82460
    },
    {
      "epoch": 2.63904,
      "grad_norm": 0.0045298924669623375,
      "learning_rate": 2.4066133333333337e-06,
      "loss": 0.0001,
      "step": 82470
    },
    {
      "epoch": 2.63936,
      "grad_norm": 0.004794491920620203,
      "learning_rate": 2.4044800000000003e-06,
      "loss": 0.0002,
      "step": 82480
    },
    {
      "epoch": 2.6396800000000002,
      "grad_norm": 0.002942256862297654,
      "learning_rate": 2.402346666666667e-06,
      "loss": 0.0003,
      "step": 82490
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.004854990169405937,
      "learning_rate": 2.4002133333333336e-06,
      "loss": 0.0002,
      "step": 82500
    },
    {
      "epoch": 2.64032,
      "grad_norm": 0.0021256518084555864,
      "learning_rate": 2.3980800000000003e-06,
      "loss": 0.0002,
      "step": 82510
    },
    {
      "epoch": 2.64064,
      "grad_norm": 0.002637067809700966,
      "learning_rate": 2.395946666666667e-06,
      "loss": 0.0043,
      "step": 82520
    },
    {
      "epoch": 2.6409599999999998,
      "grad_norm": 0.006100924219936132,
      "learning_rate": 2.3938133333333336e-06,
      "loss": 0.0002,
      "step": 82530
    },
    {
      "epoch": 2.64128,
      "grad_norm": 0.006577483378350735,
      "learning_rate": 2.39168e-06,
      "loss": 0.0504,
      "step": 82540
    },
    {
      "epoch": 2.6416,
      "grad_norm": 0.03642156347632408,
      "learning_rate": 2.389546666666667e-06,
      "loss": 0.0003,
      "step": 82550
    },
    {
      "epoch": 2.64192,
      "grad_norm": 0.004689554683864117,
      "learning_rate": 2.3874133333333335e-06,
      "loss": 0.0003,
      "step": 82560
    },
    {
      "epoch": 2.64224,
      "grad_norm": 0.003460973734036088,
      "learning_rate": 2.38528e-06,
      "loss": 0.0002,
      "step": 82570
    },
    {
      "epoch": 2.64256,
      "grad_norm": 0.005377210211008787,
      "learning_rate": 2.383146666666667e-06,
      "loss": 0.0002,
      "step": 82580
    },
    {
      "epoch": 2.64288,
      "grad_norm": 0.004361175466328859,
      "learning_rate": 2.3810133333333334e-06,
      "loss": 0.0002,
      "step": 82590
    },
    {
      "epoch": 2.6432,
      "grad_norm": 0.0032076574862003326,
      "learning_rate": 2.37888e-06,
      "loss": 0.0002,
      "step": 82600
    },
    {
      "epoch": 2.64352,
      "grad_norm": 0.0027037051040679216,
      "learning_rate": 2.3767466666666667e-06,
      "loss": 0.0031,
      "step": 82610
    },
    {
      "epoch": 2.64384,
      "grad_norm": 0.002097672550007701,
      "learning_rate": 2.3746133333333334e-06,
      "loss": 0.0002,
      "step": 82620
    },
    {
      "epoch": 2.64416,
      "grad_norm": 0.0021146610379219055,
      "learning_rate": 2.37248e-06,
      "loss": 0.0129,
      "step": 82630
    },
    {
      "epoch": 2.64448,
      "grad_norm": 0.0036435751244425774,
      "learning_rate": 2.370346666666667e-06,
      "loss": 0.0003,
      "step": 82640
    },
    {
      "epoch": 2.6448,
      "grad_norm": 0.0031737671233713627,
      "learning_rate": 2.3682133333333333e-06,
      "loss": 0.0002,
      "step": 82650
    },
    {
      "epoch": 2.64512,
      "grad_norm": 0.0047011300921440125,
      "learning_rate": 2.3660800000000004e-06,
      "loss": 0.0005,
      "step": 82660
    },
    {
      "epoch": 2.64544,
      "grad_norm": 0.009770170785486698,
      "learning_rate": 2.363946666666667e-06,
      "loss": 0.0001,
      "step": 82670
    },
    {
      "epoch": 2.64576,
      "grad_norm": 0.0026958798989653587,
      "learning_rate": 2.3618133333333333e-06,
      "loss": 0.0001,
      "step": 82680
    },
    {
      "epoch": 2.64608,
      "grad_norm": 0.0032796859741210938,
      "learning_rate": 2.3596800000000003e-06,
      "loss": 0.0002,
      "step": 82690
    },
    {
      "epoch": 2.6464,
      "grad_norm": 0.003711986355483532,
      "learning_rate": 2.357546666666667e-06,
      "loss": 0.0003,
      "step": 82700
    },
    {
      "epoch": 2.64672,
      "grad_norm": 0.0026605778839439154,
      "learning_rate": 2.3554133333333336e-06,
      "loss": 0.0007,
      "step": 82710
    },
    {
      "epoch": 2.64704,
      "grad_norm": 0.002698049647733569,
      "learning_rate": 2.3532800000000003e-06,
      "loss": 0.0046,
      "step": 82720
    },
    {
      "epoch": 2.64736,
      "grad_norm": 0.006058471743017435,
      "learning_rate": 2.351146666666667e-06,
      "loss": 0.0002,
      "step": 82730
    },
    {
      "epoch": 2.6476800000000003,
      "grad_norm": 2.1120963096618652,
      "learning_rate": 2.3490133333333336e-06,
      "loss": 0.002,
      "step": 82740
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.002404256956651807,
      "learning_rate": 2.3468800000000002e-06,
      "loss": 0.0002,
      "step": 82750
    },
    {
      "epoch": 2.64832,
      "grad_norm": 0.0021248115226626396,
      "learning_rate": 2.344746666666667e-06,
      "loss": 0.0002,
      "step": 82760
    },
    {
      "epoch": 2.64864,
      "grad_norm": 2.5813512802124023,
      "learning_rate": 2.3426133333333335e-06,
      "loss": 0.013,
      "step": 82770
    },
    {
      "epoch": 2.6489599999999998,
      "grad_norm": 0.005247330293059349,
      "learning_rate": 2.34048e-06,
      "loss": 0.0002,
      "step": 82780
    },
    {
      "epoch": 2.64928,
      "grad_norm": 0.0019604458939284086,
      "learning_rate": 2.338346666666667e-06,
      "loss": 0.0182,
      "step": 82790
    },
    {
      "epoch": 2.6496,
      "grad_norm": 0.0021378237288445234,
      "learning_rate": 2.3362133333333335e-06,
      "loss": 0.0088,
      "step": 82800
    },
    {
      "epoch": 2.64992,
      "grad_norm": 0.007381343748420477,
      "learning_rate": 2.33408e-06,
      "loss": 0.0002,
      "step": 82810
    },
    {
      "epoch": 2.65024,
      "grad_norm": 2.8758184909820557,
      "learning_rate": 2.3319466666666668e-06,
      "loss": 0.0046,
      "step": 82820
    },
    {
      "epoch": 2.65056,
      "grad_norm": 0.01079039741307497,
      "learning_rate": 2.329813333333334e-06,
      "loss": 0.0607,
      "step": 82830
    },
    {
      "epoch": 2.65088,
      "grad_norm": 0.0069094812497496605,
      "learning_rate": 2.32768e-06,
      "loss": 0.0007,
      "step": 82840
    },
    {
      "epoch": 2.6512000000000002,
      "grad_norm": 0.0022584127727895975,
      "learning_rate": 2.3255466666666667e-06,
      "loss": 0.0404,
      "step": 82850
    },
    {
      "epoch": 2.65152,
      "grad_norm": 0.003915789071470499,
      "learning_rate": 2.3234133333333338e-06,
      "loss": 0.0002,
      "step": 82860
    },
    {
      "epoch": 2.65184,
      "grad_norm": 0.004490064922720194,
      "learning_rate": 2.32128e-06,
      "loss": 0.0038,
      "step": 82870
    },
    {
      "epoch": 2.65216,
      "grad_norm": 0.002533684018999338,
      "learning_rate": 2.3191466666666666e-06,
      "loss": 0.0002,
      "step": 82880
    },
    {
      "epoch": 2.6524799999999997,
      "grad_norm": 6.50905704498291,
      "learning_rate": 2.3170133333333337e-06,
      "loss": 0.025,
      "step": 82890
    },
    {
      "epoch": 2.6528,
      "grad_norm": 0.010148616507649422,
      "learning_rate": 2.3148800000000004e-06,
      "loss": 0.0002,
      "step": 82900
    },
    {
      "epoch": 2.65312,
      "grad_norm": 0.00403706356883049,
      "learning_rate": 2.3127466666666666e-06,
      "loss": 0.0003,
      "step": 82910
    },
    {
      "epoch": 2.65344,
      "grad_norm": 0.020976081490516663,
      "learning_rate": 2.3106133333333337e-06,
      "loss": 0.0002,
      "step": 82920
    },
    {
      "epoch": 2.65376,
      "grad_norm": 0.0017544717993587255,
      "learning_rate": 2.3084800000000003e-06,
      "loss": 0.0005,
      "step": 82930
    },
    {
      "epoch": 2.65408,
      "grad_norm": 0.01934501715004444,
      "learning_rate": 2.3063466666666665e-06,
      "loss": 0.0002,
      "step": 82940
    },
    {
      "epoch": 2.6544,
      "grad_norm": 0.00955315213650465,
      "learning_rate": 2.3042133333333336e-06,
      "loss": 0.0002,
      "step": 82950
    },
    {
      "epoch": 2.65472,
      "grad_norm": 0.0037107046227902174,
      "learning_rate": 2.3020800000000002e-06,
      "loss": 0.0002,
      "step": 82960
    },
    {
      "epoch": 2.65504,
      "grad_norm": 0.003453460056334734,
      "learning_rate": 2.299946666666667e-06,
      "loss": 0.0002,
      "step": 82970
    },
    {
      "epoch": 2.65536,
      "grad_norm": 0.009910030290484428,
      "learning_rate": 2.2978133333333335e-06,
      "loss": 0.0002,
      "step": 82980
    },
    {
      "epoch": 2.6556800000000003,
      "grad_norm": 0.002626437461003661,
      "learning_rate": 2.29568e-06,
      "loss": 0.0002,
      "step": 82990
    },
    {
      "epoch": 2.656,
      "grad_norm": 6.120059013366699,
      "learning_rate": 2.293546666666667e-06,
      "loss": 0.0147,
      "step": 83000
    },
    {
      "epoch": 2.65632,
      "grad_norm": 0.0023757631424814463,
      "learning_rate": 2.2914133333333335e-06,
      "loss": 0.0587,
      "step": 83010
    },
    {
      "epoch": 2.65664,
      "grad_norm": 0.004459685180336237,
      "learning_rate": 2.28928e-06,
      "loss": 0.046,
      "step": 83020
    },
    {
      "epoch": 2.6569599999999998,
      "grad_norm": 0.00546686677262187,
      "learning_rate": 2.2871466666666668e-06,
      "loss": 0.0046,
      "step": 83030
    },
    {
      "epoch": 2.65728,
      "grad_norm": 0.0036591137759387493,
      "learning_rate": 2.2850133333333334e-06,
      "loss": 0.0002,
      "step": 83040
    },
    {
      "epoch": 2.6576,
      "grad_norm": 0.007424160372465849,
      "learning_rate": 2.28288e-06,
      "loss": 0.0668,
      "step": 83050
    },
    {
      "epoch": 2.65792,
      "grad_norm": 0.03514355421066284,
      "learning_rate": 2.280746666666667e-06,
      "loss": 0.0004,
      "step": 83060
    },
    {
      "epoch": 2.65824,
      "grad_norm": 0.011061291210353374,
      "learning_rate": 2.2786133333333334e-06,
      "loss": 0.0003,
      "step": 83070
    },
    {
      "epoch": 2.65856,
      "grad_norm": 0.0014282846823334694,
      "learning_rate": 2.27648e-06,
      "loss": 0.0001,
      "step": 83080
    },
    {
      "epoch": 2.65888,
      "grad_norm": 0.024313583970069885,
      "learning_rate": 2.274346666666667e-06,
      "loss": 0.0003,
      "step": 83090
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 0.014094162732362747,
      "learning_rate": 2.2722133333333333e-06,
      "loss": 0.0002,
      "step": 83100
    },
    {
      "epoch": 2.65952,
      "grad_norm": 0.004883052781224251,
      "learning_rate": 2.27008e-06,
      "loss": 0.0373,
      "step": 83110
    },
    {
      "epoch": 2.65984,
      "grad_norm": 0.0026071779429912567,
      "learning_rate": 2.267946666666667e-06,
      "loss": 0.0065,
      "step": 83120
    },
    {
      "epoch": 2.66016,
      "grad_norm": 0.004485501442104578,
      "learning_rate": 2.2658133333333337e-06,
      "loss": 0.0003,
      "step": 83130
    },
    {
      "epoch": 2.6604799999999997,
      "grad_norm": 3.4774465560913086,
      "learning_rate": 2.2636800000000003e-06,
      "loss": 0.0116,
      "step": 83140
    },
    {
      "epoch": 2.6608,
      "grad_norm": 0.0030078215058892965,
      "learning_rate": 2.261546666666667e-06,
      "loss": 0.0339,
      "step": 83150
    },
    {
      "epoch": 2.66112,
      "grad_norm": 0.008304577320814133,
      "learning_rate": 2.2594133333333336e-06,
      "loss": 0.0299,
      "step": 83160
    },
    {
      "epoch": 2.66144,
      "grad_norm": 0.0040048519149422646,
      "learning_rate": 2.2572800000000003e-06,
      "loss": 0.0002,
      "step": 83170
    },
    {
      "epoch": 2.66176,
      "grad_norm": 0.003101171925663948,
      "learning_rate": 2.255146666666667e-06,
      "loss": 0.0002,
      "step": 83180
    },
    {
      "epoch": 2.66208,
      "grad_norm": 0.00543063273653388,
      "learning_rate": 2.2530133333333336e-06,
      "loss": 0.0002,
      "step": 83190
    },
    {
      "epoch": 2.6624,
      "grad_norm": 0.011953244917094707,
      "learning_rate": 2.25088e-06,
      "loss": 0.0002,
      "step": 83200
    },
    {
      "epoch": 2.66272,
      "grad_norm": 0.010644729249179363,
      "learning_rate": 2.248746666666667e-06,
      "loss": 0.0009,
      "step": 83210
    },
    {
      "epoch": 2.66304,
      "grad_norm": 0.003045181045308709,
      "learning_rate": 2.2466133333333335e-06,
      "loss": 0.0002,
      "step": 83220
    },
    {
      "epoch": 2.66336,
      "grad_norm": 0.003930542152374983,
      "learning_rate": 2.24448e-06,
      "loss": 0.0005,
      "step": 83230
    },
    {
      "epoch": 2.6636800000000003,
      "grad_norm": 0.006636908743530512,
      "learning_rate": 2.242346666666667e-06,
      "loss": 0.0045,
      "step": 83240
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.005028051324188709,
      "learning_rate": 2.2402133333333334e-06,
      "loss": 0.0003,
      "step": 83250
    },
    {
      "epoch": 2.66432,
      "grad_norm": 0.03320081904530525,
      "learning_rate": 2.23808e-06,
      "loss": 0.0002,
      "step": 83260
    },
    {
      "epoch": 2.66464,
      "grad_norm": 0.005822660867124796,
      "learning_rate": 2.2359466666666667e-06,
      "loss": 0.0002,
      "step": 83270
    },
    {
      "epoch": 2.6649599999999998,
      "grad_norm": 0.014050966128706932,
      "learning_rate": 2.2338133333333334e-06,
      "loss": 0.0003,
      "step": 83280
    },
    {
      "epoch": 2.66528,
      "grad_norm": 0.0049799601547420025,
      "learning_rate": 2.23168e-06,
      "loss": 0.0429,
      "step": 83290
    },
    {
      "epoch": 2.6656,
      "grad_norm": 0.003723749192431569,
      "learning_rate": 2.2295466666666667e-06,
      "loss": 0.0002,
      "step": 83300
    },
    {
      "epoch": 2.66592,
      "grad_norm": 0.0021932562813162804,
      "learning_rate": 2.2274133333333338e-06,
      "loss": 0.0002,
      "step": 83310
    },
    {
      "epoch": 2.66624,
      "grad_norm": 0.0036452063359320164,
      "learning_rate": 2.2252800000000004e-06,
      "loss": 0.0005,
      "step": 83320
    },
    {
      "epoch": 2.66656,
      "grad_norm": 0.002131052315235138,
      "learning_rate": 2.2231466666666666e-06,
      "loss": 0.0002,
      "step": 83330
    },
    {
      "epoch": 2.66688,
      "grad_norm": 0.0023328079842031,
      "learning_rate": 2.2210133333333337e-06,
      "loss": 0.0002,
      "step": 83340
    },
    {
      "epoch": 2.6672000000000002,
      "grad_norm": 0.004659579135477543,
      "learning_rate": 2.2188800000000003e-06,
      "loss": 0.0421,
      "step": 83350
    },
    {
      "epoch": 2.66752,
      "grad_norm": 0.0031624804250895977,
      "learning_rate": 2.2167466666666666e-06,
      "loss": 0.0002,
      "step": 83360
    },
    {
      "epoch": 2.66784,
      "grad_norm": 0.07612437754869461,
      "learning_rate": 2.2146133333333336e-06,
      "loss": 0.0007,
      "step": 83370
    },
    {
      "epoch": 2.66816,
      "grad_norm": 0.003556736744940281,
      "learning_rate": 2.2124800000000003e-06,
      "loss": 0.0002,
      "step": 83380
    },
    {
      "epoch": 2.6684799999999997,
      "grad_norm": 0.0029577668756246567,
      "learning_rate": 2.210346666666667e-06,
      "loss": 0.0002,
      "step": 83390
    },
    {
      "epoch": 2.6688,
      "grad_norm": 0.0027756008785218,
      "learning_rate": 2.2082133333333336e-06,
      "loss": 0.0003,
      "step": 83400
    },
    {
      "epoch": 2.66912,
      "grad_norm": 0.0034799182321876287,
      "learning_rate": 2.2060800000000002e-06,
      "loss": 0.0002,
      "step": 83410
    },
    {
      "epoch": 2.66944,
      "grad_norm": 0.0038833012804389,
      "learning_rate": 2.203946666666667e-06,
      "loss": 0.0002,
      "step": 83420
    },
    {
      "epoch": 2.66976,
      "grad_norm": 0.002804051386192441,
      "learning_rate": 2.2018133333333335e-06,
      "loss": 0.0002,
      "step": 83430
    },
    {
      "epoch": 2.67008,
      "grad_norm": 0.005083228461444378,
      "learning_rate": 2.19968e-06,
      "loss": 0.0135,
      "step": 83440
    },
    {
      "epoch": 2.6704,
      "grad_norm": 0.0024727729614824057,
      "learning_rate": 2.197546666666667e-06,
      "loss": 0.0002,
      "step": 83450
    },
    {
      "epoch": 2.67072,
      "grad_norm": 0.003573473310098052,
      "learning_rate": 2.1954133333333335e-06,
      "loss": 0.0002,
      "step": 83460
    },
    {
      "epoch": 2.67104,
      "grad_norm": 0.0019369523506611586,
      "learning_rate": 2.19328e-06,
      "loss": 0.0433,
      "step": 83470
    },
    {
      "epoch": 2.67136,
      "grad_norm": 0.002918862970545888,
      "learning_rate": 2.191146666666667e-06,
      "loss": 0.0002,
      "step": 83480
    },
    {
      "epoch": 2.6716800000000003,
      "grad_norm": 0.0022707609459757805,
      "learning_rate": 2.1890133333333334e-06,
      "loss": 0.0219,
      "step": 83490
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.011964843608438969,
      "learning_rate": 2.18688e-06,
      "loss": 0.0002,
      "step": 83500
    },
    {
      "epoch": 2.67232,
      "grad_norm": 0.0029801379423588514,
      "learning_rate": 2.184746666666667e-06,
      "loss": 0.0003,
      "step": 83510
    },
    {
      "epoch": 2.67264,
      "grad_norm": 0.0027939225547015667,
      "learning_rate": 2.1826133333333334e-06,
      "loss": 0.0524,
      "step": 83520
    },
    {
      "epoch": 2.67296,
      "grad_norm": 0.005658695008605719,
      "learning_rate": 2.18048e-06,
      "loss": 0.0002,
      "step": 83530
    },
    {
      "epoch": 2.67328,
      "grad_norm": 0.006383770145475864,
      "learning_rate": 2.178346666666667e-06,
      "loss": 0.0324,
      "step": 83540
    },
    {
      "epoch": 2.6736,
      "grad_norm": 0.0014349357225000858,
      "learning_rate": 2.1762133333333337e-06,
      "loss": 0.0003,
      "step": 83550
    },
    {
      "epoch": 2.67392,
      "grad_norm": 0.012431089766323566,
      "learning_rate": 2.17408e-06,
      "loss": 0.0007,
      "step": 83560
    },
    {
      "epoch": 2.67424,
      "grad_norm": 0.003623766591772437,
      "learning_rate": 2.171946666666667e-06,
      "loss": 0.004,
      "step": 83570
    },
    {
      "epoch": 2.67456,
      "grad_norm": 0.004809750244021416,
      "learning_rate": 2.1698133333333337e-06,
      "loss": 0.0003,
      "step": 83580
    },
    {
      "epoch": 2.67488,
      "grad_norm": 0.0025281766429543495,
      "learning_rate": 2.16768e-06,
      "loss": 0.002,
      "step": 83590
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 0.005811838433146477,
      "learning_rate": 2.165546666666667e-06,
      "loss": 0.0017,
      "step": 83600
    },
    {
      "epoch": 2.67552,
      "grad_norm": 0.005926874466240406,
      "learning_rate": 2.1634133333333336e-06,
      "loss": 0.0003,
      "step": 83610
    },
    {
      "epoch": 2.67584,
      "grad_norm": 2.117147445678711,
      "learning_rate": 2.1612800000000003e-06,
      "loss": 0.0847,
      "step": 83620
    },
    {
      "epoch": 2.67616,
      "grad_norm": 0.00366161554120481,
      "learning_rate": 2.159146666666667e-06,
      "loss": 0.0002,
      "step": 83630
    },
    {
      "epoch": 2.6764799999999997,
      "grad_norm": 0.030187390744686127,
      "learning_rate": 2.1570133333333335e-06,
      "loss": 0.0002,
      "step": 83640
    },
    {
      "epoch": 2.6768,
      "grad_norm": 0.03149595484137535,
      "learning_rate": 2.15488e-06,
      "loss": 0.0002,
      "step": 83650
    },
    {
      "epoch": 2.67712,
      "grad_norm": 0.004193517845124006,
      "learning_rate": 2.152746666666667e-06,
      "loss": 0.0002,
      "step": 83660
    },
    {
      "epoch": 2.67744,
      "grad_norm": 0.003713475074619055,
      "learning_rate": 2.1506133333333335e-06,
      "loss": 0.0002,
      "step": 83670
    },
    {
      "epoch": 2.67776,
      "grad_norm": 0.004203830845654011,
      "learning_rate": 2.14848e-06,
      "loss": 0.0002,
      "step": 83680
    },
    {
      "epoch": 2.67808,
      "grad_norm": 0.002281554276123643,
      "learning_rate": 2.1463466666666668e-06,
      "loss": 0.0004,
      "step": 83690
    },
    {
      "epoch": 2.6784,
      "grad_norm": 0.0020647572819143534,
      "learning_rate": 2.1442133333333334e-06,
      "loss": 0.0003,
      "step": 83700
    },
    {
      "epoch": 2.67872,
      "grad_norm": 0.008783179335296154,
      "learning_rate": 2.14208e-06,
      "loss": 0.0002,
      "step": 83710
    },
    {
      "epoch": 2.67904,
      "grad_norm": 0.00492347776889801,
      "learning_rate": 2.1399466666666667e-06,
      "loss": 0.0078,
      "step": 83720
    },
    {
      "epoch": 2.67936,
      "grad_norm": 0.006652427837252617,
      "learning_rate": 2.1378133333333334e-06,
      "loss": 0.0005,
      "step": 83730
    },
    {
      "epoch": 2.67968,
      "grad_norm": 0.0036402868572622538,
      "learning_rate": 2.1356800000000004e-06,
      "loss": 0.0002,
      "step": 83740
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.038804832845926285,
      "learning_rate": 2.1335466666666667e-06,
      "loss": 0.0002,
      "step": 83750
    },
    {
      "epoch": 2.68032,
      "grad_norm": 0.003241299418732524,
      "learning_rate": 2.1314133333333333e-06,
      "loss": 0.0002,
      "step": 83760
    },
    {
      "epoch": 2.68064,
      "grad_norm": 0.0016897495370358229,
      "learning_rate": 2.1292800000000004e-06,
      "loss": 0.0139,
      "step": 83770
    },
    {
      "epoch": 2.68096,
      "grad_norm": 0.004375753458589315,
      "learning_rate": 2.1271466666666666e-06,
      "loss": 0.0236,
      "step": 83780
    },
    {
      "epoch": 2.68128,
      "grad_norm": 0.013959595933556557,
      "learning_rate": 2.1250133333333333e-06,
      "loss": 0.0407,
      "step": 83790
    },
    {
      "epoch": 2.6816,
      "grad_norm": 0.002842114306986332,
      "learning_rate": 2.1228800000000003e-06,
      "loss": 0.0002,
      "step": 83800
    },
    {
      "epoch": 2.68192,
      "grad_norm": 0.011236692778766155,
      "learning_rate": 2.120746666666667e-06,
      "loss": 0.0002,
      "step": 83810
    },
    {
      "epoch": 2.68224,
      "grad_norm": 0.006432441063225269,
      "learning_rate": 2.1186133333333336e-06,
      "loss": 0.0008,
      "step": 83820
    },
    {
      "epoch": 2.68256,
      "grad_norm": 0.007851126603782177,
      "learning_rate": 2.1164800000000003e-06,
      "loss": 0.0003,
      "step": 83830
    },
    {
      "epoch": 2.68288,
      "grad_norm": 0.004379488993436098,
      "learning_rate": 2.114346666666667e-06,
      "loss": 0.0326,
      "step": 83840
    },
    {
      "epoch": 2.6832000000000003,
      "grad_norm": 0.020675459876656532,
      "learning_rate": 2.1122133333333336e-06,
      "loss": 0.0002,
      "step": 83850
    },
    {
      "epoch": 2.68352,
      "grad_norm": 0.007547398563474417,
      "learning_rate": 2.1100800000000002e-06,
      "loss": 0.0286,
      "step": 83860
    },
    {
      "epoch": 2.68384,
      "grad_norm": 0.004825507756322622,
      "learning_rate": 2.107946666666667e-06,
      "loss": 0.0002,
      "step": 83870
    },
    {
      "epoch": 2.68416,
      "grad_norm": 0.003442778019234538,
      "learning_rate": 2.1058133333333335e-06,
      "loss": 0.0249,
      "step": 83880
    },
    {
      "epoch": 2.6844799999999998,
      "grad_norm": 0.0017778932815417647,
      "learning_rate": 2.10368e-06,
      "loss": 0.0667,
      "step": 83890
    },
    {
      "epoch": 2.6848,
      "grad_norm": 0.008342788554728031,
      "learning_rate": 2.101546666666667e-06,
      "loss": 0.0002,
      "step": 83900
    },
    {
      "epoch": 2.68512,
      "grad_norm": 0.0032242913730442524,
      "learning_rate": 2.0994133333333335e-06,
      "loss": 0.0034,
      "step": 83910
    },
    {
      "epoch": 2.68544,
      "grad_norm": 0.0035681899171322584,
      "learning_rate": 2.09728e-06,
      "loss": 0.0002,
      "step": 83920
    },
    {
      "epoch": 2.68576,
      "grad_norm": 0.023724917322397232,
      "learning_rate": 2.0951466666666668e-06,
      "loss": 0.0003,
      "step": 83930
    },
    {
      "epoch": 2.68608,
      "grad_norm": 0.01244947500526905,
      "learning_rate": 2.0930133333333334e-06,
      "loss": 0.0003,
      "step": 83940
    },
    {
      "epoch": 2.6864,
      "grad_norm": 0.007928997278213501,
      "learning_rate": 2.09088e-06,
      "loss": 0.0649,
      "step": 83950
    },
    {
      "epoch": 2.68672,
      "grad_norm": 0.003641159273684025,
      "learning_rate": 2.0887466666666667e-06,
      "loss": 0.0003,
      "step": 83960
    },
    {
      "epoch": 2.68704,
      "grad_norm": 0.0021188342943787575,
      "learning_rate": 2.0866133333333333e-06,
      "loss": 0.0004,
      "step": 83970
    },
    {
      "epoch": 2.68736,
      "grad_norm": 0.005830270703881979,
      "learning_rate": 2.08448e-06,
      "loss": 0.0003,
      "step": 83980
    },
    {
      "epoch": 2.68768,
      "grad_norm": 0.0032092868350446224,
      "learning_rate": 2.082346666666667e-06,
      "loss": 0.0003,
      "step": 83990
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.006418017204850912,
      "learning_rate": 2.0802133333333337e-06,
      "loss": 0.0196,
      "step": 84000
    },
    {
      "epoch": 2.68832,
      "grad_norm": 0.0040343827567994595,
      "learning_rate": 2.07808e-06,
      "loss": 0.003,
      "step": 84010
    },
    {
      "epoch": 2.68864,
      "grad_norm": 0.004556436091661453,
      "learning_rate": 2.075946666666667e-06,
      "loss": 0.0005,
      "step": 84020
    },
    {
      "epoch": 2.68896,
      "grad_norm": 0.006679875310510397,
      "learning_rate": 2.0738133333333336e-06,
      "loss": 0.0002,
      "step": 84030
    },
    {
      "epoch": 2.68928,
      "grad_norm": 0.0048659564927220345,
      "learning_rate": 2.07168e-06,
      "loss": 0.0002,
      "step": 84040
    },
    {
      "epoch": 2.6896,
      "grad_norm": 0.012038703076541424,
      "learning_rate": 2.069546666666667e-06,
      "loss": 0.0004,
      "step": 84050
    },
    {
      "epoch": 2.68992,
      "grad_norm": 0.0018596925074234605,
      "learning_rate": 2.0674133333333336e-06,
      "loss": 0.0003,
      "step": 84060
    },
    {
      "epoch": 2.69024,
      "grad_norm": 0.011848676018416882,
      "learning_rate": 2.0652800000000002e-06,
      "loss": 0.0005,
      "step": 84070
    },
    {
      "epoch": 2.69056,
      "grad_norm": 0.004525178577750921,
      "learning_rate": 2.063146666666667e-06,
      "loss": 0.0005,
      "step": 84080
    },
    {
      "epoch": 2.69088,
      "grad_norm": 0.010815519839525223,
      "learning_rate": 2.0610133333333335e-06,
      "loss": 0.0013,
      "step": 84090
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 0.005503608379513025,
      "learning_rate": 2.05888e-06,
      "loss": 0.0002,
      "step": 84100
    },
    {
      "epoch": 2.69152,
      "grad_norm": 0.0036222056951373816,
      "learning_rate": 2.056746666666667e-06,
      "loss": 0.0377,
      "step": 84110
    },
    {
      "epoch": 2.69184,
      "grad_norm": 0.002780963433906436,
      "learning_rate": 2.0546133333333335e-06,
      "loss": 0.0555,
      "step": 84120
    },
    {
      "epoch": 2.69216,
      "grad_norm": 3.338815927505493,
      "learning_rate": 2.05248e-06,
      "loss": 0.0079,
      "step": 84130
    },
    {
      "epoch": 2.6924799999999998,
      "grad_norm": 0.0034654957707971334,
      "learning_rate": 2.0503466666666668e-06,
      "loss": 0.0002,
      "step": 84140
    },
    {
      "epoch": 2.6928,
      "grad_norm": 0.0025428817607462406,
      "learning_rate": 2.0482133333333334e-06,
      "loss": 0.0596,
      "step": 84150
    },
    {
      "epoch": 2.69312,
      "grad_norm": 0.0031909297686070204,
      "learning_rate": 2.0460800000000005e-06,
      "loss": 0.0002,
      "step": 84160
    },
    {
      "epoch": 2.69344,
      "grad_norm": 0.002074632328003645,
      "learning_rate": 2.0439466666666667e-06,
      "loss": 0.0002,
      "step": 84170
    },
    {
      "epoch": 2.69376,
      "grad_norm": 0.00963269267231226,
      "learning_rate": 2.0418133333333334e-06,
      "loss": 0.0002,
      "step": 84180
    },
    {
      "epoch": 2.69408,
      "grad_norm": 0.003826285945251584,
      "learning_rate": 2.0396800000000004e-06,
      "loss": 0.0357,
      "step": 84190
    },
    {
      "epoch": 2.6944,
      "grad_norm": 4.019918441772461,
      "learning_rate": 2.0375466666666667e-06,
      "loss": 0.0193,
      "step": 84200
    },
    {
      "epoch": 2.6947200000000002,
      "grad_norm": 0.006037857849150896,
      "learning_rate": 2.0354133333333333e-06,
      "loss": 0.0002,
      "step": 84210
    },
    {
      "epoch": 2.69504,
      "grad_norm": 0.004776060115545988,
      "learning_rate": 2.0332800000000004e-06,
      "loss": 0.0003,
      "step": 84220
    },
    {
      "epoch": 2.69536,
      "grad_norm": 0.004119549412280321,
      "learning_rate": 2.031146666666667e-06,
      "loss": 0.021,
      "step": 84230
    },
    {
      "epoch": 2.69568,
      "grad_norm": 0.004122460726648569,
      "learning_rate": 2.0290133333333332e-06,
      "loss": 0.0002,
      "step": 84240
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 5.459557056427002,
      "learning_rate": 2.0268800000000003e-06,
      "loss": 0.0057,
      "step": 84250
    },
    {
      "epoch": 2.69632,
      "grad_norm": 0.001672624726779759,
      "learning_rate": 2.024746666666667e-06,
      "loss": 0.0004,
      "step": 84260
    },
    {
      "epoch": 2.69664,
      "grad_norm": 0.01303873211145401,
      "learning_rate": 2.022613333333333e-06,
      "loss": 0.0002,
      "step": 84270
    },
    {
      "epoch": 2.69696,
      "grad_norm": 0.0056114839389920235,
      "learning_rate": 2.0204800000000003e-06,
      "loss": 0.0002,
      "step": 84280
    },
    {
      "epoch": 2.69728,
      "grad_norm": 0.01272449642419815,
      "learning_rate": 2.018346666666667e-06,
      "loss": 0.0003,
      "step": 84290
    },
    {
      "epoch": 2.6976,
      "grad_norm": 0.005796387325972319,
      "learning_rate": 2.0162133333333336e-06,
      "loss": 0.0014,
      "step": 84300
    },
    {
      "epoch": 2.69792,
      "grad_norm": 0.006399156991392374,
      "learning_rate": 2.01408e-06,
      "loss": 0.0539,
      "step": 84310
    },
    {
      "epoch": 2.69824,
      "grad_norm": 0.008065356872975826,
      "learning_rate": 2.011946666666667e-06,
      "loss": 0.0002,
      "step": 84320
    },
    {
      "epoch": 2.69856,
      "grad_norm": 0.009128597564995289,
      "learning_rate": 2.0098133333333335e-06,
      "loss": 0.0002,
      "step": 84330
    },
    {
      "epoch": 2.69888,
      "grad_norm": 0.013473156839609146,
      "learning_rate": 2.00768e-06,
      "loss": 0.0201,
      "step": 84340
    },
    {
      "epoch": 2.6992000000000003,
      "grad_norm": 0.005966909695416689,
      "learning_rate": 2.005546666666667e-06,
      "loss": 0.0002,
      "step": 84350
    },
    {
      "epoch": 2.69952,
      "grad_norm": 0.005239318590611219,
      "learning_rate": 2.0034133333333334e-06,
      "loss": 0.0062,
      "step": 84360
    },
    {
      "epoch": 2.69984,
      "grad_norm": 0.003074407810345292,
      "learning_rate": 2.00128e-06,
      "loss": 0.0009,
      "step": 84370
    },
    {
      "epoch": 2.70016,
      "grad_norm": 0.012750431895256042,
      "learning_rate": 1.9991466666666667e-06,
      "loss": 0.0002,
      "step": 84380
    },
    {
      "epoch": 2.7004799999999998,
      "grad_norm": 0.016274213790893555,
      "learning_rate": 1.9970133333333334e-06,
      "loss": 0.0114,
      "step": 84390
    },
    {
      "epoch": 2.7008,
      "grad_norm": 0.002806722419336438,
      "learning_rate": 1.99488e-06,
      "loss": 0.0002,
      "step": 84400
    },
    {
      "epoch": 2.70112,
      "grad_norm": 0.0017952237976714969,
      "learning_rate": 1.9927466666666667e-06,
      "loss": 0.0003,
      "step": 84410
    },
    {
      "epoch": 2.70144,
      "grad_norm": 0.020960301160812378,
      "learning_rate": 1.9906133333333338e-06,
      "loss": 0.0002,
      "step": 84420
    },
    {
      "epoch": 2.70176,
      "grad_norm": 0.012248738668859005,
      "learning_rate": 1.98848e-06,
      "loss": 0.0004,
      "step": 84430
    },
    {
      "epoch": 2.70208,
      "grad_norm": 0.0020103941205888987,
      "learning_rate": 1.9863466666666666e-06,
      "loss": 0.0955,
      "step": 84440
    },
    {
      "epoch": 2.7024,
      "grad_norm": 0.0035619859118014574,
      "learning_rate": 1.9842133333333337e-06,
      "loss": 0.0005,
      "step": 84450
    },
    {
      "epoch": 2.7027200000000002,
      "grad_norm": 0.0033234748989343643,
      "learning_rate": 1.98208e-06,
      "loss": 0.0015,
      "step": 84460
    },
    {
      "epoch": 2.70304,
      "grad_norm": 0.0026021304074674845,
      "learning_rate": 1.979946666666667e-06,
      "loss": 0.0003,
      "step": 84470
    },
    {
      "epoch": 2.70336,
      "grad_norm": 0.01211613230407238,
      "learning_rate": 1.9778133333333336e-06,
      "loss": 0.0008,
      "step": 84480
    },
    {
      "epoch": 2.70368,
      "grad_norm": 0.010104997083544731,
      "learning_rate": 1.9756800000000003e-06,
      "loss": 0.0618,
      "step": 84490
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.27227210998535156,
      "learning_rate": 1.973546666666667e-06,
      "loss": 0.0005,
      "step": 84500
    },
    {
      "epoch": 2.70432,
      "grad_norm": 0.004110257141292095,
      "learning_rate": 1.9714133333333336e-06,
      "loss": 0.0005,
      "step": 84510
    },
    {
      "epoch": 2.70464,
      "grad_norm": 0.01720864698290825,
      "learning_rate": 1.9692800000000002e-06,
      "loss": 0.0003,
      "step": 84520
    },
    {
      "epoch": 2.70496,
      "grad_norm": 0.0050525907427072525,
      "learning_rate": 1.967146666666667e-06,
      "loss": 0.001,
      "step": 84530
    },
    {
      "epoch": 2.70528,
      "grad_norm": 0.004978721961379051,
      "learning_rate": 1.9650133333333335e-06,
      "loss": 0.0006,
      "step": 84540
    },
    {
      "epoch": 2.7056,
      "grad_norm": 1.1851427555084229,
      "learning_rate": 1.96288e-06,
      "loss": 0.0332,
      "step": 84550
    },
    {
      "epoch": 2.70592,
      "grad_norm": 0.01042480580508709,
      "learning_rate": 1.960746666666667e-06,
      "loss": 0.0002,
      "step": 84560
    },
    {
      "epoch": 2.70624,
      "grad_norm": 0.005913788452744484,
      "learning_rate": 1.9586133333333335e-06,
      "loss": 0.0006,
      "step": 84570
    },
    {
      "epoch": 2.70656,
      "grad_norm": 0.005573662929236889,
      "learning_rate": 1.95648e-06,
      "loss": 0.0066,
      "step": 84580
    },
    {
      "epoch": 2.70688,
      "grad_norm": 0.00617603212594986,
      "learning_rate": 1.9543466666666668e-06,
      "loss": 0.0027,
      "step": 84590
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 0.002065390581265092,
      "learning_rate": 1.9522133333333334e-06,
      "loss": 0.0008,
      "step": 84600
    },
    {
      "epoch": 2.70752,
      "grad_norm": 0.0034237259533256292,
      "learning_rate": 1.95008e-06,
      "loss": 0.0004,
      "step": 84610
    },
    {
      "epoch": 2.70784,
      "grad_norm": 0.0010787277715280652,
      "learning_rate": 1.9479466666666667e-06,
      "loss": 0.0002,
      "step": 84620
    },
    {
      "epoch": 2.70816,
      "grad_norm": 0.005460949148982763,
      "learning_rate": 1.9458133333333334e-06,
      "loss": 0.0002,
      "step": 84630
    },
    {
      "epoch": 2.7084799999999998,
      "grad_norm": 0.005504002794623375,
      "learning_rate": 1.9436800000000004e-06,
      "loss": 0.0003,
      "step": 84640
    },
    {
      "epoch": 2.7088,
      "grad_norm": 0.009309941902756691,
      "learning_rate": 1.9415466666666666e-06,
      "loss": 0.0267,
      "step": 84650
    },
    {
      "epoch": 2.70912,
      "grad_norm": 0.007840679958462715,
      "learning_rate": 1.9394133333333333e-06,
      "loss": 0.0002,
      "step": 84660
    },
    {
      "epoch": 2.70944,
      "grad_norm": 0.005865870509296656,
      "learning_rate": 1.9372800000000004e-06,
      "loss": 0.0005,
      "step": 84670
    },
    {
      "epoch": 2.70976,
      "grad_norm": 0.003341465722769499,
      "learning_rate": 1.935146666666667e-06,
      "loss": 0.0004,
      "step": 84680
    },
    {
      "epoch": 2.71008,
      "grad_norm": 0.00576084665954113,
      "learning_rate": 1.9330133333333332e-06,
      "loss": 0.0002,
      "step": 84690
    },
    {
      "epoch": 2.7104,
      "grad_norm": 0.011334392242133617,
      "learning_rate": 1.9308800000000003e-06,
      "loss": 0.0002,
      "step": 84700
    },
    {
      "epoch": 2.7107200000000002,
      "grad_norm": 0.007597075775265694,
      "learning_rate": 1.928746666666667e-06,
      "loss": 0.0005,
      "step": 84710
    },
    {
      "epoch": 2.71104,
      "grad_norm": 0.002552275313064456,
      "learning_rate": 1.9266133333333336e-06,
      "loss": 0.0003,
      "step": 84720
    },
    {
      "epoch": 2.71136,
      "grad_norm": 0.004898361396044493,
      "learning_rate": 1.9244800000000002e-06,
      "loss": 0.0003,
      "step": 84730
    },
    {
      "epoch": 2.71168,
      "grad_norm": 0.013606279157102108,
      "learning_rate": 1.922346666666667e-06,
      "loss": 0.0003,
      "step": 84740
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.0035497290082275867,
      "learning_rate": 1.9202133333333335e-06,
      "loss": 0.0003,
      "step": 84750
    },
    {
      "epoch": 2.71232,
      "grad_norm": 0.0031143208034336567,
      "learning_rate": 1.91808e-06,
      "loss": 0.0023,
      "step": 84760
    },
    {
      "epoch": 2.71264,
      "grad_norm": 0.004011670593172312,
      "learning_rate": 1.915946666666667e-06,
      "loss": 0.0028,
      "step": 84770
    },
    {
      "epoch": 2.71296,
      "grad_norm": 0.005166572518646717,
      "learning_rate": 1.9138133333333335e-06,
      "loss": 0.0087,
      "step": 84780
    },
    {
      "epoch": 2.71328,
      "grad_norm": 0.02981903776526451,
      "learning_rate": 1.91168e-06,
      "loss": 0.0002,
      "step": 84790
    },
    {
      "epoch": 2.7136,
      "grad_norm": 0.003840344725176692,
      "learning_rate": 1.9095466666666668e-06,
      "loss": 0.0002,
      "step": 84800
    },
    {
      "epoch": 2.71392,
      "grad_norm": 0.009406983852386475,
      "learning_rate": 1.9074133333333334e-06,
      "loss": 0.014,
      "step": 84810
    },
    {
      "epoch": 2.71424,
      "grad_norm": 0.002609461545944214,
      "learning_rate": 1.9052800000000003e-06,
      "loss": 0.0003,
      "step": 84820
    },
    {
      "epoch": 2.71456,
      "grad_norm": 0.008017507381737232,
      "learning_rate": 1.9031466666666667e-06,
      "loss": 0.0002,
      "step": 84830
    },
    {
      "epoch": 2.71488,
      "grad_norm": 0.0034820595756173134,
      "learning_rate": 1.9010133333333336e-06,
      "loss": 0.038,
      "step": 84840
    },
    {
      "epoch": 2.7152,
      "grad_norm": 0.006897589657455683,
      "learning_rate": 1.8988800000000002e-06,
      "loss": 0.0002,
      "step": 84850
    },
    {
      "epoch": 2.71552,
      "grad_norm": 0.013107020407915115,
      "learning_rate": 1.8967466666666667e-06,
      "loss": 0.0003,
      "step": 84860
    },
    {
      "epoch": 2.71584,
      "grad_norm": 0.0026138813700526953,
      "learning_rate": 1.8946133333333335e-06,
      "loss": 0.0003,
      "step": 84870
    },
    {
      "epoch": 2.71616,
      "grad_norm": 0.010337826795876026,
      "learning_rate": 1.8924800000000002e-06,
      "loss": 0.0003,
      "step": 84880
    },
    {
      "epoch": 2.71648,
      "grad_norm": 0.015351271256804466,
      "learning_rate": 1.8903466666666668e-06,
      "loss": 0.0007,
      "step": 84890
    },
    {
      "epoch": 2.7168,
      "grad_norm": 0.002794069703668356,
      "learning_rate": 1.8882133333333335e-06,
      "loss": 0.0002,
      "step": 84900
    },
    {
      "epoch": 2.71712,
      "grad_norm": 0.004702070727944374,
      "learning_rate": 1.8860800000000001e-06,
      "loss": 0.0226,
      "step": 84910
    },
    {
      "epoch": 2.71744,
      "grad_norm": 0.009359356015920639,
      "learning_rate": 1.8839466666666668e-06,
      "loss": 0.0002,
      "step": 84920
    },
    {
      "epoch": 2.71776,
      "grad_norm": 0.008669216185808182,
      "learning_rate": 1.8818133333333336e-06,
      "loss": 0.0127,
      "step": 84930
    },
    {
      "epoch": 2.71808,
      "grad_norm": 0.005864139646291733,
      "learning_rate": 1.87968e-06,
      "loss": 0.0002,
      "step": 84940
    },
    {
      "epoch": 2.7184,
      "grad_norm": 0.013191655278205872,
      "learning_rate": 1.8775466666666667e-06,
      "loss": 0.0016,
      "step": 84950
    },
    {
      "epoch": 2.7187200000000002,
      "grad_norm": 0.007357118185609579,
      "learning_rate": 1.8754133333333336e-06,
      "loss": 0.0003,
      "step": 84960
    },
    {
      "epoch": 2.71904,
      "grad_norm": 0.0033417092636227608,
      "learning_rate": 1.87328e-06,
      "loss": 0.1116,
      "step": 84970
    },
    {
      "epoch": 2.71936,
      "grad_norm": 3.034322738647461,
      "learning_rate": 1.8711466666666669e-06,
      "loss": 0.0045,
      "step": 84980
    },
    {
      "epoch": 2.71968,
      "grad_norm": 0.015132355503737926,
      "learning_rate": 1.8690133333333335e-06,
      "loss": 0.0004,
      "step": 84990
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.01158791035413742,
      "learning_rate": 1.8668800000000002e-06,
      "loss": 0.0003,
      "step": 85000
    },
    {
      "epoch": 2.72032,
      "grad_norm": 0.008687709458172321,
      "learning_rate": 1.864746666666667e-06,
      "loss": 0.0111,
      "step": 85010
    },
    {
      "epoch": 2.72064,
      "grad_norm": 0.002589777112007141,
      "learning_rate": 1.8626133333333335e-06,
      "loss": 0.0002,
      "step": 85020
    },
    {
      "epoch": 2.72096,
      "grad_norm": 3.0494441986083984,
      "learning_rate": 1.86048e-06,
      "loss": 0.0028,
      "step": 85030
    },
    {
      "epoch": 2.72128,
      "grad_norm": 0.019890859723091125,
      "learning_rate": 1.858346666666667e-06,
      "loss": 0.0002,
      "step": 85040
    },
    {
      "epoch": 2.7216,
      "grad_norm": 0.004680872894823551,
      "learning_rate": 1.8562133333333334e-06,
      "loss": 0.0002,
      "step": 85050
    },
    {
      "epoch": 2.72192,
      "grad_norm": 0.010113049298524857,
      "learning_rate": 1.85408e-06,
      "loss": 0.0378,
      "step": 85060
    },
    {
      "epoch": 2.72224,
      "grad_norm": 0.0019370276713743806,
      "learning_rate": 1.851946666666667e-06,
      "loss": 0.0038,
      "step": 85070
    },
    {
      "epoch": 2.72256,
      "grad_norm": 0.0073614115826785564,
      "learning_rate": 1.8498133333333335e-06,
      "loss": 0.0002,
      "step": 85080
    },
    {
      "epoch": 2.72288,
      "grad_norm": 0.037732865661382675,
      "learning_rate": 1.84768e-06,
      "loss": 0.0003,
      "step": 85090
    },
    {
      "epoch": 2.7232,
      "grad_norm": 0.00258457288146019,
      "learning_rate": 1.8455466666666668e-06,
      "loss": 0.0002,
      "step": 85100
    },
    {
      "epoch": 2.72352,
      "grad_norm": 0.003028021426871419,
      "learning_rate": 1.8434133333333335e-06,
      "loss": 0.0002,
      "step": 85110
    },
    {
      "epoch": 2.72384,
      "grad_norm": 0.008784796111285686,
      "learning_rate": 1.84128e-06,
      "loss": 0.0003,
      "step": 85120
    },
    {
      "epoch": 2.72416,
      "grad_norm": 0.002388562774285674,
      "learning_rate": 1.8391466666666668e-06,
      "loss": 0.0002,
      "step": 85130
    },
    {
      "epoch": 2.72448,
      "grad_norm": 0.0024743720423430204,
      "learning_rate": 1.8370133333333334e-06,
      "loss": 0.0002,
      "step": 85140
    },
    {
      "epoch": 2.7248,
      "grad_norm": 0.018118618056178093,
      "learning_rate": 1.8348800000000003e-06,
      "loss": 0.0002,
      "step": 85150
    },
    {
      "epoch": 2.72512,
      "grad_norm": 0.011108076199889183,
      "learning_rate": 1.832746666666667e-06,
      "loss": 0.0003,
      "step": 85160
    },
    {
      "epoch": 2.72544,
      "grad_norm": 0.012222075834870338,
      "learning_rate": 1.8306133333333334e-06,
      "loss": 0.0189,
      "step": 85170
    },
    {
      "epoch": 2.72576,
      "grad_norm": 0.013064376078546047,
      "learning_rate": 1.8284800000000002e-06,
      "loss": 0.0003,
      "step": 85180
    },
    {
      "epoch": 2.72608,
      "grad_norm": 0.007111120503395796,
      "learning_rate": 1.8263466666666669e-06,
      "loss": 0.0002,
      "step": 85190
    },
    {
      "epoch": 2.7264,
      "grad_norm": 0.0171208456158638,
      "learning_rate": 1.8242133333333333e-06,
      "loss": 0.0027,
      "step": 85200
    },
    {
      "epoch": 2.7267200000000003,
      "grad_norm": 0.012309515848755836,
      "learning_rate": 1.8220800000000002e-06,
      "loss": 0.0002,
      "step": 85210
    },
    {
      "epoch": 2.72704,
      "grad_norm": 0.008021560497581959,
      "learning_rate": 1.8199466666666668e-06,
      "loss": 0.0003,
      "step": 85220
    },
    {
      "epoch": 2.72736,
      "grad_norm": 0.006884921342134476,
      "learning_rate": 1.8178133333333335e-06,
      "loss": 0.0002,
      "step": 85230
    },
    {
      "epoch": 2.72768,
      "grad_norm": 7.486462593078613,
      "learning_rate": 1.8156800000000001e-06,
      "loss": 0.0091,
      "step": 85240
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.002971678040921688,
      "learning_rate": 1.8135466666666668e-06,
      "loss": 0.0003,
      "step": 85250
    },
    {
      "epoch": 2.72832,
      "grad_norm": 0.004259467590600252,
      "learning_rate": 1.8114133333333334e-06,
      "loss": 0.0008,
      "step": 85260
    },
    {
      "epoch": 2.72864,
      "grad_norm": 0.42376869916915894,
      "learning_rate": 1.8092800000000003e-06,
      "loss": 0.0006,
      "step": 85270
    },
    {
      "epoch": 2.72896,
      "grad_norm": 0.006096867378801107,
      "learning_rate": 1.8071466666666667e-06,
      "loss": 0.0003,
      "step": 85280
    },
    {
      "epoch": 2.72928,
      "grad_norm": 0.002663263352587819,
      "learning_rate": 1.8050133333333336e-06,
      "loss": 0.0003,
      "step": 85290
    },
    {
      "epoch": 2.7296,
      "grad_norm": 0.007856093347072601,
      "learning_rate": 1.8028800000000002e-06,
      "loss": 0.0002,
      "step": 85300
    },
    {
      "epoch": 2.72992,
      "grad_norm": 0.006559478584676981,
      "learning_rate": 1.8007466666666667e-06,
      "loss": 0.0002,
      "step": 85310
    },
    {
      "epoch": 2.7302400000000002,
      "grad_norm": 0.003966364078223705,
      "learning_rate": 1.7986133333333335e-06,
      "loss": 0.0025,
      "step": 85320
    },
    {
      "epoch": 2.73056,
      "grad_norm": 0.010554332286119461,
      "learning_rate": 1.7964800000000002e-06,
      "loss": 0.0088,
      "step": 85330
    },
    {
      "epoch": 2.73088,
      "grad_norm": 0.007932206615805626,
      "learning_rate": 1.7943466666666668e-06,
      "loss": 0.0003,
      "step": 85340
    },
    {
      "epoch": 2.7312,
      "grad_norm": 0.004672580398619175,
      "learning_rate": 1.7922133333333337e-06,
      "loss": 0.0003,
      "step": 85350
    },
    {
      "epoch": 2.7315199999999997,
      "grad_norm": 0.0051664686761796474,
      "learning_rate": 1.79008e-06,
      "loss": 0.0002,
      "step": 85360
    },
    {
      "epoch": 2.73184,
      "grad_norm": 0.07619059830904007,
      "learning_rate": 1.7879466666666668e-06,
      "loss": 0.0003,
      "step": 85370
    },
    {
      "epoch": 2.73216,
      "grad_norm": 0.0031279672402888536,
      "learning_rate": 1.7858133333333336e-06,
      "loss": 0.0002,
      "step": 85380
    },
    {
      "epoch": 2.73248,
      "grad_norm": 0.06289772689342499,
      "learning_rate": 1.78368e-06,
      "loss": 0.0003,
      "step": 85390
    },
    {
      "epoch": 2.7328,
      "grad_norm": 0.006094168405979872,
      "learning_rate": 1.7815466666666667e-06,
      "loss": 0.0804,
      "step": 85400
    },
    {
      "epoch": 2.73312,
      "grad_norm": 0.005547237116843462,
      "learning_rate": 1.7794133333333336e-06,
      "loss": 0.0184,
      "step": 85410
    },
    {
      "epoch": 2.73344,
      "grad_norm": 0.003452696604654193,
      "learning_rate": 1.7772800000000002e-06,
      "loss": 0.0005,
      "step": 85420
    },
    {
      "epoch": 2.73376,
      "grad_norm": 0.00255614728666842,
      "learning_rate": 1.7751466666666666e-06,
      "loss": 0.0003,
      "step": 85430
    },
    {
      "epoch": 2.73408,
      "grad_norm": 0.006058559287339449,
      "learning_rate": 1.7730133333333335e-06,
      "loss": 0.0003,
      "step": 85440
    },
    {
      "epoch": 2.7344,
      "grad_norm": 0.006265825126320124,
      "learning_rate": 1.7708800000000001e-06,
      "loss": 0.0002,
      "step": 85450
    },
    {
      "epoch": 2.7347200000000003,
      "grad_norm": 0.004099851008504629,
      "learning_rate": 1.768746666666667e-06,
      "loss": 0.0003,
      "step": 85460
    },
    {
      "epoch": 2.73504,
      "grad_norm": 0.5129494667053223,
      "learning_rate": 1.7666133333333334e-06,
      "loss": 0.0017,
      "step": 85470
    },
    {
      "epoch": 2.73536,
      "grad_norm": 1.8416966199874878,
      "learning_rate": 1.76448e-06,
      "loss": 0.0467,
      "step": 85480
    },
    {
      "epoch": 2.73568,
      "grad_norm": 0.023175444453954697,
      "learning_rate": 1.762346666666667e-06,
      "loss": 0.0005,
      "step": 85490
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.008985159918665886,
      "learning_rate": 1.7602133333333336e-06,
      "loss": 0.003,
      "step": 85500
    },
    {
      "epoch": 2.73632,
      "grad_norm": 0.0038378778845071793,
      "learning_rate": 1.75808e-06,
      "loss": 0.0011,
      "step": 85510
    },
    {
      "epoch": 2.73664,
      "grad_norm": 0.00921504758298397,
      "learning_rate": 1.7559466666666669e-06,
      "loss": 0.0005,
      "step": 85520
    },
    {
      "epoch": 2.73696,
      "grad_norm": 0.0082758953794837,
      "learning_rate": 1.7538133333333335e-06,
      "loss": 0.0003,
      "step": 85530
    },
    {
      "epoch": 2.73728,
      "grad_norm": 0.014353455975651741,
      "learning_rate": 1.75168e-06,
      "loss": 0.0002,
      "step": 85540
    },
    {
      "epoch": 2.7376,
      "grad_norm": 0.007297768257558346,
      "learning_rate": 1.7495466666666668e-06,
      "loss": 0.0002,
      "step": 85550
    },
    {
      "epoch": 2.73792,
      "grad_norm": 0.007596495561301708,
      "learning_rate": 1.7474133333333335e-06,
      "loss": 0.0004,
      "step": 85560
    },
    {
      "epoch": 2.7382400000000002,
      "grad_norm": 0.4785311818122864,
      "learning_rate": 1.7452800000000001e-06,
      "loss": 0.0011,
      "step": 85570
    },
    {
      "epoch": 2.73856,
      "grad_norm": 0.011104952543973923,
      "learning_rate": 1.7431466666666668e-06,
      "loss": 0.0002,
      "step": 85580
    },
    {
      "epoch": 2.73888,
      "grad_norm": 0.005339987110346556,
      "learning_rate": 1.7410133333333334e-06,
      "loss": 0.0002,
      "step": 85590
    },
    {
      "epoch": 2.7392,
      "grad_norm": 0.0045571038499474525,
      "learning_rate": 1.73888e-06,
      "loss": 0.0003,
      "step": 85600
    },
    {
      "epoch": 2.7395199999999997,
      "grad_norm": 0.02897023595869541,
      "learning_rate": 1.736746666666667e-06,
      "loss": 0.0004,
      "step": 85610
    },
    {
      "epoch": 2.73984,
      "grad_norm": 0.004514032043516636,
      "learning_rate": 1.7346133333333334e-06,
      "loss": 0.0002,
      "step": 85620
    },
    {
      "epoch": 2.74016,
      "grad_norm": 0.015655338764190674,
      "learning_rate": 1.7324800000000002e-06,
      "loss": 0.0003,
      "step": 85630
    },
    {
      "epoch": 2.74048,
      "grad_norm": 0.009131887927651405,
      "learning_rate": 1.7303466666666669e-06,
      "loss": 0.0002,
      "step": 85640
    },
    {
      "epoch": 2.7408,
      "grad_norm": 0.003966344054788351,
      "learning_rate": 1.7282133333333333e-06,
      "loss": 0.0496,
      "step": 85650
    },
    {
      "epoch": 2.74112,
      "grad_norm": 0.016605861485004425,
      "learning_rate": 1.7260800000000002e-06,
      "loss": 0.0002,
      "step": 85660
    },
    {
      "epoch": 2.74144,
      "grad_norm": 0.018102848902344704,
      "learning_rate": 1.7239466666666668e-06,
      "loss": 0.0004,
      "step": 85670
    },
    {
      "epoch": 2.74176,
      "grad_norm": 0.0015288563445210457,
      "learning_rate": 1.7218133333333335e-06,
      "loss": 0.0002,
      "step": 85680
    },
    {
      "epoch": 2.74208,
      "grad_norm": 0.002641248283907771,
      "learning_rate": 1.7196800000000003e-06,
      "loss": 0.0002,
      "step": 85690
    },
    {
      "epoch": 2.7424,
      "grad_norm": 2.4440128803253174,
      "learning_rate": 1.7175466666666668e-06,
      "loss": 0.0767,
      "step": 85700
    },
    {
      "epoch": 2.7427200000000003,
      "grad_norm": 0.0011604378232732415,
      "learning_rate": 1.7154133333333334e-06,
      "loss": 0.0002,
      "step": 85710
    },
    {
      "epoch": 2.74304,
      "grad_norm": 0.0037598428316414356,
      "learning_rate": 1.7132800000000003e-06,
      "loss": 0.0002,
      "step": 85720
    },
    {
      "epoch": 2.74336,
      "grad_norm": 0.006417650263756514,
      "learning_rate": 1.7111466666666667e-06,
      "loss": 0.0185,
      "step": 85730
    },
    {
      "epoch": 2.74368,
      "grad_norm": 0.0023055763449519873,
      "learning_rate": 1.7090133333333333e-06,
      "loss": 0.0007,
      "step": 85740
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.009004464372992516,
      "learning_rate": 1.7068800000000002e-06,
      "loss": 0.0002,
      "step": 85750
    },
    {
      "epoch": 2.74432,
      "grad_norm": 0.01920337602496147,
      "learning_rate": 1.7047466666666669e-06,
      "loss": 0.0004,
      "step": 85760
    },
    {
      "epoch": 2.74464,
      "grad_norm": 0.02464616857469082,
      "learning_rate": 1.7026133333333333e-06,
      "loss": 0.0033,
      "step": 85770
    },
    {
      "epoch": 2.74496,
      "grad_norm": 0.002644097665324807,
      "learning_rate": 1.7004800000000001e-06,
      "loss": 0.0004,
      "step": 85780
    },
    {
      "epoch": 2.74528,
      "grad_norm": 0.006463201250880957,
      "learning_rate": 1.6983466666666668e-06,
      "loss": 0.0002,
      "step": 85790
    },
    {
      "epoch": 2.7456,
      "grad_norm": 0.008691993542015553,
      "learning_rate": 1.6962133333333337e-06,
      "loss": 0.0003,
      "step": 85800
    },
    {
      "epoch": 2.74592,
      "grad_norm": 0.0029709518421441317,
      "learning_rate": 1.69408e-06,
      "loss": 0.0003,
      "step": 85810
    },
    {
      "epoch": 2.7462400000000002,
      "grad_norm": 0.0022013401612639427,
      "learning_rate": 1.6919466666666667e-06,
      "loss": 0.0006,
      "step": 85820
    },
    {
      "epoch": 2.74656,
      "grad_norm": 0.012395202182233334,
      "learning_rate": 1.6898133333333336e-06,
      "loss": 0.0003,
      "step": 85830
    },
    {
      "epoch": 2.74688,
      "grad_norm": 2.6885924339294434,
      "learning_rate": 1.6876800000000002e-06,
      "loss": 0.0569,
      "step": 85840
    },
    {
      "epoch": 2.7472,
      "grad_norm": 0.05858160927891731,
      "learning_rate": 1.6855466666666667e-06,
      "loss": 0.0002,
      "step": 85850
    },
    {
      "epoch": 2.7475199999999997,
      "grad_norm": 0.0024388954043388367,
      "learning_rate": 1.6834133333333335e-06,
      "loss": 0.0002,
      "step": 85860
    },
    {
      "epoch": 2.74784,
      "grad_norm": 0.0030729149002581835,
      "learning_rate": 1.6812800000000002e-06,
      "loss": 0.0003,
      "step": 85870
    },
    {
      "epoch": 2.74816,
      "grad_norm": 0.009586581960320473,
      "learning_rate": 1.6791466666666666e-06,
      "loss": 0.0623,
      "step": 85880
    },
    {
      "epoch": 2.74848,
      "grad_norm": 0.004506336059421301,
      "learning_rate": 1.6770133333333335e-06,
      "loss": 0.0079,
      "step": 85890
    },
    {
      "epoch": 2.7488,
      "grad_norm": 0.004345693625509739,
      "learning_rate": 1.6748800000000001e-06,
      "loss": 0.0002,
      "step": 85900
    },
    {
      "epoch": 2.74912,
      "grad_norm": 0.005260770674794912,
      "learning_rate": 1.6727466666666668e-06,
      "loss": 0.0006,
      "step": 85910
    },
    {
      "epoch": 2.74944,
      "grad_norm": 0.008631509728729725,
      "learning_rate": 1.6706133333333334e-06,
      "loss": 0.0002,
      "step": 85920
    },
    {
      "epoch": 2.74976,
      "grad_norm": 2.0375821590423584,
      "learning_rate": 1.66848e-06,
      "loss": 0.045,
      "step": 85930
    },
    {
      "epoch": 2.75008,
      "grad_norm": 0.0016006602672860026,
      "learning_rate": 1.6663466666666667e-06,
      "loss": 0.031,
      "step": 85940
    },
    {
      "epoch": 2.7504,
      "grad_norm": 0.003164943540468812,
      "learning_rate": 1.6642133333333336e-06,
      "loss": 0.0002,
      "step": 85950
    },
    {
      "epoch": 2.7507200000000003,
      "grad_norm": 0.0030678517650812864,
      "learning_rate": 1.66208e-06,
      "loss": 0.0002,
      "step": 85960
    },
    {
      "epoch": 2.75104,
      "grad_norm": 0.004482448101043701,
      "learning_rate": 1.6599466666666669e-06,
      "loss": 0.0002,
      "step": 85970
    },
    {
      "epoch": 2.75136,
      "grad_norm": 0.007726369891315699,
      "learning_rate": 1.6578133333333335e-06,
      "loss": 0.0003,
      "step": 85980
    },
    {
      "epoch": 2.75168,
      "grad_norm": 0.0026618747506290674,
      "learning_rate": 1.65568e-06,
      "loss": 0.0078,
      "step": 85990
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.01649942807853222,
      "learning_rate": 1.6535466666666668e-06,
      "loss": 0.0002,
      "step": 86000
    },
    {
      "epoch": 2.75232,
      "grad_norm": 0.004092254210263491,
      "learning_rate": 1.6514133333333335e-06,
      "loss": 0.0006,
      "step": 86010
    },
    {
      "epoch": 2.75264,
      "grad_norm": 0.023680422455072403,
      "learning_rate": 1.6492800000000001e-06,
      "loss": 0.0005,
      "step": 86020
    },
    {
      "epoch": 2.75296,
      "grad_norm": 0.0038634445518255234,
      "learning_rate": 1.647146666666667e-06,
      "loss": 0.0083,
      "step": 86030
    },
    {
      "epoch": 2.75328,
      "grad_norm": 0.006555008236318827,
      "learning_rate": 1.6450133333333334e-06,
      "loss": 0.0005,
      "step": 86040
    },
    {
      "epoch": 2.7536,
      "grad_norm": 0.004188246093690395,
      "learning_rate": 1.64288e-06,
      "loss": 0.0002,
      "step": 86050
    },
    {
      "epoch": 2.75392,
      "grad_norm": 0.003524577943608165,
      "learning_rate": 1.640746666666667e-06,
      "loss": 0.0007,
      "step": 86060
    },
    {
      "epoch": 2.7542400000000002,
      "grad_norm": 0.0030522337183356285,
      "learning_rate": 1.6386133333333334e-06,
      "loss": 0.0014,
      "step": 86070
    },
    {
      "epoch": 2.75456,
      "grad_norm": 0.01072173286229372,
      "learning_rate": 1.63648e-06,
      "loss": 0.0003,
      "step": 86080
    },
    {
      "epoch": 2.75488,
      "grad_norm": 0.005455064121633768,
      "learning_rate": 1.6343466666666669e-06,
      "loss": 0.0517,
      "step": 86090
    },
    {
      "epoch": 2.7552,
      "grad_norm": 0.006295532453805208,
      "learning_rate": 1.6322133333333335e-06,
      "loss": 0.0417,
      "step": 86100
    },
    {
      "epoch": 2.7555199999999997,
      "grad_norm": 0.003233496565371752,
      "learning_rate": 1.63008e-06,
      "loss": 0.0004,
      "step": 86110
    },
    {
      "epoch": 2.75584,
      "grad_norm": 0.008184844627976418,
      "learning_rate": 1.6279466666666668e-06,
      "loss": 0.0002,
      "step": 86120
    },
    {
      "epoch": 2.75616,
      "grad_norm": 0.005545313470065594,
      "learning_rate": 1.6258133333333334e-06,
      "loss": 0.0009,
      "step": 86130
    },
    {
      "epoch": 2.75648,
      "grad_norm": 0.00755169615149498,
      "learning_rate": 1.6236800000000003e-06,
      "loss": 0.0003,
      "step": 86140
    },
    {
      "epoch": 2.7568,
      "grad_norm": 0.0020970681216567755,
      "learning_rate": 1.6215466666666667e-06,
      "loss": 0.0008,
      "step": 86150
    },
    {
      "epoch": 2.75712,
      "grad_norm": 0.0024247090332210064,
      "learning_rate": 1.6194133333333334e-06,
      "loss": 0.0002,
      "step": 86160
    },
    {
      "epoch": 2.75744,
      "grad_norm": 0.0017332435818389058,
      "learning_rate": 1.6172800000000003e-06,
      "loss": 0.0003,
      "step": 86170
    },
    {
      "epoch": 2.75776,
      "grad_norm": 0.0070106638595461845,
      "learning_rate": 1.615146666666667e-06,
      "loss": 0.0119,
      "step": 86180
    },
    {
      "epoch": 2.75808,
      "grad_norm": 0.002373620867729187,
      "learning_rate": 1.6130133333333333e-06,
      "loss": 0.0055,
      "step": 86190
    },
    {
      "epoch": 2.7584,
      "grad_norm": 0.06944911926984787,
      "learning_rate": 1.6108800000000002e-06,
      "loss": 0.0019,
      "step": 86200
    },
    {
      "epoch": 2.75872,
      "grad_norm": 0.0030847499147057533,
      "learning_rate": 1.6087466666666668e-06,
      "loss": 0.023,
      "step": 86210
    },
    {
      "epoch": 2.75904,
      "grad_norm": 0.007784779649227858,
      "learning_rate": 1.6066133333333333e-06,
      "loss": 0.0003,
      "step": 86220
    },
    {
      "epoch": 2.75936,
      "grad_norm": 0.007163777481764555,
      "learning_rate": 1.6044800000000001e-06,
      "loss": 0.0003,
      "step": 86230
    },
    {
      "epoch": 2.75968,
      "grad_norm": 0.007461888249963522,
      "learning_rate": 1.6023466666666668e-06,
      "loss": 0.0002,
      "step": 86240
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.0036873517092317343,
      "learning_rate": 1.6002133333333334e-06,
      "loss": 0.0002,
      "step": 86250
    },
    {
      "epoch": 2.76032,
      "grad_norm": 0.008567641489207745,
      "learning_rate": 1.5980800000000003e-06,
      "loss": 0.0003,
      "step": 86260
    },
    {
      "epoch": 2.76064,
      "grad_norm": 0.023912493139505386,
      "learning_rate": 1.5959466666666667e-06,
      "loss": 0.0002,
      "step": 86270
    },
    {
      "epoch": 2.76096,
      "grad_norm": 0.008198590017855167,
      "learning_rate": 1.5938133333333336e-06,
      "loss": 0.0009,
      "step": 86280
    },
    {
      "epoch": 2.76128,
      "grad_norm": 0.002418221440166235,
      "learning_rate": 1.5916800000000002e-06,
      "loss": 0.0029,
      "step": 86290
    },
    {
      "epoch": 2.7616,
      "grad_norm": 0.02157767117023468,
      "learning_rate": 1.5895466666666667e-06,
      "loss": 0.0002,
      "step": 86300
    },
    {
      "epoch": 2.76192,
      "grad_norm": 0.07787379622459412,
      "learning_rate": 1.5874133333333335e-06,
      "loss": 0.0002,
      "step": 86310
    },
    {
      "epoch": 2.7622400000000003,
      "grad_norm": 1.053929090499878,
      "learning_rate": 1.5852800000000002e-06,
      "loss": 0.0014,
      "step": 86320
    },
    {
      "epoch": 2.76256,
      "grad_norm": 0.0033696943428367376,
      "learning_rate": 1.5831466666666668e-06,
      "loss": 0.0003,
      "step": 86330
    },
    {
      "epoch": 2.76288,
      "grad_norm": 0.024665886536240578,
      "learning_rate": 1.5810133333333335e-06,
      "loss": 0.004,
      "step": 86340
    },
    {
      "epoch": 2.7632,
      "grad_norm": 0.09497436881065369,
      "learning_rate": 1.5788800000000001e-06,
      "loss": 0.0005,
      "step": 86350
    },
    {
      "epoch": 2.7635199999999998,
      "grad_norm": 0.00439775176346302,
      "learning_rate": 1.5767466666666668e-06,
      "loss": 0.0003,
      "step": 86360
    },
    {
      "epoch": 2.76384,
      "grad_norm": 0.009297499433159828,
      "learning_rate": 1.5746133333333336e-06,
      "loss": 0.0002,
      "step": 86370
    },
    {
      "epoch": 2.76416,
      "grad_norm": 0.007404690142720938,
      "learning_rate": 1.57248e-06,
      "loss": 0.0003,
      "step": 86380
    },
    {
      "epoch": 2.76448,
      "grad_norm": 0.08033860474824905,
      "learning_rate": 1.5703466666666667e-06,
      "loss": 0.0003,
      "step": 86390
    },
    {
      "epoch": 2.7648,
      "grad_norm": 0.004657274577766657,
      "learning_rate": 1.5682133333333336e-06,
      "loss": 0.0002,
      "step": 86400
    },
    {
      "epoch": 2.76512,
      "grad_norm": 0.0054383715614676476,
      "learning_rate": 1.56608e-06,
      "loss": 0.0002,
      "step": 86410
    },
    {
      "epoch": 2.76544,
      "grad_norm": 0.006550000514835119,
      "learning_rate": 1.5639466666666667e-06,
      "loss": 0.021,
      "step": 86420
    },
    {
      "epoch": 2.76576,
      "grad_norm": 0.0029152503702789545,
      "learning_rate": 1.5618133333333335e-06,
      "loss": 0.0002,
      "step": 86430
    },
    {
      "epoch": 2.76608,
      "grad_norm": 0.01903088018298149,
      "learning_rate": 1.5596800000000002e-06,
      "loss": 0.0003,
      "step": 86440
    },
    {
      "epoch": 2.7664,
      "grad_norm": 0.012893078848719597,
      "learning_rate": 1.557546666666667e-06,
      "loss": 0.0003,
      "step": 86450
    },
    {
      "epoch": 2.76672,
      "grad_norm": 0.008001257665455341,
      "learning_rate": 1.5554133333333335e-06,
      "loss": 0.0002,
      "step": 86460
    },
    {
      "epoch": 2.76704,
      "grad_norm": 0.0023373125586658716,
      "learning_rate": 1.55328e-06,
      "loss": 0.0002,
      "step": 86470
    },
    {
      "epoch": 2.76736,
      "grad_norm": 5.125224590301514,
      "learning_rate": 1.551146666666667e-06,
      "loss": 0.0201,
      "step": 86480
    },
    {
      "epoch": 2.76768,
      "grad_norm": 0.0021615514997392893,
      "learning_rate": 1.5490133333333334e-06,
      "loss": 0.0002,
      "step": 86490
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.004005990456789732,
      "learning_rate": 1.54688e-06,
      "loss": 0.0008,
      "step": 86500
    },
    {
      "epoch": 2.76832,
      "grad_norm": 0.4267173707485199,
      "learning_rate": 1.544746666666667e-06,
      "loss": 0.0011,
      "step": 86510
    },
    {
      "epoch": 2.76864,
      "grad_norm": 0.004519328009337187,
      "learning_rate": 1.5426133333333336e-06,
      "loss": 0.0004,
      "step": 86520
    },
    {
      "epoch": 2.76896,
      "grad_norm": 4.522731781005859,
      "learning_rate": 1.54048e-06,
      "loss": 0.0076,
      "step": 86530
    },
    {
      "epoch": 2.76928,
      "grad_norm": 0.0018859521951526403,
      "learning_rate": 1.5383466666666668e-06,
      "loss": 0.0002,
      "step": 86540
    },
    {
      "epoch": 2.7696,
      "grad_norm": 0.0036453683860599995,
      "learning_rate": 1.5362133333333335e-06,
      "loss": 0.0002,
      "step": 86550
    },
    {
      "epoch": 2.76992,
      "grad_norm": 0.0038877243641763926,
      "learning_rate": 1.53408e-06,
      "loss": 0.0011,
      "step": 86560
    },
    {
      "epoch": 2.7702400000000003,
      "grad_norm": 0.010883541777729988,
      "learning_rate": 1.5319466666666668e-06,
      "loss": 0.0002,
      "step": 86570
    },
    {
      "epoch": 2.77056,
      "grad_norm": 0.002946841297671199,
      "learning_rate": 1.5298133333333334e-06,
      "loss": 0.0004,
      "step": 86580
    },
    {
      "epoch": 2.77088,
      "grad_norm": 0.0028254434000700712,
      "learning_rate": 1.52768e-06,
      "loss": 0.0002,
      "step": 86590
    },
    {
      "epoch": 2.7712,
      "grad_norm": 0.002992277964949608,
      "learning_rate": 1.525546666666667e-06,
      "loss": 0.0106,
      "step": 86600
    },
    {
      "epoch": 2.7715199999999998,
      "grad_norm": 0.011481428518891335,
      "learning_rate": 1.5234133333333334e-06,
      "loss": 0.0003,
      "step": 86610
    },
    {
      "epoch": 2.77184,
      "grad_norm": 0.007257569115608931,
      "learning_rate": 1.5212800000000002e-06,
      "loss": 0.0002,
      "step": 86620
    },
    {
      "epoch": 2.77216,
      "grad_norm": 0.00711474334821105,
      "learning_rate": 1.5191466666666669e-06,
      "loss": 0.0013,
      "step": 86630
    },
    {
      "epoch": 2.77248,
      "grad_norm": 0.0025110573042184114,
      "learning_rate": 1.5170133333333333e-06,
      "loss": 0.0003,
      "step": 86640
    },
    {
      "epoch": 2.7728,
      "grad_norm": 0.005164307542145252,
      "learning_rate": 1.5148800000000002e-06,
      "loss": 0.0002,
      "step": 86650
    },
    {
      "epoch": 2.77312,
      "grad_norm": 0.00642080744728446,
      "learning_rate": 1.5127466666666668e-06,
      "loss": 0.035,
      "step": 86660
    },
    {
      "epoch": 2.77344,
      "grad_norm": 0.004236504435539246,
      "learning_rate": 1.5106133333333335e-06,
      "loss": 0.0003,
      "step": 86670
    },
    {
      "epoch": 2.7737600000000002,
      "grad_norm": 0.0030835752841085196,
      "learning_rate": 1.5084800000000001e-06,
      "loss": 0.0002,
      "step": 86680
    },
    {
      "epoch": 2.77408,
      "grad_norm": 0.014586097560822964,
      "learning_rate": 1.5063466666666668e-06,
      "loss": 0.0002,
      "step": 86690
    },
    {
      "epoch": 2.7744,
      "grad_norm": 0.002047269605100155,
      "learning_rate": 1.5042133333333334e-06,
      "loss": 0.0002,
      "step": 86700
    },
    {
      "epoch": 2.77472,
      "grad_norm": 0.0028756463434547186,
      "learning_rate": 1.5020800000000003e-06,
      "loss": 0.0003,
      "step": 86710
    },
    {
      "epoch": 2.7750399999999997,
      "grad_norm": 0.00495130242779851,
      "learning_rate": 1.4999466666666667e-06,
      "loss": 0.0002,
      "step": 86720
    },
    {
      "epoch": 2.77536,
      "grad_norm": 0.02527722716331482,
      "learning_rate": 1.4978133333333334e-06,
      "loss": 0.0002,
      "step": 86730
    },
    {
      "epoch": 2.77568,
      "grad_norm": 0.004447986371815205,
      "learning_rate": 1.4956800000000002e-06,
      "loss": 0.001,
      "step": 86740
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.003187520895153284,
      "learning_rate": 1.4935466666666667e-06,
      "loss": 0.0001,
      "step": 86750
    },
    {
      "epoch": 2.77632,
      "grad_norm": 0.002128920517861843,
      "learning_rate": 1.4914133333333333e-06,
      "loss": 0.0003,
      "step": 86760
    },
    {
      "epoch": 2.77664,
      "grad_norm": 0.030538825318217278,
      "learning_rate": 1.4892800000000002e-06,
      "loss": 0.0005,
      "step": 86770
    },
    {
      "epoch": 2.77696,
      "grad_norm": 0.029514040797948837,
      "learning_rate": 1.4871466666666668e-06,
      "loss": 0.0002,
      "step": 86780
    },
    {
      "epoch": 2.77728,
      "grad_norm": 0.007301272824406624,
      "learning_rate": 1.4850133333333337e-06,
      "loss": 0.0002,
      "step": 86790
    },
    {
      "epoch": 2.7776,
      "grad_norm": 0.0013893871800974011,
      "learning_rate": 1.4828800000000001e-06,
      "loss": 0.0002,
      "step": 86800
    },
    {
      "epoch": 2.77792,
      "grad_norm": 0.002819204004481435,
      "learning_rate": 1.4807466666666668e-06,
      "loss": 0.0002,
      "step": 86810
    },
    {
      "epoch": 2.7782400000000003,
      "grad_norm": 0.012759240344166756,
      "learning_rate": 1.4786133333333336e-06,
      "loss": 0.0007,
      "step": 86820
    },
    {
      "epoch": 2.77856,
      "grad_norm": 0.01122821681201458,
      "learning_rate": 1.47648e-06,
      "loss": 0.0003,
      "step": 86830
    },
    {
      "epoch": 2.77888,
      "grad_norm": 0.004973160568624735,
      "learning_rate": 1.4743466666666667e-06,
      "loss": 0.1076,
      "step": 86840
    },
    {
      "epoch": 2.7792,
      "grad_norm": 0.002696381416171789,
      "learning_rate": 1.4722133333333336e-06,
      "loss": 0.0002,
      "step": 86850
    },
    {
      "epoch": 2.7795199999999998,
      "grad_norm": 0.028896529227495193,
      "learning_rate": 1.4700800000000002e-06,
      "loss": 0.0002,
      "step": 86860
    },
    {
      "epoch": 2.77984,
      "grad_norm": 0.0037821712903678417,
      "learning_rate": 1.4679466666666666e-06,
      "loss": 0.0002,
      "step": 86870
    },
    {
      "epoch": 2.78016,
      "grad_norm": 0.006846389267593622,
      "learning_rate": 1.4658133333333335e-06,
      "loss": 0.0005,
      "step": 86880
    },
    {
      "epoch": 2.78048,
      "grad_norm": 0.008363832719624043,
      "learning_rate": 1.4636800000000001e-06,
      "loss": 0.0584,
      "step": 86890
    },
    {
      "epoch": 2.7808,
      "grad_norm": 0.005399890709668398,
      "learning_rate": 1.4615466666666666e-06,
      "loss": 0.0003,
      "step": 86900
    },
    {
      "epoch": 2.78112,
      "grad_norm": 0.01400976162403822,
      "learning_rate": 1.4594133333333334e-06,
      "loss": 0.0003,
      "step": 86910
    },
    {
      "epoch": 2.78144,
      "grad_norm": 0.0038508677389472723,
      "learning_rate": 1.45728e-06,
      "loss": 0.0002,
      "step": 86920
    },
    {
      "epoch": 2.7817600000000002,
      "grad_norm": 0.003369557671248913,
      "learning_rate": 1.4551466666666667e-06,
      "loss": 0.0002,
      "step": 86930
    },
    {
      "epoch": 2.78208,
      "grad_norm": 2.63529634475708,
      "learning_rate": 1.4530133333333336e-06,
      "loss": 0.0304,
      "step": 86940
    },
    {
      "epoch": 2.7824,
      "grad_norm": 0.07580496370792389,
      "learning_rate": 1.45088e-06,
      "loss": 0.0339,
      "step": 86950
    },
    {
      "epoch": 2.78272,
      "grad_norm": 0.010890701785683632,
      "learning_rate": 1.4487466666666669e-06,
      "loss": 0.0452,
      "step": 86960
    },
    {
      "epoch": 2.7830399999999997,
      "grad_norm": 0.008712414652109146,
      "learning_rate": 1.4466133333333335e-06,
      "loss": 0.0002,
      "step": 86970
    },
    {
      "epoch": 2.78336,
      "grad_norm": 0.0017026724526658654,
      "learning_rate": 1.44448e-06,
      "loss": 0.0001,
      "step": 86980
    },
    {
      "epoch": 2.78368,
      "grad_norm": 0.006899355910718441,
      "learning_rate": 1.4423466666666668e-06,
      "loss": 0.0002,
      "step": 86990
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.005746860988438129,
      "learning_rate": 1.4402133333333335e-06,
      "loss": 0.0002,
      "step": 87000
    },
    {
      "epoch": 2.78432,
      "grad_norm": 0.01561630330979824,
      "learning_rate": 1.4380800000000001e-06,
      "loss": 0.0003,
      "step": 87010
    },
    {
      "epoch": 2.78464,
      "grad_norm": 6.257238864898682,
      "learning_rate": 1.4359466666666668e-06,
      "loss": 0.0284,
      "step": 87020
    },
    {
      "epoch": 2.78496,
      "grad_norm": 0.0035437429323792458,
      "learning_rate": 1.4338133333333334e-06,
      "loss": 0.0003,
      "step": 87030
    },
    {
      "epoch": 2.78528,
      "grad_norm": 0.002132988767698407,
      "learning_rate": 1.43168e-06,
      "loss": 0.0002,
      "step": 87040
    },
    {
      "epoch": 2.7856,
      "grad_norm": 0.0031127927359193563,
      "learning_rate": 1.429546666666667e-06,
      "loss": 0.001,
      "step": 87050
    },
    {
      "epoch": 2.78592,
      "grad_norm": 0.01370586920529604,
      "learning_rate": 1.4274133333333334e-06,
      "loss": 0.0003,
      "step": 87060
    },
    {
      "epoch": 2.7862400000000003,
      "grad_norm": 0.009064864367246628,
      "learning_rate": 1.42528e-06,
      "loss": 0.0002,
      "step": 87070
    },
    {
      "epoch": 2.78656,
      "grad_norm": 0.004743452183902264,
      "learning_rate": 1.4231466666666669e-06,
      "loss": 0.035,
      "step": 87080
    },
    {
      "epoch": 2.78688,
      "grad_norm": 0.013224759139120579,
      "learning_rate": 1.4210133333333333e-06,
      "loss": 0.0005,
      "step": 87090
    },
    {
      "epoch": 2.7872,
      "grad_norm": 0.025009743869304657,
      "learning_rate": 1.41888e-06,
      "loss": 0.0003,
      "step": 87100
    },
    {
      "epoch": 2.7875199999999998,
      "grad_norm": 0.00373315392062068,
      "learning_rate": 1.4167466666666668e-06,
      "loss": 0.0002,
      "step": 87110
    },
    {
      "epoch": 2.78784,
      "grad_norm": 0.0029051629826426506,
      "learning_rate": 1.4146133333333335e-06,
      "loss": 0.0006,
      "step": 87120
    },
    {
      "epoch": 2.78816,
      "grad_norm": 0.006246286444365978,
      "learning_rate": 1.4124800000000003e-06,
      "loss": 0.0096,
      "step": 87130
    },
    {
      "epoch": 2.78848,
      "grad_norm": 0.005090734455734491,
      "learning_rate": 1.4103466666666668e-06,
      "loss": 0.0003,
      "step": 87140
    },
    {
      "epoch": 2.7888,
      "grad_norm": 0.004204271361231804,
      "learning_rate": 1.4082133333333334e-06,
      "loss": 0.0003,
      "step": 87150
    },
    {
      "epoch": 2.78912,
      "grad_norm": 0.0038786972872912884,
      "learning_rate": 1.4060800000000003e-06,
      "loss": 0.0002,
      "step": 87160
    },
    {
      "epoch": 2.78944,
      "grad_norm": 0.0053993999026715755,
      "learning_rate": 1.4039466666666667e-06,
      "loss": 0.0002,
      "step": 87170
    },
    {
      "epoch": 2.7897600000000002,
      "grad_norm": 0.006762252654880285,
      "learning_rate": 1.4018133333333333e-06,
      "loss": 0.0142,
      "step": 87180
    },
    {
      "epoch": 2.79008,
      "grad_norm": 0.002967090578749776,
      "learning_rate": 1.3996800000000002e-06,
      "loss": 0.0002,
      "step": 87190
    },
    {
      "epoch": 2.7904,
      "grad_norm": 0.0032278788276016712,
      "learning_rate": 1.3975466666666669e-06,
      "loss": 0.0022,
      "step": 87200
    },
    {
      "epoch": 2.79072,
      "grad_norm": 0.002185299526900053,
      "learning_rate": 1.3954133333333333e-06,
      "loss": 0.0003,
      "step": 87210
    },
    {
      "epoch": 2.7910399999999997,
      "grad_norm": 0.004369624890387058,
      "learning_rate": 1.3932800000000002e-06,
      "loss": 0.0002,
      "step": 87220
    },
    {
      "epoch": 2.79136,
      "grad_norm": 0.004131454508751631,
      "learning_rate": 1.3911466666666668e-06,
      "loss": 0.0459,
      "step": 87230
    },
    {
      "epoch": 2.79168,
      "grad_norm": 0.006384221371263266,
      "learning_rate": 1.3890133333333332e-06,
      "loss": 0.0003,
      "step": 87240
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.005388393998146057,
      "learning_rate": 1.38688e-06,
      "loss": 0.0003,
      "step": 87250
    },
    {
      "epoch": 2.79232,
      "grad_norm": 0.0015621887287124991,
      "learning_rate": 1.3847466666666667e-06,
      "loss": 0.0002,
      "step": 87260
    },
    {
      "epoch": 2.79264,
      "grad_norm": 0.004195358604192734,
      "learning_rate": 1.3826133333333336e-06,
      "loss": 0.0002,
      "step": 87270
    },
    {
      "epoch": 2.79296,
      "grad_norm": 0.006563624367117882,
      "learning_rate": 1.3804800000000002e-06,
      "loss": 0.0003,
      "step": 87280
    },
    {
      "epoch": 2.79328,
      "grad_norm": 0.0015814199578016996,
      "learning_rate": 1.3783466666666667e-06,
      "loss": 0.0015,
      "step": 87290
    },
    {
      "epoch": 2.7936,
      "grad_norm": 0.005120693705976009,
      "learning_rate": 1.3762133333333335e-06,
      "loss": 0.0084,
      "step": 87300
    },
    {
      "epoch": 2.79392,
      "grad_norm": 0.004491898231208324,
      "learning_rate": 1.3740800000000002e-06,
      "loss": 0.0002,
      "step": 87310
    },
    {
      "epoch": 2.79424,
      "grad_norm": 0.004577093757688999,
      "learning_rate": 1.3719466666666666e-06,
      "loss": 0.0003,
      "step": 87320
    },
    {
      "epoch": 2.79456,
      "grad_norm": 0.002380686579272151,
      "learning_rate": 1.3698133333333335e-06,
      "loss": 0.0004,
      "step": 87330
    },
    {
      "epoch": 2.79488,
      "grad_norm": 0.0033175041899085045,
      "learning_rate": 1.3676800000000001e-06,
      "loss": 0.0012,
      "step": 87340
    },
    {
      "epoch": 2.7952,
      "grad_norm": 0.00811313558369875,
      "learning_rate": 1.3655466666666668e-06,
      "loss": 0.0003,
      "step": 87350
    },
    {
      "epoch": 2.79552,
      "grad_norm": 0.007427616510540247,
      "learning_rate": 1.3634133333333334e-06,
      "loss": 0.0002,
      "step": 87360
    },
    {
      "epoch": 2.79584,
      "grad_norm": 0.010492634028196335,
      "learning_rate": 1.36128e-06,
      "loss": 0.0002,
      "step": 87370
    },
    {
      "epoch": 2.79616,
      "grad_norm": 0.004897550214082003,
      "learning_rate": 1.3591466666666667e-06,
      "loss": 0.0002,
      "step": 87380
    },
    {
      "epoch": 2.79648,
      "grad_norm": 0.004746922757476568,
      "learning_rate": 1.3570133333333336e-06,
      "loss": 0.0003,
      "step": 87390
    },
    {
      "epoch": 2.7968,
      "grad_norm": 0.011291243135929108,
      "learning_rate": 1.35488e-06,
      "loss": 0.0005,
      "step": 87400
    },
    {
      "epoch": 2.79712,
      "grad_norm": 0.005525519605726004,
      "learning_rate": 1.3527466666666667e-06,
      "loss": 0.0008,
      "step": 87410
    },
    {
      "epoch": 2.79744,
      "grad_norm": 0.20243798196315765,
      "learning_rate": 1.3506133333333335e-06,
      "loss": 0.0008,
      "step": 87420
    },
    {
      "epoch": 2.7977600000000002,
      "grad_norm": 0.017266228795051575,
      "learning_rate": 1.34848e-06,
      "loss": 0.0007,
      "step": 87430
    },
    {
      "epoch": 2.79808,
      "grad_norm": 0.47268345952033997,
      "learning_rate": 1.3463466666666668e-06,
      "loss": 0.0011,
      "step": 87440
    },
    {
      "epoch": 2.7984,
      "grad_norm": 0.0083381999284029,
      "learning_rate": 1.3442133333333335e-06,
      "loss": 0.0002,
      "step": 87450
    },
    {
      "epoch": 2.79872,
      "grad_norm": 0.0027766001876443624,
      "learning_rate": 1.3420800000000001e-06,
      "loss": 0.0002,
      "step": 87460
    },
    {
      "epoch": 2.7990399999999998,
      "grad_norm": 0.002166108228266239,
      "learning_rate": 1.339946666666667e-06,
      "loss": 0.0002,
      "step": 87470
    },
    {
      "epoch": 2.79936,
      "grad_norm": 0.001491646165959537,
      "learning_rate": 1.3378133333333334e-06,
      "loss": 0.0006,
      "step": 87480
    },
    {
      "epoch": 2.79968,
      "grad_norm": 0.011995292268693447,
      "learning_rate": 1.33568e-06,
      "loss": 0.0002,
      "step": 87490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.0026998037938028574,
      "learning_rate": 1.333546666666667e-06,
      "loss": 0.0002,
      "step": 87500
    },
    {
      "epoch": 2.80032,
      "grad_norm": 0.011465609073638916,
      "learning_rate": 1.3314133333333334e-06,
      "loss": 0.0004,
      "step": 87510
    },
    {
      "epoch": 2.80064,
      "grad_norm": 0.011887090280652046,
      "learning_rate": 1.32928e-06,
      "loss": 0.0003,
      "step": 87520
    },
    {
      "epoch": 2.80096,
      "grad_norm": 0.0037617573980242014,
      "learning_rate": 1.3271466666666669e-06,
      "loss": 0.0001,
      "step": 87530
    },
    {
      "epoch": 2.80128,
      "grad_norm": 0.29568642377853394,
      "learning_rate": 1.3250133333333335e-06,
      "loss": 0.0004,
      "step": 87540
    },
    {
      "epoch": 2.8016,
      "grad_norm": 0.006507561542093754,
      "learning_rate": 1.32288e-06,
      "loss": 0.0002,
      "step": 87550
    },
    {
      "epoch": 2.80192,
      "grad_norm": 0.003961893729865551,
      "learning_rate": 1.3207466666666668e-06,
      "loss": 0.0002,
      "step": 87560
    },
    {
      "epoch": 2.80224,
      "grad_norm": 0.011365792714059353,
      "learning_rate": 1.3186133333333335e-06,
      "loss": 0.0002,
      "step": 87570
    },
    {
      "epoch": 2.80256,
      "grad_norm": 0.008538242429494858,
      "learning_rate": 1.3164799999999999e-06,
      "loss": 0.0002,
      "step": 87580
    },
    {
      "epoch": 2.80288,
      "grad_norm": 0.002406042069196701,
      "learning_rate": 1.3143466666666667e-06,
      "loss": 0.0001,
      "step": 87590
    },
    {
      "epoch": 2.8032,
      "grad_norm": 0.004770810250192881,
      "learning_rate": 1.3122133333333334e-06,
      "loss": 0.0001,
      "step": 87600
    },
    {
      "epoch": 2.80352,
      "grad_norm": 0.0036590967793017626,
      "learning_rate": 1.3100800000000003e-06,
      "loss": 0.0002,
      "step": 87610
    },
    {
      "epoch": 2.80384,
      "grad_norm": 0.0014505942817777395,
      "learning_rate": 1.307946666666667e-06,
      "loss": 0.0004,
      "step": 87620
    },
    {
      "epoch": 2.80416,
      "grad_norm": 0.006308841519057751,
      "learning_rate": 1.3058133333333333e-06,
      "loss": 0.0002,
      "step": 87630
    },
    {
      "epoch": 2.80448,
      "grad_norm": 0.004206283017992973,
      "learning_rate": 1.3036800000000002e-06,
      "loss": 0.0003,
      "step": 87640
    },
    {
      "epoch": 2.8048,
      "grad_norm": 0.006264257710427046,
      "learning_rate": 1.3015466666666668e-06,
      "loss": 0.0004,
      "step": 87650
    },
    {
      "epoch": 2.80512,
      "grad_norm": 0.009140602312982082,
      "learning_rate": 1.2994133333333333e-06,
      "loss": 0.0004,
      "step": 87660
    },
    {
      "epoch": 2.80544,
      "grad_norm": 0.0024523611646145582,
      "learning_rate": 1.2972800000000001e-06,
      "loss": 0.0002,
      "step": 87670
    },
    {
      "epoch": 2.8057600000000003,
      "grad_norm": 0.005025048740208149,
      "learning_rate": 1.2951466666666668e-06,
      "loss": 0.0005,
      "step": 87680
    },
    {
      "epoch": 2.80608,
      "grad_norm": 0.1850001960992813,
      "learning_rate": 1.2930133333333334e-06,
      "loss": 0.0005,
      "step": 87690
    },
    {
      "epoch": 2.8064,
      "grad_norm": 0.003862254088744521,
      "learning_rate": 1.2908800000000003e-06,
      "loss": 0.0002,
      "step": 87700
    },
    {
      "epoch": 2.80672,
      "grad_norm": 0.012770197354257107,
      "learning_rate": 1.2887466666666667e-06,
      "loss": 0.0002,
      "step": 87710
    },
    {
      "epoch": 2.8070399999999998,
      "grad_norm": 0.006773815024644136,
      "learning_rate": 1.2866133333333334e-06,
      "loss": 0.0002,
      "step": 87720
    },
    {
      "epoch": 2.80736,
      "grad_norm": 0.003551838221028447,
      "learning_rate": 1.2844800000000002e-06,
      "loss": 0.0004,
      "step": 87730
    },
    {
      "epoch": 2.80768,
      "grad_norm": 0.02224002592265606,
      "learning_rate": 1.2823466666666667e-06,
      "loss": 0.0403,
      "step": 87740
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.004250042140483856,
      "learning_rate": 1.2802133333333333e-06,
      "loss": 0.0001,
      "step": 87750
    },
    {
      "epoch": 2.80832,
      "grad_norm": 0.006555294152349234,
      "learning_rate": 1.2780800000000002e-06,
      "loss": 0.0002,
      "step": 87760
    },
    {
      "epoch": 2.80864,
      "grad_norm": 0.01847817935049534,
      "learning_rate": 1.2759466666666668e-06,
      "loss": 0.0004,
      "step": 87770
    },
    {
      "epoch": 2.80896,
      "grad_norm": 0.0031338902190327644,
      "learning_rate": 1.2738133333333335e-06,
      "loss": 0.093,
      "step": 87780
    },
    {
      "epoch": 2.80928,
      "grad_norm": 0.0036851719487458467,
      "learning_rate": 1.2716800000000001e-06,
      "loss": 0.0367,
      "step": 87790
    },
    {
      "epoch": 2.8096,
      "grad_norm": 0.00403476320207119,
      "learning_rate": 1.2695466666666668e-06,
      "loss": 0.0086,
      "step": 87800
    },
    {
      "epoch": 2.80992,
      "grad_norm": 0.004507370758801699,
      "learning_rate": 1.2674133333333336e-06,
      "loss": 0.0014,
      "step": 87810
    },
    {
      "epoch": 2.81024,
      "grad_norm": 0.006869233213365078,
      "learning_rate": 1.26528e-06,
      "loss": 0.0002,
      "step": 87820
    },
    {
      "epoch": 2.8105599999999997,
      "grad_norm": 0.0031123796943575144,
      "learning_rate": 1.2631466666666667e-06,
      "loss": 0.0002,
      "step": 87830
    },
    {
      "epoch": 2.81088,
      "grad_norm": 0.0029320677276700735,
      "learning_rate": 1.2610133333333336e-06,
      "loss": 0.0001,
      "step": 87840
    },
    {
      "epoch": 2.8112,
      "grad_norm": 0.014336937107145786,
      "learning_rate": 1.25888e-06,
      "loss": 0.0005,
      "step": 87850
    },
    {
      "epoch": 2.81152,
      "grad_norm": 0.006367529276758432,
      "learning_rate": 1.2567466666666667e-06,
      "loss": 0.0002,
      "step": 87860
    },
    {
      "epoch": 2.81184,
      "grad_norm": 0.002480987459421158,
      "learning_rate": 1.2546133333333335e-06,
      "loss": 0.0135,
      "step": 87870
    },
    {
      "epoch": 2.81216,
      "grad_norm": 0.06865039467811584,
      "learning_rate": 1.2524800000000002e-06,
      "loss": 0.0414,
      "step": 87880
    },
    {
      "epoch": 2.81248,
      "grad_norm": 0.001744336448609829,
      "learning_rate": 1.2503466666666666e-06,
      "loss": 0.0003,
      "step": 87890
    },
    {
      "epoch": 2.8128,
      "grad_norm": 0.0028788598719984293,
      "learning_rate": 1.2482133333333335e-06,
      "loss": 0.0003,
      "step": 87900
    },
    {
      "epoch": 2.81312,
      "grad_norm": 0.005034178029745817,
      "learning_rate": 1.24608e-06,
      "loss": 0.0001,
      "step": 87910
    },
    {
      "epoch": 2.81344,
      "grad_norm": 0.0016716186655685306,
      "learning_rate": 1.2439466666666668e-06,
      "loss": 0.0002,
      "step": 87920
    },
    {
      "epoch": 2.8137600000000003,
      "grad_norm": 1.5067435503005981,
      "learning_rate": 1.2418133333333334e-06,
      "loss": 0.0012,
      "step": 87930
    },
    {
      "epoch": 2.81408,
      "grad_norm": 0.002791936509311199,
      "learning_rate": 1.23968e-06,
      "loss": 0.0003,
      "step": 87940
    },
    {
      "epoch": 2.8144,
      "grad_norm": 0.006161969155073166,
      "learning_rate": 1.2375466666666667e-06,
      "loss": 0.0002,
      "step": 87950
    },
    {
      "epoch": 2.81472,
      "grad_norm": 0.0034776958636939526,
      "learning_rate": 1.2354133333333336e-06,
      "loss": 0.0002,
      "step": 87960
    },
    {
      "epoch": 2.8150399999999998,
      "grad_norm": 0.004087385721504688,
      "learning_rate": 1.23328e-06,
      "loss": 0.0002,
      "step": 87970
    },
    {
      "epoch": 2.81536,
      "grad_norm": 0.0027593919076025486,
      "learning_rate": 1.2311466666666666e-06,
      "loss": 0.0007,
      "step": 87980
    },
    {
      "epoch": 2.81568,
      "grad_norm": 0.005391673184931278,
      "learning_rate": 1.2290133333333335e-06,
      "loss": 0.0003,
      "step": 87990
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.004661103710532188,
      "learning_rate": 1.2268800000000001e-06,
      "loss": 0.0569,
      "step": 88000
    },
    {
      "epoch": 2.81632,
      "grad_norm": 0.009228795766830444,
      "learning_rate": 1.2247466666666668e-06,
      "loss": 0.0508,
      "step": 88010
    },
    {
      "epoch": 2.81664,
      "grad_norm": 0.04149489104747772,
      "learning_rate": 1.2226133333333334e-06,
      "loss": 0.0002,
      "step": 88020
    },
    {
      "epoch": 2.81696,
      "grad_norm": 0.0057152919471263885,
      "learning_rate": 1.22048e-06,
      "loss": 0.0002,
      "step": 88030
    },
    {
      "epoch": 2.8172800000000002,
      "grad_norm": 0.2918938398361206,
      "learning_rate": 1.2183466666666667e-06,
      "loss": 0.0051,
      "step": 88040
    },
    {
      "epoch": 2.8176,
      "grad_norm": 0.009143844246864319,
      "learning_rate": 1.2162133333333334e-06,
      "loss": 0.0002,
      "step": 88050
    },
    {
      "epoch": 2.81792,
      "grad_norm": 0.0025956504978239536,
      "learning_rate": 1.21408e-06,
      "loss": 0.0002,
      "step": 88060
    },
    {
      "epoch": 2.81824,
      "grad_norm": 0.0025343243032693863,
      "learning_rate": 1.2119466666666669e-06,
      "loss": 0.013,
      "step": 88070
    },
    {
      "epoch": 2.8185599999999997,
      "grad_norm": 0.003474954515695572,
      "learning_rate": 1.2098133333333333e-06,
      "loss": 0.0009,
      "step": 88080
    },
    {
      "epoch": 2.81888,
      "grad_norm": 0.003121625864878297,
      "learning_rate": 1.2076800000000002e-06,
      "loss": 0.0007,
      "step": 88090
    },
    {
      "epoch": 2.8192,
      "grad_norm": 0.04838312789797783,
      "learning_rate": 1.2055466666666668e-06,
      "loss": 0.0002,
      "step": 88100
    },
    {
      "epoch": 2.81952,
      "grad_norm": 0.0031193604227155447,
      "learning_rate": 1.2034133333333335e-06,
      "loss": 0.0004,
      "step": 88110
    },
    {
      "epoch": 2.81984,
      "grad_norm": 0.020368831232190132,
      "learning_rate": 1.2012800000000001e-06,
      "loss": 0.0002,
      "step": 88120
    },
    {
      "epoch": 2.82016,
      "grad_norm": 0.0033713586162775755,
      "learning_rate": 1.1991466666666668e-06,
      "loss": 0.049,
      "step": 88130
    },
    {
      "epoch": 2.82048,
      "grad_norm": 0.029052840545773506,
      "learning_rate": 1.1970133333333334e-06,
      "loss": 0.001,
      "step": 88140
    },
    {
      "epoch": 2.8208,
      "grad_norm": 0.00223986292257905,
      "learning_rate": 1.19488e-06,
      "loss": 0.0002,
      "step": 88150
    },
    {
      "epoch": 2.82112,
      "grad_norm": 0.012413399294018745,
      "learning_rate": 1.1927466666666667e-06,
      "loss": 0.0002,
      "step": 88160
    },
    {
      "epoch": 2.82144,
      "grad_norm": 0.002799512119963765,
      "learning_rate": 1.1906133333333336e-06,
      "loss": 0.0006,
      "step": 88170
    },
    {
      "epoch": 2.8217600000000003,
      "grad_norm": 0.006100153550505638,
      "learning_rate": 1.18848e-06,
      "loss": 0.0002,
      "step": 88180
    },
    {
      "epoch": 2.82208,
      "grad_norm": 0.01126659195870161,
      "learning_rate": 1.1863466666666667e-06,
      "loss": 0.0093,
      "step": 88190
    },
    {
      "epoch": 2.8224,
      "grad_norm": 0.010524884797632694,
      "learning_rate": 1.1842133333333335e-06,
      "loss": 0.0002,
      "step": 88200
    },
    {
      "epoch": 2.82272,
      "grad_norm": 0.007924583740532398,
      "learning_rate": 1.18208e-06,
      "loss": 0.0004,
      "step": 88210
    },
    {
      "epoch": 2.8230399999999998,
      "grad_norm": 0.030865607783198357,
      "learning_rate": 1.1799466666666668e-06,
      "loss": 0.0002,
      "step": 88220
    },
    {
      "epoch": 2.82336,
      "grad_norm": 0.004988500382751226,
      "learning_rate": 1.1778133333333335e-06,
      "loss": 0.0002,
      "step": 88230
    },
    {
      "epoch": 2.82368,
      "grad_norm": 0.002284772926941514,
      "learning_rate": 1.1756800000000001e-06,
      "loss": 0.0002,
      "step": 88240
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.0022923541255295277,
      "learning_rate": 1.1735466666666668e-06,
      "loss": 0.0002,
      "step": 88250
    },
    {
      "epoch": 2.82432,
      "grad_norm": 0.003338817972689867,
      "learning_rate": 1.1714133333333334e-06,
      "loss": 0.0002,
      "step": 88260
    },
    {
      "epoch": 2.82464,
      "grad_norm": 0.002987269312143326,
      "learning_rate": 1.16928e-06,
      "loss": 0.0002,
      "step": 88270
    },
    {
      "epoch": 2.82496,
      "grad_norm": 0.1515817791223526,
      "learning_rate": 1.1671466666666667e-06,
      "loss": 0.0006,
      "step": 88280
    },
    {
      "epoch": 2.8252800000000002,
      "grad_norm": 0.00863640196621418,
      "learning_rate": 1.1650133333333333e-06,
      "loss": 0.0003,
      "step": 88290
    },
    {
      "epoch": 2.8256,
      "grad_norm": 0.006312877871096134,
      "learning_rate": 1.1628800000000002e-06,
      "loss": 0.0003,
      "step": 88300
    },
    {
      "epoch": 2.82592,
      "grad_norm": 0.010888594202697277,
      "learning_rate": 1.1607466666666669e-06,
      "loss": 0.0006,
      "step": 88310
    },
    {
      "epoch": 2.82624,
      "grad_norm": 0.010906843468546867,
      "learning_rate": 1.1586133333333333e-06,
      "loss": 0.0006,
      "step": 88320
    },
    {
      "epoch": 2.8265599999999997,
      "grad_norm": 0.001967481803148985,
      "learning_rate": 1.1564800000000001e-06,
      "loss": 0.0002,
      "step": 88330
    },
    {
      "epoch": 2.82688,
      "grad_norm": 0.002232734579592943,
      "learning_rate": 1.1543466666666668e-06,
      "loss": 0.0001,
      "step": 88340
    },
    {
      "epoch": 2.8272,
      "grad_norm": 0.00513116829097271,
      "learning_rate": 1.1522133333333334e-06,
      "loss": 0.0002,
      "step": 88350
    },
    {
      "epoch": 2.82752,
      "grad_norm": 0.002634469186887145,
      "learning_rate": 1.15008e-06,
      "loss": 0.0003,
      "step": 88360
    },
    {
      "epoch": 2.82784,
      "grad_norm": 0.005715509410947561,
      "learning_rate": 1.1479466666666667e-06,
      "loss": 0.0002,
      "step": 88370
    },
    {
      "epoch": 2.82816,
      "grad_norm": 0.015844454988837242,
      "learning_rate": 1.1458133333333334e-06,
      "loss": 0.0002,
      "step": 88380
    },
    {
      "epoch": 2.82848,
      "grad_norm": 0.0013645605649799109,
      "learning_rate": 1.14368e-06,
      "loss": 0.0002,
      "step": 88390
    },
    {
      "epoch": 2.8288,
      "grad_norm": 0.002351079136133194,
      "learning_rate": 1.1415466666666667e-06,
      "loss": 0.0002,
      "step": 88400
    },
    {
      "epoch": 2.82912,
      "grad_norm": 0.003624215256422758,
      "learning_rate": 1.1394133333333335e-06,
      "loss": 0.0001,
      "step": 88410
    },
    {
      "epoch": 2.82944,
      "grad_norm": 0.0017050675814971328,
      "learning_rate": 1.13728e-06,
      "loss": 0.0001,
      "step": 88420
    },
    {
      "epoch": 2.8297600000000003,
      "grad_norm": 0.002841752953827381,
      "learning_rate": 1.1351466666666668e-06,
      "loss": 0.0001,
      "step": 88430
    },
    {
      "epoch": 2.83008,
      "grad_norm": 0.0034181519877165556,
      "learning_rate": 1.1330133333333335e-06,
      "loss": 0.0001,
      "step": 88440
    },
    {
      "epoch": 2.8304,
      "grad_norm": 0.017454322427511215,
      "learning_rate": 1.1308800000000001e-06,
      "loss": 0.0291,
      "step": 88450
    },
    {
      "epoch": 2.83072,
      "grad_norm": 0.0024612031411379576,
      "learning_rate": 1.1287466666666668e-06,
      "loss": 0.0002,
      "step": 88460
    },
    {
      "epoch": 2.83104,
      "grad_norm": 0.0071419160813093185,
      "learning_rate": 1.1266133333333334e-06,
      "loss": 0.0001,
      "step": 88470
    },
    {
      "epoch": 2.83136,
      "grad_norm": 0.003625681856647134,
      "learning_rate": 1.12448e-06,
      "loss": 0.0003,
      "step": 88480
    },
    {
      "epoch": 2.83168,
      "grad_norm": 0.08307517319917679,
      "learning_rate": 1.1223466666666667e-06,
      "loss": 0.0004,
      "step": 88490
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.007744308095425367,
      "learning_rate": 1.1202133333333334e-06,
      "loss": 0.0153,
      "step": 88500
    },
    {
      "epoch": 2.83232,
      "grad_norm": 0.005814566742628813,
      "learning_rate": 1.1180800000000002e-06,
      "loss": 0.0003,
      "step": 88510
    },
    {
      "epoch": 2.83264,
      "grad_norm": 0.0941770151257515,
      "learning_rate": 1.1159466666666667e-06,
      "loss": 0.0003,
      "step": 88520
    },
    {
      "epoch": 2.83296,
      "grad_norm": 0.0038329437375068665,
      "learning_rate": 1.1138133333333333e-06,
      "loss": 0.0196,
      "step": 88530
    },
    {
      "epoch": 2.8332800000000002,
      "grad_norm": 0.004474538844078779,
      "learning_rate": 1.1116800000000002e-06,
      "loss": 0.0004,
      "step": 88540
    },
    {
      "epoch": 2.8336,
      "grad_norm": 0.012796175666153431,
      "learning_rate": 1.1095466666666666e-06,
      "loss": 0.0031,
      "step": 88550
    },
    {
      "epoch": 2.83392,
      "grad_norm": 0.0022907471284270287,
      "learning_rate": 1.1074133333333335e-06,
      "loss": 0.0001,
      "step": 88560
    },
    {
      "epoch": 2.83424,
      "grad_norm": 0.01457702461630106,
      "learning_rate": 1.1052800000000001e-06,
      "loss": 0.0002,
      "step": 88570
    },
    {
      "epoch": 2.8345599999999997,
      "grad_norm": 0.002762929769232869,
      "learning_rate": 1.1031466666666668e-06,
      "loss": 0.0002,
      "step": 88580
    },
    {
      "epoch": 2.83488,
      "grad_norm": 0.003989881370216608,
      "learning_rate": 1.1010133333333334e-06,
      "loss": 0.0455,
      "step": 88590
    },
    {
      "epoch": 2.8352,
      "grad_norm": 0.0033907375764101744,
      "learning_rate": 1.09888e-06,
      "loss": 0.0002,
      "step": 88600
    },
    {
      "epoch": 2.83552,
      "grad_norm": 0.0018925993936136365,
      "learning_rate": 1.0967466666666667e-06,
      "loss": 0.0004,
      "step": 88610
    },
    {
      "epoch": 2.83584,
      "grad_norm": 0.006053718272596598,
      "learning_rate": 1.0946133333333334e-06,
      "loss": 0.0003,
      "step": 88620
    },
    {
      "epoch": 2.83616,
      "grad_norm": 0.01645149663090706,
      "learning_rate": 1.09248e-06,
      "loss": 0.0002,
      "step": 88630
    },
    {
      "epoch": 2.83648,
      "grad_norm": 0.004151626024395227,
      "learning_rate": 1.0903466666666669e-06,
      "loss": 0.0003,
      "step": 88640
    },
    {
      "epoch": 2.8368,
      "grad_norm": 0.00162785523571074,
      "learning_rate": 1.0882133333333335e-06,
      "loss": 0.0001,
      "step": 88650
    },
    {
      "epoch": 2.83712,
      "grad_norm": 0.0059216879308223724,
      "learning_rate": 1.0860800000000002e-06,
      "loss": 0.0002,
      "step": 88660
    },
    {
      "epoch": 2.83744,
      "grad_norm": 0.003133404301479459,
      "learning_rate": 1.0839466666666668e-06,
      "loss": 0.0002,
      "step": 88670
    },
    {
      "epoch": 2.83776,
      "grad_norm": 0.007986530661582947,
      "learning_rate": 1.0818133333333334e-06,
      "loss": 0.0002,
      "step": 88680
    },
    {
      "epoch": 2.83808,
      "grad_norm": 0.004966486245393753,
      "learning_rate": 1.07968e-06,
      "loss": 0.0002,
      "step": 88690
    },
    {
      "epoch": 2.8384,
      "grad_norm": 0.003667267505079508,
      "learning_rate": 1.0775466666666667e-06,
      "loss": 0.0002,
      "step": 88700
    },
    {
      "epoch": 2.83872,
      "grad_norm": 0.004405769519507885,
      "learning_rate": 1.0754133333333334e-06,
      "loss": 0.0995,
      "step": 88710
    },
    {
      "epoch": 2.83904,
      "grad_norm": 0.004093923140317202,
      "learning_rate": 1.0732800000000003e-06,
      "loss": 0.0002,
      "step": 88720
    },
    {
      "epoch": 2.83936,
      "grad_norm": 0.015587812289595604,
      "learning_rate": 1.0711466666666667e-06,
      "loss": 0.0002,
      "step": 88730
    },
    {
      "epoch": 2.83968,
      "grad_norm": 0.005781033542007208,
      "learning_rate": 1.0690133333333333e-06,
      "loss": 0.0006,
      "step": 88740
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.002624534536153078,
      "learning_rate": 1.0668800000000002e-06,
      "loss": 0.0001,
      "step": 88750
    },
    {
      "epoch": 2.84032,
      "grad_norm": 0.003407697891816497,
      "learning_rate": 1.0647466666666666e-06,
      "loss": 0.0002,
      "step": 88760
    },
    {
      "epoch": 2.84064,
      "grad_norm": 0.0034605078399181366,
      "learning_rate": 1.0626133333333335e-06,
      "loss": 0.0002,
      "step": 88770
    },
    {
      "epoch": 2.84096,
      "grad_norm": 0.004682125058025122,
      "learning_rate": 1.0604800000000001e-06,
      "loss": 0.05,
      "step": 88780
    },
    {
      "epoch": 2.8412800000000002,
      "grad_norm": 0.0057212491519749165,
      "learning_rate": 1.0583466666666668e-06,
      "loss": 0.0002,
      "step": 88790
    },
    {
      "epoch": 2.8416,
      "grad_norm": 0.07565539330244064,
      "learning_rate": 1.0562133333333334e-06,
      "loss": 0.0003,
      "step": 88800
    },
    {
      "epoch": 2.84192,
      "grad_norm": 0.003980179317295551,
      "learning_rate": 1.05408e-06,
      "loss": 0.0002,
      "step": 88810
    },
    {
      "epoch": 2.84224,
      "grad_norm": 0.0011615898692980409,
      "learning_rate": 1.0519466666666667e-06,
      "loss": 0.0006,
      "step": 88820
    },
    {
      "epoch": 2.8425599999999998,
      "grad_norm": 0.002598454710096121,
      "learning_rate": 1.0498133333333334e-06,
      "loss": 0.0008,
      "step": 88830
    },
    {
      "epoch": 2.84288,
      "grad_norm": 0.0025958246551454067,
      "learning_rate": 1.04768e-06,
      "loss": 0.0018,
      "step": 88840
    },
    {
      "epoch": 2.8432,
      "grad_norm": 0.0019452072447165847,
      "learning_rate": 1.0455466666666669e-06,
      "loss": 0.052,
      "step": 88850
    },
    {
      "epoch": 2.84352,
      "grad_norm": 0.00812310166656971,
      "learning_rate": 1.0434133333333333e-06,
      "loss": 0.0016,
      "step": 88860
    },
    {
      "epoch": 2.84384,
      "grad_norm": 0.004984009079635143,
      "learning_rate": 1.04128e-06,
      "loss": 0.003,
      "step": 88870
    },
    {
      "epoch": 2.84416,
      "grad_norm": 0.0030718964990228415,
      "learning_rate": 1.0391466666666668e-06,
      "loss": 0.0002,
      "step": 88880
    },
    {
      "epoch": 2.84448,
      "grad_norm": 0.004737198818475008,
      "learning_rate": 1.0370133333333335e-06,
      "loss": 0.0002,
      "step": 88890
    },
    {
      "epoch": 2.8448,
      "grad_norm": 0.0011011683382093906,
      "learning_rate": 1.0348800000000001e-06,
      "loss": 0.0002,
      "step": 88900
    },
    {
      "epoch": 2.84512,
      "grad_norm": 0.010893243364989758,
      "learning_rate": 1.0327466666666668e-06,
      "loss": 0.0125,
      "step": 88910
    },
    {
      "epoch": 2.84544,
      "grad_norm": 0.0032417248003184795,
      "learning_rate": 1.0306133333333334e-06,
      "loss": 0.0003,
      "step": 88920
    },
    {
      "epoch": 2.84576,
      "grad_norm": 0.0057951658964157104,
      "learning_rate": 1.02848e-06,
      "loss": 0.0004,
      "step": 88930
    },
    {
      "epoch": 2.84608,
      "grad_norm": 0.005593903828412294,
      "learning_rate": 1.0263466666666667e-06,
      "loss": 0.0002,
      "step": 88940
    },
    {
      "epoch": 2.8464,
      "grad_norm": 0.002776916604489088,
      "learning_rate": 1.0242133333333334e-06,
      "loss": 0.0002,
      "step": 88950
    },
    {
      "epoch": 2.84672,
      "grad_norm": 0.007074845489114523,
      "learning_rate": 1.02208e-06,
      "loss": 0.0002,
      "step": 88960
    },
    {
      "epoch": 2.84704,
      "grad_norm": 0.004718177020549774,
      "learning_rate": 1.0199466666666667e-06,
      "loss": 0.0005,
      "step": 88970
    },
    {
      "epoch": 2.84736,
      "grad_norm": 0.0035871886648237705,
      "learning_rate": 1.0178133333333335e-06,
      "loss": 0.0002,
      "step": 88980
    },
    {
      "epoch": 2.84768,
      "grad_norm": 0.010868693701922894,
      "learning_rate": 1.0156800000000002e-06,
      "loss": 0.0183,
      "step": 88990
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.007266496308147907,
      "learning_rate": 1.0135466666666668e-06,
      "loss": 0.0009,
      "step": 89000
    },
    {
      "epoch": 2.84832,
      "grad_norm": 0.12854602932929993,
      "learning_rate": 1.0114133333333335e-06,
      "loss": 0.0006,
      "step": 89010
    },
    {
      "epoch": 2.84864,
      "grad_norm": 0.010334387421607971,
      "learning_rate": 1.00928e-06,
      "loss": 0.0003,
      "step": 89020
    },
    {
      "epoch": 2.84896,
      "grad_norm": 0.008525618351995945,
      "learning_rate": 1.0071466666666667e-06,
      "loss": 0.0003,
      "step": 89030
    },
    {
      "epoch": 2.8492800000000003,
      "grad_norm": 0.003747429931536317,
      "learning_rate": 1.0050133333333334e-06,
      "loss": 0.0001,
      "step": 89040
    },
    {
      "epoch": 2.8496,
      "grad_norm": 0.0027741652447730303,
      "learning_rate": 1.00288e-06,
      "loss": 0.0004,
      "step": 89050
    },
    {
      "epoch": 2.84992,
      "grad_norm": 0.005455216858536005,
      "learning_rate": 1.000746666666667e-06,
      "loss": 0.0508,
      "step": 89060
    },
    {
      "epoch": 2.85024,
      "grad_norm": 0.005981966853141785,
      "learning_rate": 9.986133333333333e-07,
      "loss": 0.0479,
      "step": 89070
    },
    {
      "epoch": 2.8505599999999998,
      "grad_norm": 6.265437602996826,
      "learning_rate": 9.9648e-07,
      "loss": 0.015,
      "step": 89080
    },
    {
      "epoch": 2.85088,
      "grad_norm": 0.014548465609550476,
      "learning_rate": 9.943466666666668e-07,
      "loss": 0.0002,
      "step": 89090
    },
    {
      "epoch": 2.8512,
      "grad_norm": 0.0016703061992302537,
      "learning_rate": 9.922133333333333e-07,
      "loss": 0.0002,
      "step": 89100
    },
    {
      "epoch": 2.85152,
      "grad_norm": 0.005453748162835836,
      "learning_rate": 9.900800000000001e-07,
      "loss": 0.0003,
      "step": 89110
    },
    {
      "epoch": 2.85184,
      "grad_norm": 0.0015413654036819935,
      "learning_rate": 9.879466666666668e-07,
      "loss": 0.0461,
      "step": 89120
    },
    {
      "epoch": 2.85216,
      "grad_norm": 0.00602070102468133,
      "learning_rate": 9.858133333333334e-07,
      "loss": 0.0092,
      "step": 89130
    },
    {
      "epoch": 2.85248,
      "grad_norm": 0.006087903864681721,
      "learning_rate": 9.8368e-07,
      "loss": 0.0002,
      "step": 89140
    },
    {
      "epoch": 2.8528000000000002,
      "grad_norm": 0.0061197588220238686,
      "learning_rate": 9.815466666666667e-07,
      "loss": 0.0002,
      "step": 89150
    },
    {
      "epoch": 2.85312,
      "grad_norm": 0.003695182967931032,
      "learning_rate": 9.794133333333334e-07,
      "loss": 0.0001,
      "step": 89160
    },
    {
      "epoch": 2.85344,
      "grad_norm": 0.0052961562760174274,
      "learning_rate": 9.7728e-07,
      "loss": 0.0003,
      "step": 89170
    },
    {
      "epoch": 2.85376,
      "grad_norm": 0.004243016708642244,
      "learning_rate": 9.751466666666667e-07,
      "loss": 0.0014,
      "step": 89180
    },
    {
      "epoch": 2.8540799999999997,
      "grad_norm": 0.0031503764912486076,
      "learning_rate": 9.730133333333335e-07,
      "loss": 0.0002,
      "step": 89190
    },
    {
      "epoch": 2.8544,
      "grad_norm": 0.0032125520519912243,
      "learning_rate": 9.7088e-07,
      "loss": 0.0002,
      "step": 89200
    },
    {
      "epoch": 2.85472,
      "grad_norm": 0.027340756729245186,
      "learning_rate": 9.687466666666668e-07,
      "loss": 0.0002,
      "step": 89210
    },
    {
      "epoch": 2.85504,
      "grad_norm": 0.006363325752317905,
      "learning_rate": 9.666133333333335e-07,
      "loss": 0.0002,
      "step": 89220
    },
    {
      "epoch": 2.85536,
      "grad_norm": 0.004274056758731604,
      "learning_rate": 9.644800000000001e-07,
      "loss": 0.0002,
      "step": 89230
    },
    {
      "epoch": 2.85568,
      "grad_norm": 0.003631755942478776,
      "learning_rate": 9.623466666666668e-07,
      "loss": 0.0294,
      "step": 89240
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.004069304093718529,
      "learning_rate": 9.602133333333334e-07,
      "loss": 0.0002,
      "step": 89250
    },
    {
      "epoch": 2.85632,
      "grad_norm": 0.011262676678597927,
      "learning_rate": 9.5808e-07,
      "loss": 0.0005,
      "step": 89260
    },
    {
      "epoch": 2.85664,
      "grad_norm": 0.0077710519544780254,
      "learning_rate": 9.559466666666667e-07,
      "loss": 0.0034,
      "step": 89270
    },
    {
      "epoch": 2.85696,
      "grad_norm": 0.003192109987139702,
      "learning_rate": 9.538133333333334e-07,
      "loss": 0.0003,
      "step": 89280
    },
    {
      "epoch": 2.8572800000000003,
      "grad_norm": 0.0043395753018558025,
      "learning_rate": 9.516800000000001e-07,
      "loss": 0.0002,
      "step": 89290
    },
    {
      "epoch": 2.8576,
      "grad_norm": 0.005004077684134245,
      "learning_rate": 9.495466666666668e-07,
      "loss": 0.0002,
      "step": 89300
    },
    {
      "epoch": 2.85792,
      "grad_norm": 0.01570734940469265,
      "learning_rate": 9.474133333333334e-07,
      "loss": 0.0005,
      "step": 89310
    },
    {
      "epoch": 2.85824,
      "grad_norm": 0.002115996554493904,
      "learning_rate": 9.452800000000001e-07,
      "loss": 0.0007,
      "step": 89320
    },
    {
      "epoch": 2.8585599999999998,
      "grad_norm": 0.0052399649284780025,
      "learning_rate": 9.431466666666668e-07,
      "loss": 0.0003,
      "step": 89330
    },
    {
      "epoch": 2.85888,
      "grad_norm": 0.012356458231806755,
      "learning_rate": 9.410133333333334e-07,
      "loss": 0.0002,
      "step": 89340
    },
    {
      "epoch": 2.8592,
      "grad_norm": 0.0025646935682743788,
      "learning_rate": 9.388800000000001e-07,
      "loss": 0.0008,
      "step": 89350
    },
    {
      "epoch": 2.85952,
      "grad_norm": 0.007425489369779825,
      "learning_rate": 9.367466666666668e-07,
      "loss": 0.0002,
      "step": 89360
    },
    {
      "epoch": 2.85984,
      "grad_norm": 0.0035048481076955795,
      "learning_rate": 9.346133333333333e-07,
      "loss": 0.0002,
      "step": 89370
    },
    {
      "epoch": 2.86016,
      "grad_norm": 0.011720060370862484,
      "learning_rate": 9.3248e-07,
      "loss": 0.0003,
      "step": 89380
    },
    {
      "epoch": 2.86048,
      "grad_norm": 0.004449356347322464,
      "learning_rate": 9.303466666666668e-07,
      "loss": 0.0002,
      "step": 89390
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 0.007337575312703848,
      "learning_rate": 9.282133333333334e-07,
      "loss": 0.0004,
      "step": 89400
    },
    {
      "epoch": 2.86112,
      "grad_norm": 0.03787686675786972,
      "learning_rate": 9.2608e-07,
      "loss": 0.0002,
      "step": 89410
    },
    {
      "epoch": 2.86144,
      "grad_norm": 0.005092901643365622,
      "learning_rate": 9.239466666666667e-07,
      "loss": 0.0006,
      "step": 89420
    },
    {
      "epoch": 2.86176,
      "grad_norm": 3.7419986724853516,
      "learning_rate": 9.218133333333335e-07,
      "loss": 0.0282,
      "step": 89430
    },
    {
      "epoch": 2.8620799999999997,
      "grad_norm": 0.0021749399602413177,
      "learning_rate": 9.1968e-07,
      "loss": 0.0001,
      "step": 89440
    },
    {
      "epoch": 2.8624,
      "grad_norm": 0.006229217629879713,
      "learning_rate": 9.175466666666667e-07,
      "loss": 0.0026,
      "step": 89450
    },
    {
      "epoch": 2.86272,
      "grad_norm": 0.0036643666680902243,
      "learning_rate": 9.154133333333334e-07,
      "loss": 0.0009,
      "step": 89460
    },
    {
      "epoch": 2.86304,
      "grad_norm": 0.00387292611412704,
      "learning_rate": 9.132800000000001e-07,
      "loss": 0.0002,
      "step": 89470
    },
    {
      "epoch": 2.86336,
      "grad_norm": 0.003434515791013837,
      "learning_rate": 9.111466666666667e-07,
      "loss": 0.0006,
      "step": 89480
    },
    {
      "epoch": 2.86368,
      "grad_norm": 0.0026895347982645035,
      "learning_rate": 9.090133333333334e-07,
      "loss": 0.0002,
      "step": 89490
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.0026854791212826967,
      "learning_rate": 9.068800000000001e-07,
      "loss": 0.0003,
      "step": 89500
    },
    {
      "epoch": 2.86432,
      "grad_norm": 0.006906213238835335,
      "learning_rate": 9.047466666666667e-07,
      "loss": 0.0044,
      "step": 89510
    },
    {
      "epoch": 2.86464,
      "grad_norm": 0.004169659689068794,
      "learning_rate": 9.026133333333334e-07,
      "loss": 0.0001,
      "step": 89520
    },
    {
      "epoch": 2.86496,
      "grad_norm": 0.0024363987613469362,
      "learning_rate": 9.004800000000001e-07,
      "loss": 0.0003,
      "step": 89530
    },
    {
      "epoch": 2.8652800000000003,
      "grad_norm": 0.0018815859220921993,
      "learning_rate": 8.983466666666666e-07,
      "loss": 0.0007,
      "step": 89540
    },
    {
      "epoch": 2.8656,
      "grad_norm": 2.312434434890747,
      "learning_rate": 8.962133333333334e-07,
      "loss": 0.049,
      "step": 89550
    },
    {
      "epoch": 2.86592,
      "grad_norm": 0.004549666307866573,
      "learning_rate": 8.940800000000001e-07,
      "loss": 0.0002,
      "step": 89560
    },
    {
      "epoch": 2.86624,
      "grad_norm": 0.008539507165551186,
      "learning_rate": 8.919466666666668e-07,
      "loss": 0.0005,
      "step": 89570
    },
    {
      "epoch": 2.8665599999999998,
      "grad_norm": 0.006107899826020002,
      "learning_rate": 8.898133333333333e-07,
      "loss": 0.0003,
      "step": 89580
    },
    {
      "epoch": 2.86688,
      "grad_norm": 0.00403786962851882,
      "learning_rate": 8.876800000000001e-07,
      "loss": 0.0002,
      "step": 89590
    },
    {
      "epoch": 2.8672,
      "grad_norm": 0.010095757432281971,
      "learning_rate": 8.855466666666668e-07,
      "loss": 0.0003,
      "step": 89600
    },
    {
      "epoch": 2.86752,
      "grad_norm": 0.024692639708518982,
      "learning_rate": 8.834133333333334e-07,
      "loss": 0.0003,
      "step": 89610
    },
    {
      "epoch": 2.86784,
      "grad_norm": 0.0055793942883610725,
      "learning_rate": 8.8128e-07,
      "loss": 0.0461,
      "step": 89620
    },
    {
      "epoch": 2.86816,
      "grad_norm": 2.8485734462738037,
      "learning_rate": 8.791466666666668e-07,
      "loss": 0.0039,
      "step": 89630
    },
    {
      "epoch": 2.86848,
      "grad_norm": 0.0028616918716579676,
      "learning_rate": 8.770133333333334e-07,
      "loss": 0.0001,
      "step": 89640
    },
    {
      "epoch": 2.8688000000000002,
      "grad_norm": 0.003806322580203414,
      "learning_rate": 8.748800000000001e-07,
      "loss": 0.0002,
      "step": 89650
    },
    {
      "epoch": 2.86912,
      "grad_norm": 0.0034090164117515087,
      "learning_rate": 8.727466666666667e-07,
      "loss": 0.0002,
      "step": 89660
    },
    {
      "epoch": 2.86944,
      "grad_norm": 0.0071739330887794495,
      "learning_rate": 8.706133333333335e-07,
      "loss": 0.0239,
      "step": 89670
    },
    {
      "epoch": 2.86976,
      "grad_norm": 0.0022313448134809732,
      "learning_rate": 8.6848e-07,
      "loss": 0.0004,
      "step": 89680
    },
    {
      "epoch": 2.8700799999999997,
      "grad_norm": 0.3489665389060974,
      "learning_rate": 8.663466666666668e-07,
      "loss": 0.0049,
      "step": 89690
    },
    {
      "epoch": 2.8704,
      "grad_norm": 0.006129983346909285,
      "learning_rate": 8.642133333333334e-07,
      "loss": 0.0002,
      "step": 89700
    },
    {
      "epoch": 2.87072,
      "grad_norm": 0.0038805920630693436,
      "learning_rate": 8.620800000000002e-07,
      "loss": 0.0002,
      "step": 89710
    },
    {
      "epoch": 2.87104,
      "grad_norm": 0.00785174872726202,
      "learning_rate": 8.599466666666667e-07,
      "loss": 0.0003,
      "step": 89720
    },
    {
      "epoch": 2.87136,
      "grad_norm": 0.002985665574669838,
      "learning_rate": 8.578133333333335e-07,
      "loss": 0.0004,
      "step": 89730
    },
    {
      "epoch": 2.87168,
      "grad_norm": 0.003974050749093294,
      "learning_rate": 8.556800000000001e-07,
      "loss": 0.0002,
      "step": 89740
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.0046437522396445274,
      "learning_rate": 8.535466666666666e-07,
      "loss": 0.0002,
      "step": 89750
    },
    {
      "epoch": 2.87232,
      "grad_norm": 0.002896918449550867,
      "learning_rate": 8.514133333333334e-07,
      "loss": 0.0001,
      "step": 89760
    },
    {
      "epoch": 2.87264,
      "grad_norm": 0.0038102525286376476,
      "learning_rate": 8.492800000000002e-07,
      "loss": 0.0002,
      "step": 89770
    },
    {
      "epoch": 2.87296,
      "grad_norm": 0.008288185112178326,
      "learning_rate": 8.471466666666667e-07,
      "loss": 0.0002,
      "step": 89780
    },
    {
      "epoch": 2.87328,
      "grad_norm": 0.0015313231851905584,
      "learning_rate": 8.450133333333333e-07,
      "loss": 0.0001,
      "step": 89790
    },
    {
      "epoch": 2.8736,
      "grad_norm": 0.0068489620462059975,
      "learning_rate": 8.428800000000001e-07,
      "loss": 0.0002,
      "step": 89800
    },
    {
      "epoch": 2.87392,
      "grad_norm": 0.0026045269332826138,
      "learning_rate": 8.407466666666668e-07,
      "loss": 0.0003,
      "step": 89810
    },
    {
      "epoch": 2.87424,
      "grad_norm": 0.00568624772131443,
      "learning_rate": 8.386133333333334e-07,
      "loss": 0.0003,
      "step": 89820
    },
    {
      "epoch": 2.87456,
      "grad_norm": 0.0016560968942940235,
      "learning_rate": 8.3648e-07,
      "loss": 0.0002,
      "step": 89830
    },
    {
      "epoch": 2.87488,
      "grad_norm": 2.996504306793213,
      "learning_rate": 8.343466666666668e-07,
      "loss": 0.0027,
      "step": 89840
    },
    {
      "epoch": 2.8752,
      "grad_norm": 0.0030979833099991083,
      "learning_rate": 8.322133333333333e-07,
      "loss": 0.0001,
      "step": 89850
    },
    {
      "epoch": 2.87552,
      "grad_norm": 0.0022988230921328068,
      "learning_rate": 8.300800000000001e-07,
      "loss": 0.0001,
      "step": 89860
    },
    {
      "epoch": 2.87584,
      "grad_norm": 0.0015370531473308802,
      "learning_rate": 8.279466666666667e-07,
      "loss": 0.0002,
      "step": 89870
    },
    {
      "epoch": 2.87616,
      "grad_norm": 0.029523661360144615,
      "learning_rate": 8.258133333333335e-07,
      "loss": 0.0002,
      "step": 89880
    },
    {
      "epoch": 2.87648,
      "grad_norm": 0.004681640770286322,
      "learning_rate": 8.2368e-07,
      "loss": 0.0002,
      "step": 89890
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 0.004780719988048077,
      "learning_rate": 8.215466666666668e-07,
      "loss": 0.06,
      "step": 89900
    },
    {
      "epoch": 2.87712,
      "grad_norm": 0.004706803243607283,
      "learning_rate": 8.194133333333334e-07,
      "loss": 0.0002,
      "step": 89910
    },
    {
      "epoch": 2.87744,
      "grad_norm": 0.0119353998452425,
      "learning_rate": 8.1728e-07,
      "loss": 0.0003,
      "step": 89920
    },
    {
      "epoch": 2.87776,
      "grad_norm": 0.002255344297736883,
      "learning_rate": 8.151466666666667e-07,
      "loss": 0.0002,
      "step": 89930
    },
    {
      "epoch": 2.8780799999999997,
      "grad_norm": 0.0009086655336432159,
      "learning_rate": 8.130133333333335e-07,
      "loss": 0.0002,
      "step": 89940
    },
    {
      "epoch": 2.8784,
      "grad_norm": 0.005767702590674162,
      "learning_rate": 8.1088e-07,
      "loss": 0.0002,
      "step": 89950
    },
    {
      "epoch": 2.87872,
      "grad_norm": 0.005931642837822437,
      "learning_rate": 8.087466666666667e-07,
      "loss": 0.0087,
      "step": 89960
    },
    {
      "epoch": 2.87904,
      "grad_norm": 0.002952760551124811,
      "learning_rate": 8.066133333333334e-07,
      "loss": 0.0002,
      "step": 89970
    },
    {
      "epoch": 2.87936,
      "grad_norm": 0.0012936884304508567,
      "learning_rate": 8.044800000000002e-07,
      "loss": 0.0003,
      "step": 89980
    },
    {
      "epoch": 2.87968,
      "grad_norm": 0.0036779644433408976,
      "learning_rate": 8.023466666666667e-07,
      "loss": 0.0002,
      "step": 89990
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.006146319210529327,
      "learning_rate": 8.002133333333334e-07,
      "loss": 0.0002,
      "step": 90000
    },
    {
      "epoch": 2.88032,
      "grad_norm": 0.0019939951598644257,
      "learning_rate": 7.980800000000001e-07,
      "loss": 0.0011,
      "step": 90010
    },
    {
      "epoch": 2.88064,
      "grad_norm": 0.006515831686556339,
      "learning_rate": 7.959466666666667e-07,
      "loss": 0.0422,
      "step": 90020
    },
    {
      "epoch": 2.88096,
      "grad_norm": 0.05438276380300522,
      "learning_rate": 7.938133333333334e-07,
      "loss": 0.0112,
      "step": 90030
    },
    {
      "epoch": 2.88128,
      "grad_norm": 0.005032072775065899,
      "learning_rate": 7.916800000000001e-07,
      "loss": 0.0002,
      "step": 90040
    },
    {
      "epoch": 2.8816,
      "grad_norm": 5.647561550140381,
      "learning_rate": 7.895466666666668e-07,
      "loss": 0.018,
      "step": 90050
    },
    {
      "epoch": 2.88192,
      "grad_norm": 0.010627173818647861,
      "learning_rate": 7.874133333333334e-07,
      "loss": 0.0002,
      "step": 90060
    },
    {
      "epoch": 2.88224,
      "grad_norm": 0.005425730720162392,
      "learning_rate": 7.852800000000001e-07,
      "loss": 0.0102,
      "step": 90070
    },
    {
      "epoch": 2.88256,
      "grad_norm": 0.004571356810629368,
      "learning_rate": 7.831466666666668e-07,
      "loss": 0.0002,
      "step": 90080
    },
    {
      "epoch": 2.88288,
      "grad_norm": 0.002433055778965354,
      "learning_rate": 7.810133333333333e-07,
      "loss": 0.0002,
      "step": 90090
    },
    {
      "epoch": 2.8832,
      "grad_norm": 0.004497905261814594,
      "learning_rate": 7.7888e-07,
      "loss": 0.0001,
      "step": 90100
    },
    {
      "epoch": 2.88352,
      "grad_norm": 1.790521502494812,
      "learning_rate": 7.767466666666668e-07,
      "loss": 0.0013,
      "step": 90110
    },
    {
      "epoch": 2.88384,
      "grad_norm": 0.005103677045553923,
      "learning_rate": 7.746133333333333e-07,
      "loss": 0.0003,
      "step": 90120
    },
    {
      "epoch": 2.88416,
      "grad_norm": 0.002551742596551776,
      "learning_rate": 7.7248e-07,
      "loss": 0.0002,
      "step": 90130
    },
    {
      "epoch": 2.88448,
      "grad_norm": 0.002326186513528228,
      "learning_rate": 7.703466666666667e-07,
      "loss": 0.0002,
      "step": 90140
    },
    {
      "epoch": 2.8848000000000003,
      "grad_norm": 0.008009194396436214,
      "learning_rate": 7.682133333333335e-07,
      "loss": 0.0184,
      "step": 90150
    },
    {
      "epoch": 2.88512,
      "grad_norm": 0.005603349301964045,
      "learning_rate": 7.6608e-07,
      "loss": 0.0002,
      "step": 90160
    },
    {
      "epoch": 2.88544,
      "grad_norm": 0.0037056792061775923,
      "learning_rate": 7.639466666666667e-07,
      "loss": 0.0001,
      "step": 90170
    },
    {
      "epoch": 2.88576,
      "grad_norm": 0.00295501621440053,
      "learning_rate": 7.618133333333334e-07,
      "loss": 0.0002,
      "step": 90180
    },
    {
      "epoch": 2.8860799999999998,
      "grad_norm": 0.00435196328908205,
      "learning_rate": 7.5968e-07,
      "loss": 0.0011,
      "step": 90190
    },
    {
      "epoch": 2.8864,
      "grad_norm": 0.006654515862464905,
      "learning_rate": 7.575466666666667e-07,
      "loss": 0.0001,
      "step": 90200
    },
    {
      "epoch": 2.88672,
      "grad_norm": 0.01732434332370758,
      "learning_rate": 7.554133333333334e-07,
      "loss": 0.0005,
      "step": 90210
    },
    {
      "epoch": 2.88704,
      "grad_norm": 0.0030847061425447464,
      "learning_rate": 7.532800000000001e-07,
      "loss": 0.0001,
      "step": 90220
    },
    {
      "epoch": 2.88736,
      "grad_norm": 0.0013175628846511245,
      "learning_rate": 7.511466666666667e-07,
      "loss": 0.0439,
      "step": 90230
    },
    {
      "epoch": 2.88768,
      "grad_norm": 0.00930420309305191,
      "learning_rate": 7.490133333333334e-07,
      "loss": 0.0006,
      "step": 90240
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.006187560502439737,
      "learning_rate": 7.468800000000001e-07,
      "loss": 0.0003,
      "step": 90250
    },
    {
      "epoch": 2.88832,
      "grad_norm": 0.004242731723934412,
      "learning_rate": 7.447466666666666e-07,
      "loss": 0.0002,
      "step": 90260
    },
    {
      "epoch": 2.88864,
      "grad_norm": 0.006614251062273979,
      "learning_rate": 7.426133333333334e-07,
      "loss": 0.0488,
      "step": 90270
    },
    {
      "epoch": 2.88896,
      "grad_norm": 0.0011993711814284325,
      "learning_rate": 7.404800000000001e-07,
      "loss": 0.0004,
      "step": 90280
    },
    {
      "epoch": 2.88928,
      "grad_norm": 0.003436835017055273,
      "learning_rate": 7.383466666666668e-07,
      "loss": 0.0002,
      "step": 90290
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 0.0032119585666805506,
      "learning_rate": 7.362133333333333e-07,
      "loss": 0.0001,
      "step": 90300
    },
    {
      "epoch": 2.88992,
      "grad_norm": 0.008339294232428074,
      "learning_rate": 7.340800000000001e-07,
      "loss": 0.0002,
      "step": 90310
    },
    {
      "epoch": 2.89024,
      "grad_norm": 0.0032745010685175657,
      "learning_rate": 7.319466666666668e-07,
      "loss": 0.0006,
      "step": 90320
    },
    {
      "epoch": 2.89056,
      "grad_norm": 0.009031959809362888,
      "learning_rate": 7.298133333333334e-07,
      "loss": 0.0002,
      "step": 90330
    },
    {
      "epoch": 2.89088,
      "grad_norm": 0.013818091712892056,
      "learning_rate": 7.2768e-07,
      "loss": 0.0002,
      "step": 90340
    },
    {
      "epoch": 2.8912,
      "grad_norm": 0.0020639565773308277,
      "learning_rate": 7.255466666666668e-07,
      "loss": 0.0002,
      "step": 90350
    },
    {
      "epoch": 2.89152,
      "grad_norm": 0.0035145983565598726,
      "learning_rate": 7.234133333333333e-07,
      "loss": 0.0002,
      "step": 90360
    },
    {
      "epoch": 2.89184,
      "grad_norm": 0.01568378508090973,
      "learning_rate": 7.212800000000001e-07,
      "loss": 0.0002,
      "step": 90370
    },
    {
      "epoch": 2.89216,
      "grad_norm": 0.003557449672371149,
      "learning_rate": 7.191466666666667e-07,
      "loss": 0.0004,
      "step": 90380
    },
    {
      "epoch": 2.89248,
      "grad_norm": 0.0038607667665928602,
      "learning_rate": 7.170133333333335e-07,
      "loss": 0.0005,
      "step": 90390
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 0.003939057234674692,
      "learning_rate": 7.1488e-07,
      "loss": 0.1044,
      "step": 90400
    },
    {
      "epoch": 2.89312,
      "grad_norm": 0.004536647815257311,
      "learning_rate": 7.127466666666668e-07,
      "loss": 0.0191,
      "step": 90410
    },
    {
      "epoch": 2.89344,
      "grad_norm": 0.001560998847708106,
      "learning_rate": 7.106133333333334e-07,
      "loss": 0.0002,
      "step": 90420
    },
    {
      "epoch": 2.89376,
      "grad_norm": 0.002760682487860322,
      "learning_rate": 7.0848e-07,
      "loss": 0.0002,
      "step": 90430
    },
    {
      "epoch": 2.8940799999999998,
      "grad_norm": 0.0032733045518398285,
      "learning_rate": 7.063466666666667e-07,
      "loss": 0.0004,
      "step": 90440
    },
    {
      "epoch": 2.8944,
      "grad_norm": 0.0025263105053454638,
      "learning_rate": 7.042133333333335e-07,
      "loss": 0.0007,
      "step": 90450
    },
    {
      "epoch": 2.89472,
      "grad_norm": 4.491056442260742,
      "learning_rate": 7.020800000000001e-07,
      "loss": 0.0046,
      "step": 90460
    },
    {
      "epoch": 2.89504,
      "grad_norm": 0.010295568965375423,
      "learning_rate": 6.999466666666666e-07,
      "loss": 0.0002,
      "step": 90470
    },
    {
      "epoch": 2.89536,
      "grad_norm": 0.00520630506798625,
      "learning_rate": 6.978133333333334e-07,
      "loss": 0.0002,
      "step": 90480
    },
    {
      "epoch": 2.89568,
      "grad_norm": 0.0017415755428373814,
      "learning_rate": 6.956800000000002e-07,
      "loss": 0.0505,
      "step": 90490
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.0030133973341435194,
      "learning_rate": 6.935466666666667e-07,
      "loss": 0.0002,
      "step": 90500
    },
    {
      "epoch": 2.8963200000000002,
      "grad_norm": 0.004705741535872221,
      "learning_rate": 6.914133333333333e-07,
      "loss": 0.0002,
      "step": 90510
    },
    {
      "epoch": 2.89664,
      "grad_norm": 0.003887694561854005,
      "learning_rate": 6.892800000000001e-07,
      "loss": 0.0002,
      "step": 90520
    },
    {
      "epoch": 2.89696,
      "grad_norm": 0.0021122030448168516,
      "learning_rate": 6.871466666666666e-07,
      "loss": 0.0002,
      "step": 90530
    },
    {
      "epoch": 2.89728,
      "grad_norm": 0.006803093012422323,
      "learning_rate": 6.850133333333334e-07,
      "loss": 0.0501,
      "step": 90540
    },
    {
      "epoch": 2.8975999999999997,
      "grad_norm": 0.004712949972599745,
      "learning_rate": 6.8288e-07,
      "loss": 0.0002,
      "step": 90550
    },
    {
      "epoch": 2.89792,
      "grad_norm": 0.003329154569655657,
      "learning_rate": 6.807466666666668e-07,
      "loss": 0.0002,
      "step": 90560
    },
    {
      "epoch": 2.89824,
      "grad_norm": 0.05250195413827896,
      "learning_rate": 6.786133333333333e-07,
      "loss": 0.0005,
      "step": 90570
    },
    {
      "epoch": 2.89856,
      "grad_norm": 0.018542777746915817,
      "learning_rate": 6.764800000000001e-07,
      "loss": 0.0002,
      "step": 90580
    },
    {
      "epoch": 2.89888,
      "grad_norm": 0.009747778996825218,
      "learning_rate": 6.743466666666667e-07,
      "loss": 0.0258,
      "step": 90590
    },
    {
      "epoch": 2.8992,
      "grad_norm": 0.00828411802649498,
      "learning_rate": 6.722133333333334e-07,
      "loss": 0.0002,
      "step": 90600
    },
    {
      "epoch": 2.89952,
      "grad_norm": 0.003447100054472685,
      "learning_rate": 6.7008e-07,
      "loss": 0.0002,
      "step": 90610
    },
    {
      "epoch": 2.89984,
      "grad_norm": 0.03750636428594589,
      "learning_rate": 6.679466666666668e-07,
      "loss": 0.0003,
      "step": 90620
    },
    {
      "epoch": 2.90016,
      "grad_norm": 0.003532028989866376,
      "learning_rate": 6.658133333333334e-07,
      "loss": 0.0001,
      "step": 90630
    },
    {
      "epoch": 2.90048,
      "grad_norm": 0.0018286346457898617,
      "learning_rate": 6.6368e-07,
      "loss": 0.0003,
      "step": 90640
    },
    {
      "epoch": 2.9008000000000003,
      "grad_norm": 0.0017201240407302976,
      "learning_rate": 6.615466666666667e-07,
      "loss": 0.0422,
      "step": 90650
    },
    {
      "epoch": 2.90112,
      "grad_norm": 0.002926414832472801,
      "learning_rate": 6.594133333333335e-07,
      "loss": 0.0002,
      "step": 90660
    },
    {
      "epoch": 2.90144,
      "grad_norm": 0.005146786570549011,
      "learning_rate": 6.5728e-07,
      "loss": 0.0002,
      "step": 90670
    },
    {
      "epoch": 2.90176,
      "grad_norm": 0.005047813523560762,
      "learning_rate": 6.551466666666667e-07,
      "loss": 0.0024,
      "step": 90680
    },
    {
      "epoch": 2.9020799999999998,
      "grad_norm": 0.004016665276139975,
      "learning_rate": 6.530133333333334e-07,
      "loss": 0.0523,
      "step": 90690
    },
    {
      "epoch": 2.9024,
      "grad_norm": 0.08269926905632019,
      "learning_rate": 6.5088e-07,
      "loss": 0.0004,
      "step": 90700
    },
    {
      "epoch": 2.90272,
      "grad_norm": 0.005442050751298666,
      "learning_rate": 6.487466666666667e-07,
      "loss": 0.0003,
      "step": 90710
    },
    {
      "epoch": 2.90304,
      "grad_norm": 0.0027898026164621115,
      "learning_rate": 6.466133333333334e-07,
      "loss": 0.0002,
      "step": 90720
    },
    {
      "epoch": 2.90336,
      "grad_norm": 2.510139226913452,
      "learning_rate": 6.444800000000001e-07,
      "loss": 0.0627,
      "step": 90730
    },
    {
      "epoch": 2.90368,
      "grad_norm": 0.009744055569171906,
      "learning_rate": 6.423466666666667e-07,
      "loss": 0.0002,
      "step": 90740
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.004791157320141792,
      "learning_rate": 6.402133333333334e-07,
      "loss": 0.0002,
      "step": 90750
    },
    {
      "epoch": 2.9043200000000002,
      "grad_norm": 0.0035121350083500147,
      "learning_rate": 6.380800000000001e-07,
      "loss": 0.0001,
      "step": 90760
    },
    {
      "epoch": 2.90464,
      "grad_norm": 0.003253400092944503,
      "learning_rate": 6.359466666666667e-07,
      "loss": 0.0023,
      "step": 90770
    },
    {
      "epoch": 2.90496,
      "grad_norm": 0.00513047631829977,
      "learning_rate": 6.338133333333334e-07,
      "loss": 0.0003,
      "step": 90780
    },
    {
      "epoch": 2.90528,
      "grad_norm": 0.007719486951828003,
      "learning_rate": 6.316800000000001e-07,
      "loss": 0.0002,
      "step": 90790
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 0.011375978589057922,
      "learning_rate": 6.295466666666668e-07,
      "loss": 0.0002,
      "step": 90800
    },
    {
      "epoch": 2.90592,
      "grad_norm": 0.0023097305092960596,
      "learning_rate": 6.274133333333333e-07,
      "loss": 0.0003,
      "step": 90810
    },
    {
      "epoch": 2.90624,
      "grad_norm": 0.01779092289507389,
      "learning_rate": 6.2528e-07,
      "loss": 0.0002,
      "step": 90820
    },
    {
      "epoch": 2.90656,
      "grad_norm": 0.009938172064721584,
      "learning_rate": 6.231466666666667e-07,
      "loss": 0.0002,
      "step": 90830
    },
    {
      "epoch": 2.90688,
      "grad_norm": 10.029214859008789,
      "learning_rate": 6.210133333333333e-07,
      "loss": 0.0087,
      "step": 90840
    },
    {
      "epoch": 2.9072,
      "grad_norm": 0.1022457480430603,
      "learning_rate": 6.1888e-07,
      "loss": 0.0017,
      "step": 90850
    },
    {
      "epoch": 2.90752,
      "grad_norm": 0.003617549082264304,
      "learning_rate": 6.167466666666667e-07,
      "loss": 0.0004,
      "step": 90860
    },
    {
      "epoch": 2.90784,
      "grad_norm": 0.00254581356421113,
      "learning_rate": 6.146133333333334e-07,
      "loss": 0.0002,
      "step": 90870
    },
    {
      "epoch": 2.90816,
      "grad_norm": 0.002808470046147704,
      "learning_rate": 6.1248e-07,
      "loss": 0.0002,
      "step": 90880
    },
    {
      "epoch": 2.90848,
      "grad_norm": 0.011894122697412968,
      "learning_rate": 6.103466666666667e-07,
      "loss": 0.0002,
      "step": 90890
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 0.010074470192193985,
      "learning_rate": 6.082133333333333e-07,
      "loss": 0.0002,
      "step": 90900
    },
    {
      "epoch": 2.90912,
      "grad_norm": 0.0025505225639790297,
      "learning_rate": 6.060800000000001e-07,
      "loss": 0.0002,
      "step": 90910
    },
    {
      "epoch": 2.90944,
      "grad_norm": 0.00421621510758996,
      "learning_rate": 6.039466666666667e-07,
      "loss": 0.0003,
      "step": 90920
    },
    {
      "epoch": 2.90976,
      "grad_norm": 1.3598638772964478,
      "learning_rate": 6.018133333333334e-07,
      "loss": 0.0094,
      "step": 90930
    },
    {
      "epoch": 2.91008,
      "grad_norm": 0.004254186525940895,
      "learning_rate": 5.9968e-07,
      "loss": 0.0002,
      "step": 90940
    },
    {
      "epoch": 2.9104,
      "grad_norm": 0.001967573305591941,
      "learning_rate": 5.975466666666667e-07,
      "loss": 0.0002,
      "step": 90950
    },
    {
      "epoch": 2.91072,
      "grad_norm": 0.0039949058555066586,
      "learning_rate": 5.954133333333334e-07,
      "loss": 0.0002,
      "step": 90960
    },
    {
      "epoch": 2.91104,
      "grad_norm": 0.0329296812415123,
      "learning_rate": 5.932800000000001e-07,
      "loss": 0.0528,
      "step": 90970
    },
    {
      "epoch": 2.91136,
      "grad_norm": 0.006006095092743635,
      "learning_rate": 5.911466666666667e-07,
      "loss": 0.0002,
      "step": 90980
    },
    {
      "epoch": 2.91168,
      "grad_norm": 0.004246614873409271,
      "learning_rate": 5.890133333333334e-07,
      "loss": 0.0002,
      "step": 90990
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.008599646389484406,
      "learning_rate": 5.8688e-07,
      "loss": 0.0002,
      "step": 91000
    },
    {
      "epoch": 2.9123200000000002,
      "grad_norm": 0.002120780060067773,
      "learning_rate": 5.847466666666668e-07,
      "loss": 0.0002,
      "step": 91010
    },
    {
      "epoch": 2.91264,
      "grad_norm": 2.8255538940429688,
      "learning_rate": 5.826133333333333e-07,
      "loss": 0.0486,
      "step": 91020
    },
    {
      "epoch": 2.91296,
      "grad_norm": 0.003302739467471838,
      "learning_rate": 5.804800000000001e-07,
      "loss": 0.0117,
      "step": 91030
    },
    {
      "epoch": 2.91328,
      "grad_norm": 0.0027404464781284332,
      "learning_rate": 5.783466666666667e-07,
      "loss": 0.0002,
      "step": 91040
    },
    {
      "epoch": 2.9135999999999997,
      "grad_norm": 0.00236045615747571,
      "learning_rate": 5.762133333333334e-07,
      "loss": 0.0002,
      "step": 91050
    },
    {
      "epoch": 2.91392,
      "grad_norm": 0.011009997688233852,
      "learning_rate": 5.7408e-07,
      "loss": 0.0003,
      "step": 91060
    },
    {
      "epoch": 2.91424,
      "grad_norm": 0.013451593928039074,
      "learning_rate": 5.719466666666667e-07,
      "loss": 0.0304,
      "step": 91070
    },
    {
      "epoch": 2.91456,
      "grad_norm": 0.0036649792455136776,
      "learning_rate": 5.698133333333334e-07,
      "loss": 0.0003,
      "step": 91080
    },
    {
      "epoch": 2.91488,
      "grad_norm": 0.006613787729293108,
      "learning_rate": 5.676800000000001e-07,
      "loss": 0.0002,
      "step": 91090
    },
    {
      "epoch": 2.9152,
      "grad_norm": 0.003947125747799873,
      "learning_rate": 5.655466666666667e-07,
      "loss": 0.0002,
      "step": 91100
    },
    {
      "epoch": 2.91552,
      "grad_norm": 0.005654736887663603,
      "learning_rate": 5.634133333333334e-07,
      "loss": 0.0002,
      "step": 91110
    },
    {
      "epoch": 2.91584,
      "grad_norm": 0.0032023624517023563,
      "learning_rate": 5.6128e-07,
      "loss": 0.1033,
      "step": 91120
    },
    {
      "epoch": 2.91616,
      "grad_norm": 0.0008850849699229002,
      "learning_rate": 5.591466666666668e-07,
      "loss": 0.0292,
      "step": 91130
    },
    {
      "epoch": 2.91648,
      "grad_norm": 0.011673876084387302,
      "learning_rate": 5.570133333333334e-07,
      "loss": 0.0003,
      "step": 91140
    },
    {
      "epoch": 2.9168,
      "grad_norm": 0.003447757102549076,
      "learning_rate": 5.548800000000001e-07,
      "loss": 0.0017,
      "step": 91150
    },
    {
      "epoch": 2.91712,
      "grad_norm": 0.001887693302705884,
      "learning_rate": 5.527466666666667e-07,
      "loss": 0.0109,
      "step": 91160
    },
    {
      "epoch": 2.91744,
      "grad_norm": 0.008281839080154896,
      "learning_rate": 5.506133333333334e-07,
      "loss": 0.0357,
      "step": 91170
    },
    {
      "epoch": 2.91776,
      "grad_norm": 0.01849200390279293,
      "learning_rate": 5.484800000000001e-07,
      "loss": 0.0002,
      "step": 91180
    },
    {
      "epoch": 2.91808,
      "grad_norm": 0.0020497427321970463,
      "learning_rate": 5.463466666666666e-07,
      "loss": 0.0002,
      "step": 91190
    },
    {
      "epoch": 2.9184,
      "grad_norm": 0.004893719684332609,
      "learning_rate": 5.442133333333334e-07,
      "loss": 0.0004,
      "step": 91200
    },
    {
      "epoch": 2.91872,
      "grad_norm": 0.006227617617696524,
      "learning_rate": 5.4208e-07,
      "loss": 0.0446,
      "step": 91210
    },
    {
      "epoch": 2.91904,
      "grad_norm": 0.030495328828692436,
      "learning_rate": 5.399466666666667e-07,
      "loss": 0.0002,
      "step": 91220
    },
    {
      "epoch": 2.91936,
      "grad_norm": 0.012298612855374813,
      "learning_rate": 5.378133333333333e-07,
      "loss": 0.0025,
      "step": 91230
    },
    {
      "epoch": 2.91968,
      "grad_norm": 0.0047952947206795216,
      "learning_rate": 5.3568e-07,
      "loss": 0.0002,
      "step": 91240
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.006372386589646339,
      "learning_rate": 5.335466666666667e-07,
      "loss": 0.0002,
      "step": 91250
    },
    {
      "epoch": 2.9203200000000002,
      "grad_norm": 3.7719361782073975,
      "learning_rate": 5.314133333333334e-07,
      "loss": 0.0037,
      "step": 91260
    },
    {
      "epoch": 2.92064,
      "grad_norm": 0.024264410138130188,
      "learning_rate": 5.2928e-07,
      "loss": 0.0002,
      "step": 91270
    },
    {
      "epoch": 2.92096,
      "grad_norm": 0.007524717133492231,
      "learning_rate": 5.271466666666667e-07,
      "loss": 0.0004,
      "step": 91280
    },
    {
      "epoch": 2.92128,
      "grad_norm": 0.00455282861366868,
      "learning_rate": 5.250133333333333e-07,
      "loss": 0.0002,
      "step": 91290
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 0.004343576263636351,
      "learning_rate": 5.228800000000001e-07,
      "loss": 0.0001,
      "step": 91300
    },
    {
      "epoch": 2.92192,
      "grad_norm": 0.0043551926501095295,
      "learning_rate": 5.207466666666667e-07,
      "loss": 0.0002,
      "step": 91310
    },
    {
      "epoch": 2.92224,
      "grad_norm": 0.0030736320186406374,
      "learning_rate": 5.186133333333334e-07,
      "loss": 0.0095,
      "step": 91320
    },
    {
      "epoch": 2.92256,
      "grad_norm": 0.003927336074411869,
      "learning_rate": 5.1648e-07,
      "loss": 0.0003,
      "step": 91330
    },
    {
      "epoch": 2.92288,
      "grad_norm": 0.002020510146394372,
      "learning_rate": 5.143466666666667e-07,
      "loss": 0.0003,
      "step": 91340
    },
    {
      "epoch": 2.9232,
      "grad_norm": 0.0014323805226013064,
      "learning_rate": 5.122133333333334e-07,
      "loss": 0.043,
      "step": 91350
    },
    {
      "epoch": 2.92352,
      "grad_norm": 0.0066808974370360374,
      "learning_rate": 5.1008e-07,
      "loss": 0.0002,
      "step": 91360
    },
    {
      "epoch": 2.92384,
      "grad_norm": 0.006952445954084396,
      "learning_rate": 5.079466666666667e-07,
      "loss": 0.0003,
      "step": 91370
    },
    {
      "epoch": 2.92416,
      "grad_norm": 0.006995444186031818,
      "learning_rate": 5.058133333333334e-07,
      "loss": 0.0002,
      "step": 91380
    },
    {
      "epoch": 2.92448,
      "grad_norm": 0.0020342087373137474,
      "learning_rate": 5.0368e-07,
      "loss": 0.0369,
      "step": 91390
    },
    {
      "epoch": 2.9248,
      "grad_norm": 0.002222678856924176,
      "learning_rate": 5.015466666666667e-07,
      "loss": 0.0002,
      "step": 91400
    },
    {
      "epoch": 2.92512,
      "grad_norm": 0.0017640196019783616,
      "learning_rate": 4.994133333333333e-07,
      "loss": 0.0017,
      "step": 91410
    },
    {
      "epoch": 2.92544,
      "grad_norm": 0.0032629300840198994,
      "learning_rate": 4.972800000000001e-07,
      "loss": 0.0001,
      "step": 91420
    },
    {
      "epoch": 2.92576,
      "grad_norm": 2.6938047409057617,
      "learning_rate": 4.951466666666667e-07,
      "loss": 0.0482,
      "step": 91430
    },
    {
      "epoch": 2.92608,
      "grad_norm": 0.011523539200425148,
      "learning_rate": 4.930133333333334e-07,
      "loss": 0.0003,
      "step": 91440
    },
    {
      "epoch": 2.9264,
      "grad_norm": 0.00229266588576138,
      "learning_rate": 4.9088e-07,
      "loss": 0.0004,
      "step": 91450
    },
    {
      "epoch": 2.92672,
      "grad_norm": 0.048537466675043106,
      "learning_rate": 4.887466666666667e-07,
      "loss": 0.0003,
      "step": 91460
    },
    {
      "epoch": 2.92704,
      "grad_norm": 0.004121988080441952,
      "learning_rate": 4.866133333333334e-07,
      "loss": 0.0002,
      "step": 91470
    },
    {
      "epoch": 2.92736,
      "grad_norm": 0.018964167684316635,
      "learning_rate": 4.844800000000001e-07,
      "loss": 0.0005,
      "step": 91480
    },
    {
      "epoch": 2.92768,
      "grad_norm": 0.007160118781030178,
      "learning_rate": 4.823466666666667e-07,
      "loss": 0.0002,
      "step": 91490
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.035180650651454926,
      "learning_rate": 4.802133333333334e-07,
      "loss": 0.001,
      "step": 91500
    },
    {
      "epoch": 2.9283200000000003,
      "grad_norm": 0.006809915415942669,
      "learning_rate": 4.780800000000001e-07,
      "loss": 0.0002,
      "step": 91510
    },
    {
      "epoch": 2.92864,
      "grad_norm": 0.002780214184895158,
      "learning_rate": 4.759466666666667e-07,
      "loss": 0.0405,
      "step": 91520
    },
    {
      "epoch": 2.92896,
      "grad_norm": 0.0015717820497229695,
      "learning_rate": 4.7381333333333335e-07,
      "loss": 0.0002,
      "step": 91530
    },
    {
      "epoch": 2.92928,
      "grad_norm": 0.003094302723184228,
      "learning_rate": 4.7168000000000005e-07,
      "loss": 0.0001,
      "step": 91540
    },
    {
      "epoch": 2.9295999999999998,
      "grad_norm": 0.008640886284410954,
      "learning_rate": 4.695466666666667e-07,
      "loss": 0.0001,
      "step": 91550
    },
    {
      "epoch": 2.92992,
      "grad_norm": 0.01305093988776207,
      "learning_rate": 4.674133333333334e-07,
      "loss": 0.0003,
      "step": 91560
    },
    {
      "epoch": 2.93024,
      "grad_norm": 0.00515503017231822,
      "learning_rate": 4.6528000000000005e-07,
      "loss": 0.0006,
      "step": 91570
    },
    {
      "epoch": 2.93056,
      "grad_norm": 0.0059273215010762215,
      "learning_rate": 4.631466666666667e-07,
      "loss": 0.0305,
      "step": 91580
    },
    {
      "epoch": 2.93088,
      "grad_norm": 0.01456495188176632,
      "learning_rate": 4.610133333333334e-07,
      "loss": 0.0002,
      "step": 91590
    },
    {
      "epoch": 2.9312,
      "grad_norm": 4.09700345993042,
      "learning_rate": 4.5888000000000004e-07,
      "loss": 0.0593,
      "step": 91600
    },
    {
      "epoch": 2.93152,
      "grad_norm": 0.0019189781742170453,
      "learning_rate": 4.5674666666666674e-07,
      "loss": 0.0002,
      "step": 91610
    },
    {
      "epoch": 2.9318400000000002,
      "grad_norm": 0.004346110858023167,
      "learning_rate": 4.5461333333333334e-07,
      "loss": 0.0006,
      "step": 91620
    },
    {
      "epoch": 2.93216,
      "grad_norm": 0.005646438337862492,
      "learning_rate": 4.5248e-07,
      "loss": 0.0002,
      "step": 91630
    },
    {
      "epoch": 2.93248,
      "grad_norm": 0.011304632760584354,
      "learning_rate": 4.503466666666667e-07,
      "loss": 0.0002,
      "step": 91640
    },
    {
      "epoch": 2.9328,
      "grad_norm": 0.003883412340655923,
      "learning_rate": 4.4821333333333333e-07,
      "loss": 0.0467,
      "step": 91650
    },
    {
      "epoch": 2.9331199999999997,
      "grad_norm": 0.013689504936337471,
      "learning_rate": 4.4608000000000003e-07,
      "loss": 0.0004,
      "step": 91660
    },
    {
      "epoch": 2.93344,
      "grad_norm": 0.009376869536936283,
      "learning_rate": 4.439466666666667e-07,
      "loss": 0.0003,
      "step": 91670
    },
    {
      "epoch": 2.93376,
      "grad_norm": 0.00600065104663372,
      "learning_rate": 4.418133333333334e-07,
      "loss": 0.0004,
      "step": 91680
    },
    {
      "epoch": 2.93408,
      "grad_norm": 0.003041246673092246,
      "learning_rate": 4.3968000000000003e-07,
      "loss": 0.0008,
      "step": 91690
    },
    {
      "epoch": 2.9344,
      "grad_norm": 0.007635059300810099,
      "learning_rate": 4.375466666666667e-07,
      "loss": 0.0002,
      "step": 91700
    },
    {
      "epoch": 2.93472,
      "grad_norm": 0.006032388657331467,
      "learning_rate": 4.354133333333334e-07,
      "loss": 0.0002,
      "step": 91710
    },
    {
      "epoch": 2.93504,
      "grad_norm": 0.039559196680784225,
      "learning_rate": 4.3328e-07,
      "loss": 0.0004,
      "step": 91720
    },
    {
      "epoch": 2.93536,
      "grad_norm": 0.06763769686222076,
      "learning_rate": 4.311466666666667e-07,
      "loss": 0.0005,
      "step": 91730
    },
    {
      "epoch": 2.93568,
      "grad_norm": 0.004554644227027893,
      "learning_rate": 4.290133333333334e-07,
      "loss": 0.0003,
      "step": 91740
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.007635610643774271,
      "learning_rate": 4.2688e-07,
      "loss": 0.0004,
      "step": 91750
    },
    {
      "epoch": 2.9363200000000003,
      "grad_norm": 0.004047735594213009,
      "learning_rate": 4.247466666666667e-07,
      "loss": 0.0003,
      "step": 91760
    },
    {
      "epoch": 2.93664,
      "grad_norm": 0.007224570028483868,
      "learning_rate": 4.2261333333333337e-07,
      "loss": 0.0092,
      "step": 91770
    },
    {
      "epoch": 2.93696,
      "grad_norm": 1.3483425378799438,
      "learning_rate": 4.2048000000000007e-07,
      "loss": 0.0035,
      "step": 91780
    },
    {
      "epoch": 2.93728,
      "grad_norm": 0.003380465554073453,
      "learning_rate": 4.183466666666667e-07,
      "loss": 0.0002,
      "step": 91790
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 0.004616405814886093,
      "learning_rate": 4.162133333333334e-07,
      "loss": 0.0004,
      "step": 91800
    },
    {
      "epoch": 2.93792,
      "grad_norm": 0.0035535746719688177,
      "learning_rate": 4.1408e-07,
      "loss": 0.0003,
      "step": 91810
    },
    {
      "epoch": 2.93824,
      "grad_norm": 0.007810515351593494,
      "learning_rate": 4.1194666666666666e-07,
      "loss": 0.0002,
      "step": 91820
    },
    {
      "epoch": 2.93856,
      "grad_norm": 0.0034418043214827776,
      "learning_rate": 4.0981333333333336e-07,
      "loss": 0.0003,
      "step": 91830
    },
    {
      "epoch": 2.93888,
      "grad_norm": 0.009305767714977264,
      "learning_rate": 4.0768e-07,
      "loss": 0.0002,
      "step": 91840
    },
    {
      "epoch": 2.9392,
      "grad_norm": 0.003751229029148817,
      "learning_rate": 4.055466666666667e-07,
      "loss": 0.0002,
      "step": 91850
    },
    {
      "epoch": 2.93952,
      "grad_norm": 0.006537843029946089,
      "learning_rate": 4.0341333333333336e-07,
      "loss": 0.0006,
      "step": 91860
    },
    {
      "epoch": 2.9398400000000002,
      "grad_norm": 0.004348257556557655,
      "learning_rate": 4.0128e-07,
      "loss": 0.0002,
      "step": 91870
    },
    {
      "epoch": 2.94016,
      "grad_norm": 0.0028836200945079327,
      "learning_rate": 3.991466666666667e-07,
      "loss": 0.0002,
      "step": 91880
    },
    {
      "epoch": 2.94048,
      "grad_norm": 0.004710436798632145,
      "learning_rate": 3.9701333333333335e-07,
      "loss": 0.0123,
      "step": 91890
    },
    {
      "epoch": 2.9408,
      "grad_norm": 0.0046646129339933395,
      "learning_rate": 3.9488000000000005e-07,
      "loss": 0.0336,
      "step": 91900
    },
    {
      "epoch": 2.9411199999999997,
      "grad_norm": 0.01957644522190094,
      "learning_rate": 3.927466666666667e-07,
      "loss": 0.0002,
      "step": 91910
    },
    {
      "epoch": 2.94144,
      "grad_norm": 0.02187003381550312,
      "learning_rate": 3.906133333333334e-07,
      "loss": 0.0414,
      "step": 91920
    },
    {
      "epoch": 2.94176,
      "grad_norm": 0.007931179367005825,
      "learning_rate": 3.8848000000000005e-07,
      "loss": 0.0002,
      "step": 91930
    },
    {
      "epoch": 2.94208,
      "grad_norm": 0.0010227704187855124,
      "learning_rate": 3.863466666666667e-07,
      "loss": 0.0002,
      "step": 91940
    },
    {
      "epoch": 2.9424,
      "grad_norm": 0.007044509518891573,
      "learning_rate": 3.842133333333334e-07,
      "loss": 0.0145,
      "step": 91950
    },
    {
      "epoch": 2.94272,
      "grad_norm": 0.003080453723669052,
      "learning_rate": 3.8208000000000004e-07,
      "loss": 0.0002,
      "step": 91960
    },
    {
      "epoch": 2.94304,
      "grad_norm": 0.007046212907880545,
      "learning_rate": 3.7994666666666674e-07,
      "loss": 0.015,
      "step": 91970
    },
    {
      "epoch": 2.94336,
      "grad_norm": 0.003647101577371359,
      "learning_rate": 3.7781333333333334e-07,
      "loss": 0.0002,
      "step": 91980
    },
    {
      "epoch": 2.94368,
      "grad_norm": 0.001962695736438036,
      "learning_rate": 3.7568e-07,
      "loss": 0.0002,
      "step": 91990
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.001386365038342774,
      "learning_rate": 3.735466666666667e-07,
      "loss": 0.0003,
      "step": 92000
    },
    {
      "epoch": 2.9443200000000003,
      "grad_norm": 0.010924242436885834,
      "learning_rate": 3.7141333333333333e-07,
      "loss": 0.0002,
      "step": 92010
    },
    {
      "epoch": 2.94464,
      "grad_norm": 0.006485517602413893,
      "learning_rate": 3.6928000000000004e-07,
      "loss": 0.0002,
      "step": 92020
    },
    {
      "epoch": 2.94496,
      "grad_norm": 0.004501110874116421,
      "learning_rate": 3.671466666666667e-07,
      "loss": 0.0003,
      "step": 92030
    },
    {
      "epoch": 2.94528,
      "grad_norm": 0.0040860287845134735,
      "learning_rate": 3.6501333333333333e-07,
      "loss": 0.0001,
      "step": 92040
    },
    {
      "epoch": 2.9455999999999998,
      "grad_norm": 0.0036523924209177494,
      "learning_rate": 3.6288000000000003e-07,
      "loss": 0.0002,
      "step": 92050
    },
    {
      "epoch": 2.94592,
      "grad_norm": 0.0020564431324601173,
      "learning_rate": 3.607466666666667e-07,
      "loss": 0.0009,
      "step": 92060
    },
    {
      "epoch": 2.94624,
      "grad_norm": 0.0051480489782989025,
      "learning_rate": 3.586133333333334e-07,
      "loss": 0.0002,
      "step": 92070
    },
    {
      "epoch": 2.94656,
      "grad_norm": 0.0028469355311244726,
      "learning_rate": 3.5648e-07,
      "loss": 0.0115,
      "step": 92080
    },
    {
      "epoch": 2.94688,
      "grad_norm": 0.005219402257353067,
      "learning_rate": 3.5434666666666673e-07,
      "loss": 0.0002,
      "step": 92090
    },
    {
      "epoch": 2.9472,
      "grad_norm": 0.003940928261727095,
      "learning_rate": 3.522133333333334e-07,
      "loss": 0.0002,
      "step": 92100
    },
    {
      "epoch": 2.94752,
      "grad_norm": 0.0022588938008993864,
      "learning_rate": 3.5008e-07,
      "loss": 0.0248,
      "step": 92110
    },
    {
      "epoch": 2.9478400000000002,
      "grad_norm": 0.009719925932586193,
      "learning_rate": 3.479466666666667e-07,
      "loss": 0.0012,
      "step": 92120
    },
    {
      "epoch": 2.94816,
      "grad_norm": 0.025139199569821358,
      "learning_rate": 3.4581333333333337e-07,
      "loss": 0.0002,
      "step": 92130
    },
    {
      "epoch": 2.94848,
      "grad_norm": 0.01122189685702324,
      "learning_rate": 3.4368000000000007e-07,
      "loss": 0.0002,
      "step": 92140
    },
    {
      "epoch": 2.9488,
      "grad_norm": 0.0020742793567478657,
      "learning_rate": 3.415466666666667e-07,
      "loss": 0.0005,
      "step": 92150
    },
    {
      "epoch": 2.9491199999999997,
      "grad_norm": 0.0028400712180882692,
      "learning_rate": 3.394133333333333e-07,
      "loss": 0.0001,
      "step": 92160
    },
    {
      "epoch": 2.94944,
      "grad_norm": 0.00377129134722054,
      "learning_rate": 3.3728e-07,
      "loss": 0.0003,
      "step": 92170
    },
    {
      "epoch": 2.94976,
      "grad_norm": 0.01290940772742033,
      "learning_rate": 3.3514666666666666e-07,
      "loss": 0.0002,
      "step": 92180
    },
    {
      "epoch": 2.95008,
      "grad_norm": 0.0031896152067929506,
      "learning_rate": 3.3301333333333336e-07,
      "loss": 0.0003,
      "step": 92190
    },
    {
      "epoch": 2.9504,
      "grad_norm": 0.0038774912245571613,
      "learning_rate": 3.3088e-07,
      "loss": 0.0002,
      "step": 92200
    },
    {
      "epoch": 2.95072,
      "grad_norm": 0.00574613967910409,
      "learning_rate": 3.287466666666667e-07,
      "loss": 0.0002,
      "step": 92210
    },
    {
      "epoch": 2.95104,
      "grad_norm": 0.005648243241012096,
      "learning_rate": 3.2661333333333336e-07,
      "loss": 0.0003,
      "step": 92220
    },
    {
      "epoch": 2.95136,
      "grad_norm": 0.03986971452832222,
      "learning_rate": 3.2448e-07,
      "loss": 0.0003,
      "step": 92230
    },
    {
      "epoch": 2.95168,
      "grad_norm": 0.008737487718462944,
      "learning_rate": 3.223466666666667e-07,
      "loss": 0.0002,
      "step": 92240
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.0025353676173835993,
      "learning_rate": 3.2021333333333335e-07,
      "loss": 0.0002,
      "step": 92250
    },
    {
      "epoch": 2.9523200000000003,
      "grad_norm": 0.005208553746342659,
      "learning_rate": 3.1808000000000005e-07,
      "loss": 0.0002,
      "step": 92260
    },
    {
      "epoch": 2.95264,
      "grad_norm": 0.005817654076963663,
      "learning_rate": 3.159466666666667e-07,
      "loss": 0.0002,
      "step": 92270
    },
    {
      "epoch": 2.95296,
      "grad_norm": 0.00322382478043437,
      "learning_rate": 3.1381333333333335e-07,
      "loss": 0.0001,
      "step": 92280
    },
    {
      "epoch": 2.95328,
      "grad_norm": 0.008450961671769619,
      "learning_rate": 3.1168000000000005e-07,
      "loss": 0.0004,
      "step": 92290
    },
    {
      "epoch": 2.9536,
      "grad_norm": 0.0018437274266034365,
      "learning_rate": 3.095466666666667e-07,
      "loss": 0.0002,
      "step": 92300
    },
    {
      "epoch": 2.95392,
      "grad_norm": 0.005862434394657612,
      "learning_rate": 3.0741333333333334e-07,
      "loss": 0.0002,
      "step": 92310
    },
    {
      "epoch": 2.95424,
      "grad_norm": 0.0030636468436568975,
      "learning_rate": 3.0528000000000004e-07,
      "loss": 0.0002,
      "step": 92320
    },
    {
      "epoch": 2.95456,
      "grad_norm": 0.024508513510227203,
      "learning_rate": 3.031466666666667e-07,
      "loss": 0.0002,
      "step": 92330
    },
    {
      "epoch": 2.95488,
      "grad_norm": 0.008297680877149105,
      "learning_rate": 3.0101333333333334e-07,
      "loss": 0.0004,
      "step": 92340
    },
    {
      "epoch": 2.9552,
      "grad_norm": 0.00928778201341629,
      "learning_rate": 2.9888000000000004e-07,
      "loss": 0.0002,
      "step": 92350
    },
    {
      "epoch": 2.95552,
      "grad_norm": 0.018693529069423676,
      "learning_rate": 2.967466666666667e-07,
      "loss": 0.0002,
      "step": 92360
    },
    {
      "epoch": 2.9558400000000002,
      "grad_norm": 0.03287460282444954,
      "learning_rate": 2.9461333333333334e-07,
      "loss": 0.0003,
      "step": 92370
    },
    {
      "epoch": 2.95616,
      "grad_norm": 0.008409343659877777,
      "learning_rate": 2.9248000000000004e-07,
      "loss": 0.0002,
      "step": 92380
    },
    {
      "epoch": 2.95648,
      "grad_norm": 0.004177757538855076,
      "learning_rate": 2.903466666666667e-07,
      "loss": 0.0002,
      "step": 92390
    },
    {
      "epoch": 2.9568,
      "grad_norm": 0.004877871833741665,
      "learning_rate": 2.882133333333334e-07,
      "loss": 0.0133,
      "step": 92400
    },
    {
      "epoch": 2.9571199999999997,
      "grad_norm": 0.0017200845759361982,
      "learning_rate": 2.8608000000000003e-07,
      "loss": 0.0002,
      "step": 92410
    },
    {
      "epoch": 2.95744,
      "grad_norm": 0.011688461527228355,
      "learning_rate": 2.839466666666667e-07,
      "loss": 0.0006,
      "step": 92420
    },
    {
      "epoch": 2.95776,
      "grad_norm": 0.002906966023147106,
      "learning_rate": 2.818133333333333e-07,
      "loss": 0.0001,
      "step": 92430
    },
    {
      "epoch": 2.95808,
      "grad_norm": 0.0032879787031561136,
      "learning_rate": 2.7968000000000003e-07,
      "loss": 0.0002,
      "step": 92440
    },
    {
      "epoch": 2.9584,
      "grad_norm": 0.022789616137742996,
      "learning_rate": 2.775466666666667e-07,
      "loss": 0.0002,
      "step": 92450
    },
    {
      "epoch": 2.95872,
      "grad_norm": 0.019119033589959145,
      "learning_rate": 2.754133333333334e-07,
      "loss": 0.0002,
      "step": 92460
    },
    {
      "epoch": 2.95904,
      "grad_norm": 0.1624714732170105,
      "learning_rate": 2.7328e-07,
      "loss": 0.0621,
      "step": 92470
    },
    {
      "epoch": 2.95936,
      "grad_norm": 0.0025912162382155657,
      "learning_rate": 2.7114666666666667e-07,
      "loss": 0.0002,
      "step": 92480
    },
    {
      "epoch": 2.95968,
      "grad_norm": 0.0023135158699005842,
      "learning_rate": 2.6901333333333337e-07,
      "loss": 0.0002,
      "step": 92490
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.0019802511669695377,
      "learning_rate": 2.6688e-07,
      "loss": 0.0037,
      "step": 92500
    },
    {
      "epoch": 2.96032,
      "grad_norm": 0.0012957286089658737,
      "learning_rate": 2.647466666666667e-07,
      "loss": 0.0002,
      "step": 92510
    },
    {
      "epoch": 2.96064,
      "grad_norm": 0.006679890211671591,
      "learning_rate": 2.6261333333333337e-07,
      "loss": 0.0009,
      "step": 92520
    },
    {
      "epoch": 2.96096,
      "grad_norm": 0.002969027031213045,
      "learning_rate": 2.6048e-07,
      "loss": 0.0003,
      "step": 92530
    },
    {
      "epoch": 2.96128,
      "grad_norm": 0.0019000234315171838,
      "learning_rate": 2.5834666666666666e-07,
      "loss": 0.0004,
      "step": 92540
    },
    {
      "epoch": 2.9616,
      "grad_norm": 0.007039527874439955,
      "learning_rate": 2.5621333333333336e-07,
      "loss": 0.0002,
      "step": 92550
    },
    {
      "epoch": 2.96192,
      "grad_norm": 0.0075505697168409824,
      "learning_rate": 2.5408e-07,
      "loss": 0.0001,
      "step": 92560
    },
    {
      "epoch": 2.96224,
      "grad_norm": 0.003582421923056245,
      "learning_rate": 2.519466666666667e-07,
      "loss": 0.0139,
      "step": 92570
    },
    {
      "epoch": 2.96256,
      "grad_norm": 0.0366474948823452,
      "learning_rate": 2.4981333333333336e-07,
      "loss": 0.0553,
      "step": 92580
    },
    {
      "epoch": 2.96288,
      "grad_norm": 0.0030954915564507246,
      "learning_rate": 2.4768e-07,
      "loss": 0.0206,
      "step": 92590
    },
    {
      "epoch": 2.9632,
      "grad_norm": 0.0038494342006742954,
      "learning_rate": 2.4554666666666665e-07,
      "loss": 0.0002,
      "step": 92600
    },
    {
      "epoch": 2.96352,
      "grad_norm": 0.013281638734042645,
      "learning_rate": 2.4341333333333335e-07,
      "loss": 0.0035,
      "step": 92610
    },
    {
      "epoch": 2.9638400000000003,
      "grad_norm": 0.0029599936679005623,
      "learning_rate": 2.4128e-07,
      "loss": 0.0008,
      "step": 92620
    },
    {
      "epoch": 2.96416,
      "grad_norm": 0.17320910096168518,
      "learning_rate": 2.391466666666667e-07,
      "loss": 0.0504,
      "step": 92630
    },
    {
      "epoch": 2.96448,
      "grad_norm": 0.2422483116388321,
      "learning_rate": 2.3701333333333338e-07,
      "loss": 0.0015,
      "step": 92640
    },
    {
      "epoch": 2.9648,
      "grad_norm": 0.0037559829652309418,
      "learning_rate": 2.3488e-07,
      "loss": 0.0002,
      "step": 92650
    },
    {
      "epoch": 2.9651199999999998,
      "grad_norm": 0.006483422592282295,
      "learning_rate": 2.3274666666666667e-07,
      "loss": 0.0002,
      "step": 92660
    },
    {
      "epoch": 2.96544,
      "grad_norm": 0.0029855298344045877,
      "learning_rate": 2.3061333333333334e-07,
      "loss": 0.0003,
      "step": 92670
    },
    {
      "epoch": 2.96576,
      "grad_norm": 0.003128789132460952,
      "learning_rate": 2.2848000000000002e-07,
      "loss": 0.0003,
      "step": 92680
    },
    {
      "epoch": 2.96608,
      "grad_norm": 0.001843182137235999,
      "learning_rate": 2.263466666666667e-07,
      "loss": 0.0001,
      "step": 92690
    },
    {
      "epoch": 2.9664,
      "grad_norm": 0.005443916190415621,
      "learning_rate": 2.2421333333333337e-07,
      "loss": 0.0004,
      "step": 92700
    },
    {
      "epoch": 2.96672,
      "grad_norm": 0.004450270906090736,
      "learning_rate": 2.2208000000000001e-07,
      "loss": 0.0002,
      "step": 92710
    },
    {
      "epoch": 2.96704,
      "grad_norm": 0.008084649220108986,
      "learning_rate": 2.199466666666667e-07,
      "loss": 0.0001,
      "step": 92720
    },
    {
      "epoch": 2.96736,
      "grad_norm": 0.03458675742149353,
      "learning_rate": 2.1781333333333336e-07,
      "loss": 0.0002,
      "step": 92730
    },
    {
      "epoch": 2.96768,
      "grad_norm": 0.0015095032285898924,
      "learning_rate": 2.1568e-07,
      "loss": 0.0476,
      "step": 92740
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.0024491343647241592,
      "learning_rate": 2.1354666666666668e-07,
      "loss": 0.0003,
      "step": 92750
    },
    {
      "epoch": 2.96832,
      "grad_norm": 0.007691672537475824,
      "learning_rate": 2.1141333333333336e-07,
      "loss": 0.0378,
      "step": 92760
    },
    {
      "epoch": 2.9686399999999997,
      "grad_norm": 0.0044566369615495205,
      "learning_rate": 2.0928e-07,
      "loss": 0.0002,
      "step": 92770
    },
    {
      "epoch": 2.96896,
      "grad_norm": 0.0017539735417813063,
      "learning_rate": 2.0714666666666668e-07,
      "loss": 0.0002,
      "step": 92780
    },
    {
      "epoch": 2.96928,
      "grad_norm": 0.002945859916508198,
      "learning_rate": 2.0501333333333335e-07,
      "loss": 0.0003,
      "step": 92790
    },
    {
      "epoch": 2.9696,
      "grad_norm": 0.012862959876656532,
      "learning_rate": 2.0288000000000003e-07,
      "loss": 0.0003,
      "step": 92800
    },
    {
      "epoch": 2.96992,
      "grad_norm": 0.004160403273999691,
      "learning_rate": 2.007466666666667e-07,
      "loss": 0.0002,
      "step": 92810
    },
    {
      "epoch": 2.97024,
      "grad_norm": 0.004850880708545446,
      "learning_rate": 1.9861333333333338e-07,
      "loss": 0.0002,
      "step": 92820
    },
    {
      "epoch": 2.97056,
      "grad_norm": 0.00463980482891202,
      "learning_rate": 1.9648e-07,
      "loss": 0.0002,
      "step": 92830
    },
    {
      "epoch": 2.97088,
      "grad_norm": 0.011363046243786812,
      "learning_rate": 1.9434666666666667e-07,
      "loss": 0.0207,
      "step": 92840
    },
    {
      "epoch": 2.9712,
      "grad_norm": 0.005947367288172245,
      "learning_rate": 1.9221333333333334e-07,
      "loss": 0.0504,
      "step": 92850
    },
    {
      "epoch": 2.97152,
      "grad_norm": 0.0027645749505609274,
      "learning_rate": 1.9008000000000002e-07,
      "loss": 0.0534,
      "step": 92860
    },
    {
      "epoch": 2.9718400000000003,
      "grad_norm": 0.002321284729987383,
      "learning_rate": 1.879466666666667e-07,
      "loss": 0.0028,
      "step": 92870
    },
    {
      "epoch": 2.97216,
      "grad_norm": 0.07554369419813156,
      "learning_rate": 1.8581333333333334e-07,
      "loss": 0.0003,
      "step": 92880
    },
    {
      "epoch": 2.97248,
      "grad_norm": 0.011775126680731773,
      "learning_rate": 1.8368000000000001e-07,
      "loss": 0.0002,
      "step": 92890
    },
    {
      "epoch": 2.9728,
      "grad_norm": 0.2911672592163086,
      "learning_rate": 1.815466666666667e-07,
      "loss": 0.0236,
      "step": 92900
    },
    {
      "epoch": 2.9731199999999998,
      "grad_norm": 0.005097376648336649,
      "learning_rate": 1.7941333333333336e-07,
      "loss": 0.0507,
      "step": 92910
    },
    {
      "epoch": 2.97344,
      "grad_norm": 0.04444631561636925,
      "learning_rate": 1.7728e-07,
      "loss": 0.0003,
      "step": 92920
    },
    {
      "epoch": 2.97376,
      "grad_norm": 0.04502265527844429,
      "learning_rate": 1.7514666666666668e-07,
      "loss": 0.0003,
      "step": 92930
    },
    {
      "epoch": 2.97408,
      "grad_norm": 0.0020991514902561903,
      "learning_rate": 1.7301333333333333e-07,
      "loss": 0.0002,
      "step": 92940
    },
    {
      "epoch": 2.9744,
      "grad_norm": 0.0038732311222702265,
      "learning_rate": 1.7088e-07,
      "loss": 0.0005,
      "step": 92950
    },
    {
      "epoch": 2.97472,
      "grad_norm": 0.006667596288025379,
      "learning_rate": 1.6874666666666668e-07,
      "loss": 0.0002,
      "step": 92960
    },
    {
      "epoch": 2.97504,
      "grad_norm": 0.020042359828948975,
      "learning_rate": 1.6661333333333335e-07,
      "loss": 0.0013,
      "step": 92970
    },
    {
      "epoch": 2.9753600000000002,
      "grad_norm": 0.004936874844133854,
      "learning_rate": 1.6448000000000003e-07,
      "loss": 0.0005,
      "step": 92980
    },
    {
      "epoch": 2.97568,
      "grad_norm": 0.004037771373987198,
      "learning_rate": 1.623466666666667e-07,
      "loss": 0.0002,
      "step": 92990
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.007400608155876398,
      "learning_rate": 1.6021333333333332e-07,
      "loss": 0.0002,
      "step": 93000
    },
    {
      "epoch": 2.97632,
      "grad_norm": 0.0018419116968289018,
      "learning_rate": 1.5808e-07,
      "loss": 0.0002,
      "step": 93010
    },
    {
      "epoch": 2.9766399999999997,
      "grad_norm": 0.0018128680530935526,
      "learning_rate": 1.5594666666666667e-07,
      "loss": 0.0002,
      "step": 93020
    },
    {
      "epoch": 2.97696,
      "grad_norm": 0.008014229126274586,
      "learning_rate": 1.5381333333333334e-07,
      "loss": 0.0484,
      "step": 93030
    },
    {
      "epoch": 2.97728,
      "grad_norm": 0.0036457909736782312,
      "learning_rate": 1.5168e-07,
      "loss": 0.0002,
      "step": 93040
    },
    {
      "epoch": 2.9776,
      "grad_norm": 0.010090972296893597,
      "learning_rate": 1.4954666666666667e-07,
      "loss": 0.0002,
      "step": 93050
    },
    {
      "epoch": 2.97792,
      "grad_norm": 0.005382695700973272,
      "learning_rate": 1.4741333333333334e-07,
      "loss": 0.0005,
      "step": 93060
    },
    {
      "epoch": 2.97824,
      "grad_norm": 0.08612586557865143,
      "learning_rate": 1.4528000000000001e-07,
      "loss": 0.0004,
      "step": 93070
    },
    {
      "epoch": 2.97856,
      "grad_norm": 0.003959145862609148,
      "learning_rate": 1.431466666666667e-07,
      "loss": 0.0149,
      "step": 93080
    },
    {
      "epoch": 2.97888,
      "grad_norm": 0.028224574401974678,
      "learning_rate": 1.4101333333333334e-07,
      "loss": 0.0005,
      "step": 93090
    },
    {
      "epoch": 2.9792,
      "grad_norm": 0.009659472852945328,
      "learning_rate": 1.3888e-07,
      "loss": 0.0001,
      "step": 93100
    },
    {
      "epoch": 2.97952,
      "grad_norm": 0.0131883155554533,
      "learning_rate": 1.3674666666666668e-07,
      "loss": 0.0381,
      "step": 93110
    },
    {
      "epoch": 2.9798400000000003,
      "grad_norm": 0.0048469970934093,
      "learning_rate": 1.3461333333333336e-07,
      "loss": 0.0002,
      "step": 93120
    },
    {
      "epoch": 2.98016,
      "grad_norm": 0.004033571109175682,
      "learning_rate": 1.3248e-07,
      "loss": 0.0003,
      "step": 93130
    },
    {
      "epoch": 2.98048,
      "grad_norm": 0.0033895689994096756,
      "learning_rate": 1.3034666666666668e-07,
      "loss": 0.0152,
      "step": 93140
    },
    {
      "epoch": 2.9808,
      "grad_norm": 0.010151783935725689,
      "learning_rate": 1.2821333333333335e-07,
      "loss": 0.0329,
      "step": 93150
    },
    {
      "epoch": 2.9811199999999998,
      "grad_norm": 0.002793935127556324,
      "learning_rate": 1.2608e-07,
      "loss": 0.0002,
      "step": 93160
    },
    {
      "epoch": 2.98144,
      "grad_norm": 0.00308097037486732,
      "learning_rate": 1.2394666666666668e-07,
      "loss": 0.0001,
      "step": 93170
    },
    {
      "epoch": 2.98176,
      "grad_norm": 0.0017595741664990783,
      "learning_rate": 1.2181333333333335e-07,
      "loss": 0.0003,
      "step": 93180
    },
    {
      "epoch": 2.98208,
      "grad_norm": 0.008863396942615509,
      "learning_rate": 1.1968e-07,
      "loss": 0.0002,
      "step": 93190
    },
    {
      "epoch": 2.9824,
      "grad_norm": 0.016109729185700417,
      "learning_rate": 1.1754666666666667e-07,
      "loss": 0.0004,
      "step": 93200
    },
    {
      "epoch": 2.98272,
      "grad_norm": 0.002828580094501376,
      "learning_rate": 1.1541333333333335e-07,
      "loss": 0.0003,
      "step": 93210
    },
    {
      "epoch": 2.98304,
      "grad_norm": 0.041120801120996475,
      "learning_rate": 1.1328e-07,
      "loss": 0.0015,
      "step": 93220
    },
    {
      "epoch": 2.9833600000000002,
      "grad_norm": 0.0037366002798080444,
      "learning_rate": 1.1114666666666668e-07,
      "loss": 0.0002,
      "step": 93230
    },
    {
      "epoch": 2.98368,
      "grad_norm": 0.004693388938903809,
      "learning_rate": 1.0901333333333335e-07,
      "loss": 0.0002,
      "step": 93240
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.07269549369812012,
      "learning_rate": 1.0688e-07,
      "loss": 0.0009,
      "step": 93250
    },
    {
      "epoch": 2.98432,
      "grad_norm": 0.004895120859146118,
      "learning_rate": 1.0474666666666668e-07,
      "loss": 0.0003,
      "step": 93260
    },
    {
      "epoch": 2.9846399999999997,
      "grad_norm": 0.06087323650717735,
      "learning_rate": 1.0261333333333335e-07,
      "loss": 0.0007,
      "step": 93270
    },
    {
      "epoch": 2.98496,
      "grad_norm": 0.015812791883945465,
      "learning_rate": 1.0048000000000001e-07,
      "loss": 0.0485,
      "step": 93280
    },
    {
      "epoch": 2.98528,
      "grad_norm": 0.003875833237543702,
      "learning_rate": 9.834666666666667e-08,
      "loss": 0.0002,
      "step": 93290
    },
    {
      "epoch": 2.9856,
      "grad_norm": 0.004064340144395828,
      "learning_rate": 9.621333333333335e-08,
      "loss": 0.0003,
      "step": 93300
    },
    {
      "epoch": 2.98592,
      "grad_norm": 0.007639950606971979,
      "learning_rate": 9.408e-08,
      "loss": 0.0027,
      "step": 93310
    },
    {
      "epoch": 2.98624,
      "grad_norm": 0.013137495145201683,
      "learning_rate": 9.194666666666668e-08,
      "loss": 0.0001,
      "step": 93320
    },
    {
      "epoch": 2.98656,
      "grad_norm": 0.006263308692723513,
      "learning_rate": 8.981333333333333e-08,
      "loss": 0.0001,
      "step": 93330
    },
    {
      "epoch": 2.98688,
      "grad_norm": 0.002404981991276145,
      "learning_rate": 8.768e-08,
      "loss": 0.0003,
      "step": 93340
    },
    {
      "epoch": 2.9872,
      "grad_norm": 0.0024094986729323864,
      "learning_rate": 8.554666666666668e-08,
      "loss": 0.0003,
      "step": 93350
    },
    {
      "epoch": 2.98752,
      "grad_norm": 0.0024613395798951387,
      "learning_rate": 8.341333333333334e-08,
      "loss": 0.0004,
      "step": 93360
    },
    {
      "epoch": 2.9878400000000003,
      "grad_norm": 0.011875727213919163,
      "learning_rate": 8.128000000000001e-08,
      "loss": 0.0004,
      "step": 93370
    },
    {
      "epoch": 2.98816,
      "grad_norm": 0.08830610662698746,
      "learning_rate": 7.914666666666667e-08,
      "loss": 0.0003,
      "step": 93380
    },
    {
      "epoch": 2.98848,
      "grad_norm": 0.012747874483466148,
      "learning_rate": 7.701333333333335e-08,
      "loss": 0.0003,
      "step": 93390
    },
    {
      "epoch": 2.9888,
      "grad_norm": 0.004561268258839846,
      "learning_rate": 7.488e-08,
      "loss": 0.0002,
      "step": 93400
    },
    {
      "epoch": 2.9891199999999998,
      "grad_norm": 0.0026889119762927294,
      "learning_rate": 7.274666666666667e-08,
      "loss": 0.0002,
      "step": 93410
    },
    {
      "epoch": 2.98944,
      "grad_norm": 0.002465941710397601,
      "learning_rate": 7.061333333333334e-08,
      "loss": 0.0002,
      "step": 93420
    },
    {
      "epoch": 2.98976,
      "grad_norm": 0.010370878502726555,
      "learning_rate": 6.848e-08,
      "loss": 0.0002,
      "step": 93430
    },
    {
      "epoch": 2.99008,
      "grad_norm": 0.004574465099722147,
      "learning_rate": 6.634666666666666e-08,
      "loss": 0.0002,
      "step": 93440
    },
    {
      "epoch": 2.9904,
      "grad_norm": 0.004871791694313288,
      "learning_rate": 6.421333333333334e-08,
      "loss": 0.0002,
      "step": 93450
    },
    {
      "epoch": 2.99072,
      "grad_norm": 0.003269393928349018,
      "learning_rate": 6.208000000000001e-08,
      "loss": 0.0002,
      "step": 93460
    },
    {
      "epoch": 2.99104,
      "grad_norm": 0.0016610855236649513,
      "learning_rate": 5.994666666666667e-08,
      "loss": 0.0002,
      "step": 93470
    },
    {
      "epoch": 2.9913600000000002,
      "grad_norm": 0.0018161119660362601,
      "learning_rate": 5.781333333333334e-08,
      "loss": 0.0002,
      "step": 93480
    },
    {
      "epoch": 2.99168,
      "grad_norm": 0.003140978515148163,
      "learning_rate": 5.5680000000000006e-08,
      "loss": 0.0004,
      "step": 93490
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.0042840056121349335,
      "learning_rate": 5.354666666666667e-08,
      "loss": 0.0002,
      "step": 93500
    },
    {
      "epoch": 2.99232,
      "grad_norm": 0.006773729342967272,
      "learning_rate": 5.141333333333334e-08,
      "loss": 0.0034,
      "step": 93510
    },
    {
      "epoch": 2.9926399999999997,
      "grad_norm": 0.004730384796857834,
      "learning_rate": 4.928e-08,
      "loss": 0.0002,
      "step": 93520
    },
    {
      "epoch": 2.99296,
      "grad_norm": 0.004102386999875307,
      "learning_rate": 4.714666666666667e-08,
      "loss": 0.0002,
      "step": 93530
    },
    {
      "epoch": 2.99328,
      "grad_norm": 0.003792339935898781,
      "learning_rate": 4.5013333333333343e-08,
      "loss": 0.0002,
      "step": 93540
    },
    {
      "epoch": 2.9936,
      "grad_norm": 0.0028688055463135242,
      "learning_rate": 4.2880000000000004e-08,
      "loss": 0.0002,
      "step": 93550
    },
    {
      "epoch": 2.99392,
      "grad_norm": 0.010418814606964588,
      "learning_rate": 4.074666666666667e-08,
      "loss": 0.0006,
      "step": 93560
    },
    {
      "epoch": 2.99424,
      "grad_norm": 0.005948417820036411,
      "learning_rate": 3.861333333333334e-08,
      "loss": 0.0004,
      "step": 93570
    },
    {
      "epoch": 2.99456,
      "grad_norm": 0.0035217024851590395,
      "learning_rate": 3.6480000000000006e-08,
      "loss": 0.0002,
      "step": 93580
    },
    {
      "epoch": 2.99488,
      "grad_norm": 0.006110894959419966,
      "learning_rate": 3.434666666666667e-08,
      "loss": 0.0002,
      "step": 93590
    },
    {
      "epoch": 2.9952,
      "grad_norm": 0.012955853715538979,
      "learning_rate": 3.2213333333333335e-08,
      "loss": 0.0002,
      "step": 93600
    },
    {
      "epoch": 2.99552,
      "grad_norm": 0.023846833035349846,
      "learning_rate": 3.008e-08,
      "loss": 0.0027,
      "step": 93610
    },
    {
      "epoch": 2.99584,
      "grad_norm": 0.011257920414209366,
      "learning_rate": 2.794666666666667e-08,
      "loss": 0.0003,
      "step": 93620
    },
    {
      "epoch": 2.99616,
      "grad_norm": 0.0045846025459468365,
      "learning_rate": 2.5813333333333337e-08,
      "loss": 0.0002,
      "step": 93630
    },
    {
      "epoch": 2.99648,
      "grad_norm": 0.044071946293115616,
      "learning_rate": 2.368e-08,
      "loss": 0.0002,
      "step": 93640
    },
    {
      "epoch": 2.9968,
      "grad_norm": 0.00329414289444685,
      "learning_rate": 2.154666666666667e-08,
      "loss": 0.0005,
      "step": 93650
    },
    {
      "epoch": 2.99712,
      "grad_norm": 0.007249642629176378,
      "learning_rate": 1.9413333333333336e-08,
      "loss": 0.0001,
      "step": 93660
    },
    {
      "epoch": 2.99744,
      "grad_norm": 0.006579522974789143,
      "learning_rate": 1.728e-08,
      "loss": 0.0002,
      "step": 93670
    },
    {
      "epoch": 2.99776,
      "grad_norm": 0.005405227653682232,
      "learning_rate": 1.5146666666666667e-08,
      "loss": 0.0938,
      "step": 93680
    },
    {
      "epoch": 2.99808,
      "grad_norm": 0.00349619728513062,
      "learning_rate": 1.3013333333333335e-08,
      "loss": 0.0711,
      "step": 93690
    },
    {
      "epoch": 2.9984,
      "grad_norm": 0.00318688596598804,
      "learning_rate": 1.088e-08,
      "loss": 0.0002,
      "step": 93700
    },
    {
      "epoch": 2.99872,
      "grad_norm": 0.004148733336478472,
      "learning_rate": 8.746666666666668e-09,
      "loss": 0.0468,
      "step": 93710
    },
    {
      "epoch": 2.99904,
      "grad_norm": 0.004356472287327051,
      "learning_rate": 6.613333333333334e-09,
      "loss": 0.0004,
      "step": 93720
    },
    {
      "epoch": 2.9993600000000002,
      "grad_norm": 0.006062098313122988,
      "learning_rate": 4.48e-09,
      "loss": 0.047,
      "step": 93730
    },
    {
      "epoch": 2.99968,
      "grad_norm": 0.0309654138982296,
      "learning_rate": 2.346666666666667e-09,
      "loss": 0.0003,
      "step": 93740
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.006268488708883524,
      "learning_rate": 2.1333333333333337e-10,
      "loss": 0.0004,
      "step": 93750
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.0013,
      "eval_f1": 0.0012999101169919106,
      "eval_loss": 10.459956169128418,
      "eval_precision": 0.0012956872851965571,
      "eval_recall": 0.0013047257871364143,
      "eval_runtime": 43.7091,
      "eval_samples_per_second": 228.785,
      "eval_steps_per_second": 14.299,
      "step": 93750
    }
  ],
  "logging_steps": 10,
  "max_steps": 93750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 2
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.2892679414268544e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
