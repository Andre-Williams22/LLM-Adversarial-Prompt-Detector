{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 1000,
  "global_step": 62500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 3.2e-05,
      "grad_norm": 0.5772296190261841,
      "learning_rate": 1e-05,
      "loss": 0.6816,
      "step": 1
    },
    {
      "epoch": 0.00032,
      "grad_norm": 0.6372267603874207,
      "learning_rate": 9.99856e-06,
      "loss": 0.6945,
      "step": 10
    },
    {
      "epoch": 0.00064,
      "grad_norm": 0.5988937020301819,
      "learning_rate": 9.996960000000002e-06,
      "loss": 0.6881,
      "step": 20
    },
    {
      "epoch": 0.00096,
      "grad_norm": 0.5195396542549133,
      "learning_rate": 9.99536e-06,
      "loss": 0.6837,
      "step": 30
    },
    {
      "epoch": 0.00128,
      "grad_norm": 0.7454258799552917,
      "learning_rate": 9.993760000000001e-06,
      "loss": 0.6785,
      "step": 40
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.746166467666626,
      "learning_rate": 9.992160000000001e-06,
      "loss": 0.6774,
      "step": 50
    },
    {
      "epoch": 0.00192,
      "grad_norm": 0.4932015836238861,
      "learning_rate": 9.990560000000001e-06,
      "loss": 0.6746,
      "step": 60
    },
    {
      "epoch": 0.00224,
      "grad_norm": 0.629021167755127,
      "learning_rate": 9.988960000000001e-06,
      "loss": 0.6626,
      "step": 70
    },
    {
      "epoch": 0.00256,
      "grad_norm": 0.5954501628875732,
      "learning_rate": 9.987360000000001e-06,
      "loss": 0.6599,
      "step": 80
    },
    {
      "epoch": 0.00288,
      "grad_norm": 0.6759225726127625,
      "learning_rate": 9.98576e-06,
      "loss": 0.6575,
      "step": 90
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.6227260828018188,
      "learning_rate": 9.98416e-06,
      "loss": 0.6553,
      "step": 100
    },
    {
      "epoch": 0.00352,
      "grad_norm": 1.1263768672943115,
      "learning_rate": 9.98256e-06,
      "loss": 0.655,
      "step": 110
    },
    {
      "epoch": 0.00384,
      "grad_norm": 0.667783796787262,
      "learning_rate": 9.98096e-06,
      "loss": 0.6416,
      "step": 120
    },
    {
      "epoch": 0.00416,
      "grad_norm": 0.7752240896224976,
      "learning_rate": 9.979360000000002e-06,
      "loss": 0.6375,
      "step": 130
    },
    {
      "epoch": 0.00448,
      "grad_norm": 0.6826844811439514,
      "learning_rate": 9.97776e-06,
      "loss": 0.6333,
      "step": 140
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.5180217027664185,
      "learning_rate": 9.97616e-06,
      "loss": 0.6299,
      "step": 150
    },
    {
      "epoch": 0.00512,
      "grad_norm": 0.9415282607078552,
      "learning_rate": 9.974560000000002e-06,
      "loss": 0.6194,
      "step": 160
    },
    {
      "epoch": 0.00544,
      "grad_norm": 0.6713254451751709,
      "learning_rate": 9.97296e-06,
      "loss": 0.6107,
      "step": 170
    },
    {
      "epoch": 0.00576,
      "grad_norm": 0.8516808152198792,
      "learning_rate": 9.971360000000001e-06,
      "loss": 0.6064,
      "step": 180
    },
    {
      "epoch": 0.00608,
      "grad_norm": 0.9603063464164734,
      "learning_rate": 9.969760000000001e-06,
      "loss": 0.6036,
      "step": 190
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.7154183983802795,
      "learning_rate": 9.968160000000001e-06,
      "loss": 0.6003,
      "step": 200
    },
    {
      "epoch": 0.00672,
      "grad_norm": 0.9223616123199463,
      "learning_rate": 9.966560000000001e-06,
      "loss": 0.5906,
      "step": 210
    },
    {
      "epoch": 0.00704,
      "grad_norm": 0.8758302927017212,
      "learning_rate": 9.964960000000001e-06,
      "loss": 0.5801,
      "step": 220
    },
    {
      "epoch": 0.00736,
      "grad_norm": 0.7211931347846985,
      "learning_rate": 9.96336e-06,
      "loss": 0.5719,
      "step": 230
    },
    {
      "epoch": 0.00768,
      "grad_norm": 0.712308943271637,
      "learning_rate": 9.96176e-06,
      "loss": 0.5637,
      "step": 240
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.8140082955360413,
      "learning_rate": 9.96016e-06,
      "loss": 0.5434,
      "step": 250
    },
    {
      "epoch": 0.00832,
      "grad_norm": 0.9518082141876221,
      "learning_rate": 9.95856e-06,
      "loss": 0.5421,
      "step": 260
    },
    {
      "epoch": 0.00864,
      "grad_norm": 0.660990297794342,
      "learning_rate": 9.95696e-06,
      "loss": 0.5353,
      "step": 270
    },
    {
      "epoch": 0.00896,
      "grad_norm": 0.8362196087837219,
      "learning_rate": 9.95536e-06,
      "loss": 0.5232,
      "step": 280
    },
    {
      "epoch": 0.00928,
      "grad_norm": 0.9231534600257874,
      "learning_rate": 9.953760000000002e-06,
      "loss": 0.5139,
      "step": 290
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.920041024684906,
      "learning_rate": 9.95216e-06,
      "loss": 0.4982,
      "step": 300
    },
    {
      "epoch": 0.00992,
      "grad_norm": 0.8972070813179016,
      "learning_rate": 9.950560000000002e-06,
      "loss": 0.4966,
      "step": 310
    },
    {
      "epoch": 0.01024,
      "grad_norm": 1.0785545110702515,
      "learning_rate": 9.948960000000001e-06,
      "loss": 0.4845,
      "step": 320
    },
    {
      "epoch": 0.01056,
      "grad_norm": 0.8938458561897278,
      "learning_rate": 9.94736e-06,
      "loss": 0.4677,
      "step": 330
    },
    {
      "epoch": 0.01088,
      "grad_norm": 1.0320535898208618,
      "learning_rate": 9.945760000000001e-06,
      "loss": 0.4745,
      "step": 340
    },
    {
      "epoch": 0.0112,
      "grad_norm": 1.0443198680877686,
      "learning_rate": 9.944160000000001e-06,
      "loss": 0.4623,
      "step": 350
    },
    {
      "epoch": 0.01152,
      "grad_norm": 1.1428415775299072,
      "learning_rate": 9.942560000000001e-06,
      "loss": 0.4347,
      "step": 360
    },
    {
      "epoch": 0.01184,
      "grad_norm": 0.9861016869544983,
      "learning_rate": 9.94096e-06,
      "loss": 0.4314,
      "step": 370
    },
    {
      "epoch": 0.01216,
      "grad_norm": 0.9384735226631165,
      "learning_rate": 9.93936e-06,
      "loss": 0.4096,
      "step": 380
    },
    {
      "epoch": 0.01248,
      "grad_norm": 0.9838609099388123,
      "learning_rate": 9.93776e-06,
      "loss": 0.4242,
      "step": 390
    },
    {
      "epoch": 0.0128,
      "grad_norm": 1.0516349077224731,
      "learning_rate": 9.936160000000002e-06,
      "loss": 0.3865,
      "step": 400
    },
    {
      "epoch": 0.01312,
      "grad_norm": 0.9321200251579285,
      "learning_rate": 9.93456e-06,
      "loss": 0.3926,
      "step": 410
    },
    {
      "epoch": 0.01344,
      "grad_norm": 0.9206990003585815,
      "learning_rate": 9.93296e-06,
      "loss": 0.3884,
      "step": 420
    },
    {
      "epoch": 0.01376,
      "grad_norm": 0.9689248204231262,
      "learning_rate": 9.931360000000002e-06,
      "loss": 0.3576,
      "step": 430
    },
    {
      "epoch": 0.01408,
      "grad_norm": 0.8768617510795593,
      "learning_rate": 9.92976e-06,
      "loss": 0.3663,
      "step": 440
    },
    {
      "epoch": 0.0144,
      "grad_norm": 1.0592268705368042,
      "learning_rate": 9.928160000000002e-06,
      "loss": 0.3517,
      "step": 450
    },
    {
      "epoch": 0.01472,
      "grad_norm": 0.8322001099586487,
      "learning_rate": 9.926560000000001e-06,
      "loss": 0.3209,
      "step": 460
    },
    {
      "epoch": 0.01504,
      "grad_norm": 0.9722844362258911,
      "learning_rate": 9.924960000000001e-06,
      "loss": 0.3374,
      "step": 470
    },
    {
      "epoch": 0.01536,
      "grad_norm": 0.826374351978302,
      "learning_rate": 9.923360000000001e-06,
      "loss": 0.3052,
      "step": 480
    },
    {
      "epoch": 0.01568,
      "grad_norm": 0.8560382723808289,
      "learning_rate": 9.921760000000001e-06,
      "loss": 0.3229,
      "step": 490
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.9964046478271484,
      "learning_rate": 9.920160000000001e-06,
      "loss": 0.306,
      "step": 500
    },
    {
      "epoch": 0.01632,
      "grad_norm": 0.705923318862915,
      "learning_rate": 9.91856e-06,
      "loss": 0.2896,
      "step": 510
    },
    {
      "epoch": 0.01664,
      "grad_norm": 0.7603698372840881,
      "learning_rate": 9.91696e-06,
      "loss": 0.3008,
      "step": 520
    },
    {
      "epoch": 0.01696,
      "grad_norm": 0.655163586139679,
      "learning_rate": 9.91536e-06,
      "loss": 0.2706,
      "step": 530
    },
    {
      "epoch": 0.01728,
      "grad_norm": 0.6583874225616455,
      "learning_rate": 9.91376e-06,
      "loss": 0.2799,
      "step": 540
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.7613275051116943,
      "learning_rate": 9.91216e-06,
      "loss": 0.2632,
      "step": 550
    },
    {
      "epoch": 0.01792,
      "grad_norm": 0.5605890154838562,
      "learning_rate": 9.910560000000002e-06,
      "loss": 0.2642,
      "step": 560
    },
    {
      "epoch": 0.01824,
      "grad_norm": 0.5870200395584106,
      "learning_rate": 9.90896e-06,
      "loss": 0.262,
      "step": 570
    },
    {
      "epoch": 0.01856,
      "grad_norm": 0.7241846323013306,
      "learning_rate": 9.90736e-06,
      "loss": 0.257,
      "step": 580
    },
    {
      "epoch": 0.01888,
      "grad_norm": 0.5218434929847717,
      "learning_rate": 9.905760000000002e-06,
      "loss": 0.2606,
      "step": 590
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.5974805355072021,
      "learning_rate": 9.90416e-06,
      "loss": 0.266,
      "step": 600
    },
    {
      "epoch": 0.01952,
      "grad_norm": 0.5320665240287781,
      "learning_rate": 9.902560000000001e-06,
      "loss": 0.2838,
      "step": 610
    },
    {
      "epoch": 0.01984,
      "grad_norm": 0.8858120441436768,
      "learning_rate": 9.900960000000001e-06,
      "loss": 0.2494,
      "step": 620
    },
    {
      "epoch": 0.02016,
      "grad_norm": 0.4226350784301758,
      "learning_rate": 9.899360000000001e-06,
      "loss": 0.2491,
      "step": 630
    },
    {
      "epoch": 0.02048,
      "grad_norm": 0.6483362913131714,
      "learning_rate": 9.897760000000001e-06,
      "loss": 0.2441,
      "step": 640
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.5092102289199829,
      "learning_rate": 9.89616e-06,
      "loss": 0.2346,
      "step": 650
    },
    {
      "epoch": 0.02112,
      "grad_norm": 0.3699677884578705,
      "learning_rate": 9.89456e-06,
      "loss": 0.2503,
      "step": 660
    },
    {
      "epoch": 0.02144,
      "grad_norm": 0.31335949897766113,
      "learning_rate": 9.89296e-06,
      "loss": 0.2284,
      "step": 670
    },
    {
      "epoch": 0.02176,
      "grad_norm": 0.2763538360595703,
      "learning_rate": 9.89136e-06,
      "loss": 0.2341,
      "step": 680
    },
    {
      "epoch": 0.02208,
      "grad_norm": 0.5583370923995972,
      "learning_rate": 9.88976e-06,
      "loss": 0.2362,
      "step": 690
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.378378301858902,
      "learning_rate": 9.888160000000002e-06,
      "loss": 0.2633,
      "step": 700
    },
    {
      "epoch": 0.02272,
      "grad_norm": 0.31343305110931396,
      "learning_rate": 9.88656e-06,
      "loss": 0.2238,
      "step": 710
    },
    {
      "epoch": 0.02304,
      "grad_norm": 0.27027666568756104,
      "learning_rate": 9.884960000000002e-06,
      "loss": 0.2336,
      "step": 720
    },
    {
      "epoch": 0.02336,
      "grad_norm": 0.2853204309940338,
      "learning_rate": 9.883360000000002e-06,
      "loss": 0.242,
      "step": 730
    },
    {
      "epoch": 0.02368,
      "grad_norm": 0.3040961027145386,
      "learning_rate": 9.881760000000001e-06,
      "loss": 0.2285,
      "step": 740
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.2546136677265167,
      "learning_rate": 9.880160000000001e-06,
      "loss": 0.2652,
      "step": 750
    },
    {
      "epoch": 0.02432,
      "grad_norm": 0.3063100278377533,
      "learning_rate": 9.878560000000001e-06,
      "loss": 0.2275,
      "step": 760
    },
    {
      "epoch": 0.02464,
      "grad_norm": 0.22690585255622864,
      "learning_rate": 9.876960000000001e-06,
      "loss": 0.2377,
      "step": 770
    },
    {
      "epoch": 0.02496,
      "grad_norm": 0.1431588977575302,
      "learning_rate": 9.875360000000001e-06,
      "loss": 0.217,
      "step": 780
    },
    {
      "epoch": 0.02528,
      "grad_norm": 0.18514566123485565,
      "learning_rate": 9.87376e-06,
      "loss": 0.2274,
      "step": 790
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.2729806900024414,
      "learning_rate": 9.87216e-06,
      "loss": 0.2248,
      "step": 800
    },
    {
      "epoch": 0.02592,
      "grad_norm": 0.23500068485736847,
      "learning_rate": 9.87056e-06,
      "loss": 0.2252,
      "step": 810
    },
    {
      "epoch": 0.02624,
      "grad_norm": 0.44541487097740173,
      "learning_rate": 9.86896e-06,
      "loss": 0.2262,
      "step": 820
    },
    {
      "epoch": 0.02656,
      "grad_norm": 0.6283864378929138,
      "learning_rate": 9.86736e-06,
      "loss": 0.2457,
      "step": 830
    },
    {
      "epoch": 0.02688,
      "grad_norm": 0.7423155307769775,
      "learning_rate": 9.86576e-06,
      "loss": 0.2224,
      "step": 840
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.30704256892204285,
      "learning_rate": 9.86416e-06,
      "loss": 0.217,
      "step": 850
    },
    {
      "epoch": 0.02752,
      "grad_norm": 0.3856132924556732,
      "learning_rate": 9.862560000000002e-06,
      "loss": 0.2156,
      "step": 860
    },
    {
      "epoch": 0.02784,
      "grad_norm": 0.38380035758018494,
      "learning_rate": 9.86096e-06,
      "loss": 0.2141,
      "step": 870
    },
    {
      "epoch": 0.02816,
      "grad_norm": 0.14702527225017548,
      "learning_rate": 9.859360000000001e-06,
      "loss": 0.2199,
      "step": 880
    },
    {
      "epoch": 0.02848,
      "grad_norm": 0.10707229375839233,
      "learning_rate": 9.857760000000001e-06,
      "loss": 0.2321,
      "step": 890
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.30148905515670776,
      "learning_rate": 9.856160000000001e-06,
      "loss": 0.2413,
      "step": 900
    },
    {
      "epoch": 0.02912,
      "grad_norm": 0.5294326543807983,
      "learning_rate": 9.854560000000001e-06,
      "loss": 0.2153,
      "step": 910
    },
    {
      "epoch": 0.02944,
      "grad_norm": 0.3028925955295563,
      "learning_rate": 9.852960000000001e-06,
      "loss": 0.2228,
      "step": 920
    },
    {
      "epoch": 0.02976,
      "grad_norm": 1.2595795392990112,
      "learning_rate": 9.85136e-06,
      "loss": 0.2115,
      "step": 930
    },
    {
      "epoch": 0.03008,
      "grad_norm": 0.16329142451286316,
      "learning_rate": 9.84976e-06,
      "loss": 0.223,
      "step": 940
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.08415767550468445,
      "learning_rate": 9.84816e-06,
      "loss": 0.2079,
      "step": 950
    },
    {
      "epoch": 0.03072,
      "grad_norm": 0.27255934476852417,
      "learning_rate": 9.84656e-06,
      "loss": 0.2209,
      "step": 960
    },
    {
      "epoch": 0.03104,
      "grad_norm": 0.4128131866455078,
      "learning_rate": 9.84496e-06,
      "loss": 0.2447,
      "step": 970
    },
    {
      "epoch": 0.03136,
      "grad_norm": 0.10424142330884933,
      "learning_rate": 9.84336e-06,
      "loss": 0.2082,
      "step": 980
    },
    {
      "epoch": 0.03168,
      "grad_norm": 0.04370850697159767,
      "learning_rate": 9.841760000000002e-06,
      "loss": 0.2224,
      "step": 990
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.0747571587562561,
      "learning_rate": 9.84016e-06,
      "loss": 0.2034,
      "step": 1000
    },
    {
      "epoch": 0.032,
      "eval_runtime": 51.2259,
      "eval_samples_per_second": 195.214,
      "eval_steps_per_second": 12.201,
      "step": 1000
    },
    {
      "epoch": 0.03232,
      "grad_norm": 0.25935348868370056,
      "learning_rate": 9.83856e-06,
      "loss": 0.2189,
      "step": 1010
    },
    {
      "epoch": 0.03264,
      "grad_norm": 0.280911386013031,
      "learning_rate": 9.836960000000001e-06,
      "loss": 0.2194,
      "step": 1020
    },
    {
      "epoch": 0.03296,
      "grad_norm": 0.394784152507782,
      "learning_rate": 9.83536e-06,
      "loss": 0.2063,
      "step": 1030
    },
    {
      "epoch": 0.03328,
      "grad_norm": 0.4260733723640442,
      "learning_rate": 9.833760000000001e-06,
      "loss": 0.2154,
      "step": 1040
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.17332586646080017,
      "learning_rate": 9.832160000000001e-06,
      "loss": 0.214,
      "step": 1050
    },
    {
      "epoch": 0.03392,
      "grad_norm": 0.3117387294769287,
      "learning_rate": 9.830560000000001e-06,
      "loss": 0.2075,
      "step": 1060
    },
    {
      "epoch": 0.03424,
      "grad_norm": 0.10418199002742767,
      "learning_rate": 9.82896e-06,
      "loss": 0.2147,
      "step": 1070
    },
    {
      "epoch": 0.03456,
      "grad_norm": 0.20805612206459045,
      "learning_rate": 9.82736e-06,
      "loss": 0.207,
      "step": 1080
    },
    {
      "epoch": 0.03488,
      "grad_norm": 0.6205736994743347,
      "learning_rate": 9.82576e-06,
      "loss": 0.2179,
      "step": 1090
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.19869309663772583,
      "learning_rate": 9.82416e-06,
      "loss": 0.2065,
      "step": 1100
    },
    {
      "epoch": 0.03552,
      "grad_norm": 0.3145264983177185,
      "learning_rate": 9.82256e-06,
      "loss": 0.2091,
      "step": 1110
    },
    {
      "epoch": 0.03584,
      "grad_norm": 0.1180107370018959,
      "learning_rate": 9.82096e-06,
      "loss": 0.2115,
      "step": 1120
    },
    {
      "epoch": 0.03616,
      "grad_norm": 0.35546016693115234,
      "learning_rate": 9.819360000000002e-06,
      "loss": 0.2098,
      "step": 1130
    },
    {
      "epoch": 0.03648,
      "grad_norm": 0.04094008356332779,
      "learning_rate": 9.81776e-06,
      "loss": 0.2043,
      "step": 1140
    },
    {
      "epoch": 0.0368,
      "grad_norm": 1.4854077100753784,
      "learning_rate": 9.816160000000002e-06,
      "loss": 0.2456,
      "step": 1150
    },
    {
      "epoch": 0.03712,
      "grad_norm": 0.29139581322669983,
      "learning_rate": 9.814560000000001e-06,
      "loss": 0.22,
      "step": 1160
    },
    {
      "epoch": 0.03744,
      "grad_norm": 0.18137048184871674,
      "learning_rate": 9.81296e-06,
      "loss": 0.2048,
      "step": 1170
    },
    {
      "epoch": 0.03776,
      "grad_norm": 0.29550889134407043,
      "learning_rate": 9.811360000000001e-06,
      "loss": 0.208,
      "step": 1180
    },
    {
      "epoch": 0.03808,
      "grad_norm": 0.2611566185951233,
      "learning_rate": 9.809760000000001e-06,
      "loss": 0.2246,
      "step": 1190
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.09063445031642914,
      "learning_rate": 9.808160000000001e-06,
      "loss": 0.2175,
      "step": 1200
    },
    {
      "epoch": 0.03872,
      "grad_norm": 0.07872077077627182,
      "learning_rate": 9.806560000000001e-06,
      "loss": 0.2319,
      "step": 1210
    },
    {
      "epoch": 0.03904,
      "grad_norm": 0.4095418453216553,
      "learning_rate": 9.80496e-06,
      "loss": 0.2097,
      "step": 1220
    },
    {
      "epoch": 0.03936,
      "grad_norm": 0.14809654653072357,
      "learning_rate": 9.80336e-06,
      "loss": 0.2179,
      "step": 1230
    },
    {
      "epoch": 0.03968,
      "grad_norm": 0.07879509776830673,
      "learning_rate": 9.80176e-06,
      "loss": 0.2364,
      "step": 1240
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.04738736152648926,
      "learning_rate": 9.80016e-06,
      "loss": 0.205,
      "step": 1250
    },
    {
      "epoch": 0.04032,
      "grad_norm": 0.31266075372695923,
      "learning_rate": 9.79856e-06,
      "loss": 0.213,
      "step": 1260
    },
    {
      "epoch": 0.04064,
      "grad_norm": 0.10330208390951157,
      "learning_rate": 9.79696e-06,
      "loss": 0.2044,
      "step": 1270
    },
    {
      "epoch": 0.04096,
      "grad_norm": 0.1431484818458557,
      "learning_rate": 9.79536e-06,
      "loss": 0.2084,
      "step": 1280
    },
    {
      "epoch": 0.04128,
      "grad_norm": 0.057074546813964844,
      "learning_rate": 9.793760000000002e-06,
      "loss": 0.2226,
      "step": 1290
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.03340713307261467,
      "learning_rate": 9.79216e-06,
      "loss": 0.2025,
      "step": 1300
    },
    {
      "epoch": 0.04192,
      "grad_norm": 0.07505746930837631,
      "learning_rate": 9.790560000000001e-06,
      "loss": 0.2306,
      "step": 1310
    },
    {
      "epoch": 0.04224,
      "grad_norm": 0.04363001137971878,
      "learning_rate": 9.788960000000001e-06,
      "loss": 0.206,
      "step": 1320
    },
    {
      "epoch": 0.04256,
      "grad_norm": 0.033244889229536057,
      "learning_rate": 9.787360000000001e-06,
      "loss": 0.2047,
      "step": 1330
    },
    {
      "epoch": 0.04288,
      "grad_norm": 0.04435243085026741,
      "learning_rate": 9.785760000000001e-06,
      "loss": 0.2423,
      "step": 1340
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.1461954563856125,
      "learning_rate": 9.784160000000001e-06,
      "loss": 0.2066,
      "step": 1350
    },
    {
      "epoch": 0.04352,
      "grad_norm": 1.5244520902633667,
      "learning_rate": 9.78256e-06,
      "loss": 0.2109,
      "step": 1360
    },
    {
      "epoch": 0.04384,
      "grad_norm": 0.03816159814596176,
      "learning_rate": 9.78096e-06,
      "loss": 0.2145,
      "step": 1370
    },
    {
      "epoch": 0.04416,
      "grad_norm": 0.07349918782711029,
      "learning_rate": 9.77936e-06,
      "loss": 0.2082,
      "step": 1380
    },
    {
      "epoch": 0.04448,
      "grad_norm": 0.1543935090303421,
      "learning_rate": 9.77776e-06,
      "loss": 0.2133,
      "step": 1390
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.9257737994194031,
      "learning_rate": 9.776160000000002e-06,
      "loss": 0.2197,
      "step": 1400
    },
    {
      "epoch": 0.04512,
      "grad_norm": 0.03225787356495857,
      "learning_rate": 9.77456e-06,
      "loss": 0.2058,
      "step": 1410
    },
    {
      "epoch": 0.04544,
      "grad_norm": 0.2527458667755127,
      "learning_rate": 9.772960000000002e-06,
      "loss": 0.203,
      "step": 1420
    },
    {
      "epoch": 0.04576,
      "grad_norm": 0.07092259079217911,
      "learning_rate": 9.771360000000002e-06,
      "loss": 0.2087,
      "step": 1430
    },
    {
      "epoch": 0.04608,
      "grad_norm": 0.13046926259994507,
      "learning_rate": 9.76976e-06,
      "loss": 0.2037,
      "step": 1440
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.06817924976348877,
      "learning_rate": 9.768160000000001e-06,
      "loss": 0.201,
      "step": 1450
    },
    {
      "epoch": 0.04672,
      "grad_norm": 0.09784440696239471,
      "learning_rate": 9.766560000000001e-06,
      "loss": 0.2157,
      "step": 1460
    },
    {
      "epoch": 0.04704,
      "grad_norm": 0.07664968073368073,
      "learning_rate": 9.764960000000001e-06,
      "loss": 0.2017,
      "step": 1470
    },
    {
      "epoch": 0.04736,
      "grad_norm": 0.6253470182418823,
      "learning_rate": 9.763360000000001e-06,
      "loss": 0.2131,
      "step": 1480
    },
    {
      "epoch": 0.04768,
      "grad_norm": 0.04476781561970711,
      "learning_rate": 9.761760000000001e-06,
      "loss": 0.208,
      "step": 1490
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.15738646686077118,
      "learning_rate": 9.76016e-06,
      "loss": 0.2043,
      "step": 1500
    },
    {
      "epoch": 0.04832,
      "grad_norm": 0.26666295528411865,
      "learning_rate": 9.75856e-06,
      "loss": 0.2081,
      "step": 1510
    },
    {
      "epoch": 0.04864,
      "grad_norm": 0.24138139188289642,
      "learning_rate": 9.75696e-06,
      "loss": 0.214,
      "step": 1520
    },
    {
      "epoch": 0.04896,
      "grad_norm": 0.7028571367263794,
      "learning_rate": 9.75536e-06,
      "loss": 0.2269,
      "step": 1530
    },
    {
      "epoch": 0.04928,
      "grad_norm": 0.5340948700904846,
      "learning_rate": 9.75376e-06,
      "loss": 0.2072,
      "step": 1540
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.08876685798168182,
      "learning_rate": 9.75216e-06,
      "loss": 0.2099,
      "step": 1550
    },
    {
      "epoch": 0.04992,
      "grad_norm": 0.6777289509773254,
      "learning_rate": 9.750560000000002e-06,
      "loss": 0.2217,
      "step": 1560
    },
    {
      "epoch": 0.05024,
      "grad_norm": 0.47742390632629395,
      "learning_rate": 9.74896e-06,
      "loss": 0.2115,
      "step": 1570
    },
    {
      "epoch": 0.05056,
      "grad_norm": 0.09437701106071472,
      "learning_rate": 9.747360000000001e-06,
      "loss": 0.2121,
      "step": 1580
    },
    {
      "epoch": 0.05088,
      "grad_norm": 0.10868493467569351,
      "learning_rate": 9.745760000000001e-06,
      "loss": 0.21,
      "step": 1590
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.9988599419593811,
      "learning_rate": 9.74416e-06,
      "loss": 0.219,
      "step": 1600
    },
    {
      "epoch": 0.05152,
      "grad_norm": 0.183588907122612,
      "learning_rate": 9.742560000000001e-06,
      "loss": 0.227,
      "step": 1610
    },
    {
      "epoch": 0.05184,
      "grad_norm": 0.05970442667603493,
      "learning_rate": 9.740960000000001e-06,
      "loss": 0.2183,
      "step": 1620
    },
    {
      "epoch": 0.05216,
      "grad_norm": 0.7156170010566711,
      "learning_rate": 9.739360000000001e-06,
      "loss": 0.2157,
      "step": 1630
    },
    {
      "epoch": 0.05248,
      "grad_norm": 0.19000929594039917,
      "learning_rate": 9.73776e-06,
      "loss": 0.2195,
      "step": 1640
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.6606351137161255,
      "learning_rate": 9.73616e-06,
      "loss": 0.203,
      "step": 1650
    },
    {
      "epoch": 0.05312,
      "grad_norm": 0.041393958032131195,
      "learning_rate": 9.73456e-06,
      "loss": 0.2013,
      "step": 1660
    },
    {
      "epoch": 0.05344,
      "grad_norm": 0.6462761163711548,
      "learning_rate": 9.732960000000002e-06,
      "loss": 0.2284,
      "step": 1670
    },
    {
      "epoch": 0.05376,
      "grad_norm": 0.06848060339689255,
      "learning_rate": 9.73136e-06,
      "loss": 0.2016,
      "step": 1680
    },
    {
      "epoch": 0.05408,
      "grad_norm": 0.6407340168952942,
      "learning_rate": 9.72976e-06,
      "loss": 0.2327,
      "step": 1690
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.0397396981716156,
      "learning_rate": 9.728160000000002e-06,
      "loss": 0.2284,
      "step": 1700
    },
    {
      "epoch": 0.05472,
      "grad_norm": 0.0847674235701561,
      "learning_rate": 9.72656e-06,
      "loss": 0.2213,
      "step": 1710
    },
    {
      "epoch": 0.05504,
      "grad_norm": 0.42512375116348267,
      "learning_rate": 9.724960000000001e-06,
      "loss": 0.2145,
      "step": 1720
    },
    {
      "epoch": 0.05536,
      "grad_norm": 0.22750172019004822,
      "learning_rate": 9.723360000000001e-06,
      "loss": 0.2065,
      "step": 1730
    },
    {
      "epoch": 0.05568,
      "grad_norm": 0.23211221396923065,
      "learning_rate": 9.721760000000001e-06,
      "loss": 0.2085,
      "step": 1740
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.0481233112514019,
      "learning_rate": 9.720160000000001e-06,
      "loss": 0.2083,
      "step": 1750
    },
    {
      "epoch": 0.05632,
      "grad_norm": 0.21107998490333557,
      "learning_rate": 9.718560000000001e-06,
      "loss": 0.2031,
      "step": 1760
    },
    {
      "epoch": 0.05664,
      "grad_norm": 0.05298091098666191,
      "learning_rate": 9.716960000000001e-06,
      "loss": 0.2012,
      "step": 1770
    },
    {
      "epoch": 0.05696,
      "grad_norm": 0.021620217710733414,
      "learning_rate": 9.71536e-06,
      "loss": 0.205,
      "step": 1780
    },
    {
      "epoch": 0.05728,
      "grad_norm": 0.11616045236587524,
      "learning_rate": 9.71376e-06,
      "loss": 0.2018,
      "step": 1790
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.032477784901857376,
      "learning_rate": 9.71216e-06,
      "loss": 0.2069,
      "step": 1800
    },
    {
      "epoch": 0.05792,
      "grad_norm": 0.03119688108563423,
      "learning_rate": 9.71056e-06,
      "loss": 0.2145,
      "step": 1810
    },
    {
      "epoch": 0.05824,
      "grad_norm": 0.05584878847002983,
      "learning_rate": 9.70896e-06,
      "loss": 0.2022,
      "step": 1820
    },
    {
      "epoch": 0.05856,
      "grad_norm": 0.04058930650353432,
      "learning_rate": 9.707360000000002e-06,
      "loss": 0.2133,
      "step": 1830
    },
    {
      "epoch": 0.05888,
      "grad_norm": 0.07539470493793488,
      "learning_rate": 9.70576e-06,
      "loss": 0.2241,
      "step": 1840
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.07198011130094528,
      "learning_rate": 9.70416e-06,
      "loss": 0.2055,
      "step": 1850
    },
    {
      "epoch": 0.05952,
      "grad_norm": 0.042023248970508575,
      "learning_rate": 9.702560000000002e-06,
      "loss": 0.2211,
      "step": 1860
    },
    {
      "epoch": 0.05984,
      "grad_norm": 0.0849735215306282,
      "learning_rate": 9.70096e-06,
      "loss": 0.2063,
      "step": 1870
    },
    {
      "epoch": 0.06016,
      "grad_norm": 0.040422771126031876,
      "learning_rate": 9.699360000000001e-06,
      "loss": 0.2077,
      "step": 1880
    },
    {
      "epoch": 0.06048,
      "grad_norm": 0.035364266484975815,
      "learning_rate": 9.697760000000001e-06,
      "loss": 0.2195,
      "step": 1890
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.05248803645372391,
      "learning_rate": 9.696160000000001e-06,
      "loss": 0.2025,
      "step": 1900
    },
    {
      "epoch": 0.06112,
      "grad_norm": 0.04960120841860771,
      "learning_rate": 9.694560000000001e-06,
      "loss": 0.2033,
      "step": 1910
    },
    {
      "epoch": 0.06144,
      "grad_norm": 0.07702748477458954,
      "learning_rate": 9.69296e-06,
      "loss": 0.2194,
      "step": 1920
    },
    {
      "epoch": 0.06176,
      "grad_norm": 0.13156281411647797,
      "learning_rate": 9.69136e-06,
      "loss": 0.2083,
      "step": 1930
    },
    {
      "epoch": 0.06208,
      "grad_norm": 0.06517354398965836,
      "learning_rate": 9.68976e-06,
      "loss": 0.2012,
      "step": 1940
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.540458083152771,
      "learning_rate": 9.68816e-06,
      "loss": 0.2074,
      "step": 1950
    },
    {
      "epoch": 0.06272,
      "grad_norm": 0.029077354818582535,
      "learning_rate": 9.68656e-06,
      "loss": 0.2048,
      "step": 1960
    },
    {
      "epoch": 0.06304,
      "grad_norm": 0.26913368701934814,
      "learning_rate": 9.68496e-06,
      "loss": 0.2219,
      "step": 1970
    },
    {
      "epoch": 0.06336,
      "grad_norm": 0.09967519342899323,
      "learning_rate": 9.68336e-06,
      "loss": 0.2275,
      "step": 1980
    },
    {
      "epoch": 0.06368,
      "grad_norm": 0.044186461716890335,
      "learning_rate": 9.681760000000002e-06,
      "loss": 0.2045,
      "step": 1990
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.12718890607357025,
      "learning_rate": 9.68016e-06,
      "loss": 0.2266,
      "step": 2000
    },
    {
      "epoch": 0.064,
      "eval_runtime": 51.6038,
      "eval_samples_per_second": 193.784,
      "eval_steps_per_second": 12.112,
      "step": 2000
    },
    {
      "epoch": 0.06432,
      "grad_norm": 0.02565907873213291,
      "learning_rate": 9.678560000000001e-06,
      "loss": 0.2099,
      "step": 2010
    },
    {
      "epoch": 0.06464,
      "grad_norm": 0.04827062785625458,
      "learning_rate": 9.676960000000001e-06,
      "loss": 0.2053,
      "step": 2020
    },
    {
      "epoch": 0.06496,
      "grad_norm": 0.0269980039447546,
      "learning_rate": 9.67536e-06,
      "loss": 0.242,
      "step": 2030
    },
    {
      "epoch": 0.06528,
      "grad_norm": 0.7254326939582825,
      "learning_rate": 9.673760000000001e-06,
      "loss": 0.2101,
      "step": 2040
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.019166911020874977,
      "learning_rate": 9.672160000000001e-06,
      "loss": 0.2,
      "step": 2050
    },
    {
      "epoch": 0.06592,
      "grad_norm": 0.3596416711807251,
      "learning_rate": 9.67056e-06,
      "loss": 0.2065,
      "step": 2060
    },
    {
      "epoch": 0.06624,
      "grad_norm": 0.12531867623329163,
      "learning_rate": 9.66896e-06,
      "loss": 0.2049,
      "step": 2070
    },
    {
      "epoch": 0.06656,
      "grad_norm": 0.5148455500602722,
      "learning_rate": 9.66736e-06,
      "loss": 0.2039,
      "step": 2080
    },
    {
      "epoch": 0.06688,
      "grad_norm": 0.04818730056285858,
      "learning_rate": 9.66576e-06,
      "loss": 0.2122,
      "step": 2090
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.7253904342651367,
      "learning_rate": 9.664160000000002e-06,
      "loss": 0.2572,
      "step": 2100
    },
    {
      "epoch": 0.06752,
      "grad_norm": 0.5048514604568481,
      "learning_rate": 9.66256e-06,
      "loss": 0.2068,
      "step": 2110
    },
    {
      "epoch": 0.06784,
      "grad_norm": 0.07465649396181107,
      "learning_rate": 9.66096e-06,
      "loss": 0.2037,
      "step": 2120
    },
    {
      "epoch": 0.06816,
      "grad_norm": 0.02178817428648472,
      "learning_rate": 9.659360000000002e-06,
      "loss": 0.2109,
      "step": 2130
    },
    {
      "epoch": 0.06848,
      "grad_norm": 0.044111739844083786,
      "learning_rate": 9.65776e-06,
      "loss": 0.2053,
      "step": 2140
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.7942686676979065,
      "learning_rate": 9.656160000000001e-06,
      "loss": 0.2049,
      "step": 2150
    },
    {
      "epoch": 0.06912,
      "grad_norm": 0.04766009747982025,
      "learning_rate": 9.654560000000001e-06,
      "loss": 0.2033,
      "step": 2160
    },
    {
      "epoch": 0.06944,
      "grad_norm": 0.9078080654144287,
      "learning_rate": 9.652960000000001e-06,
      "loss": 0.209,
      "step": 2170
    },
    {
      "epoch": 0.06976,
      "grad_norm": 0.7671841979026794,
      "learning_rate": 9.651360000000001e-06,
      "loss": 0.2314,
      "step": 2180
    },
    {
      "epoch": 0.07008,
      "grad_norm": 0.023934530094265938,
      "learning_rate": 9.649760000000001e-06,
      "loss": 0.2013,
      "step": 2190
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.03527502343058586,
      "learning_rate": 9.64816e-06,
      "loss": 0.2217,
      "step": 2200
    },
    {
      "epoch": 0.07072,
      "grad_norm": 0.03683052584528923,
      "learning_rate": 9.64656e-06,
      "loss": 0.2168,
      "step": 2210
    },
    {
      "epoch": 0.07104,
      "grad_norm": 0.1432175636291504,
      "learning_rate": 9.64496e-06,
      "loss": 0.208,
      "step": 2220
    },
    {
      "epoch": 0.07136,
      "grad_norm": 0.022945035248994827,
      "learning_rate": 9.64336e-06,
      "loss": 0.2021,
      "step": 2230
    },
    {
      "epoch": 0.07168,
      "grad_norm": 0.07673115283250809,
      "learning_rate": 9.64176e-06,
      "loss": 0.2196,
      "step": 2240
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.7664859294891357,
      "learning_rate": 9.64016e-06,
      "loss": 0.207,
      "step": 2250
    },
    {
      "epoch": 0.07232,
      "grad_norm": 0.025821300223469734,
      "learning_rate": 9.638560000000002e-06,
      "loss": 0.2004,
      "step": 2260
    },
    {
      "epoch": 0.07264,
      "grad_norm": 0.04060683399438858,
      "learning_rate": 9.63696e-06,
      "loss": 0.2012,
      "step": 2270
    },
    {
      "epoch": 0.07296,
      "grad_norm": 0.09738358855247498,
      "learning_rate": 9.63536e-06,
      "loss": 0.2065,
      "step": 2280
    },
    {
      "epoch": 0.07328,
      "grad_norm": 0.051302313804626465,
      "learning_rate": 9.633760000000001e-06,
      "loss": 0.2074,
      "step": 2290
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.17609503865242004,
      "learning_rate": 9.63216e-06,
      "loss": 0.2015,
      "step": 2300
    },
    {
      "epoch": 0.07392,
      "grad_norm": 0.043375492095947266,
      "learning_rate": 9.630560000000001e-06,
      "loss": 0.2015,
      "step": 2310
    },
    {
      "epoch": 0.07424,
      "grad_norm": 0.03445035591721535,
      "learning_rate": 9.628960000000001e-06,
      "loss": 0.2038,
      "step": 2320
    },
    {
      "epoch": 0.07456,
      "grad_norm": 0.08494958281517029,
      "learning_rate": 9.627360000000001e-06,
      "loss": 0.2068,
      "step": 2330
    },
    {
      "epoch": 0.07488,
      "grad_norm": 0.5679529309272766,
      "learning_rate": 9.62576e-06,
      "loss": 0.2035,
      "step": 2340
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.0766739472746849,
      "learning_rate": 9.62416e-06,
      "loss": 0.2033,
      "step": 2350
    },
    {
      "epoch": 0.07552,
      "grad_norm": 0.028945239260792732,
      "learning_rate": 9.62256e-06,
      "loss": 0.2037,
      "step": 2360
    },
    {
      "epoch": 0.07584,
      "grad_norm": 0.7960630059242249,
      "learning_rate": 9.62096e-06,
      "loss": 0.2119,
      "step": 2370
    },
    {
      "epoch": 0.07616,
      "grad_norm": 0.03308844938874245,
      "learning_rate": 9.61936e-06,
      "loss": 0.221,
      "step": 2380
    },
    {
      "epoch": 0.07648,
      "grad_norm": 0.04956900700926781,
      "learning_rate": 9.61776e-06,
      "loss": 0.1998,
      "step": 2390
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.9590281248092651,
      "learning_rate": 9.616160000000002e-06,
      "loss": 0.2214,
      "step": 2400
    },
    {
      "epoch": 0.07712,
      "grad_norm": 0.03131728619337082,
      "learning_rate": 9.61456e-06,
      "loss": 0.1998,
      "step": 2410
    },
    {
      "epoch": 0.07744,
      "grad_norm": 0.02162615768611431,
      "learning_rate": 9.612960000000002e-06,
      "loss": 0.2036,
      "step": 2420
    },
    {
      "epoch": 0.07776,
      "grad_norm": 0.09437144547700882,
      "learning_rate": 9.611360000000001e-06,
      "loss": 0.2061,
      "step": 2430
    },
    {
      "epoch": 0.07808,
      "grad_norm": 0.21268098056316376,
      "learning_rate": 9.609760000000001e-06,
      "loss": 0.2197,
      "step": 2440
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.13543245196342468,
      "learning_rate": 9.608160000000001e-06,
      "loss": 0.2014,
      "step": 2450
    },
    {
      "epoch": 0.07872,
      "grad_norm": 0.10978122055530548,
      "learning_rate": 9.606560000000001e-06,
      "loss": 0.2061,
      "step": 2460
    },
    {
      "epoch": 0.07904,
      "grad_norm": 0.04087606444954872,
      "learning_rate": 9.604960000000001e-06,
      "loss": 0.2031,
      "step": 2470
    },
    {
      "epoch": 0.07936,
      "grad_norm": 0.018225831910967827,
      "learning_rate": 9.60336e-06,
      "loss": 0.2007,
      "step": 2480
    },
    {
      "epoch": 0.07968,
      "grad_norm": 0.03175780549645424,
      "learning_rate": 9.60176e-06,
      "loss": 0.2357,
      "step": 2490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.03628714755177498,
      "learning_rate": 9.60016e-06,
      "loss": 0.2046,
      "step": 2500
    },
    {
      "epoch": 0.08032,
      "grad_norm": 0.04242571443319321,
      "learning_rate": 9.59856e-06,
      "loss": 0.2063,
      "step": 2510
    },
    {
      "epoch": 0.08064,
      "grad_norm": 0.7655386328697205,
      "learning_rate": 9.59696e-06,
      "loss": 0.2165,
      "step": 2520
    },
    {
      "epoch": 0.08096,
      "grad_norm": 0.21508994698524475,
      "learning_rate": 9.59536e-06,
      "loss": 0.2009,
      "step": 2530
    },
    {
      "epoch": 0.08128,
      "grad_norm": 0.10282114148139954,
      "learning_rate": 9.59376e-06,
      "loss": 0.2009,
      "step": 2540
    },
    {
      "epoch": 0.0816,
      "grad_norm": 1.0908467769622803,
      "learning_rate": 9.59216e-06,
      "loss": 0.222,
      "step": 2550
    },
    {
      "epoch": 0.08192,
      "grad_norm": 0.05285560339689255,
      "learning_rate": 9.590560000000002e-06,
      "loss": 0.208,
      "step": 2560
    },
    {
      "epoch": 0.08224,
      "grad_norm": 0.05884011462330818,
      "learning_rate": 9.58896e-06,
      "loss": 0.2014,
      "step": 2570
    },
    {
      "epoch": 0.08256,
      "grad_norm": 0.03535221517086029,
      "learning_rate": 9.587360000000001e-06,
      "loss": 0.2063,
      "step": 2580
    },
    {
      "epoch": 0.08288,
      "grad_norm": 0.2206360250711441,
      "learning_rate": 9.585760000000001e-06,
      "loss": 0.2004,
      "step": 2590
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.046821653842926025,
      "learning_rate": 9.584160000000001e-06,
      "loss": 0.2039,
      "step": 2600
    },
    {
      "epoch": 0.08352,
      "grad_norm": 0.027020273730158806,
      "learning_rate": 9.582560000000001e-06,
      "loss": 0.2002,
      "step": 2610
    },
    {
      "epoch": 0.08384,
      "grad_norm": 0.10659585893154144,
      "learning_rate": 9.58096e-06,
      "loss": 0.2036,
      "step": 2620
    },
    {
      "epoch": 0.08416,
      "grad_norm": 0.7631987929344177,
      "learning_rate": 9.57936e-06,
      "loss": 0.2312,
      "step": 2630
    },
    {
      "epoch": 0.08448,
      "grad_norm": 0.05411718785762787,
      "learning_rate": 9.57776e-06,
      "loss": 0.204,
      "step": 2640
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.854601263999939,
      "learning_rate": 9.57616e-06,
      "loss": 0.2033,
      "step": 2650
    },
    {
      "epoch": 0.08512,
      "grad_norm": 0.07086345553398132,
      "learning_rate": 9.57456e-06,
      "loss": 0.2068,
      "step": 2660
    },
    {
      "epoch": 0.08544,
      "grad_norm": 2.086801290512085,
      "learning_rate": 9.572960000000002e-06,
      "loss": 0.2221,
      "step": 2670
    },
    {
      "epoch": 0.08576,
      "grad_norm": 0.03661193326115608,
      "learning_rate": 9.57136e-06,
      "loss": 0.2015,
      "step": 2680
    },
    {
      "epoch": 0.08608,
      "grad_norm": 0.05123550817370415,
      "learning_rate": 9.569760000000002e-06,
      "loss": 0.2048,
      "step": 2690
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.028043638914823532,
      "learning_rate": 9.568160000000002e-06,
      "loss": 0.2053,
      "step": 2700
    },
    {
      "epoch": 0.08672,
      "grad_norm": 0.24127225577831268,
      "learning_rate": 9.56656e-06,
      "loss": 0.2007,
      "step": 2710
    },
    {
      "epoch": 0.08704,
      "grad_norm": 1.5851905345916748,
      "learning_rate": 9.564960000000001e-06,
      "loss": 0.2189,
      "step": 2720
    },
    {
      "epoch": 0.08736,
      "grad_norm": 0.014819245785474777,
      "learning_rate": 9.563360000000001e-06,
      "loss": 0.2007,
      "step": 2730
    },
    {
      "epoch": 0.08768,
      "grad_norm": 0.07373855262994766,
      "learning_rate": 9.561760000000001e-06,
      "loss": 0.2281,
      "step": 2740
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.03947298228740692,
      "learning_rate": 9.560160000000001e-06,
      "loss": 0.2009,
      "step": 2750
    },
    {
      "epoch": 0.08832,
      "grad_norm": 0.03881335258483887,
      "learning_rate": 9.558560000000001e-06,
      "loss": 0.2001,
      "step": 2760
    },
    {
      "epoch": 0.08864,
      "grad_norm": 0.03844192624092102,
      "learning_rate": 9.55696e-06,
      "loss": 0.2298,
      "step": 2770
    },
    {
      "epoch": 0.08896,
      "grad_norm": 0.054982393980026245,
      "learning_rate": 9.55536e-06,
      "loss": 0.2063,
      "step": 2780
    },
    {
      "epoch": 0.08928,
      "grad_norm": 0.030924122780561447,
      "learning_rate": 9.55376e-06,
      "loss": 0.2,
      "step": 2790
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.4368175268173218,
      "learning_rate": 9.55216e-06,
      "loss": 0.2015,
      "step": 2800
    },
    {
      "epoch": 0.08992,
      "grad_norm": 0.028874369338154793,
      "learning_rate": 9.55056e-06,
      "loss": 0.2118,
      "step": 2810
    },
    {
      "epoch": 0.09024,
      "grad_norm": 0.0597529411315918,
      "learning_rate": 9.54896e-06,
      "loss": 0.2087,
      "step": 2820
    },
    {
      "epoch": 0.09056,
      "grad_norm": 0.02708190679550171,
      "learning_rate": 9.547360000000002e-06,
      "loss": 0.2026,
      "step": 2830
    },
    {
      "epoch": 0.09088,
      "grad_norm": 0.32797083258628845,
      "learning_rate": 9.54576e-06,
      "loss": 0.2022,
      "step": 2840
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.053379807621240616,
      "learning_rate": 9.544160000000001e-06,
      "loss": 0.2006,
      "step": 2850
    },
    {
      "epoch": 0.09152,
      "grad_norm": 0.25769221782684326,
      "learning_rate": 9.542560000000001e-06,
      "loss": 0.2105,
      "step": 2860
    },
    {
      "epoch": 0.09184,
      "grad_norm": 0.11981187015771866,
      "learning_rate": 9.540960000000001e-06,
      "loss": 0.2123,
      "step": 2870
    },
    {
      "epoch": 0.09216,
      "grad_norm": 0.06526552885770798,
      "learning_rate": 9.539360000000001e-06,
      "loss": 0.2045,
      "step": 2880
    },
    {
      "epoch": 0.09248,
      "grad_norm": 0.4021829068660736,
      "learning_rate": 9.537760000000001e-06,
      "loss": 0.2027,
      "step": 2890
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.06294393539428711,
      "learning_rate": 9.536160000000001e-06,
      "loss": 0.209,
      "step": 2900
    },
    {
      "epoch": 0.09312,
      "grad_norm": 0.07324199378490448,
      "learning_rate": 9.53456e-06,
      "loss": 0.2014,
      "step": 2910
    },
    {
      "epoch": 0.09344,
      "grad_norm": 0.03333302214741707,
      "learning_rate": 9.53296e-06,
      "loss": 0.2009,
      "step": 2920
    },
    {
      "epoch": 0.09376,
      "grad_norm": 0.02363278903067112,
      "learning_rate": 9.53136e-06,
      "loss": 0.2147,
      "step": 2930
    },
    {
      "epoch": 0.09408,
      "grad_norm": 0.022570185363292694,
      "learning_rate": 9.52976e-06,
      "loss": 0.2363,
      "step": 2940
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.03520102798938751,
      "learning_rate": 9.52816e-06,
      "loss": 0.2282,
      "step": 2950
    },
    {
      "epoch": 0.09472,
      "grad_norm": 0.06452064216136932,
      "learning_rate": 9.52656e-06,
      "loss": 0.2037,
      "step": 2960
    },
    {
      "epoch": 0.09504,
      "grad_norm": 1.4304499626159668,
      "learning_rate": 9.52496e-06,
      "loss": 0.2158,
      "step": 2970
    },
    {
      "epoch": 0.09536,
      "grad_norm": 0.026090160012245178,
      "learning_rate": 9.52336e-06,
      "loss": 0.2101,
      "step": 2980
    },
    {
      "epoch": 0.09568,
      "grad_norm": 0.03490987420082092,
      "learning_rate": 9.521760000000001e-06,
      "loss": 0.2123,
      "step": 2990
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.05156046897172928,
      "learning_rate": 9.52016e-06,
      "loss": 0.2009,
      "step": 3000
    },
    {
      "epoch": 0.096,
      "eval_runtime": 52.4629,
      "eval_samples_per_second": 190.611,
      "eval_steps_per_second": 11.913,
      "step": 3000
    },
    {
      "epoch": 0.09632,
      "grad_norm": 0.38358500599861145,
      "learning_rate": 9.518560000000001e-06,
      "loss": 0.2103,
      "step": 3010
    },
    {
      "epoch": 0.09664,
      "grad_norm": 0.030489766970276833,
      "learning_rate": 9.516960000000001e-06,
      "loss": 0.2085,
      "step": 3020
    },
    {
      "epoch": 0.09696,
      "grad_norm": 0.1948373168706894,
      "learning_rate": 9.515360000000001e-06,
      "loss": 0.2006,
      "step": 3030
    },
    {
      "epoch": 0.09728,
      "grad_norm": 0.04044917970895767,
      "learning_rate": 9.513760000000001e-06,
      "loss": 0.2183,
      "step": 3040
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.026114730164408684,
      "learning_rate": 9.51216e-06,
      "loss": 0.2081,
      "step": 3050
    },
    {
      "epoch": 0.09792,
      "grad_norm": 0.17103339731693268,
      "learning_rate": 9.51056e-06,
      "loss": 0.2005,
      "step": 3060
    },
    {
      "epoch": 0.09824,
      "grad_norm": 0.05964943394064903,
      "learning_rate": 9.50896e-06,
      "loss": 0.2004,
      "step": 3070
    },
    {
      "epoch": 0.09856,
      "grad_norm": 0.03607504814863205,
      "learning_rate": 9.50736e-06,
      "loss": 0.2028,
      "step": 3080
    },
    {
      "epoch": 0.09888,
      "grad_norm": 0.031820397824048996,
      "learning_rate": 9.50576e-06,
      "loss": 0.2172,
      "step": 3090
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.04093528911471367,
      "learning_rate": 9.504160000000002e-06,
      "loss": 0.2011,
      "step": 3100
    },
    {
      "epoch": 0.09952,
      "grad_norm": 0.035448748618364334,
      "learning_rate": 9.50256e-06,
      "loss": 0.2052,
      "step": 3110
    },
    {
      "epoch": 0.09984,
      "grad_norm": 0.036229457706213,
      "learning_rate": 9.500960000000002e-06,
      "loss": 0.2026,
      "step": 3120
    },
    {
      "epoch": 0.10016,
      "grad_norm": 0.0396597720682621,
      "learning_rate": 9.499360000000001e-06,
      "loss": 0.2036,
      "step": 3130
    },
    {
      "epoch": 0.10048,
      "grad_norm": 0.03992583230137825,
      "learning_rate": 9.49776e-06,
      "loss": 0.2048,
      "step": 3140
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.04626331478357315,
      "learning_rate": 9.496160000000001e-06,
      "loss": 0.2006,
      "step": 3150
    },
    {
      "epoch": 0.10112,
      "grad_norm": 0.11568070948123932,
      "learning_rate": 9.494560000000001e-06,
      "loss": 0.201,
      "step": 3160
    },
    {
      "epoch": 0.10144,
      "grad_norm": 1.2623798847198486,
      "learning_rate": 9.492960000000001e-06,
      "loss": 0.211,
      "step": 3170
    },
    {
      "epoch": 0.10176,
      "grad_norm": 0.13182400166988373,
      "learning_rate": 9.491360000000001e-06,
      "loss": 0.2113,
      "step": 3180
    },
    {
      "epoch": 0.10208,
      "grad_norm": 0.05932435393333435,
      "learning_rate": 9.48976e-06,
      "loss": 0.2051,
      "step": 3190
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.8529485464096069,
      "learning_rate": 9.48816e-06,
      "loss": 0.2385,
      "step": 3200
    },
    {
      "epoch": 0.10272,
      "grad_norm": 0.2758465111255646,
      "learning_rate": 9.48656e-06,
      "loss": 0.2096,
      "step": 3210
    },
    {
      "epoch": 0.10304,
      "grad_norm": 0.02268761210143566,
      "learning_rate": 9.48496e-06,
      "loss": 0.2022,
      "step": 3220
    },
    {
      "epoch": 0.10336,
      "grad_norm": 0.3398053050041199,
      "learning_rate": 9.48336e-06,
      "loss": 0.2042,
      "step": 3230
    },
    {
      "epoch": 0.10368,
      "grad_norm": 0.047729019075632095,
      "learning_rate": 9.48176e-06,
      "loss": 0.2111,
      "step": 3240
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.02729477733373642,
      "learning_rate": 9.48016e-06,
      "loss": 0.2017,
      "step": 3250
    },
    {
      "epoch": 0.10432,
      "grad_norm": 0.024674953892827034,
      "learning_rate": 9.478560000000002e-06,
      "loss": 0.2202,
      "step": 3260
    },
    {
      "epoch": 0.10464,
      "grad_norm": 1.125299334526062,
      "learning_rate": 9.47696e-06,
      "loss": 0.2114,
      "step": 3270
    },
    {
      "epoch": 0.10496,
      "grad_norm": 0.09878882020711899,
      "learning_rate": 9.475360000000001e-06,
      "loss": 0.2073,
      "step": 3280
    },
    {
      "epoch": 0.10528,
      "grad_norm": 0.06867758184671402,
      "learning_rate": 9.473760000000001e-06,
      "loss": 0.2015,
      "step": 3290
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.10576169192790985,
      "learning_rate": 9.47216e-06,
      "loss": 0.2043,
      "step": 3300
    },
    {
      "epoch": 0.10592,
      "grad_norm": 0.015627065673470497,
      "learning_rate": 9.470560000000001e-06,
      "loss": 0.2069,
      "step": 3310
    },
    {
      "epoch": 0.10624,
      "grad_norm": 0.09769342839717865,
      "learning_rate": 9.468960000000001e-06,
      "loss": 0.2155,
      "step": 3320
    },
    {
      "epoch": 0.10656,
      "grad_norm": 0.024162882938981056,
      "learning_rate": 9.46736e-06,
      "loss": 0.2176,
      "step": 3330
    },
    {
      "epoch": 0.10688,
      "grad_norm": 0.025672229006886482,
      "learning_rate": 9.46576e-06,
      "loss": 0.1997,
      "step": 3340
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.05036315694451332,
      "learning_rate": 9.46416e-06,
      "loss": 0.2076,
      "step": 3350
    },
    {
      "epoch": 0.10752,
      "grad_norm": 0.021590275689959526,
      "learning_rate": 9.46256e-06,
      "loss": 0.2029,
      "step": 3360
    },
    {
      "epoch": 0.10784,
      "grad_norm": 0.17042115330696106,
      "learning_rate": 9.460960000000002e-06,
      "loss": 0.2005,
      "step": 3370
    },
    {
      "epoch": 0.10816,
      "grad_norm": 0.022433964535593987,
      "learning_rate": 9.45936e-06,
      "loss": 0.1995,
      "step": 3380
    },
    {
      "epoch": 0.10848,
      "grad_norm": 0.02025977149605751,
      "learning_rate": 9.45776e-06,
      "loss": 0.2009,
      "step": 3390
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.07737642526626587,
      "learning_rate": 9.456160000000002e-06,
      "loss": 0.2012,
      "step": 3400
    },
    {
      "epoch": 0.10912,
      "grad_norm": 0.030636226758360863,
      "learning_rate": 9.45456e-06,
      "loss": 0.2014,
      "step": 3410
    },
    {
      "epoch": 0.10944,
      "grad_norm": 0.18513832986354828,
      "learning_rate": 9.452960000000001e-06,
      "loss": 0.215,
      "step": 3420
    },
    {
      "epoch": 0.10976,
      "grad_norm": 0.027425484731793404,
      "learning_rate": 9.451360000000001e-06,
      "loss": 0.2236,
      "step": 3430
    },
    {
      "epoch": 0.11008,
      "grad_norm": 0.037247028201818466,
      "learning_rate": 9.449760000000001e-06,
      "loss": 0.2172,
      "step": 3440
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.051554568111896515,
      "learning_rate": 9.448160000000001e-06,
      "loss": 0.2041,
      "step": 3450
    },
    {
      "epoch": 0.11072,
      "grad_norm": 0.10929406434297562,
      "learning_rate": 9.446560000000001e-06,
      "loss": 0.2196,
      "step": 3460
    },
    {
      "epoch": 0.11104,
      "grad_norm": 0.05460644140839577,
      "learning_rate": 9.44496e-06,
      "loss": 0.2008,
      "step": 3470
    },
    {
      "epoch": 0.11136,
      "grad_norm": 0.046413734555244446,
      "learning_rate": 9.44336e-06,
      "loss": 0.2,
      "step": 3480
    },
    {
      "epoch": 0.11168,
      "grad_norm": 0.03037809021770954,
      "learning_rate": 9.44176e-06,
      "loss": 0.2271,
      "step": 3490
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.5084421038627625,
      "learning_rate": 9.44016e-06,
      "loss": 0.2059,
      "step": 3500
    },
    {
      "epoch": 0.11232,
      "grad_norm": 0.03571571037173271,
      "learning_rate": 9.43856e-06,
      "loss": 0.2105,
      "step": 3510
    },
    {
      "epoch": 0.11264,
      "grad_norm": 0.17268207669258118,
      "learning_rate": 9.43696e-06,
      "loss": 0.2098,
      "step": 3520
    },
    {
      "epoch": 0.11296,
      "grad_norm": 0.03529130667448044,
      "learning_rate": 9.435360000000002e-06,
      "loss": 0.2011,
      "step": 3530
    },
    {
      "epoch": 0.11328,
      "grad_norm": 0.024855615571141243,
      "learning_rate": 9.43376e-06,
      "loss": 0.2015,
      "step": 3540
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.022320497781038284,
      "learning_rate": 9.432160000000002e-06,
      "loss": 0.2013,
      "step": 3550
    },
    {
      "epoch": 0.11392,
      "grad_norm": 0.1652650237083435,
      "learning_rate": 9.430560000000001e-06,
      "loss": 0.2,
      "step": 3560
    },
    {
      "epoch": 0.11424,
      "grad_norm": 0.33685940504074097,
      "learning_rate": 9.42896e-06,
      "loss": 0.201,
      "step": 3570
    },
    {
      "epoch": 0.11456,
      "grad_norm": 0.1026301383972168,
      "learning_rate": 9.427360000000001e-06,
      "loss": 0.203,
      "step": 3580
    },
    {
      "epoch": 0.11488,
      "grad_norm": 0.04404614120721817,
      "learning_rate": 9.425760000000001e-06,
      "loss": 0.2008,
      "step": 3590
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.034850314259529114,
      "learning_rate": 9.424160000000001e-06,
      "loss": 0.2117,
      "step": 3600
    },
    {
      "epoch": 0.11552,
      "grad_norm": 0.03243733569979668,
      "learning_rate": 9.42256e-06,
      "loss": 0.2024,
      "step": 3610
    },
    {
      "epoch": 0.11584,
      "grad_norm": 0.021822648122906685,
      "learning_rate": 9.42096e-06,
      "loss": 0.2118,
      "step": 3620
    },
    {
      "epoch": 0.11616,
      "grad_norm": 0.9334460496902466,
      "learning_rate": 9.41936e-06,
      "loss": 0.2131,
      "step": 3630
    },
    {
      "epoch": 0.11648,
      "grad_norm": 0.04133696109056473,
      "learning_rate": 9.41776e-06,
      "loss": 0.202,
      "step": 3640
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.10026032477617264,
      "learning_rate": 9.41616e-06,
      "loss": 0.2011,
      "step": 3650
    },
    {
      "epoch": 0.11712,
      "grad_norm": 0.03566240146756172,
      "learning_rate": 9.41456e-06,
      "loss": 0.2006,
      "step": 3660
    },
    {
      "epoch": 0.11744,
      "grad_norm": 0.028559615835547447,
      "learning_rate": 9.412960000000002e-06,
      "loss": 0.2,
      "step": 3670
    },
    {
      "epoch": 0.11776,
      "grad_norm": 0.06540734320878983,
      "learning_rate": 9.41136e-06,
      "loss": 0.2126,
      "step": 3680
    },
    {
      "epoch": 0.11808,
      "grad_norm": 0.07988737523555756,
      "learning_rate": 9.409760000000002e-06,
      "loss": 0.2005,
      "step": 3690
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.04876966401934624,
      "learning_rate": 9.408160000000001e-06,
      "loss": 0.2089,
      "step": 3700
    },
    {
      "epoch": 0.11872,
      "grad_norm": 0.042669206857681274,
      "learning_rate": 9.406560000000001e-06,
      "loss": 0.2195,
      "step": 3710
    },
    {
      "epoch": 0.11904,
      "grad_norm": 0.046064842492341995,
      "learning_rate": 9.404960000000001e-06,
      "loss": 0.2149,
      "step": 3720
    },
    {
      "epoch": 0.11936,
      "grad_norm": 0.015510139055550098,
      "learning_rate": 9.403360000000001e-06,
      "loss": 0.2023,
      "step": 3730
    },
    {
      "epoch": 0.11968,
      "grad_norm": 0.020685730502009392,
      "learning_rate": 9.401760000000001e-06,
      "loss": 0.2037,
      "step": 3740
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.04174621030688286,
      "learning_rate": 9.40016e-06,
      "loss": 0.2006,
      "step": 3750
    },
    {
      "epoch": 0.12032,
      "grad_norm": 0.05112943798303604,
      "learning_rate": 9.39856e-06,
      "loss": 0.2005,
      "step": 3760
    },
    {
      "epoch": 0.12064,
      "grad_norm": 0.7362382411956787,
      "learning_rate": 9.39696e-06,
      "loss": 0.2022,
      "step": 3770
    },
    {
      "epoch": 0.12096,
      "grad_norm": 0.04138370230793953,
      "learning_rate": 9.39536e-06,
      "loss": 0.2014,
      "step": 3780
    },
    {
      "epoch": 0.12128,
      "grad_norm": 0.05956019088625908,
      "learning_rate": 9.39376e-06,
      "loss": 0.2104,
      "step": 3790
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.12702657282352448,
      "learning_rate": 9.392160000000002e-06,
      "loss": 0.2004,
      "step": 3800
    },
    {
      "epoch": 0.12192,
      "grad_norm": 0.04235279560089111,
      "learning_rate": 9.39056e-06,
      "loss": 0.2005,
      "step": 3810
    },
    {
      "epoch": 0.12224,
      "grad_norm": 0.029644953086972237,
      "learning_rate": 9.38896e-06,
      "loss": 0.2006,
      "step": 3820
    },
    {
      "epoch": 0.12256,
      "grad_norm": 0.04382853955030441,
      "learning_rate": 9.387360000000002e-06,
      "loss": 0.2155,
      "step": 3830
    },
    {
      "epoch": 0.12288,
      "grad_norm": 0.019566979259252548,
      "learning_rate": 9.38576e-06,
      "loss": 0.2047,
      "step": 3840
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.04554476588964462,
      "learning_rate": 9.384160000000001e-06,
      "loss": 0.1995,
      "step": 3850
    },
    {
      "epoch": 0.12352,
      "grad_norm": 0.027083968743681908,
      "learning_rate": 9.382560000000001e-06,
      "loss": 0.205,
      "step": 3860
    },
    {
      "epoch": 0.12384,
      "grad_norm": 0.03686343505978584,
      "learning_rate": 9.380960000000001e-06,
      "loss": 0.2028,
      "step": 3870
    },
    {
      "epoch": 0.12416,
      "grad_norm": 0.030292516574263573,
      "learning_rate": 9.379360000000001e-06,
      "loss": 0.2019,
      "step": 3880
    },
    {
      "epoch": 0.12448,
      "grad_norm": 0.022871484979987144,
      "learning_rate": 9.37776e-06,
      "loss": 0.2143,
      "step": 3890
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.02783011831343174,
      "learning_rate": 9.37616e-06,
      "loss": 0.2006,
      "step": 3900
    },
    {
      "epoch": 0.12512,
      "grad_norm": 0.13706305623054504,
      "learning_rate": 9.37456e-06,
      "loss": 0.2009,
      "step": 3910
    },
    {
      "epoch": 0.12544,
      "grad_norm": 0.03576076775789261,
      "learning_rate": 9.37296e-06,
      "loss": 0.2161,
      "step": 3920
    },
    {
      "epoch": 0.12576,
      "grad_norm": 0.03657717630267143,
      "learning_rate": 9.37136e-06,
      "loss": 0.2012,
      "step": 3930
    },
    {
      "epoch": 0.12608,
      "grad_norm": 0.04175899177789688,
      "learning_rate": 9.369760000000002e-06,
      "loss": 0.2094,
      "step": 3940
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.02535965107381344,
      "learning_rate": 9.36816e-06,
      "loss": 0.2021,
      "step": 3950
    },
    {
      "epoch": 0.12672,
      "grad_norm": 0.040489643812179565,
      "learning_rate": 9.366560000000002e-06,
      "loss": 0.2,
      "step": 3960
    },
    {
      "epoch": 0.12704,
      "grad_norm": 0.031108146533370018,
      "learning_rate": 9.364960000000002e-06,
      "loss": 0.2031,
      "step": 3970
    },
    {
      "epoch": 0.12736,
      "grad_norm": 0.02833913452923298,
      "learning_rate": 9.36336e-06,
      "loss": 0.2057,
      "step": 3980
    },
    {
      "epoch": 0.12768,
      "grad_norm": 0.09500463306903839,
      "learning_rate": 9.361760000000001e-06,
      "loss": 0.2246,
      "step": 3990
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.10459057986736298,
      "learning_rate": 9.360160000000001e-06,
      "loss": 0.2084,
      "step": 4000
    },
    {
      "epoch": 0.128,
      "eval_runtime": 51.5508,
      "eval_samples_per_second": 193.984,
      "eval_steps_per_second": 12.124,
      "step": 4000
    },
    {
      "epoch": 0.12832,
      "grad_norm": 0.16051378846168518,
      "learning_rate": 9.358560000000001e-06,
      "loss": 0.2006,
      "step": 4010
    },
    {
      "epoch": 0.12864,
      "grad_norm": 0.04377879202365875,
      "learning_rate": 9.356960000000001e-06,
      "loss": 0.2007,
      "step": 4020
    },
    {
      "epoch": 0.12896,
      "grad_norm": 0.03030584752559662,
      "learning_rate": 9.35536e-06,
      "loss": 0.2006,
      "step": 4030
    },
    {
      "epoch": 0.12928,
      "grad_norm": 0.060176510363817215,
      "learning_rate": 9.35376e-06,
      "loss": 0.201,
      "step": 4040
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.013841303996741772,
      "learning_rate": 9.35216e-06,
      "loss": 0.2014,
      "step": 4050
    },
    {
      "epoch": 0.12992,
      "grad_norm": 0.021890174597501755,
      "learning_rate": 9.35056e-06,
      "loss": 0.2004,
      "step": 4060
    },
    {
      "epoch": 0.13024,
      "grad_norm": 0.026628278195858,
      "learning_rate": 9.34896e-06,
      "loss": 0.1999,
      "step": 4070
    },
    {
      "epoch": 0.13056,
      "grad_norm": 0.10358383506536484,
      "learning_rate": 9.34736e-06,
      "loss": 0.2226,
      "step": 4080
    },
    {
      "epoch": 0.13088,
      "grad_norm": 0.03744886443018913,
      "learning_rate": 9.34576e-06,
      "loss": 0.2142,
      "step": 4090
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.17345045506954193,
      "learning_rate": 9.344160000000002e-06,
      "loss": 0.2005,
      "step": 4100
    },
    {
      "epoch": 0.13152,
      "grad_norm": 0.031414054334163666,
      "learning_rate": 9.34256e-06,
      "loss": 0.2,
      "step": 4110
    },
    {
      "epoch": 0.13184,
      "grad_norm": 0.03133522346615791,
      "learning_rate": 9.340960000000001e-06,
      "loss": 0.1996,
      "step": 4120
    },
    {
      "epoch": 0.13216,
      "grad_norm": 0.014612285420298576,
      "learning_rate": 9.339360000000001e-06,
      "loss": 0.2069,
      "step": 4130
    },
    {
      "epoch": 0.13248,
      "grad_norm": 0.01904108375310898,
      "learning_rate": 9.337760000000001e-06,
      "loss": 0.1996,
      "step": 4140
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.23030398786067963,
      "learning_rate": 9.336160000000001e-06,
      "loss": 0.2016,
      "step": 4150
    },
    {
      "epoch": 0.13312,
      "grad_norm": 0.021882595494389534,
      "learning_rate": 9.334560000000001e-06,
      "loss": 0.2046,
      "step": 4160
    },
    {
      "epoch": 0.13344,
      "grad_norm": 0.022451000288128853,
      "learning_rate": 9.332960000000001e-06,
      "loss": 0.2067,
      "step": 4170
    },
    {
      "epoch": 0.13376,
      "grad_norm": 0.022983001545071602,
      "learning_rate": 9.33136e-06,
      "loss": 0.2191,
      "step": 4180
    },
    {
      "epoch": 0.13408,
      "grad_norm": 0.21545787155628204,
      "learning_rate": 9.32976e-06,
      "loss": 0.219,
      "step": 4190
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.047071728855371475,
      "learning_rate": 9.32816e-06,
      "loss": 0.2013,
      "step": 4200
    },
    {
      "epoch": 0.13472,
      "grad_norm": 0.024068104103207588,
      "learning_rate": 9.32656e-06,
      "loss": 0.2381,
      "step": 4210
    },
    {
      "epoch": 0.13504,
      "grad_norm": 0.02413974143564701,
      "learning_rate": 9.32496e-06,
      "loss": 0.2028,
      "step": 4220
    },
    {
      "epoch": 0.13536,
      "grad_norm": 0.03311595693230629,
      "learning_rate": 9.323360000000002e-06,
      "loss": 0.2076,
      "step": 4230
    },
    {
      "epoch": 0.13568,
      "grad_norm": 0.8304014205932617,
      "learning_rate": 9.32176e-06,
      "loss": 0.2024,
      "step": 4240
    },
    {
      "epoch": 0.136,
      "grad_norm": 1.3628337383270264,
      "learning_rate": 9.32016e-06,
      "loss": 0.2163,
      "step": 4250
    },
    {
      "epoch": 0.13632,
      "grad_norm": 0.04553357511758804,
      "learning_rate": 9.318560000000001e-06,
      "loss": 0.2177,
      "step": 4260
    },
    {
      "epoch": 0.13664,
      "grad_norm": 0.07507701963186264,
      "learning_rate": 9.31696e-06,
      "loss": 0.2015,
      "step": 4270
    },
    {
      "epoch": 0.13696,
      "grad_norm": 0.025422343984246254,
      "learning_rate": 9.315360000000001e-06,
      "loss": 0.2,
      "step": 4280
    },
    {
      "epoch": 0.13728,
      "grad_norm": 0.07192689180374146,
      "learning_rate": 9.313760000000001e-06,
      "loss": 0.2048,
      "step": 4290
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.03475669398903847,
      "learning_rate": 9.312160000000001e-06,
      "loss": 0.2218,
      "step": 4300
    },
    {
      "epoch": 0.13792,
      "grad_norm": 0.02529713325202465,
      "learning_rate": 9.310560000000001e-06,
      "loss": 0.206,
      "step": 4310
    },
    {
      "epoch": 0.13824,
      "grad_norm": 0.03185316175222397,
      "learning_rate": 9.30896e-06,
      "loss": 0.2037,
      "step": 4320
    },
    {
      "epoch": 0.13856,
      "grad_norm": 0.025501912459731102,
      "learning_rate": 9.30736e-06,
      "loss": 0.2011,
      "step": 4330
    },
    {
      "epoch": 0.13888,
      "grad_norm": 0.6590728759765625,
      "learning_rate": 9.30576e-06,
      "loss": 0.2023,
      "step": 4340
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.5154156684875488,
      "learning_rate": 9.30416e-06,
      "loss": 0.2135,
      "step": 4350
    },
    {
      "epoch": 0.13952,
      "grad_norm": 0.07339571416378021,
      "learning_rate": 9.30256e-06,
      "loss": 0.1998,
      "step": 4360
    },
    {
      "epoch": 0.13984,
      "grad_norm": 0.023745397105813026,
      "learning_rate": 9.300960000000002e-06,
      "loss": 0.2035,
      "step": 4370
    },
    {
      "epoch": 0.14016,
      "grad_norm": 0.05023600906133652,
      "learning_rate": 9.29936e-06,
      "loss": 0.2055,
      "step": 4380
    },
    {
      "epoch": 0.14048,
      "grad_norm": 0.0258554108440876,
      "learning_rate": 9.297760000000002e-06,
      "loss": 0.1999,
      "step": 4390
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.030154114589095116,
      "learning_rate": 9.296160000000001e-06,
      "loss": 0.2075,
      "step": 4400
    },
    {
      "epoch": 0.14112,
      "grad_norm": 0.048760220408439636,
      "learning_rate": 9.29456e-06,
      "loss": 0.2502,
      "step": 4410
    },
    {
      "epoch": 0.14144,
      "grad_norm": 0.055055100470781326,
      "learning_rate": 9.292960000000001e-06,
      "loss": 0.2029,
      "step": 4420
    },
    {
      "epoch": 0.14176,
      "grad_norm": 0.37320470809936523,
      "learning_rate": 9.291360000000001e-06,
      "loss": 0.201,
      "step": 4430
    },
    {
      "epoch": 0.14208,
      "grad_norm": 0.027169452980160713,
      "learning_rate": 9.289760000000001e-06,
      "loss": 0.2277,
      "step": 4440
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.05881790816783905,
      "learning_rate": 9.288160000000001e-06,
      "loss": 0.2021,
      "step": 4450
    },
    {
      "epoch": 0.14272,
      "grad_norm": 0.0367683544754982,
      "learning_rate": 9.28656e-06,
      "loss": 0.2283,
      "step": 4460
    },
    {
      "epoch": 0.14304,
      "grad_norm": 0.05609859898686409,
      "learning_rate": 9.28496e-06,
      "loss": 0.2017,
      "step": 4470
    },
    {
      "epoch": 0.14336,
      "grad_norm": 0.058214157819747925,
      "learning_rate": 9.28336e-06,
      "loss": 0.2044,
      "step": 4480
    },
    {
      "epoch": 0.14368,
      "grad_norm": 0.041554391384124756,
      "learning_rate": 9.28176e-06,
      "loss": 0.209,
      "step": 4490
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.4140273332595825,
      "learning_rate": 9.28016e-06,
      "loss": 0.2032,
      "step": 4500
    },
    {
      "epoch": 0.14432,
      "grad_norm": 0.02343597821891308,
      "learning_rate": 9.27856e-06,
      "loss": 0.2155,
      "step": 4510
    },
    {
      "epoch": 0.14464,
      "grad_norm": 0.03727685660123825,
      "learning_rate": 9.27696e-06,
      "loss": 0.2109,
      "step": 4520
    },
    {
      "epoch": 0.14496,
      "grad_norm": 0.030558105558156967,
      "learning_rate": 9.275360000000002e-06,
      "loss": 0.2019,
      "step": 4530
    },
    {
      "epoch": 0.14528,
      "grad_norm": 0.028837624937295914,
      "learning_rate": 9.27376e-06,
      "loss": 0.2006,
      "step": 4540
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.017092125490307808,
      "learning_rate": 9.272160000000001e-06,
      "loss": 0.2188,
      "step": 4550
    },
    {
      "epoch": 0.14592,
      "grad_norm": 0.050605084747076035,
      "learning_rate": 9.270560000000001e-06,
      "loss": 0.2089,
      "step": 4560
    },
    {
      "epoch": 0.14624,
      "grad_norm": 0.04192711040377617,
      "learning_rate": 9.268960000000001e-06,
      "loss": 0.2023,
      "step": 4570
    },
    {
      "epoch": 0.14656,
      "grad_norm": 0.12895530462265015,
      "learning_rate": 9.267360000000001e-06,
      "loss": 0.2062,
      "step": 4580
    },
    {
      "epoch": 0.14688,
      "grad_norm": 0.015979215502738953,
      "learning_rate": 9.265760000000001e-06,
      "loss": 0.2017,
      "step": 4590
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.04565514996647835,
      "learning_rate": 9.26416e-06,
      "loss": 0.2003,
      "step": 4600
    },
    {
      "epoch": 0.14752,
      "grad_norm": 1.2815771102905273,
      "learning_rate": 9.26256e-06,
      "loss": 0.2033,
      "step": 4610
    },
    {
      "epoch": 0.14784,
      "grad_norm": 0.017197849228978157,
      "learning_rate": 9.26096e-06,
      "loss": 0.2001,
      "step": 4620
    },
    {
      "epoch": 0.14816,
      "grad_norm": 0.05990368500351906,
      "learning_rate": 9.25936e-06,
      "loss": 0.2174,
      "step": 4630
    },
    {
      "epoch": 0.14848,
      "grad_norm": 0.053036127239465714,
      "learning_rate": 9.257760000000002e-06,
      "loss": 0.2025,
      "step": 4640
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.03763894364237785,
      "learning_rate": 9.25616e-06,
      "loss": 0.2134,
      "step": 4650
    },
    {
      "epoch": 0.14912,
      "grad_norm": 0.06460879743099213,
      "learning_rate": 9.25456e-06,
      "loss": 0.2018,
      "step": 4660
    },
    {
      "epoch": 0.14944,
      "grad_norm": 0.016924604773521423,
      "learning_rate": 9.252960000000002e-06,
      "loss": 0.2049,
      "step": 4670
    },
    {
      "epoch": 0.14976,
      "grad_norm": 0.0370369590818882,
      "learning_rate": 9.25136e-06,
      "loss": 0.2181,
      "step": 4680
    },
    {
      "epoch": 0.15008,
      "grad_norm": 0.21504046022891998,
      "learning_rate": 9.249760000000001e-06,
      "loss": 0.2025,
      "step": 4690
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.022939765825867653,
      "learning_rate": 9.248160000000001e-06,
      "loss": 0.2041,
      "step": 4700
    },
    {
      "epoch": 0.15072,
      "grad_norm": 0.029251324012875557,
      "learning_rate": 9.246560000000001e-06,
      "loss": 0.201,
      "step": 4710
    },
    {
      "epoch": 0.15104,
      "grad_norm": 0.028234167024493217,
      "learning_rate": 9.244960000000001e-06,
      "loss": 0.2048,
      "step": 4720
    },
    {
      "epoch": 0.15136,
      "grad_norm": 0.02303302474319935,
      "learning_rate": 9.243360000000001e-06,
      "loss": 0.1998,
      "step": 4730
    },
    {
      "epoch": 0.15168,
      "grad_norm": 0.024756375700235367,
      "learning_rate": 9.24176e-06,
      "loss": 0.2085,
      "step": 4740
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.053296055644750595,
      "learning_rate": 9.24016e-06,
      "loss": 0.2004,
      "step": 4750
    },
    {
      "epoch": 0.15232,
      "grad_norm": 0.03879344463348389,
      "learning_rate": 9.23856e-06,
      "loss": 0.2028,
      "step": 4760
    },
    {
      "epoch": 0.15264,
      "grad_norm": 0.028179196640849113,
      "learning_rate": 9.23696e-06,
      "loss": 0.2162,
      "step": 4770
    },
    {
      "epoch": 0.15296,
      "grad_norm": 0.04245305061340332,
      "learning_rate": 9.23536e-06,
      "loss": 0.1998,
      "step": 4780
    },
    {
      "epoch": 0.15328,
      "grad_norm": 0.03633202612400055,
      "learning_rate": 9.23376e-06,
      "loss": 0.1997,
      "step": 4790
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.060278505086898804,
      "learning_rate": 9.232160000000002e-06,
      "loss": 0.2347,
      "step": 4800
    },
    {
      "epoch": 0.15392,
      "grad_norm": 0.06424904614686966,
      "learning_rate": 9.23056e-06,
      "loss": 0.2102,
      "step": 4810
    },
    {
      "epoch": 0.15424,
      "grad_norm": 0.021167581900954247,
      "learning_rate": 9.228960000000002e-06,
      "loss": 0.1994,
      "step": 4820
    },
    {
      "epoch": 0.15456,
      "grad_norm": 0.059676915407180786,
      "learning_rate": 9.227360000000001e-06,
      "loss": 0.2095,
      "step": 4830
    },
    {
      "epoch": 0.15488,
      "grad_norm": 0.01669364422559738,
      "learning_rate": 9.22576e-06,
      "loss": 0.2326,
      "step": 4840
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.03848804906010628,
      "learning_rate": 9.224160000000001e-06,
      "loss": 0.2004,
      "step": 4850
    },
    {
      "epoch": 0.15552,
      "grad_norm": 0.04430073872208595,
      "learning_rate": 9.222560000000001e-06,
      "loss": 0.1995,
      "step": 4860
    },
    {
      "epoch": 0.15584,
      "grad_norm": 0.04343534633517265,
      "learning_rate": 9.220960000000001e-06,
      "loss": 0.2,
      "step": 4870
    },
    {
      "epoch": 0.15616,
      "grad_norm": 0.16201063990592957,
      "learning_rate": 9.21936e-06,
      "loss": 0.224,
      "step": 4880
    },
    {
      "epoch": 0.15648,
      "grad_norm": 0.013477910310029984,
      "learning_rate": 9.21776e-06,
      "loss": 0.201,
      "step": 4890
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.015656515955924988,
      "learning_rate": 9.21616e-06,
      "loss": 0.2315,
      "step": 4900
    },
    {
      "epoch": 0.15712,
      "grad_norm": 0.05208176374435425,
      "learning_rate": 9.214560000000002e-06,
      "loss": 0.2004,
      "step": 4910
    },
    {
      "epoch": 0.15744,
      "grad_norm": 0.032959599047899246,
      "learning_rate": 9.21296e-06,
      "loss": 0.1996,
      "step": 4920
    },
    {
      "epoch": 0.15776,
      "grad_norm": 0.030765142291784286,
      "learning_rate": 9.21136e-06,
      "loss": 0.2033,
      "step": 4930
    },
    {
      "epoch": 0.15808,
      "grad_norm": 0.024349728599190712,
      "learning_rate": 9.209760000000002e-06,
      "loss": 0.2043,
      "step": 4940
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.9341525435447693,
      "learning_rate": 9.20816e-06,
      "loss": 0.2006,
      "step": 4950
    },
    {
      "epoch": 0.15872,
      "grad_norm": 0.04595784470438957,
      "learning_rate": 9.206560000000002e-06,
      "loss": 0.2023,
      "step": 4960
    },
    {
      "epoch": 0.15904,
      "grad_norm": 0.05770380422472954,
      "learning_rate": 9.204960000000001e-06,
      "loss": 0.1996,
      "step": 4970
    },
    {
      "epoch": 0.15936,
      "grad_norm": 0.11932555586099625,
      "learning_rate": 9.203360000000001e-06,
      "loss": 0.2006,
      "step": 4980
    },
    {
      "epoch": 0.15968,
      "grad_norm": 0.029431674629449844,
      "learning_rate": 9.201760000000001e-06,
      "loss": 0.2161,
      "step": 4990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.0867200642824173,
      "learning_rate": 9.200160000000001e-06,
      "loss": 0.2083,
      "step": 5000
    },
    {
      "epoch": 0.16,
      "eval_runtime": 53.0048,
      "eval_samples_per_second": 188.662,
      "eval_steps_per_second": 11.791,
      "step": 5000
    },
    {
      "epoch": 0.16032,
      "grad_norm": 0.02696770615875721,
      "learning_rate": 9.198560000000001e-06,
      "loss": 0.2016,
      "step": 5010
    },
    {
      "epoch": 0.16064,
      "grad_norm": 0.0910327211022377,
      "learning_rate": 9.19696e-06,
      "loss": 0.1998,
      "step": 5020
    },
    {
      "epoch": 0.16096,
      "grad_norm": 0.022707020863890648,
      "learning_rate": 9.19536e-06,
      "loss": 0.1997,
      "step": 5030
    },
    {
      "epoch": 0.16128,
      "grad_norm": 0.01914074458181858,
      "learning_rate": 9.19376e-06,
      "loss": 0.2002,
      "step": 5040
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.015078859403729439,
      "learning_rate": 9.19216e-06,
      "loss": 0.1999,
      "step": 5050
    },
    {
      "epoch": 0.16192,
      "grad_norm": 0.04523082077503204,
      "learning_rate": 9.19056e-06,
      "loss": 0.1998,
      "step": 5060
    },
    {
      "epoch": 0.16224,
      "grad_norm": 0.032420773059129715,
      "learning_rate": 9.188960000000002e-06,
      "loss": 0.208,
      "step": 5070
    },
    {
      "epoch": 0.16256,
      "grad_norm": 0.039906907826662064,
      "learning_rate": 9.18736e-06,
      "loss": 0.2,
      "step": 5080
    },
    {
      "epoch": 0.16288,
      "grad_norm": 0.04314257577061653,
      "learning_rate": 9.18576e-06,
      "loss": 0.1996,
      "step": 5090
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.01715928502380848,
      "learning_rate": 9.184160000000002e-06,
      "loss": 0.2167,
      "step": 5100
    },
    {
      "epoch": 0.16352,
      "grad_norm": 0.03906736895442009,
      "learning_rate": 9.18256e-06,
      "loss": 0.2001,
      "step": 5110
    },
    {
      "epoch": 0.16384,
      "grad_norm": 0.033029280602931976,
      "learning_rate": 9.180960000000001e-06,
      "loss": 0.2264,
      "step": 5120
    },
    {
      "epoch": 0.16416,
      "grad_norm": 0.04093272611498833,
      "learning_rate": 9.179360000000001e-06,
      "loss": 0.2011,
      "step": 5130
    },
    {
      "epoch": 0.16448,
      "grad_norm": 0.9221647381782532,
      "learning_rate": 9.177760000000001e-06,
      "loss": 0.236,
      "step": 5140
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.03737794607877731,
      "learning_rate": 9.176160000000001e-06,
      "loss": 0.2144,
      "step": 5150
    },
    {
      "epoch": 0.16512,
      "grad_norm": 0.08649598062038422,
      "learning_rate": 9.17456e-06,
      "loss": 0.201,
      "step": 5160
    },
    {
      "epoch": 0.16544,
      "grad_norm": 0.6462164521217346,
      "learning_rate": 9.17296e-06,
      "loss": 0.2019,
      "step": 5170
    },
    {
      "epoch": 0.16576,
      "grad_norm": 0.07341955602169037,
      "learning_rate": 9.17136e-06,
      "loss": 0.2,
      "step": 5180
    },
    {
      "epoch": 0.16608,
      "grad_norm": 0.03896484151482582,
      "learning_rate": 9.16976e-06,
      "loss": 0.2435,
      "step": 5190
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.03853299468755722,
      "learning_rate": 9.16816e-06,
      "loss": 0.2007,
      "step": 5200
    },
    {
      "epoch": 0.16672,
      "grad_norm": 0.0636252909898758,
      "learning_rate": 9.16656e-06,
      "loss": 0.2004,
      "step": 5210
    },
    {
      "epoch": 0.16704,
      "grad_norm": 0.027315407991409302,
      "learning_rate": 9.16496e-06,
      "loss": 0.1994,
      "step": 5220
    },
    {
      "epoch": 0.16736,
      "grad_norm": 0.03965302184224129,
      "learning_rate": 9.163360000000002e-06,
      "loss": 0.2159,
      "step": 5230
    },
    {
      "epoch": 0.16768,
      "grad_norm": 2.385559558868408,
      "learning_rate": 9.16176e-06,
      "loss": 0.2222,
      "step": 5240
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.03863179311156273,
      "learning_rate": 9.160160000000001e-06,
      "loss": 0.2007,
      "step": 5250
    },
    {
      "epoch": 0.16832,
      "grad_norm": 0.8764930367469788,
      "learning_rate": 9.158560000000001e-06,
      "loss": 0.2168,
      "step": 5260
    },
    {
      "epoch": 0.16864,
      "grad_norm": 0.0417182520031929,
      "learning_rate": 9.15696e-06,
      "loss": 0.2024,
      "step": 5270
    },
    {
      "epoch": 0.16896,
      "grad_norm": 1.7989037036895752,
      "learning_rate": 9.155360000000001e-06,
      "loss": 0.2094,
      "step": 5280
    },
    {
      "epoch": 0.16928,
      "grad_norm": 0.6813876032829285,
      "learning_rate": 9.153760000000001e-06,
      "loss": 0.2159,
      "step": 5290
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.02873212844133377,
      "learning_rate": 9.15216e-06,
      "loss": 0.2036,
      "step": 5300
    },
    {
      "epoch": 0.16992,
      "grad_norm": 0.06396017968654633,
      "learning_rate": 9.15056e-06,
      "loss": 0.1995,
      "step": 5310
    },
    {
      "epoch": 0.17024,
      "grad_norm": 0.02473650500178337,
      "learning_rate": 9.14896e-06,
      "loss": 0.201,
      "step": 5320
    },
    {
      "epoch": 0.17056,
      "grad_norm": 0.13795994222164154,
      "learning_rate": 9.14736e-06,
      "loss": 0.1997,
      "step": 5330
    },
    {
      "epoch": 0.17088,
      "grad_norm": 0.04986606538295746,
      "learning_rate": 9.14576e-06,
      "loss": 0.2012,
      "step": 5340
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.056602396070957184,
      "learning_rate": 9.14416e-06,
      "loss": 0.1996,
      "step": 5350
    },
    {
      "epoch": 0.17152,
      "grad_norm": 0.06565208733081818,
      "learning_rate": 9.14256e-06,
      "loss": 0.1999,
      "step": 5360
    },
    {
      "epoch": 0.17184,
      "grad_norm": 0.027284439653158188,
      "learning_rate": 9.140960000000002e-06,
      "loss": 0.1994,
      "step": 5370
    },
    {
      "epoch": 0.17216,
      "grad_norm": 0.042418625205755234,
      "learning_rate": 9.13936e-06,
      "loss": 0.2069,
      "step": 5380
    },
    {
      "epoch": 0.17248,
      "grad_norm": 0.029310675337910652,
      "learning_rate": 9.137760000000001e-06,
      "loss": 0.201,
      "step": 5390
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.015058797784149647,
      "learning_rate": 9.136160000000001e-06,
      "loss": 0.2002,
      "step": 5400
    },
    {
      "epoch": 0.17312,
      "grad_norm": 0.02418571524322033,
      "learning_rate": 9.134560000000001e-06,
      "loss": 0.2001,
      "step": 5410
    },
    {
      "epoch": 0.17344,
      "grad_norm": 0.04427656903862953,
      "learning_rate": 9.132960000000001e-06,
      "loss": 0.1998,
      "step": 5420
    },
    {
      "epoch": 0.17376,
      "grad_norm": 0.024537868797779083,
      "learning_rate": 9.131360000000001e-06,
      "loss": 0.232,
      "step": 5430
    },
    {
      "epoch": 0.17408,
      "grad_norm": 0.04040578752756119,
      "learning_rate": 9.12976e-06,
      "loss": 0.2002,
      "step": 5440
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.057821258902549744,
      "learning_rate": 9.12816e-06,
      "loss": 0.1998,
      "step": 5450
    },
    {
      "epoch": 0.17472,
      "grad_norm": 0.022961491718888283,
      "learning_rate": 9.12656e-06,
      "loss": 0.1997,
      "step": 5460
    },
    {
      "epoch": 0.17504,
      "grad_norm": 0.027689186856150627,
      "learning_rate": 9.12496e-06,
      "loss": 0.2008,
      "step": 5470
    },
    {
      "epoch": 0.17536,
      "grad_norm": 0.020388079807162285,
      "learning_rate": 9.12336e-06,
      "loss": 0.2032,
      "step": 5480
    },
    {
      "epoch": 0.17568,
      "grad_norm": 0.04339306056499481,
      "learning_rate": 9.12176e-06,
      "loss": 0.2011,
      "step": 5490
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.02491137944161892,
      "learning_rate": 9.120160000000002e-06,
      "loss": 0.2189,
      "step": 5500
    },
    {
      "epoch": 0.17632,
      "grad_norm": 0.04573614150285721,
      "learning_rate": 9.11856e-06,
      "loss": 0.2018,
      "step": 5510
    },
    {
      "epoch": 0.17664,
      "grad_norm": 0.02520780637860298,
      "learning_rate": 9.11696e-06,
      "loss": 0.2057,
      "step": 5520
    },
    {
      "epoch": 0.17696,
      "grad_norm": 0.057339273393154144,
      "learning_rate": 9.115360000000001e-06,
      "loss": 0.1999,
      "step": 5530
    },
    {
      "epoch": 0.17728,
      "grad_norm": 0.04269605502486229,
      "learning_rate": 9.11376e-06,
      "loss": 0.2169,
      "step": 5540
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.15626277029514313,
      "learning_rate": 9.112160000000001e-06,
      "loss": 0.2,
      "step": 5550
    },
    {
      "epoch": 0.17792,
      "grad_norm": 0.02568974904716015,
      "learning_rate": 9.110560000000001e-06,
      "loss": 0.1996,
      "step": 5560
    },
    {
      "epoch": 0.17824,
      "grad_norm": 0.028412312269210815,
      "learning_rate": 9.108960000000001e-06,
      "loss": 0.1996,
      "step": 5570
    },
    {
      "epoch": 0.17856,
      "grad_norm": 0.03636777400970459,
      "learning_rate": 9.10736e-06,
      "loss": 0.2137,
      "step": 5580
    },
    {
      "epoch": 0.17888,
      "grad_norm": 0.01645437255501747,
      "learning_rate": 9.10576e-06,
      "loss": 0.201,
      "step": 5590
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.04481817036867142,
      "learning_rate": 9.10416e-06,
      "loss": 0.1998,
      "step": 5600
    },
    {
      "epoch": 0.17952,
      "grad_norm": 0.028207192197442055,
      "learning_rate": 9.10256e-06,
      "loss": 0.2242,
      "step": 5610
    },
    {
      "epoch": 0.17984,
      "grad_norm": 0.027511609718203545,
      "learning_rate": 9.10096e-06,
      "loss": 0.1997,
      "step": 5620
    },
    {
      "epoch": 0.18016,
      "grad_norm": 0.03513740375638008,
      "learning_rate": 9.09936e-06,
      "loss": 0.2151,
      "step": 5630
    },
    {
      "epoch": 0.18048,
      "grad_norm": 0.016396095976233482,
      "learning_rate": 9.097760000000002e-06,
      "loss": 0.2069,
      "step": 5640
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.02586469054222107,
      "learning_rate": 9.09616e-06,
      "loss": 0.2028,
      "step": 5650
    },
    {
      "epoch": 0.18112,
      "grad_norm": 0.029202010482549667,
      "learning_rate": 9.094560000000002e-06,
      "loss": 0.2012,
      "step": 5660
    },
    {
      "epoch": 0.18144,
      "grad_norm": 0.06680785864591599,
      "learning_rate": 9.092960000000001e-06,
      "loss": 0.2069,
      "step": 5670
    },
    {
      "epoch": 0.18176,
      "grad_norm": 0.23652231693267822,
      "learning_rate": 9.091360000000001e-06,
      "loss": 0.2193,
      "step": 5680
    },
    {
      "epoch": 0.18208,
      "grad_norm": 0.042473457753658295,
      "learning_rate": 9.089760000000001e-06,
      "loss": 0.2316,
      "step": 5690
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.04249192401766777,
      "learning_rate": 9.088160000000001e-06,
      "loss": 0.2048,
      "step": 5700
    },
    {
      "epoch": 0.18272,
      "grad_norm": 0.0438808798789978,
      "learning_rate": 9.086560000000001e-06,
      "loss": 0.201,
      "step": 5710
    },
    {
      "epoch": 0.18304,
      "grad_norm": 0.013506969437003136,
      "learning_rate": 9.084960000000001e-06,
      "loss": 0.1996,
      "step": 5720
    },
    {
      "epoch": 0.18336,
      "grad_norm": 0.06747736781835556,
      "learning_rate": 9.08336e-06,
      "loss": 0.1997,
      "step": 5730
    },
    {
      "epoch": 0.18368,
      "grad_norm": 0.02447822503745556,
      "learning_rate": 9.08176e-06,
      "loss": 0.1997,
      "step": 5740
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.03465714305639267,
      "learning_rate": 9.08016e-06,
      "loss": 0.205,
      "step": 5750
    },
    {
      "epoch": 0.18432,
      "grad_norm": 0.050518400967121124,
      "learning_rate": 9.07856e-06,
      "loss": 0.2004,
      "step": 5760
    },
    {
      "epoch": 0.18464,
      "grad_norm": 0.029684998095035553,
      "learning_rate": 9.07696e-06,
      "loss": 0.2173,
      "step": 5770
    },
    {
      "epoch": 0.18496,
      "grad_norm": 0.044476546347141266,
      "learning_rate": 9.07536e-06,
      "loss": 0.23,
      "step": 5780
    },
    {
      "epoch": 0.18528,
      "grad_norm": 0.02180168777704239,
      "learning_rate": 9.07376e-06,
      "loss": 0.2019,
      "step": 5790
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.029275868088006973,
      "learning_rate": 9.072160000000002e-06,
      "loss": 0.1996,
      "step": 5800
    },
    {
      "epoch": 0.18592,
      "grad_norm": 0.025683552026748657,
      "learning_rate": 9.07056e-06,
      "loss": 0.2137,
      "step": 5810
    },
    {
      "epoch": 0.18624,
      "grad_norm": 0.037886183708906174,
      "learning_rate": 9.068960000000001e-06,
      "loss": 0.2054,
      "step": 5820
    },
    {
      "epoch": 0.18656,
      "grad_norm": 0.7592833638191223,
      "learning_rate": 9.067360000000001e-06,
      "loss": 0.2172,
      "step": 5830
    },
    {
      "epoch": 0.18688,
      "grad_norm": 0.04005112871527672,
      "learning_rate": 9.065760000000001e-06,
      "loss": 0.2135,
      "step": 5840
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.025486642494797707,
      "learning_rate": 9.064160000000001e-06,
      "loss": 0.2002,
      "step": 5850
    },
    {
      "epoch": 0.18752,
      "grad_norm": 0.08778523653745651,
      "learning_rate": 9.062560000000001e-06,
      "loss": 0.2007,
      "step": 5860
    },
    {
      "epoch": 0.18784,
      "grad_norm": 1.0505067110061646,
      "learning_rate": 9.06096e-06,
      "loss": 0.2124,
      "step": 5870
    },
    {
      "epoch": 0.18816,
      "grad_norm": 0.1054336279630661,
      "learning_rate": 9.05936e-06,
      "loss": 0.215,
      "step": 5880
    },
    {
      "epoch": 0.18848,
      "grad_norm": 0.013600497506558895,
      "learning_rate": 9.05776e-06,
      "loss": 0.2047,
      "step": 5890
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.033769235014915466,
      "learning_rate": 9.05616e-06,
      "loss": 0.2004,
      "step": 5900
    },
    {
      "epoch": 0.18912,
      "grad_norm": 0.027982527390122414,
      "learning_rate": 9.054560000000002e-06,
      "loss": 0.2127,
      "step": 5910
    },
    {
      "epoch": 0.18944,
      "grad_norm": 0.0393974706530571,
      "learning_rate": 9.05296e-06,
      "loss": 0.2153,
      "step": 5920
    },
    {
      "epoch": 0.18976,
      "grad_norm": 0.01852426491677761,
      "learning_rate": 9.051360000000002e-06,
      "loss": 0.2007,
      "step": 5930
    },
    {
      "epoch": 0.19008,
      "grad_norm": 1.3414667844772339,
      "learning_rate": 9.049760000000002e-06,
      "loss": 0.2142,
      "step": 5940
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.01670982502400875,
      "learning_rate": 9.04816e-06,
      "loss": 0.1995,
      "step": 5950
    },
    {
      "epoch": 0.19072,
      "grad_norm": 0.034196171909570694,
      "learning_rate": 9.046560000000001e-06,
      "loss": 0.1994,
      "step": 5960
    },
    {
      "epoch": 0.19104,
      "grad_norm": 0.024783127009868622,
      "learning_rate": 9.044960000000001e-06,
      "loss": 0.1995,
      "step": 5970
    },
    {
      "epoch": 0.19136,
      "grad_norm": 0.030743308365345,
      "learning_rate": 9.043360000000001e-06,
      "loss": 0.1998,
      "step": 5980
    },
    {
      "epoch": 0.19168,
      "grad_norm": 0.017430735751986504,
      "learning_rate": 9.041760000000001e-06,
      "loss": 0.2002,
      "step": 5990
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.06625410169363022,
      "learning_rate": 9.040160000000001e-06,
      "loss": 0.1997,
      "step": 6000
    },
    {
      "epoch": 0.192,
      "eval_runtime": 62.0959,
      "eval_samples_per_second": 161.041,
      "eval_steps_per_second": 10.065,
      "step": 6000
    },
    {
      "epoch": 0.19232,
      "grad_norm": 0.038904622197151184,
      "learning_rate": 9.03856e-06,
      "loss": 0.2,
      "step": 6010
    },
    {
      "epoch": 0.19264,
      "grad_norm": 0.034304313361644745,
      "learning_rate": 9.03696e-06,
      "loss": 0.1994,
      "step": 6020
    },
    {
      "epoch": 0.19296,
      "grad_norm": 0.043652329593896866,
      "learning_rate": 9.03536e-06,
      "loss": 0.2265,
      "step": 6030
    },
    {
      "epoch": 0.19328,
      "grad_norm": 0.04719015955924988,
      "learning_rate": 9.03376e-06,
      "loss": 0.2021,
      "step": 6040
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.02534722536802292,
      "learning_rate": 9.03216e-06,
      "loss": 0.1997,
      "step": 6050
    },
    {
      "epoch": 0.19392,
      "grad_norm": 0.6570029854774475,
      "learning_rate": 9.03056e-06,
      "loss": 0.2132,
      "step": 6060
    },
    {
      "epoch": 0.19424,
      "grad_norm": 0.08041742444038391,
      "learning_rate": 9.028960000000002e-06,
      "loss": 0.1995,
      "step": 6070
    },
    {
      "epoch": 0.19456,
      "grad_norm": 0.02186586894094944,
      "learning_rate": 9.02736e-06,
      "loss": 0.2,
      "step": 6080
    },
    {
      "epoch": 0.19488,
      "grad_norm": 0.0551755428314209,
      "learning_rate": 9.025760000000001e-06,
      "loss": 0.2001,
      "step": 6090
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.023542078211903572,
      "learning_rate": 9.024160000000001e-06,
      "loss": 0.2168,
      "step": 6100
    },
    {
      "epoch": 0.19552,
      "grad_norm": 0.04768252745270729,
      "learning_rate": 9.02256e-06,
      "loss": 0.1994,
      "step": 6110
    },
    {
      "epoch": 0.19584,
      "grad_norm": 0.059738852083683014,
      "learning_rate": 9.020960000000001e-06,
      "loss": 0.2006,
      "step": 6120
    },
    {
      "epoch": 0.19616,
      "grad_norm": 0.18337799608707428,
      "learning_rate": 9.019360000000001e-06,
      "loss": 0.2159,
      "step": 6130
    },
    {
      "epoch": 0.19648,
      "grad_norm": 0.028334299102425575,
      "learning_rate": 9.017760000000001e-06,
      "loss": 0.1996,
      "step": 6140
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.02966427244246006,
      "learning_rate": 9.01616e-06,
      "loss": 0.1998,
      "step": 6150
    },
    {
      "epoch": 0.19712,
      "grad_norm": 0.03826509788632393,
      "learning_rate": 9.01456e-06,
      "loss": 0.2273,
      "step": 6160
    },
    {
      "epoch": 0.19744,
      "grad_norm": 0.028057007119059563,
      "learning_rate": 9.01296e-06,
      "loss": 0.1996,
      "step": 6170
    },
    {
      "epoch": 0.19776,
      "grad_norm": 0.03713807836174965,
      "learning_rate": 9.01136e-06,
      "loss": 0.1997,
      "step": 6180
    },
    {
      "epoch": 0.19808,
      "grad_norm": 0.18246859312057495,
      "learning_rate": 9.00976e-06,
      "loss": 0.2208,
      "step": 6190
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.012282418087124825,
      "learning_rate": 9.00816e-06,
      "loss": 0.2,
      "step": 6200
    },
    {
      "epoch": 0.19872,
      "grad_norm": 0.0367434062063694,
      "learning_rate": 9.00656e-06,
      "loss": 0.1995,
      "step": 6210
    },
    {
      "epoch": 0.19904,
      "grad_norm": 0.02664782665669918,
      "learning_rate": 9.00496e-06,
      "loss": 0.2004,
      "step": 6220
    },
    {
      "epoch": 0.19936,
      "grad_norm": 0.01760476641356945,
      "learning_rate": 9.003360000000001e-06,
      "loss": 0.1997,
      "step": 6230
    },
    {
      "epoch": 0.19968,
      "grad_norm": 0.03006972186267376,
      "learning_rate": 9.00176e-06,
      "loss": 0.2,
      "step": 6240
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.0320717878639698,
      "learning_rate": 9.000160000000001e-06,
      "loss": 0.1995,
      "step": 6250
    },
    {
      "epoch": 0.20032,
      "grad_norm": 0.04380830377340317,
      "learning_rate": 8.998560000000001e-06,
      "loss": 0.2128,
      "step": 6260
    },
    {
      "epoch": 0.20064,
      "grad_norm": 0.22749148309230804,
      "learning_rate": 8.996960000000001e-06,
      "loss": 0.2003,
      "step": 6270
    },
    {
      "epoch": 0.20096,
      "grad_norm": 0.06291281431913376,
      "learning_rate": 8.995360000000001e-06,
      "loss": 0.2075,
      "step": 6280
    },
    {
      "epoch": 0.20128,
      "grad_norm": 0.025211576372385025,
      "learning_rate": 8.99376e-06,
      "loss": 0.1998,
      "step": 6290
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.028531160205602646,
      "learning_rate": 8.99216e-06,
      "loss": 0.1994,
      "step": 6300
    },
    {
      "epoch": 0.20192,
      "grad_norm": 0.05387987196445465,
      "learning_rate": 8.99056e-06,
      "loss": 0.1996,
      "step": 6310
    },
    {
      "epoch": 0.20224,
      "grad_norm": 0.0423206128180027,
      "learning_rate": 8.98896e-06,
      "loss": 0.1997,
      "step": 6320
    },
    {
      "epoch": 0.20256,
      "grad_norm": 0.01327524147927761,
      "learning_rate": 8.98736e-06,
      "loss": 0.1995,
      "step": 6330
    },
    {
      "epoch": 0.20288,
      "grad_norm": 0.021376168355345726,
      "learning_rate": 8.985760000000002e-06,
      "loss": 0.1994,
      "step": 6340
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.10922997444868088,
      "learning_rate": 8.98416e-06,
      "loss": 0.2133,
      "step": 6350
    },
    {
      "epoch": 0.20352,
      "grad_norm": 0.03219742327928543,
      "learning_rate": 8.982560000000002e-06,
      "loss": 0.2037,
      "step": 6360
    },
    {
      "epoch": 0.20384,
      "grad_norm": 0.04638539254665375,
      "learning_rate": 8.980960000000002e-06,
      "loss": 0.1996,
      "step": 6370
    },
    {
      "epoch": 0.20416,
      "grad_norm": 0.04519229009747505,
      "learning_rate": 8.97936e-06,
      "loss": 0.2145,
      "step": 6380
    },
    {
      "epoch": 0.20448,
      "grad_norm": 0.03965494781732559,
      "learning_rate": 8.977760000000001e-06,
      "loss": 0.1995,
      "step": 6390
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.03430721536278725,
      "learning_rate": 8.976160000000001e-06,
      "loss": 0.1996,
      "step": 6400
    },
    {
      "epoch": 0.20512,
      "grad_norm": 0.034940455108881,
      "learning_rate": 8.974560000000001e-06,
      "loss": 0.2192,
      "step": 6410
    },
    {
      "epoch": 0.20544,
      "grad_norm": 0.035923104733228683,
      "learning_rate": 8.972960000000001e-06,
      "loss": 0.1995,
      "step": 6420
    },
    {
      "epoch": 0.20576,
      "grad_norm": 0.24649588763713837,
      "learning_rate": 8.97136e-06,
      "loss": 0.2004,
      "step": 6430
    },
    {
      "epoch": 0.20608,
      "grad_norm": 0.042509689927101135,
      "learning_rate": 8.96976e-06,
      "loss": 0.2011,
      "step": 6440
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.036259014159440994,
      "learning_rate": 8.96816e-06,
      "loss": 0.2009,
      "step": 6450
    },
    {
      "epoch": 0.20672,
      "grad_norm": 0.030375700443983078,
      "learning_rate": 8.96656e-06,
      "loss": 0.2,
      "step": 6460
    },
    {
      "epoch": 0.20704,
      "grad_norm": 0.026962406933307648,
      "learning_rate": 8.96496e-06,
      "loss": 0.2027,
      "step": 6470
    },
    {
      "epoch": 0.20736,
      "grad_norm": 0.026722382754087448,
      "learning_rate": 8.96336e-06,
      "loss": 0.2152,
      "step": 6480
    },
    {
      "epoch": 0.20768,
      "grad_norm": 0.024336280301213264,
      "learning_rate": 8.96176e-06,
      "loss": 0.2012,
      "step": 6490
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.04549703374505043,
      "learning_rate": 8.960160000000002e-06,
      "loss": 0.2136,
      "step": 6500
    },
    {
      "epoch": 0.20832,
      "grad_norm": 0.014913782477378845,
      "learning_rate": 8.95856e-06,
      "loss": 0.2,
      "step": 6510
    },
    {
      "epoch": 0.20864,
      "grad_norm": 0.06341242790222168,
      "learning_rate": 8.956960000000001e-06,
      "loss": 0.2166,
      "step": 6520
    },
    {
      "epoch": 0.20896,
      "grad_norm": 0.034978143870830536,
      "learning_rate": 8.955360000000001e-06,
      "loss": 0.2008,
      "step": 6530
    },
    {
      "epoch": 0.20928,
      "grad_norm": 0.027659257873892784,
      "learning_rate": 8.95376e-06,
      "loss": 0.2111,
      "step": 6540
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.05889730900526047,
      "learning_rate": 8.952160000000001e-06,
      "loss": 0.2165,
      "step": 6550
    },
    {
      "epoch": 0.20992,
      "grad_norm": 0.05460640415549278,
      "learning_rate": 8.950560000000001e-06,
      "loss": 0.1994,
      "step": 6560
    },
    {
      "epoch": 0.21024,
      "grad_norm": 0.04246795177459717,
      "learning_rate": 8.94896e-06,
      "loss": 0.1994,
      "step": 6570
    },
    {
      "epoch": 0.21056,
      "grad_norm": 0.04652630165219307,
      "learning_rate": 8.94736e-06,
      "loss": 0.2091,
      "step": 6580
    },
    {
      "epoch": 0.21088,
      "grad_norm": 0.08096481114625931,
      "learning_rate": 8.94576e-06,
      "loss": 0.2154,
      "step": 6590
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.02490728162229061,
      "learning_rate": 8.94416e-06,
      "loss": 0.2121,
      "step": 6600
    },
    {
      "epoch": 0.21152,
      "grad_norm": 0.03845697268843651,
      "learning_rate": 8.942560000000002e-06,
      "loss": 0.1996,
      "step": 6610
    },
    {
      "epoch": 0.21184,
      "grad_norm": 0.06041591987013817,
      "learning_rate": 8.94096e-06,
      "loss": 0.2056,
      "step": 6620
    },
    {
      "epoch": 0.21216,
      "grad_norm": 0.0294454637914896,
      "learning_rate": 8.93936e-06,
      "loss": 0.1995,
      "step": 6630
    },
    {
      "epoch": 0.21248,
      "grad_norm": 0.024677440524101257,
      "learning_rate": 8.937760000000002e-06,
      "loss": 0.2122,
      "step": 6640
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.039127085357904434,
      "learning_rate": 8.93616e-06,
      "loss": 0.2241,
      "step": 6650
    },
    {
      "epoch": 0.21312,
      "grad_norm": 0.06244649738073349,
      "learning_rate": 8.934560000000001e-06,
      "loss": 0.2002,
      "step": 6660
    },
    {
      "epoch": 0.21344,
      "grad_norm": 0.11528455466032028,
      "learning_rate": 8.932960000000001e-06,
      "loss": 0.1999,
      "step": 6670
    },
    {
      "epoch": 0.21376,
      "grad_norm": 0.7935788631439209,
      "learning_rate": 8.931360000000001e-06,
      "loss": 0.2015,
      "step": 6680
    },
    {
      "epoch": 0.21408,
      "grad_norm": 0.15257373452186584,
      "learning_rate": 8.929760000000001e-06,
      "loss": 0.2098,
      "step": 6690
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.02459382638335228,
      "learning_rate": 8.928160000000001e-06,
      "loss": 0.1996,
      "step": 6700
    },
    {
      "epoch": 0.21472,
      "grad_norm": 0.05265635997056961,
      "learning_rate": 8.92656e-06,
      "loss": 0.1998,
      "step": 6710
    },
    {
      "epoch": 0.21504,
      "grad_norm": 0.03586871922016144,
      "learning_rate": 8.92496e-06,
      "loss": 0.2055,
      "step": 6720
    },
    {
      "epoch": 0.21536,
      "grad_norm": 0.05219121277332306,
      "learning_rate": 8.92336e-06,
      "loss": 0.2161,
      "step": 6730
    },
    {
      "epoch": 0.21568,
      "grad_norm": 0.03985300287604332,
      "learning_rate": 8.92176e-06,
      "loss": 0.2151,
      "step": 6740
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.04520290344953537,
      "learning_rate": 8.92016e-06,
      "loss": 0.1996,
      "step": 6750
    },
    {
      "epoch": 0.21632,
      "grad_norm": 0.07429426163434982,
      "learning_rate": 8.91856e-06,
      "loss": 0.1997,
      "step": 6760
    },
    {
      "epoch": 0.21664,
      "grad_norm": 0.03274295851588249,
      "learning_rate": 8.916960000000002e-06,
      "loss": 0.2157,
      "step": 6770
    },
    {
      "epoch": 0.21696,
      "grad_norm": 0.009152829647064209,
      "learning_rate": 8.91536e-06,
      "loss": 0.2001,
      "step": 6780
    },
    {
      "epoch": 0.21728,
      "grad_norm": 0.028649482876062393,
      "learning_rate": 8.91376e-06,
      "loss": 0.2003,
      "step": 6790
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.02243087813258171,
      "learning_rate": 8.912160000000001e-06,
      "loss": 0.2145,
      "step": 6800
    },
    {
      "epoch": 0.21792,
      "grad_norm": 0.7111563086509705,
      "learning_rate": 8.91056e-06,
      "loss": 0.2163,
      "step": 6810
    },
    {
      "epoch": 0.21824,
      "grad_norm": 0.02984900400042534,
      "learning_rate": 8.908960000000001e-06,
      "loss": 0.1998,
      "step": 6820
    },
    {
      "epoch": 0.21856,
      "grad_norm": 0.0494750551879406,
      "learning_rate": 8.907360000000001e-06,
      "loss": 0.1995,
      "step": 6830
    },
    {
      "epoch": 0.21888,
      "grad_norm": 0.020477229729294777,
      "learning_rate": 8.905760000000001e-06,
      "loss": 0.1996,
      "step": 6840
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.037271782755851746,
      "learning_rate": 8.90416e-06,
      "loss": 0.214,
      "step": 6850
    },
    {
      "epoch": 0.21952,
      "grad_norm": 0.027117660269141197,
      "learning_rate": 8.90256e-06,
      "loss": 0.2004,
      "step": 6860
    },
    {
      "epoch": 0.21984,
      "grad_norm": 0.026208214461803436,
      "learning_rate": 8.90096e-06,
      "loss": 0.2005,
      "step": 6870
    },
    {
      "epoch": 0.22016,
      "grad_norm": 0.029335003346204758,
      "learning_rate": 8.89936e-06,
      "loss": 0.2136,
      "step": 6880
    },
    {
      "epoch": 0.22048,
      "grad_norm": 0.03406091406941414,
      "learning_rate": 8.89776e-06,
      "loss": 0.2006,
      "step": 6890
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.02957390621304512,
      "learning_rate": 8.89616e-06,
      "loss": 0.1996,
      "step": 6900
    },
    {
      "epoch": 0.22112,
      "grad_norm": 0.03979214280843735,
      "learning_rate": 8.894560000000002e-06,
      "loss": 0.1994,
      "step": 6910
    },
    {
      "epoch": 0.22144,
      "grad_norm": 0.04111882299184799,
      "learning_rate": 8.89296e-06,
      "loss": 0.2026,
      "step": 6920
    },
    {
      "epoch": 0.22176,
      "grad_norm": 0.017461292445659637,
      "learning_rate": 8.891360000000002e-06,
      "loss": 0.1994,
      "step": 6930
    },
    {
      "epoch": 0.22208,
      "grad_norm": 0.10203719139099121,
      "learning_rate": 8.889760000000001e-06,
      "loss": 0.2017,
      "step": 6940
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.02756480686366558,
      "learning_rate": 8.888160000000001e-06,
      "loss": 0.2152,
      "step": 6950
    },
    {
      "epoch": 0.22272,
      "grad_norm": 0.06790480762720108,
      "learning_rate": 8.886560000000001e-06,
      "loss": 0.2205,
      "step": 6960
    },
    {
      "epoch": 0.22304,
      "grad_norm": 0.7522640824317932,
      "learning_rate": 8.884960000000001e-06,
      "loss": 0.245,
      "step": 6970
    },
    {
      "epoch": 0.22336,
      "grad_norm": 0.1089787483215332,
      "learning_rate": 8.883360000000001e-06,
      "loss": 0.1997,
      "step": 6980
    },
    {
      "epoch": 0.22368,
      "grad_norm": 0.05502306669950485,
      "learning_rate": 8.88176e-06,
      "loss": 0.1996,
      "step": 6990
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.03594072163105011,
      "learning_rate": 8.88016e-06,
      "loss": 0.2257,
      "step": 7000
    },
    {
      "epoch": 0.224,
      "eval_runtime": 59.9033,
      "eval_samples_per_second": 166.936,
      "eval_steps_per_second": 10.433,
      "step": 7000
    },
    {
      "epoch": 0.22432,
      "grad_norm": 0.022554097697138786,
      "learning_rate": 8.87856e-06,
      "loss": 0.2161,
      "step": 7010
    },
    {
      "epoch": 0.22464,
      "grad_norm": 0.06253290176391602,
      "learning_rate": 8.87696e-06,
      "loss": 0.2009,
      "step": 7020
    },
    {
      "epoch": 0.22496,
      "grad_norm": 0.06768381595611572,
      "learning_rate": 8.87536e-06,
      "loss": 0.2138,
      "step": 7030
    },
    {
      "epoch": 0.22528,
      "grad_norm": 0.04202193394303322,
      "learning_rate": 8.873760000000002e-06,
      "loss": 0.1997,
      "step": 7040
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.04707035422325134,
      "learning_rate": 8.87216e-06,
      "loss": 0.2001,
      "step": 7050
    },
    {
      "epoch": 0.22592,
      "grad_norm": 0.031070949509739876,
      "learning_rate": 8.87056e-06,
      "loss": 0.1996,
      "step": 7060
    },
    {
      "epoch": 0.22624,
      "grad_norm": 0.03648748993873596,
      "learning_rate": 8.868960000000002e-06,
      "loss": 0.2162,
      "step": 7070
    },
    {
      "epoch": 0.22656,
      "grad_norm": 0.011819781735539436,
      "learning_rate": 8.86736e-06,
      "loss": 0.1996,
      "step": 7080
    },
    {
      "epoch": 0.22688,
      "grad_norm": 1.1725751161575317,
      "learning_rate": 8.865760000000001e-06,
      "loss": 0.2102,
      "step": 7090
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.04538675770163536,
      "learning_rate": 8.864160000000001e-06,
      "loss": 0.2002,
      "step": 7100
    },
    {
      "epoch": 0.22752,
      "grad_norm": 0.021783897653222084,
      "learning_rate": 8.862560000000001e-06,
      "loss": 0.201,
      "step": 7110
    },
    {
      "epoch": 0.22784,
      "grad_norm": 0.014452810399234295,
      "learning_rate": 8.860960000000001e-06,
      "loss": 0.1993,
      "step": 7120
    },
    {
      "epoch": 0.22816,
      "grad_norm": 0.022959351539611816,
      "learning_rate": 8.85936e-06,
      "loss": 0.1993,
      "step": 7130
    },
    {
      "epoch": 0.22848,
      "grad_norm": 0.039033688604831696,
      "learning_rate": 8.85776e-06,
      "loss": 0.2006,
      "step": 7140
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.03281832113862038,
      "learning_rate": 8.85616e-06,
      "loss": 0.2148,
      "step": 7150
    },
    {
      "epoch": 0.22912,
      "grad_norm": 1.4354000091552734,
      "learning_rate": 8.85456e-06,
      "loss": 0.2009,
      "step": 7160
    },
    {
      "epoch": 0.22944,
      "grad_norm": 0.04705050587654114,
      "learning_rate": 8.85296e-06,
      "loss": 0.212,
      "step": 7170
    },
    {
      "epoch": 0.22976,
      "grad_norm": 0.017833365127444267,
      "learning_rate": 8.85136e-06,
      "loss": 0.1993,
      "step": 7180
    },
    {
      "epoch": 0.23008,
      "grad_norm": 0.04636804014444351,
      "learning_rate": 8.84976e-06,
      "loss": 0.2124,
      "step": 7190
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.031102217733860016,
      "learning_rate": 8.848160000000002e-06,
      "loss": 0.1997,
      "step": 7200
    },
    {
      "epoch": 0.23072,
      "grad_norm": 0.024127129465341568,
      "learning_rate": 8.84656e-06,
      "loss": 0.1996,
      "step": 7210
    },
    {
      "epoch": 0.23104,
      "grad_norm": 0.015290647745132446,
      "learning_rate": 8.84496e-06,
      "loss": 0.2151,
      "step": 7220
    },
    {
      "epoch": 0.23136,
      "grad_norm": 0.1293436586856842,
      "learning_rate": 8.843360000000001e-06,
      "loss": 0.2002,
      "step": 7230
    },
    {
      "epoch": 0.23168,
      "grad_norm": 0.020159699022769928,
      "learning_rate": 8.84176e-06,
      "loss": 0.1997,
      "step": 7240
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.02716178633272648,
      "learning_rate": 8.840160000000001e-06,
      "loss": 0.2169,
      "step": 7250
    },
    {
      "epoch": 0.23232,
      "grad_norm": 0.058879297226667404,
      "learning_rate": 8.838560000000001e-06,
      "loss": 0.2004,
      "step": 7260
    },
    {
      "epoch": 0.23264,
      "grad_norm": 0.03466873615980148,
      "learning_rate": 8.836960000000001e-06,
      "loss": 0.1994,
      "step": 7270
    },
    {
      "epoch": 0.23296,
      "grad_norm": 0.04879751801490784,
      "learning_rate": 8.83536e-06,
      "loss": 0.1997,
      "step": 7280
    },
    {
      "epoch": 0.23328,
      "grad_norm": 0.02900007553398609,
      "learning_rate": 8.83376e-06,
      "loss": 0.2,
      "step": 7290
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.012692853808403015,
      "learning_rate": 8.83216e-06,
      "loss": 0.1992,
      "step": 7300
    },
    {
      "epoch": 0.23392,
      "grad_norm": 0.05795001983642578,
      "learning_rate": 8.83056e-06,
      "loss": 0.1994,
      "step": 7310
    },
    {
      "epoch": 0.23424,
      "grad_norm": 0.03090074099600315,
      "learning_rate": 8.82896e-06,
      "loss": 0.2055,
      "step": 7320
    },
    {
      "epoch": 0.23456,
      "grad_norm": 0.01906977966427803,
      "learning_rate": 8.82736e-06,
      "loss": 0.1999,
      "step": 7330
    },
    {
      "epoch": 0.23488,
      "grad_norm": 0.021751126274466515,
      "learning_rate": 8.825760000000002e-06,
      "loss": 0.2155,
      "step": 7340
    },
    {
      "epoch": 0.2352,
      "grad_norm": 3.201991319656372,
      "learning_rate": 8.82416e-06,
      "loss": 0.2083,
      "step": 7350
    },
    {
      "epoch": 0.23552,
      "grad_norm": 0.030717598274350166,
      "learning_rate": 8.822560000000001e-06,
      "loss": 0.1998,
      "step": 7360
    },
    {
      "epoch": 0.23584,
      "grad_norm": 0.023965904489159584,
      "learning_rate": 8.820960000000001e-06,
      "loss": 0.1998,
      "step": 7370
    },
    {
      "epoch": 0.23616,
      "grad_norm": 0.018886979669332504,
      "learning_rate": 8.819360000000001e-06,
      "loss": 0.2005,
      "step": 7380
    },
    {
      "epoch": 0.23648,
      "grad_norm": 0.026730665937066078,
      "learning_rate": 8.817760000000001e-06,
      "loss": 0.2177,
      "step": 7390
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.036144550889730453,
      "learning_rate": 8.816160000000001e-06,
      "loss": 0.2012,
      "step": 7400
    },
    {
      "epoch": 0.23712,
      "grad_norm": 0.011329666711390018,
      "learning_rate": 8.814560000000001e-06,
      "loss": 0.1992,
      "step": 7410
    },
    {
      "epoch": 0.23744,
      "grad_norm": 0.01896347478032112,
      "learning_rate": 8.81296e-06,
      "loss": 0.2167,
      "step": 7420
    },
    {
      "epoch": 0.23776,
      "grad_norm": 0.6201649904251099,
      "learning_rate": 8.81136e-06,
      "loss": 0.2452,
      "step": 7430
    },
    {
      "epoch": 0.23808,
      "grad_norm": 0.0651165246963501,
      "learning_rate": 8.80976e-06,
      "loss": 0.1997,
      "step": 7440
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.08369508385658264,
      "learning_rate": 8.80816e-06,
      "loss": 0.2,
      "step": 7450
    },
    {
      "epoch": 0.23872,
      "grad_norm": 0.021900000050663948,
      "learning_rate": 8.80656e-06,
      "loss": 0.1995,
      "step": 7460
    },
    {
      "epoch": 0.23904,
      "grad_norm": 0.03702719882130623,
      "learning_rate": 8.80496e-06,
      "loss": 0.1993,
      "step": 7470
    },
    {
      "epoch": 0.23936,
      "grad_norm": 0.02824127860367298,
      "learning_rate": 8.80336e-06,
      "loss": 0.1999,
      "step": 7480
    },
    {
      "epoch": 0.23968,
      "grad_norm": 0.03893260285258293,
      "learning_rate": 8.80176e-06,
      "loss": 0.2019,
      "step": 7490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.03122418001294136,
      "learning_rate": 8.800160000000001e-06,
      "loss": 0.208,
      "step": 7500
    },
    {
      "epoch": 0.24032,
      "grad_norm": 0.02826831489801407,
      "learning_rate": 8.79856e-06,
      "loss": 0.1994,
      "step": 7510
    },
    {
      "epoch": 0.24064,
      "grad_norm": 0.012911920435726643,
      "learning_rate": 8.796960000000001e-06,
      "loss": 0.2018,
      "step": 7520
    },
    {
      "epoch": 0.24096,
      "grad_norm": 0.013558522798120975,
      "learning_rate": 8.795360000000001e-06,
      "loss": 0.1999,
      "step": 7530
    },
    {
      "epoch": 0.24128,
      "grad_norm": 0.028918912634253502,
      "learning_rate": 8.793760000000001e-06,
      "loss": 0.2114,
      "step": 7540
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.6857119798660278,
      "learning_rate": 8.792160000000001e-06,
      "loss": 0.2279,
      "step": 7550
    },
    {
      "epoch": 0.24192,
      "grad_norm": 0.01682342402637005,
      "learning_rate": 8.79056e-06,
      "loss": 0.1992,
      "step": 7560
    },
    {
      "epoch": 0.24224,
      "grad_norm": 0.029550092294812202,
      "learning_rate": 8.78896e-06,
      "loss": 0.2001,
      "step": 7570
    },
    {
      "epoch": 0.24256,
      "grad_norm": 0.02161564864218235,
      "learning_rate": 8.78736e-06,
      "loss": 0.1998,
      "step": 7580
    },
    {
      "epoch": 0.24288,
      "grad_norm": 0.0658678486943245,
      "learning_rate": 8.78576e-06,
      "loss": 0.1994,
      "step": 7590
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.04453371837735176,
      "learning_rate": 8.78416e-06,
      "loss": 0.2163,
      "step": 7600
    },
    {
      "epoch": 0.24352,
      "grad_norm": 0.019083434715867043,
      "learning_rate": 8.782560000000002e-06,
      "loss": 0.2144,
      "step": 7610
    },
    {
      "epoch": 0.24384,
      "grad_norm": 0.8861984610557556,
      "learning_rate": 8.78096e-06,
      "loss": 0.2151,
      "step": 7620
    },
    {
      "epoch": 0.24416,
      "grad_norm": 0.014820145443081856,
      "learning_rate": 8.779360000000002e-06,
      "loss": 0.1995,
      "step": 7630
    },
    {
      "epoch": 0.24448,
      "grad_norm": 0.03014843352138996,
      "learning_rate": 8.777760000000001e-06,
      "loss": 0.2185,
      "step": 7640
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.023409515619277954,
      "learning_rate": 8.77616e-06,
      "loss": 0.1999,
      "step": 7650
    },
    {
      "epoch": 0.24512,
      "grad_norm": 0.11150632053613663,
      "learning_rate": 8.774560000000001e-06,
      "loss": 0.1999,
      "step": 7660
    },
    {
      "epoch": 0.24544,
      "grad_norm": 0.05388616397976875,
      "learning_rate": 8.772960000000001e-06,
      "loss": 0.2154,
      "step": 7670
    },
    {
      "epoch": 0.24576,
      "grad_norm": 0.050771377980709076,
      "learning_rate": 8.771360000000001e-06,
      "loss": 0.1994,
      "step": 7680
    },
    {
      "epoch": 0.24608,
      "grad_norm": 0.018550362437963486,
      "learning_rate": 8.769760000000001e-06,
      "loss": 0.1994,
      "step": 7690
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.02129381336271763,
      "learning_rate": 8.76816e-06,
      "loss": 0.1997,
      "step": 7700
    },
    {
      "epoch": 0.24672,
      "grad_norm": 0.04232124611735344,
      "learning_rate": 8.76656e-06,
      "loss": 0.1997,
      "step": 7710
    },
    {
      "epoch": 0.24704,
      "grad_norm": 0.025238648056983948,
      "learning_rate": 8.76496e-06,
      "loss": 0.1998,
      "step": 7720
    },
    {
      "epoch": 0.24736,
      "grad_norm": 0.025045305490493774,
      "learning_rate": 8.76336e-06,
      "loss": 0.2177,
      "step": 7730
    },
    {
      "epoch": 0.24768,
      "grad_norm": 0.03683193027973175,
      "learning_rate": 8.76176e-06,
      "loss": 0.2167,
      "step": 7740
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.03201961889863014,
      "learning_rate": 8.76016e-06,
      "loss": 0.2147,
      "step": 7750
    },
    {
      "epoch": 0.24832,
      "grad_norm": 0.0461808480322361,
      "learning_rate": 8.75856e-06,
      "loss": 0.1996,
      "step": 7760
    },
    {
      "epoch": 0.24864,
      "grad_norm": 0.026839591562747955,
      "learning_rate": 8.756960000000002e-06,
      "loss": 0.1993,
      "step": 7770
    },
    {
      "epoch": 0.24896,
      "grad_norm": 0.052094705402851105,
      "learning_rate": 8.75536e-06,
      "loss": 0.1994,
      "step": 7780
    },
    {
      "epoch": 0.24928,
      "grad_norm": 0.05876755341887474,
      "learning_rate": 8.753760000000001e-06,
      "loss": 0.1993,
      "step": 7790
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.07415895164012909,
      "learning_rate": 8.752160000000001e-06,
      "loss": 0.2173,
      "step": 7800
    },
    {
      "epoch": 0.24992,
      "grad_norm": 0.9622887969017029,
      "learning_rate": 8.75056e-06,
      "loss": 0.2192,
      "step": 7810
    },
    {
      "epoch": 0.25024,
      "grad_norm": 0.007621704135090113,
      "learning_rate": 8.748960000000001e-06,
      "loss": 0.2044,
      "step": 7820
    },
    {
      "epoch": 0.25056,
      "grad_norm": 0.024208253249526024,
      "learning_rate": 8.747360000000001e-06,
      "loss": 0.2449,
      "step": 7830
    },
    {
      "epoch": 0.25088,
      "grad_norm": 0.05956602469086647,
      "learning_rate": 8.74576e-06,
      "loss": 0.2336,
      "step": 7840
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.09201112389564514,
      "learning_rate": 8.74416e-06,
      "loss": 0.2002,
      "step": 7850
    },
    {
      "epoch": 0.25152,
      "grad_norm": 0.060130391269922256,
      "learning_rate": 8.74256e-06,
      "loss": 0.1997,
      "step": 7860
    },
    {
      "epoch": 0.25184,
      "grad_norm": 0.047419462352991104,
      "learning_rate": 8.74096e-06,
      "loss": 0.1997,
      "step": 7870
    },
    {
      "epoch": 0.25216,
      "grad_norm": 0.044856663793325424,
      "learning_rate": 8.739360000000002e-06,
      "loss": 0.2002,
      "step": 7880
    },
    {
      "epoch": 0.25248,
      "grad_norm": 0.050350919365882874,
      "learning_rate": 8.73776e-06,
      "loss": 0.2155,
      "step": 7890
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.03945639729499817,
      "learning_rate": 8.73616e-06,
      "loss": 0.1997,
      "step": 7900
    },
    {
      "epoch": 0.25312,
      "grad_norm": 0.05274520814418793,
      "learning_rate": 8.734560000000002e-06,
      "loss": 0.2116,
      "step": 7910
    },
    {
      "epoch": 0.25344,
      "grad_norm": 0.018195943906903267,
      "learning_rate": 8.73296e-06,
      "loss": 0.2479,
      "step": 7920
    },
    {
      "epoch": 0.25376,
      "grad_norm": 0.030552256852388382,
      "learning_rate": 8.731360000000001e-06,
      "loss": 0.2013,
      "step": 7930
    },
    {
      "epoch": 0.25408,
      "grad_norm": 0.019189903512597084,
      "learning_rate": 8.729760000000001e-06,
      "loss": 0.1998,
      "step": 7940
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.069541335105896,
      "learning_rate": 8.728160000000001e-06,
      "loss": 0.1996,
      "step": 7950
    },
    {
      "epoch": 0.25472,
      "grad_norm": 0.01760011352598667,
      "learning_rate": 8.726560000000001e-06,
      "loss": 0.2045,
      "step": 7960
    },
    {
      "epoch": 0.25504,
      "grad_norm": 0.04304350167512894,
      "learning_rate": 8.724960000000001e-06,
      "loss": 0.2006,
      "step": 7970
    },
    {
      "epoch": 0.25536,
      "grad_norm": 0.03367311507463455,
      "learning_rate": 8.72336e-06,
      "loss": 0.1996,
      "step": 7980
    },
    {
      "epoch": 0.25568,
      "grad_norm": 0.11939264088869095,
      "learning_rate": 8.72176e-06,
      "loss": 0.1998,
      "step": 7990
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.01989023946225643,
      "learning_rate": 8.72016e-06,
      "loss": 0.1997,
      "step": 8000
    },
    {
      "epoch": 0.256,
      "eval_runtime": 59.5232,
      "eval_samples_per_second": 168.002,
      "eval_steps_per_second": 10.5,
      "step": 8000
    },
    {
      "epoch": 0.25632,
      "grad_norm": 0.022194402292370796,
      "learning_rate": 8.71856e-06,
      "loss": 0.1992,
      "step": 8010
    },
    {
      "epoch": 0.25664,
      "grad_norm": 0.01808713935315609,
      "learning_rate": 8.71696e-06,
      "loss": 0.2266,
      "step": 8020
    },
    {
      "epoch": 0.25696,
      "grad_norm": 0.027568500488996506,
      "learning_rate": 8.71536e-06,
      "loss": 0.1998,
      "step": 8030
    },
    {
      "epoch": 0.25728,
      "grad_norm": 0.02976289577782154,
      "learning_rate": 8.713760000000002e-06,
      "loss": 0.2167,
      "step": 8040
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.06048334017395973,
      "learning_rate": 8.71216e-06,
      "loss": 0.2107,
      "step": 8050
    },
    {
      "epoch": 0.25792,
      "grad_norm": 0.022387178614735603,
      "learning_rate": 8.710560000000002e-06,
      "loss": 0.211,
      "step": 8060
    },
    {
      "epoch": 0.25824,
      "grad_norm": 0.02404084801673889,
      "learning_rate": 8.708960000000001e-06,
      "loss": 0.1992,
      "step": 8070
    },
    {
      "epoch": 0.25856,
      "grad_norm": 0.04554463550448418,
      "learning_rate": 8.70736e-06,
      "loss": 0.1999,
      "step": 8080
    },
    {
      "epoch": 0.25888,
      "grad_norm": 0.019658518955111504,
      "learning_rate": 8.705760000000001e-06,
      "loss": 0.211,
      "step": 8090
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.016495302319526672,
      "learning_rate": 8.704160000000001e-06,
      "loss": 0.2002,
      "step": 8100
    },
    {
      "epoch": 0.25952,
      "grad_norm": 0.06227254495024681,
      "learning_rate": 8.702560000000001e-06,
      "loss": 0.1998,
      "step": 8110
    },
    {
      "epoch": 0.25984,
      "grad_norm": 0.014957833103835583,
      "learning_rate": 8.70096e-06,
      "loss": 0.1998,
      "step": 8120
    },
    {
      "epoch": 0.26016,
      "grad_norm": 0.018781118094921112,
      "learning_rate": 8.69936e-06,
      "loss": 0.2138,
      "step": 8130
    },
    {
      "epoch": 0.26048,
      "grad_norm": 0.016399193555116653,
      "learning_rate": 8.69776e-06,
      "loss": 0.1995,
      "step": 8140
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.023883134126663208,
      "learning_rate": 8.69616e-06,
      "loss": 0.2005,
      "step": 8150
    },
    {
      "epoch": 0.26112,
      "grad_norm": 0.049854837357997894,
      "learning_rate": 8.69456e-06,
      "loss": 0.1995,
      "step": 8160
    },
    {
      "epoch": 0.26144,
      "grad_norm": 0.0237155482172966,
      "learning_rate": 8.69296e-06,
      "loss": 0.1996,
      "step": 8170
    },
    {
      "epoch": 0.26176,
      "grad_norm": 0.038529977202415466,
      "learning_rate": 8.69136e-06,
      "loss": 0.2008,
      "step": 8180
    },
    {
      "epoch": 0.26208,
      "grad_norm": 0.0601792074739933,
      "learning_rate": 8.68976e-06,
      "loss": 0.2136,
      "step": 8190
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.022817840799689293,
      "learning_rate": 8.688160000000002e-06,
      "loss": 0.2164,
      "step": 8200
    },
    {
      "epoch": 0.26272,
      "grad_norm": 0.039545219391584396,
      "learning_rate": 8.68656e-06,
      "loss": 0.1993,
      "step": 8210
    },
    {
      "epoch": 0.26304,
      "grad_norm": 0.02395203709602356,
      "learning_rate": 8.684960000000001e-06,
      "loss": 0.2001,
      "step": 8220
    },
    {
      "epoch": 0.26336,
      "grad_norm": 0.03272869065403938,
      "learning_rate": 8.683360000000001e-06,
      "loss": 0.2158,
      "step": 8230
    },
    {
      "epoch": 0.26368,
      "grad_norm": 0.021776285022497177,
      "learning_rate": 8.68176e-06,
      "loss": 0.2001,
      "step": 8240
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.09801530838012695,
      "learning_rate": 8.680160000000001e-06,
      "loss": 0.2056,
      "step": 8250
    },
    {
      "epoch": 0.26432,
      "grad_norm": 0.05822518467903137,
      "learning_rate": 8.67856e-06,
      "loss": 0.2009,
      "step": 8260
    },
    {
      "epoch": 0.26464,
      "grad_norm": 0.01519918255507946,
      "learning_rate": 8.67696e-06,
      "loss": 0.2151,
      "step": 8270
    },
    {
      "epoch": 0.26496,
      "grad_norm": 0.2629779875278473,
      "learning_rate": 8.67536e-06,
      "loss": 0.2212,
      "step": 8280
    },
    {
      "epoch": 0.26528,
      "grad_norm": 0.04798468202352524,
      "learning_rate": 8.67376e-06,
      "loss": 0.1996,
      "step": 8290
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.021010415628552437,
      "learning_rate": 8.67216e-06,
      "loss": 0.1993,
      "step": 8300
    },
    {
      "epoch": 0.26592,
      "grad_norm": 0.05134814605116844,
      "learning_rate": 8.670560000000002e-06,
      "loss": 0.2042,
      "step": 8310
    },
    {
      "epoch": 0.26624,
      "grad_norm": 0.04607895016670227,
      "learning_rate": 8.66896e-06,
      "loss": 0.1994,
      "step": 8320
    },
    {
      "epoch": 0.26656,
      "grad_norm": 0.05219727009534836,
      "learning_rate": 8.66736e-06,
      "loss": 0.2148,
      "step": 8330
    },
    {
      "epoch": 0.26688,
      "grad_norm": 0.018131501972675323,
      "learning_rate": 8.665760000000002e-06,
      "loss": 0.1995,
      "step": 8340
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.022534793242812157,
      "learning_rate": 8.66416e-06,
      "loss": 0.2118,
      "step": 8350
    },
    {
      "epoch": 0.26752,
      "grad_norm": 0.047220345586538315,
      "learning_rate": 8.662560000000001e-06,
      "loss": 0.2012,
      "step": 8360
    },
    {
      "epoch": 0.26784,
      "grad_norm": 0.019177695736289024,
      "learning_rate": 8.660960000000001e-06,
      "loss": 0.2003,
      "step": 8370
    },
    {
      "epoch": 0.26816,
      "grad_norm": 0.027139069512486458,
      "learning_rate": 8.659360000000001e-06,
      "loss": 0.2145,
      "step": 8380
    },
    {
      "epoch": 0.26848,
      "grad_norm": 0.05259396508336067,
      "learning_rate": 8.657760000000001e-06,
      "loss": 0.1994,
      "step": 8390
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.021387606859207153,
      "learning_rate": 8.65616e-06,
      "loss": 0.2014,
      "step": 8400
    },
    {
      "epoch": 0.26912,
      "grad_norm": 0.023115385323762894,
      "learning_rate": 8.65456e-06,
      "loss": 0.1995,
      "step": 8410
    },
    {
      "epoch": 0.26944,
      "grad_norm": 0.0467601977288723,
      "learning_rate": 8.65296e-06,
      "loss": 0.1994,
      "step": 8420
    },
    {
      "epoch": 0.26976,
      "grad_norm": 0.009560136124491692,
      "learning_rate": 8.65136e-06,
      "loss": 0.1994,
      "step": 8430
    },
    {
      "epoch": 0.27008,
      "grad_norm": 0.06878845393657684,
      "learning_rate": 8.64976e-06,
      "loss": 0.1998,
      "step": 8440
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.017998620867729187,
      "learning_rate": 8.64816e-06,
      "loss": 0.2226,
      "step": 8450
    },
    {
      "epoch": 0.27072,
      "grad_norm": 0.0339888259768486,
      "learning_rate": 8.64656e-06,
      "loss": 0.1998,
      "step": 8460
    },
    {
      "epoch": 0.27104,
      "grad_norm": 0.015002594329416752,
      "learning_rate": 8.644960000000002e-06,
      "loss": 0.1993,
      "step": 8470
    },
    {
      "epoch": 0.27136,
      "grad_norm": 0.03335518762469292,
      "learning_rate": 8.64336e-06,
      "loss": 0.2076,
      "step": 8480
    },
    {
      "epoch": 0.27168,
      "grad_norm": 0.03406780958175659,
      "learning_rate": 8.641760000000001e-06,
      "loss": 0.1994,
      "step": 8490
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.027836041525006294,
      "learning_rate": 8.640160000000001e-06,
      "loss": 0.1999,
      "step": 8500
    },
    {
      "epoch": 0.27232,
      "grad_norm": 0.05438898876309395,
      "learning_rate": 8.63856e-06,
      "loss": 0.1996,
      "step": 8510
    },
    {
      "epoch": 0.27264,
      "grad_norm": 0.07707583159208298,
      "learning_rate": 8.636960000000001e-06,
      "loss": 0.1994,
      "step": 8520
    },
    {
      "epoch": 0.27296,
      "grad_norm": 0.02658994495868683,
      "learning_rate": 8.635360000000001e-06,
      "loss": 0.2146,
      "step": 8530
    },
    {
      "epoch": 0.27328,
      "grad_norm": 0.7275611758232117,
      "learning_rate": 8.63376e-06,
      "loss": 0.2,
      "step": 8540
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.016850555315613747,
      "learning_rate": 8.63216e-06,
      "loss": 0.1999,
      "step": 8550
    },
    {
      "epoch": 0.27392,
      "grad_norm": 0.13854864239692688,
      "learning_rate": 8.63056e-06,
      "loss": 0.2023,
      "step": 8560
    },
    {
      "epoch": 0.27424,
      "grad_norm": 0.0176031906157732,
      "learning_rate": 8.62896e-06,
      "loss": 0.1993,
      "step": 8570
    },
    {
      "epoch": 0.27456,
      "grad_norm": 0.05913656949996948,
      "learning_rate": 8.62736e-06,
      "loss": 0.1994,
      "step": 8580
    },
    {
      "epoch": 0.27488,
      "grad_norm": 0.021260209381580353,
      "learning_rate": 8.62576e-06,
      "loss": 0.2168,
      "step": 8590
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.0372408963739872,
      "learning_rate": 8.62416e-06,
      "loss": 0.221,
      "step": 8600
    },
    {
      "epoch": 0.27552,
      "grad_norm": 0.019219238311052322,
      "learning_rate": 8.622560000000002e-06,
      "loss": 0.2011,
      "step": 8610
    },
    {
      "epoch": 0.27584,
      "grad_norm": 0.013626021333038807,
      "learning_rate": 8.62096e-06,
      "loss": 0.2006,
      "step": 8620
    },
    {
      "epoch": 0.27616,
      "grad_norm": 0.037665314972400665,
      "learning_rate": 8.619360000000001e-06,
      "loss": 0.1998,
      "step": 8630
    },
    {
      "epoch": 0.27648,
      "grad_norm": 0.01403864473104477,
      "learning_rate": 8.617760000000001e-06,
      "loss": 0.1994,
      "step": 8640
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.027958495542407036,
      "learning_rate": 8.616160000000001e-06,
      "loss": 0.1996,
      "step": 8650
    },
    {
      "epoch": 0.27712,
      "grad_norm": 0.03247351571917534,
      "learning_rate": 8.614560000000001e-06,
      "loss": 0.1995,
      "step": 8660
    },
    {
      "epoch": 0.27744,
      "grad_norm": 0.03132112696766853,
      "learning_rate": 8.612960000000001e-06,
      "loss": 0.1992,
      "step": 8670
    },
    {
      "epoch": 0.27776,
      "grad_norm": 0.022812610492110252,
      "learning_rate": 8.61136e-06,
      "loss": 0.2165,
      "step": 8680
    },
    {
      "epoch": 0.27808,
      "grad_norm": 0.09691013395786285,
      "learning_rate": 8.60976e-06,
      "loss": 0.1997,
      "step": 8690
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.017085375264286995,
      "learning_rate": 8.60816e-06,
      "loss": 0.2019,
      "step": 8700
    },
    {
      "epoch": 0.27872,
      "grad_norm": 0.2220587432384491,
      "learning_rate": 8.60656e-06,
      "loss": 0.212,
      "step": 8710
    },
    {
      "epoch": 0.27904,
      "grad_norm": 0.1380661278963089,
      "learning_rate": 8.60496e-06,
      "loss": 0.2014,
      "step": 8720
    },
    {
      "epoch": 0.27936,
      "grad_norm": 0.038210269063711166,
      "learning_rate": 8.60336e-06,
      "loss": 0.1996,
      "step": 8730
    },
    {
      "epoch": 0.27968,
      "grad_norm": 0.03298995643854141,
      "learning_rate": 8.601760000000002e-06,
      "loss": 0.2,
      "step": 8740
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.02725040167570114,
      "learning_rate": 8.60016e-06,
      "loss": 0.225,
      "step": 8750
    },
    {
      "epoch": 0.28032,
      "grad_norm": 0.015228026546537876,
      "learning_rate": 8.59856e-06,
      "loss": 0.2004,
      "step": 8760
    },
    {
      "epoch": 0.28064,
      "grad_norm": 0.053598981350660324,
      "learning_rate": 8.596960000000001e-06,
      "loss": 0.2073,
      "step": 8770
    },
    {
      "epoch": 0.28096,
      "grad_norm": 0.07605123519897461,
      "learning_rate": 8.59536e-06,
      "loss": 0.2001,
      "step": 8780
    },
    {
      "epoch": 0.28128,
      "grad_norm": 0.04565686360001564,
      "learning_rate": 8.593760000000001e-06,
      "loss": 0.2125,
      "step": 8790
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.056440696120262146,
      "learning_rate": 8.592160000000001e-06,
      "loss": 0.2009,
      "step": 8800
    },
    {
      "epoch": 0.28192,
      "grad_norm": 0.03903956711292267,
      "learning_rate": 8.590560000000001e-06,
      "loss": 0.2176,
      "step": 8810
    },
    {
      "epoch": 0.28224,
      "grad_norm": 0.03376695513725281,
      "learning_rate": 8.588960000000001e-06,
      "loss": 0.1999,
      "step": 8820
    },
    {
      "epoch": 0.28256,
      "grad_norm": 0.011579347774386406,
      "learning_rate": 8.58736e-06,
      "loss": 0.1995,
      "step": 8830
    },
    {
      "epoch": 0.28288,
      "grad_norm": 0.3561129868030548,
      "learning_rate": 8.58576e-06,
      "loss": 0.2005,
      "step": 8840
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.018039561808109283,
      "learning_rate": 8.58416e-06,
      "loss": 0.2008,
      "step": 8850
    },
    {
      "epoch": 0.28352,
      "grad_norm": 0.04661617428064346,
      "learning_rate": 8.58256e-06,
      "loss": 0.1995,
      "step": 8860
    },
    {
      "epoch": 0.28384,
      "grad_norm": 0.0705133005976677,
      "learning_rate": 8.58096e-06,
      "loss": 0.2058,
      "step": 8870
    },
    {
      "epoch": 0.28416,
      "grad_norm": 0.02407713793218136,
      "learning_rate": 8.579360000000002e-06,
      "loss": 0.2142,
      "step": 8880
    },
    {
      "epoch": 0.28448,
      "grad_norm": 0.25629034638404846,
      "learning_rate": 8.57776e-06,
      "loss": 0.2013,
      "step": 8890
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.012880677357316017,
      "learning_rate": 8.576160000000002e-06,
      "loss": 0.2002,
      "step": 8900
    },
    {
      "epoch": 0.28512,
      "grad_norm": 0.01513015665113926,
      "learning_rate": 8.574560000000001e-06,
      "loss": 0.205,
      "step": 8910
    },
    {
      "epoch": 0.28544,
      "grad_norm": 0.051157332956790924,
      "learning_rate": 8.57296e-06,
      "loss": 0.2001,
      "step": 8920
    },
    {
      "epoch": 0.28576,
      "grad_norm": 0.018923791125416756,
      "learning_rate": 8.571360000000001e-06,
      "loss": 0.1995,
      "step": 8930
    },
    {
      "epoch": 0.28608,
      "grad_norm": 0.02499670907855034,
      "learning_rate": 8.569760000000001e-06,
      "loss": 0.1998,
      "step": 8940
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.11250001192092896,
      "learning_rate": 8.568160000000001e-06,
      "loss": 0.1994,
      "step": 8950
    },
    {
      "epoch": 0.28672,
      "grad_norm": 0.01536030787974596,
      "learning_rate": 8.566560000000001e-06,
      "loss": 0.1992,
      "step": 8960
    },
    {
      "epoch": 0.28704,
      "grad_norm": 0.4583163857460022,
      "learning_rate": 8.56496e-06,
      "loss": 0.1998,
      "step": 8970
    },
    {
      "epoch": 0.28736,
      "grad_norm": 0.03724265843629837,
      "learning_rate": 8.56336e-06,
      "loss": 0.1998,
      "step": 8980
    },
    {
      "epoch": 0.28768,
      "grad_norm": 0.014644581824541092,
      "learning_rate": 8.56176e-06,
      "loss": 0.2288,
      "step": 8990
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.027334701269865036,
      "learning_rate": 8.56016e-06,
      "loss": 0.1995,
      "step": 9000
    },
    {
      "epoch": 0.288,
      "eval_runtime": 59.6685,
      "eval_samples_per_second": 167.593,
      "eval_steps_per_second": 10.475,
      "step": 9000
    },
    {
      "epoch": 0.28832,
      "grad_norm": 0.019453981891274452,
      "learning_rate": 8.55856e-06,
      "loss": 0.1998,
      "step": 9010
    },
    {
      "epoch": 0.28864,
      "grad_norm": 0.031273651868104935,
      "learning_rate": 8.55696e-06,
      "loss": 0.1998,
      "step": 9020
    },
    {
      "epoch": 0.28896,
      "grad_norm": 0.018962744623422623,
      "learning_rate": 8.55536e-06,
      "loss": 0.2135,
      "step": 9030
    },
    {
      "epoch": 0.28928,
      "grad_norm": 0.030907299369573593,
      "learning_rate": 8.553760000000002e-06,
      "loss": 0.2319,
      "step": 9040
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.02673337608575821,
      "learning_rate": 8.55216e-06,
      "loss": 0.1995,
      "step": 9050
    },
    {
      "epoch": 0.28992,
      "grad_norm": 0.027871711179614067,
      "learning_rate": 8.550560000000001e-06,
      "loss": 0.1993,
      "step": 9060
    },
    {
      "epoch": 0.29024,
      "grad_norm": 0.037209123373031616,
      "learning_rate": 8.548960000000001e-06,
      "loss": 0.2146,
      "step": 9070
    },
    {
      "epoch": 0.29056,
      "grad_norm": 0.028931856155395508,
      "learning_rate": 8.547360000000001e-06,
      "loss": 0.2156,
      "step": 9080
    },
    {
      "epoch": 0.29088,
      "grad_norm": 0.016996044665575027,
      "learning_rate": 8.545760000000001e-06,
      "loss": 0.1995,
      "step": 9090
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.030520889908075333,
      "learning_rate": 8.544160000000001e-06,
      "loss": 0.2003,
      "step": 9100
    },
    {
      "epoch": 0.29152,
      "grad_norm": 2.1277823448181152,
      "learning_rate": 8.54256e-06,
      "loss": 0.2267,
      "step": 9110
    },
    {
      "epoch": 0.29184,
      "grad_norm": 0.026477418839931488,
      "learning_rate": 8.54096e-06,
      "loss": 0.2004,
      "step": 9120
    },
    {
      "epoch": 0.29216,
      "grad_norm": 0.03477870672941208,
      "learning_rate": 8.53936e-06,
      "loss": 0.2144,
      "step": 9130
    },
    {
      "epoch": 0.29248,
      "grad_norm": 3.9041664600372314,
      "learning_rate": 8.53776e-06,
      "loss": 0.2222,
      "step": 9140
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.025777067989110947,
      "learning_rate": 8.53616e-06,
      "loss": 0.2145,
      "step": 9150
    },
    {
      "epoch": 0.29312,
      "grad_norm": 0.053627822548151016,
      "learning_rate": 8.53456e-06,
      "loss": 0.2141,
      "step": 9160
    },
    {
      "epoch": 0.29344,
      "grad_norm": 0.027059460058808327,
      "learning_rate": 8.532960000000002e-06,
      "loss": 0.2,
      "step": 9170
    },
    {
      "epoch": 0.29376,
      "grad_norm": 0.028499696403741837,
      "learning_rate": 8.53136e-06,
      "loss": 0.2141,
      "step": 9180
    },
    {
      "epoch": 0.29408,
      "grad_norm": 2.6578075885772705,
      "learning_rate": 8.52976e-06,
      "loss": 0.2081,
      "step": 9190
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.05806652829051018,
      "learning_rate": 8.528160000000001e-06,
      "loss": 0.1996,
      "step": 9200
    },
    {
      "epoch": 0.29472,
      "grad_norm": 0.018331527709960938,
      "learning_rate": 8.52656e-06,
      "loss": 0.2071,
      "step": 9210
    },
    {
      "epoch": 0.29504,
      "grad_norm": 0.05696830153465271,
      "learning_rate": 8.524960000000001e-06,
      "loss": 0.2283,
      "step": 9220
    },
    {
      "epoch": 0.29536,
      "grad_norm": 0.0479184128344059,
      "learning_rate": 8.523360000000001e-06,
      "loss": 0.1995,
      "step": 9230
    },
    {
      "epoch": 0.29568,
      "grad_norm": 0.0880110040307045,
      "learning_rate": 8.521760000000001e-06,
      "loss": 0.1995,
      "step": 9240
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.02303391322493553,
      "learning_rate": 8.52016e-06,
      "loss": 0.2012,
      "step": 9250
    },
    {
      "epoch": 0.29632,
      "grad_norm": 0.028760869055986404,
      "learning_rate": 8.51856e-06,
      "loss": 0.1998,
      "step": 9260
    },
    {
      "epoch": 0.29664,
      "grad_norm": 0.059897713363170624,
      "learning_rate": 8.51696e-06,
      "loss": 0.1997,
      "step": 9270
    },
    {
      "epoch": 0.29696,
      "grad_norm": 0.017908256500959396,
      "learning_rate": 8.51536e-06,
      "loss": 0.1994,
      "step": 9280
    },
    {
      "epoch": 0.29728,
      "grad_norm": 0.16621656715869904,
      "learning_rate": 8.51376e-06,
      "loss": 0.2019,
      "step": 9290
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.053830474615097046,
      "learning_rate": 8.51216e-06,
      "loss": 0.1994,
      "step": 9300
    },
    {
      "epoch": 0.29792,
      "grad_norm": 0.02146793156862259,
      "learning_rate": 8.510560000000002e-06,
      "loss": 0.1994,
      "step": 9310
    },
    {
      "epoch": 0.29824,
      "grad_norm": 0.013478768058121204,
      "learning_rate": 8.50896e-06,
      "loss": 0.1993,
      "step": 9320
    },
    {
      "epoch": 0.29856,
      "grad_norm": 0.015756309032440186,
      "learning_rate": 8.507360000000002e-06,
      "loss": 0.2254,
      "step": 9330
    },
    {
      "epoch": 0.29888,
      "grad_norm": 0.051101941615343094,
      "learning_rate": 8.505760000000001e-06,
      "loss": 0.2001,
      "step": 9340
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.01554777380079031,
      "learning_rate": 8.50416e-06,
      "loss": 0.1996,
      "step": 9350
    },
    {
      "epoch": 0.29952,
      "grad_norm": 0.013157106004655361,
      "learning_rate": 8.502560000000001e-06,
      "loss": 0.1995,
      "step": 9360
    },
    {
      "epoch": 0.29984,
      "grad_norm": 0.010753664188086987,
      "learning_rate": 8.500960000000001e-06,
      "loss": 0.2141,
      "step": 9370
    },
    {
      "epoch": 0.30016,
      "grad_norm": 0.04280681535601616,
      "learning_rate": 8.499360000000001e-06,
      "loss": 0.211,
      "step": 9380
    },
    {
      "epoch": 0.30048,
      "grad_norm": 0.05135289952158928,
      "learning_rate": 8.49776e-06,
      "loss": 0.1994,
      "step": 9390
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.022791597992181778,
      "learning_rate": 8.49616e-06,
      "loss": 0.1994,
      "step": 9400
    },
    {
      "epoch": 0.30112,
      "grad_norm": 0.05569811165332794,
      "learning_rate": 8.49456e-06,
      "loss": 0.2056,
      "step": 9410
    },
    {
      "epoch": 0.30144,
      "grad_norm": 0.01666499301791191,
      "learning_rate": 8.49296e-06,
      "loss": 0.1992,
      "step": 9420
    },
    {
      "epoch": 0.30176,
      "grad_norm": 0.031291887164115906,
      "learning_rate": 8.49136e-06,
      "loss": 0.2209,
      "step": 9430
    },
    {
      "epoch": 0.30208,
      "grad_norm": 0.06839477270841599,
      "learning_rate": 8.48976e-06,
      "loss": 0.1992,
      "step": 9440
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.020224347710609436,
      "learning_rate": 8.48816e-06,
      "loss": 0.1992,
      "step": 9450
    },
    {
      "epoch": 0.30272,
      "grad_norm": 0.01641615480184555,
      "learning_rate": 8.48656e-06,
      "loss": 0.1995,
      "step": 9460
    },
    {
      "epoch": 0.30304,
      "grad_norm": 0.040981996804475784,
      "learning_rate": 8.484960000000002e-06,
      "loss": 0.2064,
      "step": 9470
    },
    {
      "epoch": 0.30336,
      "grad_norm": 0.03566465899348259,
      "learning_rate": 8.48336e-06,
      "loss": 0.2112,
      "step": 9480
    },
    {
      "epoch": 0.30368,
      "grad_norm": 0.05738513544201851,
      "learning_rate": 8.481760000000001e-06,
      "loss": 0.2001,
      "step": 9490
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.024715177714824677,
      "learning_rate": 8.480160000000001e-06,
      "loss": 0.1992,
      "step": 9500
    },
    {
      "epoch": 0.30432,
      "grad_norm": 0.02250315621495247,
      "learning_rate": 8.478560000000001e-06,
      "loss": 0.2171,
      "step": 9510
    },
    {
      "epoch": 0.30464,
      "grad_norm": 0.0314595066010952,
      "learning_rate": 8.476960000000001e-06,
      "loss": 0.1994,
      "step": 9520
    },
    {
      "epoch": 0.30496,
      "grad_norm": 0.14259770512580872,
      "learning_rate": 8.47536e-06,
      "loss": 0.1995,
      "step": 9530
    },
    {
      "epoch": 0.30528,
      "grad_norm": 0.03716934844851494,
      "learning_rate": 8.47376e-06,
      "loss": 0.2117,
      "step": 9540
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.0255262553691864,
      "learning_rate": 8.47216e-06,
      "loss": 0.1997,
      "step": 9550
    },
    {
      "epoch": 0.30592,
      "grad_norm": 0.044943567365407944,
      "learning_rate": 8.47056e-06,
      "loss": 0.1994,
      "step": 9560
    },
    {
      "epoch": 0.30624,
      "grad_norm": 0.024015972390770912,
      "learning_rate": 8.46896e-06,
      "loss": 0.1993,
      "step": 9570
    },
    {
      "epoch": 0.30656,
      "grad_norm": 0.03827937692403793,
      "learning_rate": 8.467360000000002e-06,
      "loss": 0.1994,
      "step": 9580
    },
    {
      "epoch": 0.30688,
      "grad_norm": 0.02129133976995945,
      "learning_rate": 8.46576e-06,
      "loss": 0.2017,
      "step": 9590
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.055676691234111786,
      "learning_rate": 8.46416e-06,
      "loss": 0.1995,
      "step": 9600
    },
    {
      "epoch": 0.30752,
      "grad_norm": 0.021334074437618256,
      "learning_rate": 8.462560000000002e-06,
      "loss": 0.2136,
      "step": 9610
    },
    {
      "epoch": 0.30784,
      "grad_norm": 1.5486528873443604,
      "learning_rate": 8.46096e-06,
      "loss": 0.222,
      "step": 9620
    },
    {
      "epoch": 0.30816,
      "grad_norm": 0.03665321692824364,
      "learning_rate": 8.459360000000001e-06,
      "loss": 0.2146,
      "step": 9630
    },
    {
      "epoch": 0.30848,
      "grad_norm": 0.04269421100616455,
      "learning_rate": 8.457760000000001e-06,
      "loss": 0.1995,
      "step": 9640
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.018663626164197922,
      "learning_rate": 8.456160000000001e-06,
      "loss": 0.1993,
      "step": 9650
    },
    {
      "epoch": 0.30912,
      "grad_norm": 0.03803599253296852,
      "learning_rate": 8.454560000000001e-06,
      "loss": 0.1995,
      "step": 9660
    },
    {
      "epoch": 0.30944,
      "grad_norm": 0.015280386433005333,
      "learning_rate": 8.45296e-06,
      "loss": 0.1997,
      "step": 9670
    },
    {
      "epoch": 0.30976,
      "grad_norm": 0.05345053970813751,
      "learning_rate": 8.45136e-06,
      "loss": 0.1994,
      "step": 9680
    },
    {
      "epoch": 0.31008,
      "grad_norm": 0.2331778109073639,
      "learning_rate": 8.44976e-06,
      "loss": 0.1997,
      "step": 9690
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.013426769524812698,
      "learning_rate": 8.44816e-06,
      "loss": 0.2153,
      "step": 9700
    },
    {
      "epoch": 0.31072,
      "grad_norm": 0.02738313004374504,
      "learning_rate": 8.44656e-06,
      "loss": 0.2002,
      "step": 9710
    },
    {
      "epoch": 0.31104,
      "grad_norm": 0.017363885417580605,
      "learning_rate": 8.44496e-06,
      "loss": 0.1994,
      "step": 9720
    },
    {
      "epoch": 0.31136,
      "grad_norm": 0.030774524435400963,
      "learning_rate": 8.44336e-06,
      "loss": 0.2006,
      "step": 9730
    },
    {
      "epoch": 0.31168,
      "grad_norm": 0.045242153108119965,
      "learning_rate": 8.441760000000002e-06,
      "loss": 0.2144,
      "step": 9740
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.036199163645505905,
      "learning_rate": 8.44016e-06,
      "loss": 0.214,
      "step": 9750
    },
    {
      "epoch": 0.31232,
      "grad_norm": 0.037436049431562424,
      "learning_rate": 8.438560000000001e-06,
      "loss": 0.1995,
      "step": 9760
    },
    {
      "epoch": 0.31264,
      "grad_norm": 0.046000633388757706,
      "learning_rate": 8.436960000000001e-06,
      "loss": 0.2296,
      "step": 9770
    },
    {
      "epoch": 0.31296,
      "grad_norm": 0.05220455303788185,
      "learning_rate": 8.43536e-06,
      "loss": 0.1995,
      "step": 9780
    },
    {
      "epoch": 0.31328,
      "grad_norm": 0.03935007378458977,
      "learning_rate": 8.433760000000001e-06,
      "loss": 0.2,
      "step": 9790
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.020518025383353233,
      "learning_rate": 8.432160000000001e-06,
      "loss": 0.1996,
      "step": 9800
    },
    {
      "epoch": 0.31392,
      "grad_norm": 0.03008115477859974,
      "learning_rate": 8.43056e-06,
      "loss": 0.2116,
      "step": 9810
    },
    {
      "epoch": 0.31424,
      "grad_norm": 0.023435097187757492,
      "learning_rate": 8.42896e-06,
      "loss": 0.2158,
      "step": 9820
    },
    {
      "epoch": 0.31456,
      "grad_norm": 0.07584885507822037,
      "learning_rate": 8.42736e-06,
      "loss": 0.2128,
      "step": 9830
    },
    {
      "epoch": 0.31488,
      "grad_norm": 0.05638981983065605,
      "learning_rate": 8.42576e-06,
      "loss": 0.2004,
      "step": 9840
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.017985157668590546,
      "learning_rate": 8.424160000000002e-06,
      "loss": 0.2051,
      "step": 9850
    },
    {
      "epoch": 0.31552,
      "grad_norm": 0.07716121524572372,
      "learning_rate": 8.42256e-06,
      "loss": 0.1998,
      "step": 9860
    },
    {
      "epoch": 0.31584,
      "grad_norm": 0.7570006847381592,
      "learning_rate": 8.42096e-06,
      "loss": 0.2164,
      "step": 9870
    },
    {
      "epoch": 0.31616,
      "grad_norm": 0.03927347809076309,
      "learning_rate": 8.419360000000002e-06,
      "loss": 0.2012,
      "step": 9880
    },
    {
      "epoch": 0.31648,
      "grad_norm": 0.0855848416686058,
      "learning_rate": 8.41776e-06,
      "loss": 0.2027,
      "step": 9890
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.014528193511068821,
      "learning_rate": 8.416160000000001e-06,
      "loss": 0.2121,
      "step": 9900
    },
    {
      "epoch": 0.31712,
      "grad_norm": 0.08674010634422302,
      "learning_rate": 8.414560000000001e-06,
      "loss": 0.201,
      "step": 9910
    },
    {
      "epoch": 0.31744,
      "grad_norm": 0.03172925487160683,
      "learning_rate": 8.412960000000001e-06,
      "loss": 0.1994,
      "step": 9920
    },
    {
      "epoch": 0.31776,
      "grad_norm": 0.7745568752288818,
      "learning_rate": 8.411360000000001e-06,
      "loss": 0.2237,
      "step": 9930
    },
    {
      "epoch": 0.31808,
      "grad_norm": 0.05825332552194595,
      "learning_rate": 8.409760000000001e-06,
      "loss": 0.1996,
      "step": 9940
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.022395659238100052,
      "learning_rate": 8.40816e-06,
      "loss": 0.1994,
      "step": 9950
    },
    {
      "epoch": 0.31872,
      "grad_norm": 0.0218435637652874,
      "learning_rate": 8.40656e-06,
      "loss": 0.1993,
      "step": 9960
    },
    {
      "epoch": 0.31904,
      "grad_norm": 0.0507754422724247,
      "learning_rate": 8.40496e-06,
      "loss": 0.2251,
      "step": 9970
    },
    {
      "epoch": 0.31936,
      "grad_norm": 0.033555448055267334,
      "learning_rate": 8.40336e-06,
      "loss": 0.2143,
      "step": 9980
    },
    {
      "epoch": 0.31968,
      "grad_norm": 0.023796672001481056,
      "learning_rate": 8.40176e-06,
      "loss": 0.1995,
      "step": 9990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.03058471903204918,
      "learning_rate": 8.40016e-06,
      "loss": 0.2153,
      "step": 10000
    },
    {
      "epoch": 0.32,
      "eval_runtime": 51.9734,
      "eval_samples_per_second": 192.406,
      "eval_steps_per_second": 12.025,
      "step": 10000
    },
    {
      "epoch": 0.32032,
      "grad_norm": 0.0288980882614851,
      "learning_rate": 8.398560000000002e-06,
      "loss": 0.1993,
      "step": 10010
    },
    {
      "epoch": 0.32064,
      "grad_norm": 0.4556099474430084,
      "learning_rate": 8.39696e-06,
      "loss": 0.2003,
      "step": 10020
    },
    {
      "epoch": 0.32096,
      "grad_norm": 0.04342021048069,
      "learning_rate": 8.39536e-06,
      "loss": 0.1997,
      "step": 10030
    },
    {
      "epoch": 0.32128,
      "grad_norm": 0.01537397876381874,
      "learning_rate": 8.393760000000001e-06,
      "loss": 0.2033,
      "step": 10040
    },
    {
      "epoch": 0.3216,
      "grad_norm": 2.4043338298797607,
      "learning_rate": 8.39216e-06,
      "loss": 0.2106,
      "step": 10050
    },
    {
      "epoch": 0.32192,
      "grad_norm": 0.02438693307340145,
      "learning_rate": 8.390560000000001e-06,
      "loss": 0.2003,
      "step": 10060
    },
    {
      "epoch": 0.32224,
      "grad_norm": 0.021104654297232628,
      "learning_rate": 8.388960000000001e-06,
      "loss": 0.1997,
      "step": 10070
    },
    {
      "epoch": 0.32256,
      "grad_norm": 0.026554901152849197,
      "learning_rate": 8.387360000000001e-06,
      "loss": 0.1996,
      "step": 10080
    },
    {
      "epoch": 0.32288,
      "grad_norm": 0.016434624791145325,
      "learning_rate": 8.38576e-06,
      "loss": 0.2192,
      "step": 10090
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.08620865643024445,
      "learning_rate": 8.38416e-06,
      "loss": 0.2124,
      "step": 10100
    },
    {
      "epoch": 0.32352,
      "grad_norm": 0.02050977386534214,
      "learning_rate": 8.38256e-06,
      "loss": 0.1994,
      "step": 10110
    },
    {
      "epoch": 0.32384,
      "grad_norm": 0.0615510530769825,
      "learning_rate": 8.38096e-06,
      "loss": 0.2052,
      "step": 10120
    },
    {
      "epoch": 0.32416,
      "grad_norm": 0.05048070475459099,
      "learning_rate": 8.37936e-06,
      "loss": 0.2027,
      "step": 10130
    },
    {
      "epoch": 0.32448,
      "grad_norm": 0.01759052462875843,
      "learning_rate": 8.37776e-06,
      "loss": 0.1994,
      "step": 10140
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.04378291592001915,
      "learning_rate": 8.37616e-06,
      "loss": 0.1995,
      "step": 10150
    },
    {
      "epoch": 0.32512,
      "grad_norm": 0.02738482691347599,
      "learning_rate": 8.37456e-06,
      "loss": 0.215,
      "step": 10160
    },
    {
      "epoch": 0.32544,
      "grad_norm": 0.029601633548736572,
      "learning_rate": 8.372960000000002e-06,
      "loss": 0.1997,
      "step": 10170
    },
    {
      "epoch": 0.32576,
      "grad_norm": 0.04225873574614525,
      "learning_rate": 8.37136e-06,
      "loss": 0.1995,
      "step": 10180
    },
    {
      "epoch": 0.32608,
      "grad_norm": 0.020823920145630836,
      "learning_rate": 8.369760000000001e-06,
      "loss": 0.2129,
      "step": 10190
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.05996553227305412,
      "learning_rate": 8.368160000000001e-06,
      "loss": 0.2025,
      "step": 10200
    },
    {
      "epoch": 0.32672,
      "grad_norm": 0.7448873519897461,
      "learning_rate": 8.36656e-06,
      "loss": 0.2177,
      "step": 10210
    },
    {
      "epoch": 0.32704,
      "grad_norm": 0.01881357841193676,
      "learning_rate": 8.364960000000001e-06,
      "loss": 0.2005,
      "step": 10220
    },
    {
      "epoch": 0.32736,
      "grad_norm": 0.19270889461040497,
      "learning_rate": 8.363360000000001e-06,
      "loss": 0.1997,
      "step": 10230
    },
    {
      "epoch": 0.32768,
      "grad_norm": 0.06299469619989395,
      "learning_rate": 8.36176e-06,
      "loss": 0.1993,
      "step": 10240
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.038590606302022934,
      "learning_rate": 8.36016e-06,
      "loss": 0.1993,
      "step": 10250
    },
    {
      "epoch": 0.32832,
      "grad_norm": 0.012310180813074112,
      "learning_rate": 8.35856e-06,
      "loss": 0.2018,
      "step": 10260
    },
    {
      "epoch": 0.32864,
      "grad_norm": 0.05396478250622749,
      "learning_rate": 8.35696e-06,
      "loss": 0.1994,
      "step": 10270
    },
    {
      "epoch": 0.32896,
      "grad_norm": 0.8140143156051636,
      "learning_rate": 8.35536e-06,
      "loss": 0.2332,
      "step": 10280
    },
    {
      "epoch": 0.32928,
      "grad_norm": 0.03156409040093422,
      "learning_rate": 8.35376e-06,
      "loss": 0.2055,
      "step": 10290
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.034550607204437256,
      "learning_rate": 8.35216e-06,
      "loss": 0.2001,
      "step": 10300
    },
    {
      "epoch": 0.32992,
      "grad_norm": 0.014887510798871517,
      "learning_rate": 8.350560000000002e-06,
      "loss": 0.2038,
      "step": 10310
    },
    {
      "epoch": 0.33024,
      "grad_norm": 0.035214703530073166,
      "learning_rate": 8.34896e-06,
      "loss": 0.215,
      "step": 10320
    },
    {
      "epoch": 0.33056,
      "grad_norm": 0.02779979817569256,
      "learning_rate": 8.347360000000001e-06,
      "loss": 0.2142,
      "step": 10330
    },
    {
      "epoch": 0.33088,
      "grad_norm": 0.0650436133146286,
      "learning_rate": 8.345760000000001e-06,
      "loss": 0.2109,
      "step": 10340
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.023513799533247948,
      "learning_rate": 8.344160000000001e-06,
      "loss": 0.2002,
      "step": 10350
    },
    {
      "epoch": 0.33152,
      "grad_norm": 0.03472474217414856,
      "learning_rate": 8.342560000000001e-06,
      "loss": 0.1994,
      "step": 10360
    },
    {
      "epoch": 0.33184,
      "grad_norm": 0.03278345242142677,
      "learning_rate": 8.340960000000001e-06,
      "loss": 0.213,
      "step": 10370
    },
    {
      "epoch": 0.33216,
      "grad_norm": 0.030393658205866814,
      "learning_rate": 8.33936e-06,
      "loss": 0.2044,
      "step": 10380
    },
    {
      "epoch": 0.33248,
      "grad_norm": 0.7827227115631104,
      "learning_rate": 8.33776e-06,
      "loss": 0.2104,
      "step": 10390
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.17860247194766998,
      "learning_rate": 8.33616e-06,
      "loss": 0.2013,
      "step": 10400
    },
    {
      "epoch": 0.33312,
      "grad_norm": 0.05570356175303459,
      "learning_rate": 8.33456e-06,
      "loss": 0.1992,
      "step": 10410
    },
    {
      "epoch": 0.33344,
      "grad_norm": 0.01377628929913044,
      "learning_rate": 8.33296e-06,
      "loss": 0.1995,
      "step": 10420
    },
    {
      "epoch": 0.33376,
      "grad_norm": 0.01749788597226143,
      "learning_rate": 8.33136e-06,
      "loss": 0.2,
      "step": 10430
    },
    {
      "epoch": 0.33408,
      "grad_norm": 0.01725447177886963,
      "learning_rate": 8.329760000000002e-06,
      "loss": 0.1994,
      "step": 10440
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.013276767916977406,
      "learning_rate": 8.32816e-06,
      "loss": 0.2148,
      "step": 10450
    },
    {
      "epoch": 0.33472,
      "grad_norm": 0.0273171104490757,
      "learning_rate": 8.32656e-06,
      "loss": 0.1995,
      "step": 10460
    },
    {
      "epoch": 0.33504,
      "grad_norm": 0.05494430288672447,
      "learning_rate": 8.324960000000001e-06,
      "loss": 0.1995,
      "step": 10470
    },
    {
      "epoch": 0.33536,
      "grad_norm": 0.061869509518146515,
      "learning_rate": 8.32336e-06,
      "loss": 0.2054,
      "step": 10480
    },
    {
      "epoch": 0.33568,
      "grad_norm": 0.057399213314056396,
      "learning_rate": 8.321760000000001e-06,
      "loss": 0.1995,
      "step": 10490
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.0386638343334198,
      "learning_rate": 8.320160000000001e-06,
      "loss": 0.1992,
      "step": 10500
    },
    {
      "epoch": 0.33632,
      "grad_norm": 0.04775230959057808,
      "learning_rate": 8.318560000000001e-06,
      "loss": 0.2119,
      "step": 10510
    },
    {
      "epoch": 0.33664,
      "grad_norm": 0.019832197576761246,
      "learning_rate": 8.31696e-06,
      "loss": 0.1997,
      "step": 10520
    },
    {
      "epoch": 0.33696,
      "grad_norm": 0.018868014216423035,
      "learning_rate": 8.31536e-06,
      "loss": 0.1992,
      "step": 10530
    },
    {
      "epoch": 0.33728,
      "grad_norm": 0.01874016970396042,
      "learning_rate": 8.31376e-06,
      "loss": 0.1994,
      "step": 10540
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.017287297174334526,
      "learning_rate": 8.31216e-06,
      "loss": 0.2154,
      "step": 10550
    },
    {
      "epoch": 0.33792,
      "grad_norm": 0.023864666000008583,
      "learning_rate": 8.31056e-06,
      "loss": 0.2002,
      "step": 10560
    },
    {
      "epoch": 0.33824,
      "grad_norm": 0.03793907165527344,
      "learning_rate": 8.30896e-06,
      "loss": 0.2148,
      "step": 10570
    },
    {
      "epoch": 0.33856,
      "grad_norm": 0.41631707549095154,
      "learning_rate": 8.307360000000002e-06,
      "loss": 0.1999,
      "step": 10580
    },
    {
      "epoch": 0.33888,
      "grad_norm": 0.024511830881237984,
      "learning_rate": 8.30576e-06,
      "loss": 0.2058,
      "step": 10590
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.02128409408032894,
      "learning_rate": 8.304160000000001e-06,
      "loss": 0.1994,
      "step": 10600
    },
    {
      "epoch": 0.33952,
      "grad_norm": 0.27377915382385254,
      "learning_rate": 8.302560000000001e-06,
      "loss": 0.2136,
      "step": 10610
    },
    {
      "epoch": 0.33984,
      "grad_norm": 0.26016077399253845,
      "learning_rate": 8.30096e-06,
      "loss": 0.1996,
      "step": 10620
    },
    {
      "epoch": 0.34016,
      "grad_norm": 0.02872316725552082,
      "learning_rate": 8.299360000000001e-06,
      "loss": 0.1998,
      "step": 10630
    },
    {
      "epoch": 0.34048,
      "grad_norm": 0.03267889469861984,
      "learning_rate": 8.297760000000001e-06,
      "loss": 0.2172,
      "step": 10640
    },
    {
      "epoch": 0.3408,
      "grad_norm": 1.0007165670394897,
      "learning_rate": 8.296160000000001e-06,
      "loss": 0.2072,
      "step": 10650
    },
    {
      "epoch": 0.34112,
      "grad_norm": 0.08784240484237671,
      "learning_rate": 8.29456e-06,
      "loss": 0.2007,
      "step": 10660
    },
    {
      "epoch": 0.34144,
      "grad_norm": 0.1622488647699356,
      "learning_rate": 8.29296e-06,
      "loss": 0.217,
      "step": 10670
    },
    {
      "epoch": 0.34176,
      "grad_norm": 0.05523460730910301,
      "learning_rate": 8.29136e-06,
      "loss": 0.201,
      "step": 10680
    },
    {
      "epoch": 0.34208,
      "grad_norm": 0.02612304873764515,
      "learning_rate": 8.28976e-06,
      "loss": 0.2008,
      "step": 10690
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.01888231746852398,
      "learning_rate": 8.28816e-06,
      "loss": 0.216,
      "step": 10700
    },
    {
      "epoch": 0.34272,
      "grad_norm": 0.026409868150949478,
      "learning_rate": 8.28656e-06,
      "loss": 0.2289,
      "step": 10710
    },
    {
      "epoch": 0.34304,
      "grad_norm": 0.04491101950407028,
      "learning_rate": 8.28496e-06,
      "loss": 0.2163,
      "step": 10720
    },
    {
      "epoch": 0.34336,
      "grad_norm": 0.056422509253025055,
      "learning_rate": 8.28336e-06,
      "loss": 0.2068,
      "step": 10730
    },
    {
      "epoch": 0.34368,
      "grad_norm": 0.029398052021861076,
      "learning_rate": 8.281760000000001e-06,
      "loss": 0.1998,
      "step": 10740
    },
    {
      "epoch": 0.344,
      "grad_norm": 1.8545780181884766,
      "learning_rate": 8.28016e-06,
      "loss": 0.2078,
      "step": 10750
    },
    {
      "epoch": 0.34432,
      "grad_norm": 0.025252019986510277,
      "learning_rate": 8.278560000000001e-06,
      "loss": 0.2139,
      "step": 10760
    },
    {
      "epoch": 0.34464,
      "grad_norm": 0.056677673012018204,
      "learning_rate": 8.276960000000001e-06,
      "loss": 0.1993,
      "step": 10770
    },
    {
      "epoch": 0.34496,
      "grad_norm": 0.7304871678352356,
      "learning_rate": 8.275360000000001e-06,
      "loss": 0.2003,
      "step": 10780
    },
    {
      "epoch": 0.34528,
      "grad_norm": 0.0380546897649765,
      "learning_rate": 8.273760000000001e-06,
      "loss": 0.1995,
      "step": 10790
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.052263155579566956,
      "learning_rate": 8.27216e-06,
      "loss": 0.2147,
      "step": 10800
    },
    {
      "epoch": 0.34592,
      "grad_norm": 0.02351444773375988,
      "learning_rate": 8.27056e-06,
      "loss": 0.2263,
      "step": 10810
    },
    {
      "epoch": 0.34624,
      "grad_norm": 0.0354924313724041,
      "learning_rate": 8.26896e-06,
      "loss": 0.1998,
      "step": 10820
    },
    {
      "epoch": 0.34656,
      "grad_norm": 0.6273773312568665,
      "learning_rate": 8.26736e-06,
      "loss": 0.2243,
      "step": 10830
    },
    {
      "epoch": 0.34688,
      "grad_norm": 0.08068113774061203,
      "learning_rate": 8.26576e-06,
      "loss": 0.2005,
      "step": 10840
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.08654932677745819,
      "learning_rate": 8.264160000000002e-06,
      "loss": 0.1994,
      "step": 10850
    },
    {
      "epoch": 0.34752,
      "grad_norm": 0.03629731759428978,
      "learning_rate": 8.26256e-06,
      "loss": 0.1993,
      "step": 10860
    },
    {
      "epoch": 0.34784,
      "grad_norm": 0.047138575464487076,
      "learning_rate": 8.260960000000002e-06,
      "loss": 0.1993,
      "step": 10870
    },
    {
      "epoch": 0.34816,
      "grad_norm": 0.06385770440101624,
      "learning_rate": 8.259360000000002e-06,
      "loss": 0.2123,
      "step": 10880
    },
    {
      "epoch": 0.34848,
      "grad_norm": 0.06030707061290741,
      "learning_rate": 8.25776e-06,
      "loss": 0.1996,
      "step": 10890
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.016496503725647926,
      "learning_rate": 8.256160000000001e-06,
      "loss": 0.2017,
      "step": 10900
    },
    {
      "epoch": 0.34912,
      "grad_norm": 0.02036985754966736,
      "learning_rate": 8.254560000000001e-06,
      "loss": 0.1992,
      "step": 10910
    },
    {
      "epoch": 0.34944,
      "grad_norm": 0.037499550729990005,
      "learning_rate": 8.252960000000001e-06,
      "loss": 0.2225,
      "step": 10920
    },
    {
      "epoch": 0.34976,
      "grad_norm": 0.02753475308418274,
      "learning_rate": 8.251360000000001e-06,
      "loss": 0.2449,
      "step": 10930
    },
    {
      "epoch": 0.35008,
      "grad_norm": 0.06444723159074783,
      "learning_rate": 8.24976e-06,
      "loss": 0.1997,
      "step": 10940
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.03689892962574959,
      "learning_rate": 8.24816e-06,
      "loss": 0.2302,
      "step": 10950
    },
    {
      "epoch": 0.35072,
      "grad_norm": 0.0528915636241436,
      "learning_rate": 8.24656e-06,
      "loss": 0.2002,
      "step": 10960
    },
    {
      "epoch": 0.35104,
      "grad_norm": 0.036061689257621765,
      "learning_rate": 8.24496e-06,
      "loss": 0.2077,
      "step": 10970
    },
    {
      "epoch": 0.35136,
      "grad_norm": 0.014541150070726871,
      "learning_rate": 8.24336e-06,
      "loss": 0.2013,
      "step": 10980
    },
    {
      "epoch": 0.35168,
      "grad_norm": 0.43043696880340576,
      "learning_rate": 8.24176e-06,
      "loss": 0.2005,
      "step": 10990
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.023701468482613564,
      "learning_rate": 8.24016e-06,
      "loss": 0.2179,
      "step": 11000
    },
    {
      "epoch": 0.352,
      "eval_runtime": 51.834,
      "eval_samples_per_second": 192.924,
      "eval_steps_per_second": 12.058,
      "step": 11000
    },
    {
      "epoch": 0.35232,
      "grad_norm": 0.05886673927307129,
      "learning_rate": 8.238560000000002e-06,
      "loss": 0.2029,
      "step": 11010
    },
    {
      "epoch": 0.35264,
      "grad_norm": 0.015126511454582214,
      "learning_rate": 8.23696e-06,
      "loss": 0.1994,
      "step": 11020
    },
    {
      "epoch": 0.35296,
      "grad_norm": 0.03461446985602379,
      "learning_rate": 8.235360000000001e-06,
      "loss": 0.1994,
      "step": 11030
    },
    {
      "epoch": 0.35328,
      "grad_norm": 0.03487185016274452,
      "learning_rate": 8.233760000000001e-06,
      "loss": 0.2137,
      "step": 11040
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.020416678860783577,
      "learning_rate": 8.23216e-06,
      "loss": 0.2139,
      "step": 11050
    },
    {
      "epoch": 0.35392,
      "grad_norm": 0.041627004742622375,
      "learning_rate": 8.230560000000001e-06,
      "loss": 0.1995,
      "step": 11060
    },
    {
      "epoch": 0.35424,
      "grad_norm": 0.10101497918367386,
      "learning_rate": 8.228960000000001e-06,
      "loss": 0.1997,
      "step": 11070
    },
    {
      "epoch": 0.35456,
      "grad_norm": 0.020833253860473633,
      "learning_rate": 8.22736e-06,
      "loss": 0.2152,
      "step": 11080
    },
    {
      "epoch": 0.35488,
      "grad_norm": 0.014156575314700603,
      "learning_rate": 8.22576e-06,
      "loss": 0.1996,
      "step": 11090
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.04672406613826752,
      "learning_rate": 8.22416e-06,
      "loss": 0.2034,
      "step": 11100
    },
    {
      "epoch": 0.35552,
      "grad_norm": 0.020765649154782295,
      "learning_rate": 8.22256e-06,
      "loss": 0.1997,
      "step": 11110
    },
    {
      "epoch": 0.35584,
      "grad_norm": 0.026902824640274048,
      "learning_rate": 8.220960000000002e-06,
      "loss": 0.2,
      "step": 11120
    },
    {
      "epoch": 0.35616,
      "grad_norm": 0.025147706270217896,
      "learning_rate": 8.21936e-06,
      "loss": 0.1992,
      "step": 11130
    },
    {
      "epoch": 0.35648,
      "grad_norm": 0.025116045027971268,
      "learning_rate": 8.21776e-06,
      "loss": 0.2019,
      "step": 11140
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.02005181647837162,
      "learning_rate": 8.216160000000002e-06,
      "loss": 0.1995,
      "step": 11150
    },
    {
      "epoch": 0.35712,
      "grad_norm": 0.03923961520195007,
      "learning_rate": 8.21456e-06,
      "loss": 0.216,
      "step": 11160
    },
    {
      "epoch": 0.35744,
      "grad_norm": 0.017712734639644623,
      "learning_rate": 8.212960000000001e-06,
      "loss": 0.2282,
      "step": 11170
    },
    {
      "epoch": 0.35776,
      "grad_norm": 0.04196785390377045,
      "learning_rate": 8.211360000000001e-06,
      "loss": 0.2043,
      "step": 11180
    },
    {
      "epoch": 0.35808,
      "grad_norm": 0.051193851977586746,
      "learning_rate": 8.209760000000001e-06,
      "loss": 0.1995,
      "step": 11190
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.03529534116387367,
      "learning_rate": 8.208160000000001e-06,
      "loss": 0.2004,
      "step": 11200
    },
    {
      "epoch": 0.35872,
      "grad_norm": 0.028367066755890846,
      "learning_rate": 8.206560000000001e-06,
      "loss": 0.1994,
      "step": 11210
    },
    {
      "epoch": 0.35904,
      "grad_norm": 0.0472186878323555,
      "learning_rate": 8.20496e-06,
      "loss": 0.2142,
      "step": 11220
    },
    {
      "epoch": 0.35936,
      "grad_norm": 0.034169431775808334,
      "learning_rate": 8.20336e-06,
      "loss": 0.1995,
      "step": 11230
    },
    {
      "epoch": 0.35968,
      "grad_norm": 0.03995267301797867,
      "learning_rate": 8.20176e-06,
      "loss": 0.1993,
      "step": 11240
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.02856168895959854,
      "learning_rate": 8.20016e-06,
      "loss": 0.1992,
      "step": 11250
    },
    {
      "epoch": 0.36032,
      "grad_norm": 0.045421015471220016,
      "learning_rate": 8.19856e-06,
      "loss": 0.1997,
      "step": 11260
    },
    {
      "epoch": 0.36064,
      "grad_norm": 0.12324310094118118,
      "learning_rate": 8.19696e-06,
      "loss": 0.1994,
      "step": 11270
    },
    {
      "epoch": 0.36096,
      "grad_norm": 0.026108434423804283,
      "learning_rate": 8.195360000000002e-06,
      "loss": 0.1993,
      "step": 11280
    },
    {
      "epoch": 0.36128,
      "grad_norm": 0.015622582286596298,
      "learning_rate": 8.19376e-06,
      "loss": 0.2137,
      "step": 11290
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.04123340919613838,
      "learning_rate": 8.192160000000002e-06,
      "loss": 0.1993,
      "step": 11300
    },
    {
      "epoch": 0.36192,
      "grad_norm": 0.051312681287527084,
      "learning_rate": 8.190560000000001e-06,
      "loss": 0.1995,
      "step": 11310
    },
    {
      "epoch": 0.36224,
      "grad_norm": 0.02793988771736622,
      "learning_rate": 8.18896e-06,
      "loss": 0.2004,
      "step": 11320
    },
    {
      "epoch": 0.36256,
      "grad_norm": 0.050179336220026016,
      "learning_rate": 8.187360000000001e-06,
      "loss": 0.1998,
      "step": 11330
    },
    {
      "epoch": 0.36288,
      "grad_norm": 0.04154445230960846,
      "learning_rate": 8.185760000000001e-06,
      "loss": 0.2034,
      "step": 11340
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.011921578086912632,
      "learning_rate": 8.184160000000001e-06,
      "loss": 0.1999,
      "step": 11350
    },
    {
      "epoch": 0.36352,
      "grad_norm": 0.049928005784749985,
      "learning_rate": 8.18256e-06,
      "loss": 0.2039,
      "step": 11360
    },
    {
      "epoch": 0.36384,
      "grad_norm": 0.10433875024318695,
      "learning_rate": 8.18096e-06,
      "loss": 0.2178,
      "step": 11370
    },
    {
      "epoch": 0.36416,
      "grad_norm": 0.7396916747093201,
      "learning_rate": 8.17936e-06,
      "loss": 0.2325,
      "step": 11380
    },
    {
      "epoch": 0.36448,
      "grad_norm": 0.03406767547130585,
      "learning_rate": 8.17776e-06,
      "loss": 0.2136,
      "step": 11390
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.03693503141403198,
      "learning_rate": 8.17616e-06,
      "loss": 0.1996,
      "step": 11400
    },
    {
      "epoch": 0.36512,
      "grad_norm": 0.07357091456651688,
      "learning_rate": 8.17456e-06,
      "loss": 0.1994,
      "step": 11410
    },
    {
      "epoch": 0.36544,
      "grad_norm": 0.03687779977917671,
      "learning_rate": 8.17296e-06,
      "loss": 0.1996,
      "step": 11420
    },
    {
      "epoch": 0.36576,
      "grad_norm": 0.028399838134646416,
      "learning_rate": 8.17136e-06,
      "loss": 0.1993,
      "step": 11430
    },
    {
      "epoch": 0.36608,
      "grad_norm": 0.027446554973721504,
      "learning_rate": 8.169760000000002e-06,
      "loss": 0.2314,
      "step": 11440
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.052738018333911896,
      "learning_rate": 8.16816e-06,
      "loss": 0.1998,
      "step": 11450
    },
    {
      "epoch": 0.36672,
      "grad_norm": 0.023405952379107475,
      "learning_rate": 8.166560000000001e-06,
      "loss": 0.1994,
      "step": 11460
    },
    {
      "epoch": 0.36704,
      "grad_norm": 0.022165702655911446,
      "learning_rate": 8.164960000000001e-06,
      "loss": 0.2007,
      "step": 11470
    },
    {
      "epoch": 0.36736,
      "grad_norm": 0.03401441127061844,
      "learning_rate": 8.16336e-06,
      "loss": 0.1997,
      "step": 11480
    },
    {
      "epoch": 0.36768,
      "grad_norm": 0.02083433046936989,
      "learning_rate": 8.161760000000001e-06,
      "loss": 0.1994,
      "step": 11490
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.8074040412902832,
      "learning_rate": 8.16016e-06,
      "loss": 0.2238,
      "step": 11500
    },
    {
      "epoch": 0.36832,
      "grad_norm": 0.019460080191493034,
      "learning_rate": 8.15856e-06,
      "loss": 0.1996,
      "step": 11510
    },
    {
      "epoch": 0.36864,
      "grad_norm": 0.06640299409627914,
      "learning_rate": 8.15696e-06,
      "loss": 0.2166,
      "step": 11520
    },
    {
      "epoch": 0.36896,
      "grad_norm": 0.027341699227690697,
      "learning_rate": 8.15536e-06,
      "loss": 0.1995,
      "step": 11530
    },
    {
      "epoch": 0.36928,
      "grad_norm": 0.056671325117349625,
      "learning_rate": 8.15376e-06,
      "loss": 0.2153,
      "step": 11540
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.1299319863319397,
      "learning_rate": 8.152160000000002e-06,
      "loss": 0.2011,
      "step": 11550
    },
    {
      "epoch": 0.36992,
      "grad_norm": 0.03566870093345642,
      "learning_rate": 8.15056e-06,
      "loss": 0.1997,
      "step": 11560
    },
    {
      "epoch": 0.37024,
      "grad_norm": 0.11844068020582199,
      "learning_rate": 8.14896e-06,
      "loss": 0.2003,
      "step": 11570
    },
    {
      "epoch": 0.37056,
      "grad_norm": 0.040554992854595184,
      "learning_rate": 8.147360000000002e-06,
      "loss": 0.2037,
      "step": 11580
    },
    {
      "epoch": 0.37088,
      "grad_norm": 0.03985410928726196,
      "learning_rate": 8.14576e-06,
      "loss": 0.1994,
      "step": 11590
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.03690878674387932,
      "learning_rate": 8.144160000000001e-06,
      "loss": 0.2282,
      "step": 11600
    },
    {
      "epoch": 0.37152,
      "grad_norm": 0.024600056931376457,
      "learning_rate": 8.142560000000001e-06,
      "loss": 0.2152,
      "step": 11610
    },
    {
      "epoch": 0.37184,
      "grad_norm": 0.022169481962919235,
      "learning_rate": 8.140960000000001e-06,
      "loss": 0.2014,
      "step": 11620
    },
    {
      "epoch": 0.37216,
      "grad_norm": 0.0371878556907177,
      "learning_rate": 8.139360000000001e-06,
      "loss": 0.2141,
      "step": 11630
    },
    {
      "epoch": 0.37248,
      "grad_norm": 1.130220890045166,
      "learning_rate": 8.13776e-06,
      "loss": 0.2185,
      "step": 11640
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.08922723680734634,
      "learning_rate": 8.13616e-06,
      "loss": 0.1995,
      "step": 11650
    },
    {
      "epoch": 0.37312,
      "grad_norm": 0.19595803320407867,
      "learning_rate": 8.13456e-06,
      "loss": 0.2117,
      "step": 11660
    },
    {
      "epoch": 0.37344,
      "grad_norm": 0.0675262063741684,
      "learning_rate": 8.13296e-06,
      "loss": 0.2013,
      "step": 11670
    },
    {
      "epoch": 0.37376,
      "grad_norm": 0.03457595407962799,
      "learning_rate": 8.13136e-06,
      "loss": 0.1995,
      "step": 11680
    },
    {
      "epoch": 0.37408,
      "grad_norm": 0.05255628749728203,
      "learning_rate": 8.12976e-06,
      "loss": 0.1995,
      "step": 11690
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.025825969874858856,
      "learning_rate": 8.12816e-06,
      "loss": 0.2123,
      "step": 11700
    },
    {
      "epoch": 0.37472,
      "grad_norm": 0.026172365993261337,
      "learning_rate": 8.126560000000002e-06,
      "loss": 0.1992,
      "step": 11710
    },
    {
      "epoch": 0.37504,
      "grad_norm": 0.038691017776727676,
      "learning_rate": 8.12496e-06,
      "loss": 0.1998,
      "step": 11720
    },
    {
      "epoch": 0.37536,
      "grad_norm": 0.046589404344558716,
      "learning_rate": 8.12336e-06,
      "loss": 0.1994,
      "step": 11730
    },
    {
      "epoch": 0.37568,
      "grad_norm": 0.022809987887740135,
      "learning_rate": 8.121760000000001e-06,
      "loss": 0.1994,
      "step": 11740
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.038986898958683014,
      "learning_rate": 8.12016e-06,
      "loss": 0.1993,
      "step": 11750
    },
    {
      "epoch": 0.37632,
      "grad_norm": 0.010150562971830368,
      "learning_rate": 8.118560000000001e-06,
      "loss": 0.1992,
      "step": 11760
    },
    {
      "epoch": 0.37664,
      "grad_norm": 0.029025038704276085,
      "learning_rate": 8.116960000000001e-06,
      "loss": 0.1995,
      "step": 11770
    },
    {
      "epoch": 0.37696,
      "grad_norm": 0.031181389465928078,
      "learning_rate": 8.115360000000001e-06,
      "loss": 0.2003,
      "step": 11780
    },
    {
      "epoch": 0.37728,
      "grad_norm": 0.0434940829873085,
      "learning_rate": 8.11376e-06,
      "loss": 0.1994,
      "step": 11790
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.044163160026073456,
      "learning_rate": 8.11216e-06,
      "loss": 0.2143,
      "step": 11800
    },
    {
      "epoch": 0.37792,
      "grad_norm": 0.02645692229270935,
      "learning_rate": 8.11056e-06,
      "loss": 0.1995,
      "step": 11810
    },
    {
      "epoch": 0.37824,
      "grad_norm": 0.021561460569500923,
      "learning_rate": 8.10896e-06,
      "loss": 0.1994,
      "step": 11820
    },
    {
      "epoch": 0.37856,
      "grad_norm": 0.0901491567492485,
      "learning_rate": 8.10736e-06,
      "loss": 0.1994,
      "step": 11830
    },
    {
      "epoch": 0.37888,
      "grad_norm": 0.018052591010928154,
      "learning_rate": 8.10576e-06,
      "loss": 0.1993,
      "step": 11840
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.0488077849149704,
      "learning_rate": 8.104160000000002e-06,
      "loss": 0.2169,
      "step": 11850
    },
    {
      "epoch": 0.37952,
      "grad_norm": 0.0360170342028141,
      "learning_rate": 8.10256e-06,
      "loss": 0.1993,
      "step": 11860
    },
    {
      "epoch": 0.37984,
      "grad_norm": 0.007901471108198166,
      "learning_rate": 8.100960000000001e-06,
      "loss": 0.2101,
      "step": 11870
    },
    {
      "epoch": 0.38016,
      "grad_norm": 0.04673924669623375,
      "learning_rate": 8.099360000000001e-06,
      "loss": 0.1996,
      "step": 11880
    },
    {
      "epoch": 0.38048,
      "grad_norm": 0.05916750431060791,
      "learning_rate": 8.097760000000001e-06,
      "loss": 0.1996,
      "step": 11890
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.03482367843389511,
      "learning_rate": 8.096160000000001e-06,
      "loss": 0.213,
      "step": 11900
    },
    {
      "epoch": 0.38112,
      "grad_norm": 0.0334756076335907,
      "learning_rate": 8.094560000000001e-06,
      "loss": 0.1998,
      "step": 11910
    },
    {
      "epoch": 0.38144,
      "grad_norm": 0.025893138721585274,
      "learning_rate": 8.092960000000001e-06,
      "loss": 0.1995,
      "step": 11920
    },
    {
      "epoch": 0.38176,
      "grad_norm": 0.054943472146987915,
      "learning_rate": 8.09136e-06,
      "loss": 0.1996,
      "step": 11930
    },
    {
      "epoch": 0.38208,
      "grad_norm": 0.013118447735905647,
      "learning_rate": 8.08976e-06,
      "loss": 0.1999,
      "step": 11940
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.026560194790363312,
      "learning_rate": 8.08816e-06,
      "loss": 0.2058,
      "step": 11950
    },
    {
      "epoch": 0.38272,
      "grad_norm": 0.021587610244750977,
      "learning_rate": 8.08656e-06,
      "loss": 0.1994,
      "step": 11960
    },
    {
      "epoch": 0.38304,
      "grad_norm": 1.102460265159607,
      "learning_rate": 8.08496e-06,
      "loss": 0.2181,
      "step": 11970
    },
    {
      "epoch": 0.38336,
      "grad_norm": 2.7229111194610596,
      "learning_rate": 8.083360000000002e-06,
      "loss": 0.2054,
      "step": 11980
    },
    {
      "epoch": 0.38368,
      "grad_norm": 0.043307628482580185,
      "learning_rate": 8.08176e-06,
      "loss": 0.1992,
      "step": 11990
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.03350289538502693,
      "learning_rate": 8.08016e-06,
      "loss": 0.2089,
      "step": 12000
    },
    {
      "epoch": 0.384,
      "eval_runtime": 52.9348,
      "eval_samples_per_second": 188.912,
      "eval_steps_per_second": 11.807,
      "step": 12000
    },
    {
      "epoch": 0.38432,
      "grad_norm": 0.04242371395230293,
      "learning_rate": 8.078560000000001e-06,
      "loss": 0.1997,
      "step": 12010
    },
    {
      "epoch": 0.38464,
      "grad_norm": 0.12957601249217987,
      "learning_rate": 8.07696e-06,
      "loss": 0.2068,
      "step": 12020
    },
    {
      "epoch": 0.38496,
      "grad_norm": 0.029797488823533058,
      "learning_rate": 8.075360000000001e-06,
      "loss": 0.1992,
      "step": 12030
    },
    {
      "epoch": 0.38528,
      "grad_norm": 0.033457111567258835,
      "learning_rate": 8.073760000000001e-06,
      "loss": 0.1996,
      "step": 12040
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.02654019370675087,
      "learning_rate": 8.072160000000001e-06,
      "loss": 0.1992,
      "step": 12050
    },
    {
      "epoch": 0.38592,
      "grad_norm": 0.039129674434661865,
      "learning_rate": 8.070560000000001e-06,
      "loss": 0.2138,
      "step": 12060
    },
    {
      "epoch": 0.38624,
      "grad_norm": 0.02662009559571743,
      "learning_rate": 8.06896e-06,
      "loss": 0.1993,
      "step": 12070
    },
    {
      "epoch": 0.38656,
      "grad_norm": 0.03591371700167656,
      "learning_rate": 8.06736e-06,
      "loss": 0.1994,
      "step": 12080
    },
    {
      "epoch": 0.38688,
      "grad_norm": 0.03178415820002556,
      "learning_rate": 8.06576e-06,
      "loss": 0.1994,
      "step": 12090
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.1064111664891243,
      "learning_rate": 8.06416e-06,
      "loss": 0.2083,
      "step": 12100
    },
    {
      "epoch": 0.38752,
      "grad_norm": 0.016173914074897766,
      "learning_rate": 8.06256e-06,
      "loss": 0.2154,
      "step": 12110
    },
    {
      "epoch": 0.38784,
      "grad_norm": 0.037001099437475204,
      "learning_rate": 8.060960000000002e-06,
      "loss": 0.1993,
      "step": 12120
    },
    {
      "epoch": 0.38816,
      "grad_norm": 0.032982904464006424,
      "learning_rate": 8.05936e-06,
      "loss": 0.1996,
      "step": 12130
    },
    {
      "epoch": 0.38848,
      "grad_norm": 0.04607921466231346,
      "learning_rate": 8.057760000000002e-06,
      "loss": 0.1993,
      "step": 12140
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.05947955697774887,
      "learning_rate": 8.056160000000001e-06,
      "loss": 0.1993,
      "step": 12150
    },
    {
      "epoch": 0.38912,
      "grad_norm": 0.05157815292477608,
      "learning_rate": 8.05456e-06,
      "loss": 0.2144,
      "step": 12160
    },
    {
      "epoch": 0.38944,
      "grad_norm": 0.01342212688177824,
      "learning_rate": 8.052960000000001e-06,
      "loss": 0.1998,
      "step": 12170
    },
    {
      "epoch": 0.38976,
      "grad_norm": 0.022940872237086296,
      "learning_rate": 8.051360000000001e-06,
      "loss": 0.1993,
      "step": 12180
    },
    {
      "epoch": 0.39008,
      "grad_norm": 0.03517945855855942,
      "learning_rate": 8.049760000000001e-06,
      "loss": 0.2148,
      "step": 12190
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.020684530958533287,
      "learning_rate": 8.048160000000001e-06,
      "loss": 0.1993,
      "step": 12200
    },
    {
      "epoch": 0.39072,
      "grad_norm": 0.028712259605526924,
      "learning_rate": 8.04656e-06,
      "loss": 0.1999,
      "step": 12210
    },
    {
      "epoch": 0.39104,
      "grad_norm": 0.03341015428304672,
      "learning_rate": 8.04496e-06,
      "loss": 0.2133,
      "step": 12220
    },
    {
      "epoch": 0.39136,
      "grad_norm": 0.03455822169780731,
      "learning_rate": 8.04336e-06,
      "loss": 0.1993,
      "step": 12230
    },
    {
      "epoch": 0.39168,
      "grad_norm": 0.024584461003541946,
      "learning_rate": 8.04176e-06,
      "loss": 0.2059,
      "step": 12240
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.06381744146347046,
      "learning_rate": 8.04016e-06,
      "loss": 0.2022,
      "step": 12250
    },
    {
      "epoch": 0.39232,
      "grad_norm": 0.020895790308713913,
      "learning_rate": 8.03856e-06,
      "loss": 0.1994,
      "step": 12260
    },
    {
      "epoch": 0.39264,
      "grad_norm": 0.023110661655664444,
      "learning_rate": 8.03696e-06,
      "loss": 0.1996,
      "step": 12270
    },
    {
      "epoch": 0.39296,
      "grad_norm": 0.05272606387734413,
      "learning_rate": 8.035360000000002e-06,
      "loss": 0.1995,
      "step": 12280
    },
    {
      "epoch": 0.39328,
      "grad_norm": 0.016268953680992126,
      "learning_rate": 8.03376e-06,
      "loss": 0.2255,
      "step": 12290
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.04624444618821144,
      "learning_rate": 8.032160000000001e-06,
      "loss": 0.1994,
      "step": 12300
    },
    {
      "epoch": 0.39392,
      "grad_norm": 0.02876574546098709,
      "learning_rate": 8.030560000000001e-06,
      "loss": 0.1994,
      "step": 12310
    },
    {
      "epoch": 0.39424,
      "grad_norm": 0.07118891924619675,
      "learning_rate": 8.028960000000001e-06,
      "loss": 0.1995,
      "step": 12320
    },
    {
      "epoch": 0.39456,
      "grad_norm": 0.2322344332933426,
      "learning_rate": 8.027360000000001e-06,
      "loss": 0.1999,
      "step": 12330
    },
    {
      "epoch": 0.39488,
      "grad_norm": 0.045007068663835526,
      "learning_rate": 8.025760000000001e-06,
      "loss": 0.1995,
      "step": 12340
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.01497492752969265,
      "learning_rate": 8.02416e-06,
      "loss": 0.2118,
      "step": 12350
    },
    {
      "epoch": 0.39552,
      "grad_norm": 0.08719859272241592,
      "learning_rate": 8.02256e-06,
      "loss": 0.1995,
      "step": 12360
    },
    {
      "epoch": 0.39584,
      "grad_norm": 0.030724911019206047,
      "learning_rate": 8.02096e-06,
      "loss": 0.2155,
      "step": 12370
    },
    {
      "epoch": 0.39616,
      "grad_norm": 0.03276493400335312,
      "learning_rate": 8.01936e-06,
      "loss": 0.1996,
      "step": 12380
    },
    {
      "epoch": 0.39648,
      "grad_norm": 0.15224716067314148,
      "learning_rate": 8.01776e-06,
      "loss": 0.1997,
      "step": 12390
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.028256073594093323,
      "learning_rate": 8.01616e-06,
      "loss": 0.2166,
      "step": 12400
    },
    {
      "epoch": 0.39712,
      "grad_norm": 0.03156321868300438,
      "learning_rate": 8.01456e-06,
      "loss": 0.1994,
      "step": 12410
    },
    {
      "epoch": 0.39744,
      "grad_norm": 0.03227861225605011,
      "learning_rate": 8.01296e-06,
      "loss": 0.1993,
      "step": 12420
    },
    {
      "epoch": 0.39776,
      "grad_norm": 0.03733054921030998,
      "learning_rate": 8.01136e-06,
      "loss": 0.2044,
      "step": 12430
    },
    {
      "epoch": 0.39808,
      "grad_norm": 0.06449835002422333,
      "learning_rate": 8.009760000000001e-06,
      "loss": 0.1993,
      "step": 12440
    },
    {
      "epoch": 0.3984,
      "grad_norm": 2.0213704109191895,
      "learning_rate": 8.00816e-06,
      "loss": 0.2419,
      "step": 12450
    },
    {
      "epoch": 0.39872,
      "grad_norm": 0.02487846650183201,
      "learning_rate": 8.006560000000001e-06,
      "loss": 0.1993,
      "step": 12460
    },
    {
      "epoch": 0.39904,
      "grad_norm": 0.031684257090091705,
      "learning_rate": 8.004960000000001e-06,
      "loss": 0.228,
      "step": 12470
    },
    {
      "epoch": 0.39936,
      "grad_norm": 0.013851839117705822,
      "learning_rate": 8.003360000000001e-06,
      "loss": 0.2041,
      "step": 12480
    },
    {
      "epoch": 0.39968,
      "grad_norm": 0.022882338613271713,
      "learning_rate": 8.00176e-06,
      "loss": 0.2069,
      "step": 12490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.03295683115720749,
      "learning_rate": 8.00016e-06,
      "loss": 0.2271,
      "step": 12500
    },
    {
      "epoch": 0.40032,
      "grad_norm": 0.0739046037197113,
      "learning_rate": 7.99856e-06,
      "loss": 0.1995,
      "step": 12510
    },
    {
      "epoch": 0.40064,
      "grad_norm": 0.014470034278929234,
      "learning_rate": 7.99696e-06,
      "loss": 0.1993,
      "step": 12520
    },
    {
      "epoch": 0.40096,
      "grad_norm": 0.11031385511159897,
      "learning_rate": 7.99536e-06,
      "loss": 0.1994,
      "step": 12530
    },
    {
      "epoch": 0.40128,
      "grad_norm": 0.020311908796429634,
      "learning_rate": 7.99376e-06,
      "loss": 0.1994,
      "step": 12540
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.05985870584845543,
      "learning_rate": 7.992160000000002e-06,
      "loss": 0.1993,
      "step": 12550
    },
    {
      "epoch": 0.40192,
      "grad_norm": 0.028034601360559464,
      "learning_rate": 7.99056e-06,
      "loss": 0.1995,
      "step": 12560
    },
    {
      "epoch": 0.40224,
      "grad_norm": 0.024359919130802155,
      "learning_rate": 7.988960000000002e-06,
      "loss": 0.2153,
      "step": 12570
    },
    {
      "epoch": 0.40256,
      "grad_norm": 0.023780377581715584,
      "learning_rate": 7.987360000000001e-06,
      "loss": 0.1993,
      "step": 12580
    },
    {
      "epoch": 0.40288,
      "grad_norm": 0.03432633355259895,
      "learning_rate": 7.98576e-06,
      "loss": 0.1994,
      "step": 12590
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.010577687062323093,
      "learning_rate": 7.984160000000001e-06,
      "loss": 0.1993,
      "step": 12600
    },
    {
      "epoch": 0.40352,
      "grad_norm": 0.02254975400865078,
      "learning_rate": 7.982560000000001e-06,
      "loss": 0.1994,
      "step": 12610
    },
    {
      "epoch": 0.40384,
      "grad_norm": 0.038539331406354904,
      "learning_rate": 7.980960000000001e-06,
      "loss": 0.1994,
      "step": 12620
    },
    {
      "epoch": 0.40416,
      "grad_norm": 0.028508076444268227,
      "learning_rate": 7.97936e-06,
      "loss": 0.1995,
      "step": 12630
    },
    {
      "epoch": 0.40448,
      "grad_norm": 0.022620683535933495,
      "learning_rate": 7.97776e-06,
      "loss": 0.2025,
      "step": 12640
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.013596105389297009,
      "learning_rate": 7.97616e-06,
      "loss": 0.1992,
      "step": 12650
    },
    {
      "epoch": 0.40512,
      "grad_norm": 0.02490045689046383,
      "learning_rate": 7.97456e-06,
      "loss": 0.1992,
      "step": 12660
    },
    {
      "epoch": 0.40544,
      "grad_norm": 0.02588641829788685,
      "learning_rate": 7.97296e-06,
      "loss": 0.1993,
      "step": 12670
    },
    {
      "epoch": 0.40576,
      "grad_norm": 0.032825738191604614,
      "learning_rate": 7.97136e-06,
      "loss": 0.1993,
      "step": 12680
    },
    {
      "epoch": 0.40608,
      "grad_norm": 0.04845224320888519,
      "learning_rate": 7.96976e-06,
      "loss": 0.1994,
      "step": 12690
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.032134391367435455,
      "learning_rate": 7.96816e-06,
      "loss": 0.1993,
      "step": 12700
    },
    {
      "epoch": 0.40672,
      "grad_norm": 0.03030581772327423,
      "learning_rate": 7.966560000000002e-06,
      "loss": 0.2162,
      "step": 12710
    },
    {
      "epoch": 0.40704,
      "grad_norm": 0.021777639165520668,
      "learning_rate": 7.96496e-06,
      "loss": 0.2153,
      "step": 12720
    },
    {
      "epoch": 0.40736,
      "grad_norm": 0.054504215717315674,
      "learning_rate": 7.963360000000001e-06,
      "loss": 0.1998,
      "step": 12730
    },
    {
      "epoch": 0.40768,
      "grad_norm": 0.7855976819992065,
      "learning_rate": 7.961760000000001e-06,
      "loss": 0.2013,
      "step": 12740
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.7605182528495789,
      "learning_rate": 7.96016e-06,
      "loss": 0.2163,
      "step": 12750
    },
    {
      "epoch": 0.40832,
      "grad_norm": 0.03416240215301514,
      "learning_rate": 7.958560000000001e-06,
      "loss": 0.1994,
      "step": 12760
    },
    {
      "epoch": 0.40864,
      "grad_norm": 0.04218108579516411,
      "learning_rate": 7.95696e-06,
      "loss": 0.1992,
      "step": 12770
    },
    {
      "epoch": 0.40896,
      "grad_norm": 0.5318617820739746,
      "learning_rate": 7.95536e-06,
      "loss": 0.2056,
      "step": 12780
    },
    {
      "epoch": 0.40928,
      "grad_norm": 0.027236154302954674,
      "learning_rate": 7.95376e-06,
      "loss": 0.1996,
      "step": 12790
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.025620948523283005,
      "learning_rate": 7.95216e-06,
      "loss": 0.2157,
      "step": 12800
    },
    {
      "epoch": 0.40992,
      "grad_norm": 0.03677302971482277,
      "learning_rate": 7.95056e-06,
      "loss": 0.1995,
      "step": 12810
    },
    {
      "epoch": 0.41024,
      "grad_norm": 0.03986987844109535,
      "learning_rate": 7.948960000000002e-06,
      "loss": 0.1992,
      "step": 12820
    },
    {
      "epoch": 0.41056,
      "grad_norm": 0.05012354627251625,
      "learning_rate": 7.94736e-06,
      "loss": 0.1994,
      "step": 12830
    },
    {
      "epoch": 0.41088,
      "grad_norm": 0.030390342697501183,
      "learning_rate": 7.94576e-06,
      "loss": 0.2,
      "step": 12840
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.06336340308189392,
      "learning_rate": 7.944160000000002e-06,
      "loss": 0.1994,
      "step": 12850
    },
    {
      "epoch": 0.41152,
      "grad_norm": 0.7500767111778259,
      "learning_rate": 7.94256e-06,
      "loss": 0.2253,
      "step": 12860
    },
    {
      "epoch": 0.41184,
      "grad_norm": 0.021321076899766922,
      "learning_rate": 7.940960000000001e-06,
      "loss": 0.1993,
      "step": 12870
    },
    {
      "epoch": 0.41216,
      "grad_norm": 0.061185143887996674,
      "learning_rate": 7.939360000000001e-06,
      "loss": 0.2308,
      "step": 12880
    },
    {
      "epoch": 0.41248,
      "grad_norm": 0.14887763559818268,
      "learning_rate": 7.937760000000001e-06,
      "loss": 0.2001,
      "step": 12890
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.027857746928930283,
      "learning_rate": 7.936160000000001e-06,
      "loss": 0.1997,
      "step": 12900
    },
    {
      "epoch": 0.41312,
      "grad_norm": 0.03570249676704407,
      "learning_rate": 7.93456e-06,
      "loss": 0.209,
      "step": 12910
    },
    {
      "epoch": 0.41344,
      "grad_norm": 0.055601347237825394,
      "learning_rate": 7.93296e-06,
      "loss": 0.1993,
      "step": 12920
    },
    {
      "epoch": 0.41376,
      "grad_norm": 0.016337404027581215,
      "learning_rate": 7.93136e-06,
      "loss": 0.2145,
      "step": 12930
    },
    {
      "epoch": 0.41408,
      "grad_norm": 0.0127702746540308,
      "learning_rate": 7.92976e-06,
      "loss": 0.1994,
      "step": 12940
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.034126002341508865,
      "learning_rate": 7.92816e-06,
      "loss": 0.1995,
      "step": 12950
    },
    {
      "epoch": 0.41472,
      "grad_norm": 0.028823411092162132,
      "learning_rate": 7.92656e-06,
      "loss": 0.1993,
      "step": 12960
    },
    {
      "epoch": 0.41504,
      "grad_norm": 0.021981073543429375,
      "learning_rate": 7.92496e-06,
      "loss": 0.2119,
      "step": 12970
    },
    {
      "epoch": 0.41536,
      "grad_norm": 0.05486683547496796,
      "learning_rate": 7.923360000000002e-06,
      "loss": 0.2156,
      "step": 12980
    },
    {
      "epoch": 0.41568,
      "grad_norm": 0.03051202930510044,
      "learning_rate": 7.92176e-06,
      "loss": 0.1992,
      "step": 12990
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.03240128979086876,
      "learning_rate": 7.920160000000001e-06,
      "loss": 0.1994,
      "step": 13000
    },
    {
      "epoch": 0.416,
      "eval_runtime": 55.6949,
      "eval_samples_per_second": 179.55,
      "eval_steps_per_second": 11.222,
      "step": 13000
    },
    {
      "epoch": 0.41632,
      "grad_norm": 0.023240268230438232,
      "learning_rate": 7.918560000000001e-06,
      "loss": 0.1993,
      "step": 13010
    },
    {
      "epoch": 0.41664,
      "grad_norm": 0.027025602757930756,
      "learning_rate": 7.91696e-06,
      "loss": 0.2128,
      "step": 13020
    },
    {
      "epoch": 0.41696,
      "grad_norm": 0.043057482689619064,
      "learning_rate": 7.915360000000001e-06,
      "loss": 0.1991,
      "step": 13030
    },
    {
      "epoch": 0.41728,
      "grad_norm": 0.11623572558164597,
      "learning_rate": 7.913760000000001e-06,
      "loss": 0.1994,
      "step": 13040
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.021296516060829163,
      "learning_rate": 7.91216e-06,
      "loss": 0.2154,
      "step": 13050
    },
    {
      "epoch": 0.41792,
      "grad_norm": 0.014503954909741879,
      "learning_rate": 7.91056e-06,
      "loss": 0.2128,
      "step": 13060
    },
    {
      "epoch": 0.41824,
      "grad_norm": 0.04262160882353783,
      "learning_rate": 7.90896e-06,
      "loss": 0.2141,
      "step": 13070
    },
    {
      "epoch": 0.41856,
      "grad_norm": 0.03520657494664192,
      "learning_rate": 7.90736e-06,
      "loss": 0.1997,
      "step": 13080
    },
    {
      "epoch": 0.41888,
      "grad_norm": 0.012125661596655846,
      "learning_rate": 7.90576e-06,
      "loss": 0.2007,
      "step": 13090
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.0394970141351223,
      "learning_rate": 7.90416e-06,
      "loss": 0.2196,
      "step": 13100
    },
    {
      "epoch": 0.41952,
      "grad_norm": 0.025813452899456024,
      "learning_rate": 7.90256e-06,
      "loss": 0.1994,
      "step": 13110
    },
    {
      "epoch": 0.41984,
      "grad_norm": 0.05962839350104332,
      "learning_rate": 7.900960000000002e-06,
      "loss": 0.212,
      "step": 13120
    },
    {
      "epoch": 0.42016,
      "grad_norm": 0.031519513577222824,
      "learning_rate": 7.89936e-06,
      "loss": 0.1993,
      "step": 13130
    },
    {
      "epoch": 0.42048,
      "grad_norm": 0.04603371396660805,
      "learning_rate": 7.897760000000001e-06,
      "loss": 0.2024,
      "step": 13140
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.040644172579050064,
      "learning_rate": 7.896160000000001e-06,
      "loss": 0.2121,
      "step": 13150
    },
    {
      "epoch": 0.42112,
      "grad_norm": 0.022377150133252144,
      "learning_rate": 7.894560000000001e-06,
      "loss": 0.1994,
      "step": 13160
    },
    {
      "epoch": 0.42144,
      "grad_norm": 0.041028402745723724,
      "learning_rate": 7.892960000000001e-06,
      "loss": 0.1996,
      "step": 13170
    },
    {
      "epoch": 0.42176,
      "grad_norm": 0.028110792860388756,
      "learning_rate": 7.891360000000001e-06,
      "loss": 0.1993,
      "step": 13180
    },
    {
      "epoch": 0.42208,
      "grad_norm": 0.0426122322678566,
      "learning_rate": 7.88976e-06,
      "loss": 0.2317,
      "step": 13190
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.02516450546681881,
      "learning_rate": 7.88816e-06,
      "loss": 0.2082,
      "step": 13200
    },
    {
      "epoch": 0.42272,
      "grad_norm": 0.03219403326511383,
      "learning_rate": 7.88656e-06,
      "loss": 0.2203,
      "step": 13210
    },
    {
      "epoch": 0.42304,
      "grad_norm": 0.042376961559057236,
      "learning_rate": 7.88496e-06,
      "loss": 0.1993,
      "step": 13220
    },
    {
      "epoch": 0.42336,
      "grad_norm": 0.02919044718146324,
      "learning_rate": 7.88336e-06,
      "loss": 0.1995,
      "step": 13230
    },
    {
      "epoch": 0.42368,
      "grad_norm": 0.030781395733356476,
      "learning_rate": 7.88176e-06,
      "loss": 0.224,
      "step": 13240
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.02697509154677391,
      "learning_rate": 7.880160000000002e-06,
      "loss": 0.1995,
      "step": 13250
    },
    {
      "epoch": 0.42432,
      "grad_norm": 0.016942430287599564,
      "learning_rate": 7.87856e-06,
      "loss": 0.2012,
      "step": 13260
    },
    {
      "epoch": 0.42464,
      "grad_norm": 0.026482047513127327,
      "learning_rate": 7.87696e-06,
      "loss": 0.1994,
      "step": 13270
    },
    {
      "epoch": 0.42496,
      "grad_norm": 0.03596359118819237,
      "learning_rate": 7.875360000000001e-06,
      "loss": 0.21,
      "step": 13280
    },
    {
      "epoch": 0.42528,
      "grad_norm": 0.015236440114676952,
      "learning_rate": 7.87376e-06,
      "loss": 0.1992,
      "step": 13290
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.02629404328763485,
      "learning_rate": 7.872160000000001e-06,
      "loss": 0.1993,
      "step": 13300
    },
    {
      "epoch": 0.42592,
      "grad_norm": 0.02697782963514328,
      "learning_rate": 7.870560000000001e-06,
      "loss": 0.1993,
      "step": 13310
    },
    {
      "epoch": 0.42624,
      "grad_norm": 0.026872796937823296,
      "learning_rate": 7.868960000000001e-06,
      "loss": 0.1997,
      "step": 13320
    },
    {
      "epoch": 0.42656,
      "grad_norm": 0.0347137488424778,
      "learning_rate": 7.867360000000001e-06,
      "loss": 0.2191,
      "step": 13330
    },
    {
      "epoch": 0.42688,
      "grad_norm": 0.0886070728302002,
      "learning_rate": 7.86576e-06,
      "loss": 0.1994,
      "step": 13340
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.019458450376987457,
      "learning_rate": 7.86416e-06,
      "loss": 0.2,
      "step": 13350
    },
    {
      "epoch": 0.42752,
      "grad_norm": 3.378933906555176,
      "learning_rate": 7.86256e-06,
      "loss": 0.2141,
      "step": 13360
    },
    {
      "epoch": 0.42784,
      "grad_norm": 0.02740837261080742,
      "learning_rate": 7.86096e-06,
      "loss": 0.2159,
      "step": 13370
    },
    {
      "epoch": 0.42816,
      "grad_norm": 0.053512196987867355,
      "learning_rate": 7.85936e-06,
      "loss": 0.2268,
      "step": 13380
    },
    {
      "epoch": 0.42848,
      "grad_norm": 0.024996988475322723,
      "learning_rate": 7.85776e-06,
      "loss": 0.1995,
      "step": 13390
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.06395066529512405,
      "learning_rate": 7.85616e-06,
      "loss": 0.2159,
      "step": 13400
    },
    {
      "epoch": 0.42912,
      "grad_norm": 0.03751375898718834,
      "learning_rate": 7.854560000000002e-06,
      "loss": 0.1995,
      "step": 13410
    },
    {
      "epoch": 0.42944,
      "grad_norm": 0.039230335503816605,
      "learning_rate": 7.85296e-06,
      "loss": 0.1993,
      "step": 13420
    },
    {
      "epoch": 0.42976,
      "grad_norm": 0.02542436681687832,
      "learning_rate": 7.85136e-06,
      "loss": 0.1994,
      "step": 13430
    },
    {
      "epoch": 0.43008,
      "grad_norm": 0.022955475375056267,
      "learning_rate": 7.849760000000001e-06,
      "loss": 0.1993,
      "step": 13440
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.029510075226426125,
      "learning_rate": 7.84816e-06,
      "loss": 0.1995,
      "step": 13450
    },
    {
      "epoch": 0.43072,
      "grad_norm": 0.01480503473430872,
      "learning_rate": 7.846560000000001e-06,
      "loss": 0.1993,
      "step": 13460
    },
    {
      "epoch": 0.43104,
      "grad_norm": 0.017938684672117233,
      "learning_rate": 7.844960000000001e-06,
      "loss": 0.1993,
      "step": 13470
    },
    {
      "epoch": 0.43136,
      "grad_norm": 0.035749807953834534,
      "learning_rate": 7.84336e-06,
      "loss": 0.2141,
      "step": 13480
    },
    {
      "epoch": 0.43168,
      "grad_norm": 0.05120782181620598,
      "learning_rate": 7.84176e-06,
      "loss": 0.1992,
      "step": 13490
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.0625486969947815,
      "learning_rate": 7.84016e-06,
      "loss": 0.2142,
      "step": 13500
    },
    {
      "epoch": 0.43232,
      "grad_norm": 0.08057587593793869,
      "learning_rate": 7.83856e-06,
      "loss": 0.1994,
      "step": 13510
    },
    {
      "epoch": 0.43264,
      "grad_norm": 0.024558458477258682,
      "learning_rate": 7.83696e-06,
      "loss": 0.2001,
      "step": 13520
    },
    {
      "epoch": 0.43296,
      "grad_norm": 0.01252063550055027,
      "learning_rate": 7.83536e-06,
      "loss": 0.1993,
      "step": 13530
    },
    {
      "epoch": 0.43328,
      "grad_norm": 0.04468650370836258,
      "learning_rate": 7.83376e-06,
      "loss": 0.2116,
      "step": 13540
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.023580282926559448,
      "learning_rate": 7.832160000000002e-06,
      "loss": 0.1993,
      "step": 13550
    },
    {
      "epoch": 0.43392,
      "grad_norm": 0.034612007439136505,
      "learning_rate": 7.83056e-06,
      "loss": 0.232,
      "step": 13560
    },
    {
      "epoch": 0.43424,
      "grad_norm": 0.02432337775826454,
      "learning_rate": 7.828960000000001e-06,
      "loss": 0.1995,
      "step": 13570
    },
    {
      "epoch": 0.43456,
      "grad_norm": 0.06937164068222046,
      "learning_rate": 7.827360000000001e-06,
      "loss": 0.1995,
      "step": 13580
    },
    {
      "epoch": 0.43488,
      "grad_norm": 0.025943594053387642,
      "learning_rate": 7.825760000000001e-06,
      "loss": 0.1995,
      "step": 13590
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.05330671742558479,
      "learning_rate": 7.824160000000001e-06,
      "loss": 0.2133,
      "step": 13600
    },
    {
      "epoch": 0.43552,
      "grad_norm": 0.013101550750434399,
      "learning_rate": 7.822560000000001e-06,
      "loss": 0.2017,
      "step": 13610
    },
    {
      "epoch": 0.43584,
      "grad_norm": 0.008040149696171284,
      "learning_rate": 7.82096e-06,
      "loss": 0.213,
      "step": 13620
    },
    {
      "epoch": 0.43616,
      "grad_norm": 0.040128037333488464,
      "learning_rate": 7.81936e-06,
      "loss": 0.1993,
      "step": 13630
    },
    {
      "epoch": 0.43648,
      "grad_norm": 0.02137051150202751,
      "learning_rate": 7.81776e-06,
      "loss": 0.2134,
      "step": 13640
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.08125607669353485,
      "learning_rate": 7.81616e-06,
      "loss": 0.1994,
      "step": 13650
    },
    {
      "epoch": 0.43712,
      "grad_norm": 0.02770383469760418,
      "learning_rate": 7.81456e-06,
      "loss": 0.1995,
      "step": 13660
    },
    {
      "epoch": 0.43744,
      "grad_norm": 0.7774343490600586,
      "learning_rate": 7.81296e-06,
      "loss": 0.2159,
      "step": 13670
    },
    {
      "epoch": 0.43776,
      "grad_norm": 0.023860501125454903,
      "learning_rate": 7.811360000000002e-06,
      "loss": 0.2139,
      "step": 13680
    },
    {
      "epoch": 0.43808,
      "grad_norm": 0.023592732846736908,
      "learning_rate": 7.80976e-06,
      "loss": 0.2,
      "step": 13690
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.03930836170911789,
      "learning_rate": 7.80816e-06,
      "loss": 0.2153,
      "step": 13700
    },
    {
      "epoch": 0.43872,
      "grad_norm": 0.02015068754553795,
      "learning_rate": 7.806560000000001e-06,
      "loss": 0.2148,
      "step": 13710
    },
    {
      "epoch": 0.43904,
      "grad_norm": 0.055203404277563095,
      "learning_rate": 7.80496e-06,
      "loss": 0.1998,
      "step": 13720
    },
    {
      "epoch": 0.43936,
      "grad_norm": 0.0699821189045906,
      "learning_rate": 7.803360000000001e-06,
      "loss": 0.1999,
      "step": 13730
    },
    {
      "epoch": 0.43968,
      "grad_norm": 0.01905914396047592,
      "learning_rate": 7.801760000000001e-06,
      "loss": 0.2384,
      "step": 13740
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.09250304102897644,
      "learning_rate": 7.800160000000001e-06,
      "loss": 0.216,
      "step": 13750
    },
    {
      "epoch": 0.44032,
      "grad_norm": 1.2693363428115845,
      "learning_rate": 7.79856e-06,
      "loss": 0.2105,
      "step": 13760
    },
    {
      "epoch": 0.44064,
      "grad_norm": 0.03388233855366707,
      "learning_rate": 7.79696e-06,
      "loss": 0.1993,
      "step": 13770
    },
    {
      "epoch": 0.44096,
      "grad_norm": 0.042686861008405685,
      "learning_rate": 7.79536e-06,
      "loss": 0.2155,
      "step": 13780
    },
    {
      "epoch": 0.44128,
      "grad_norm": 0.04531148076057434,
      "learning_rate": 7.79376e-06,
      "loss": 0.2005,
      "step": 13790
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.02952522411942482,
      "learning_rate": 7.79216e-06,
      "loss": 0.1992,
      "step": 13800
    },
    {
      "epoch": 0.44192,
      "grad_norm": 0.04237392544746399,
      "learning_rate": 7.79056e-06,
      "loss": 0.1994,
      "step": 13810
    },
    {
      "epoch": 0.44224,
      "grad_norm": 0.05876984819769859,
      "learning_rate": 7.788960000000002e-06,
      "loss": 0.1996,
      "step": 13820
    },
    {
      "epoch": 0.44256,
      "grad_norm": 0.012005629017949104,
      "learning_rate": 7.78736e-06,
      "loss": 0.1999,
      "step": 13830
    },
    {
      "epoch": 0.44288,
      "grad_norm": 0.012104704044759274,
      "learning_rate": 7.785760000000001e-06,
      "loss": 0.2209,
      "step": 13840
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.017390122637152672,
      "learning_rate": 7.784160000000001e-06,
      "loss": 0.1993,
      "step": 13850
    },
    {
      "epoch": 0.44352,
      "grad_norm": 0.040333639830350876,
      "learning_rate": 7.78256e-06,
      "loss": 0.2141,
      "step": 13860
    },
    {
      "epoch": 0.44384,
      "grad_norm": 0.02359650284051895,
      "learning_rate": 7.780960000000001e-06,
      "loss": 0.2001,
      "step": 13870
    },
    {
      "epoch": 0.44416,
      "grad_norm": 0.0191782359033823,
      "learning_rate": 7.779360000000001e-06,
      "loss": 0.2009,
      "step": 13880
    },
    {
      "epoch": 0.44448,
      "grad_norm": 0.029610006138682365,
      "learning_rate": 7.777760000000001e-06,
      "loss": 0.2001,
      "step": 13890
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.05822460725903511,
      "learning_rate": 7.77616e-06,
      "loss": 0.1997,
      "step": 13900
    },
    {
      "epoch": 0.44512,
      "grad_norm": 0.04316132888197899,
      "learning_rate": 7.77456e-06,
      "loss": 0.2007,
      "step": 13910
    },
    {
      "epoch": 0.44544,
      "grad_norm": 0.01944943703711033,
      "learning_rate": 7.77296e-06,
      "loss": 0.2141,
      "step": 13920
    },
    {
      "epoch": 0.44576,
      "grad_norm": 0.01698051206767559,
      "learning_rate": 7.77136e-06,
      "loss": 0.1993,
      "step": 13930
    },
    {
      "epoch": 0.44608,
      "grad_norm": 0.0241454616189003,
      "learning_rate": 7.76976e-06,
      "loss": 0.1992,
      "step": 13940
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.02511109970510006,
      "learning_rate": 7.76816e-06,
      "loss": 0.1996,
      "step": 13950
    },
    {
      "epoch": 0.44672,
      "grad_norm": 0.027055928483605385,
      "learning_rate": 7.76656e-06,
      "loss": 0.1992,
      "step": 13960
    },
    {
      "epoch": 0.44704,
      "grad_norm": 0.021654317155480385,
      "learning_rate": 7.76496e-06,
      "loss": 0.1994,
      "step": 13970
    },
    {
      "epoch": 0.44736,
      "grad_norm": 0.02485903538763523,
      "learning_rate": 7.763360000000002e-06,
      "loss": 0.2111,
      "step": 13980
    },
    {
      "epoch": 0.44768,
      "grad_norm": 0.03765132650732994,
      "learning_rate": 7.76176e-06,
      "loss": 0.1994,
      "step": 13990
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.026558302342891693,
      "learning_rate": 7.760160000000001e-06,
      "loss": 0.1993,
      "step": 14000
    },
    {
      "epoch": 0.448,
      "eval_runtime": 52.728,
      "eval_samples_per_second": 189.653,
      "eval_steps_per_second": 11.853,
      "step": 14000
    },
    {
      "epoch": 0.44832,
      "grad_norm": 0.02537621185183525,
      "learning_rate": 7.758560000000001e-06,
      "loss": 0.1997,
      "step": 14010
    },
    {
      "epoch": 0.44864,
      "grad_norm": 0.1810462474822998,
      "learning_rate": 7.756960000000001e-06,
      "loss": 0.2,
      "step": 14020
    },
    {
      "epoch": 0.44896,
      "grad_norm": 0.7076647281646729,
      "learning_rate": 7.755360000000001e-06,
      "loss": 0.2128,
      "step": 14030
    },
    {
      "epoch": 0.44928,
      "grad_norm": 0.7266632914543152,
      "learning_rate": 7.75376e-06,
      "loss": 0.2165,
      "step": 14040
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.02622915245592594,
      "learning_rate": 7.75216e-06,
      "loss": 0.2032,
      "step": 14050
    },
    {
      "epoch": 0.44992,
      "grad_norm": 0.011491535231471062,
      "learning_rate": 7.75056e-06,
      "loss": 0.1993,
      "step": 14060
    },
    {
      "epoch": 0.45024,
      "grad_norm": 0.021924110129475594,
      "learning_rate": 7.74896e-06,
      "loss": 0.2172,
      "step": 14070
    },
    {
      "epoch": 0.45056,
      "grad_norm": 0.029195310547947884,
      "learning_rate": 7.74736e-06,
      "loss": 0.2139,
      "step": 14080
    },
    {
      "epoch": 0.45088,
      "grad_norm": 0.020047219470143318,
      "learning_rate": 7.745760000000002e-06,
      "loss": 0.1996,
      "step": 14090
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.019472308456897736,
      "learning_rate": 7.74416e-06,
      "loss": 0.2003,
      "step": 14100
    },
    {
      "epoch": 0.45152,
      "grad_norm": 0.057953283190727234,
      "learning_rate": 7.742560000000002e-06,
      "loss": 0.204,
      "step": 14110
    },
    {
      "epoch": 0.45184,
      "grad_norm": 1.0139422416687012,
      "learning_rate": 7.740960000000002e-06,
      "loss": 0.2154,
      "step": 14120
    },
    {
      "epoch": 0.45216,
      "grad_norm": 0.01718965172767639,
      "learning_rate": 7.73936e-06,
      "loss": 0.1993,
      "step": 14130
    },
    {
      "epoch": 0.45248,
      "grad_norm": 0.022310785949230194,
      "learning_rate": 7.737760000000001e-06,
      "loss": 0.1992,
      "step": 14140
    },
    {
      "epoch": 0.4528,
      "grad_norm": 2.261600971221924,
      "learning_rate": 7.736160000000001e-06,
      "loss": 0.2052,
      "step": 14150
    },
    {
      "epoch": 0.45312,
      "grad_norm": 0.021973956376314163,
      "learning_rate": 7.734560000000001e-06,
      "loss": 0.2078,
      "step": 14160
    },
    {
      "epoch": 0.45344,
      "grad_norm": 0.05332549288868904,
      "learning_rate": 7.732960000000001e-06,
      "loss": 0.1994,
      "step": 14170
    },
    {
      "epoch": 0.45376,
      "grad_norm": 0.027972914278507233,
      "learning_rate": 7.73136e-06,
      "loss": 0.1991,
      "step": 14180
    },
    {
      "epoch": 0.45408,
      "grad_norm": 0.500636637210846,
      "learning_rate": 7.72976e-06,
      "loss": 0.2002,
      "step": 14190
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.027691083028912544,
      "learning_rate": 7.72816e-06,
      "loss": 0.2164,
      "step": 14200
    },
    {
      "epoch": 0.45472,
      "grad_norm": 0.024625390768051147,
      "learning_rate": 7.72656e-06,
      "loss": 0.1994,
      "step": 14210
    },
    {
      "epoch": 0.45504,
      "grad_norm": 0.03412948176264763,
      "learning_rate": 7.72496e-06,
      "loss": 0.1995,
      "step": 14220
    },
    {
      "epoch": 0.45536,
      "grad_norm": 0.742174506187439,
      "learning_rate": 7.72336e-06,
      "loss": 0.2161,
      "step": 14230
    },
    {
      "epoch": 0.45568,
      "grad_norm": 1.4724540710449219,
      "learning_rate": 7.72176e-06,
      "loss": 0.2229,
      "step": 14240
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.02649640291929245,
      "learning_rate": 7.720160000000002e-06,
      "loss": 0.2004,
      "step": 14250
    },
    {
      "epoch": 0.45632,
      "grad_norm": 0.03299326077103615,
      "learning_rate": 7.71856e-06,
      "loss": 0.1992,
      "step": 14260
    },
    {
      "epoch": 0.45664,
      "grad_norm": 0.047270942479372025,
      "learning_rate": 7.716960000000001e-06,
      "loss": 0.2158,
      "step": 14270
    },
    {
      "epoch": 0.45696,
      "grad_norm": 0.03804272413253784,
      "learning_rate": 7.715360000000001e-06,
      "loss": 0.2118,
      "step": 14280
    },
    {
      "epoch": 0.45728,
      "grad_norm": 0.046460773795843124,
      "learning_rate": 7.71376e-06,
      "loss": 0.2014,
      "step": 14290
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.03859565407037735,
      "learning_rate": 7.712160000000001e-06,
      "loss": 0.2299,
      "step": 14300
    },
    {
      "epoch": 0.45792,
      "grad_norm": 0.018042460083961487,
      "learning_rate": 7.710560000000001e-06,
      "loss": 0.1993,
      "step": 14310
    },
    {
      "epoch": 0.45824,
      "grad_norm": 0.027088584378361702,
      "learning_rate": 7.70896e-06,
      "loss": 0.2011,
      "step": 14320
    },
    {
      "epoch": 0.45856,
      "grad_norm": 0.03378098085522652,
      "learning_rate": 7.70736e-06,
      "loss": 0.203,
      "step": 14330
    },
    {
      "epoch": 0.45888,
      "grad_norm": 0.033799197524785995,
      "learning_rate": 7.70576e-06,
      "loss": 0.1993,
      "step": 14340
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.04026472568511963,
      "learning_rate": 7.70416e-06,
      "loss": 0.2017,
      "step": 14350
    },
    {
      "epoch": 0.45952,
      "grad_norm": 0.01784277893602848,
      "learning_rate": 7.70256e-06,
      "loss": 0.2066,
      "step": 14360
    },
    {
      "epoch": 0.45984,
      "grad_norm": 0.9638417363166809,
      "learning_rate": 7.70096e-06,
      "loss": 0.2164,
      "step": 14370
    },
    {
      "epoch": 0.46016,
      "grad_norm": 0.06301531195640564,
      "learning_rate": 7.69936e-06,
      "loss": 0.2146,
      "step": 14380
    },
    {
      "epoch": 0.46048,
      "grad_norm": 0.011440667323768139,
      "learning_rate": 7.69776e-06,
      "loss": 0.1995,
      "step": 14390
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.14242936670780182,
      "learning_rate": 7.69616e-06,
      "loss": 0.1995,
      "step": 14400
    },
    {
      "epoch": 0.46112,
      "grad_norm": 0.01549596805125475,
      "learning_rate": 7.694560000000001e-06,
      "loss": 0.2144,
      "step": 14410
    },
    {
      "epoch": 0.46144,
      "grad_norm": 0.014450890012085438,
      "learning_rate": 7.69296e-06,
      "loss": 0.1994,
      "step": 14420
    },
    {
      "epoch": 0.46176,
      "grad_norm": 0.041349273175001144,
      "learning_rate": 7.691360000000001e-06,
      "loss": 0.1995,
      "step": 14430
    },
    {
      "epoch": 0.46208,
      "grad_norm": 0.03465958312153816,
      "learning_rate": 7.689760000000001e-06,
      "loss": 0.2004,
      "step": 14440
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.010455045849084854,
      "learning_rate": 7.688160000000001e-06,
      "loss": 0.2175,
      "step": 14450
    },
    {
      "epoch": 0.46272,
      "grad_norm": 0.027120908722281456,
      "learning_rate": 7.68656e-06,
      "loss": 0.1998,
      "step": 14460
    },
    {
      "epoch": 0.46304,
      "grad_norm": 0.033551424741744995,
      "learning_rate": 7.68496e-06,
      "loss": 0.1996,
      "step": 14470
    },
    {
      "epoch": 0.46336,
      "grad_norm": 0.02216891571879387,
      "learning_rate": 7.68336e-06,
      "loss": 0.2148,
      "step": 14480
    },
    {
      "epoch": 0.46368,
      "grad_norm": 0.021518496796488762,
      "learning_rate": 7.68176e-06,
      "loss": 0.2019,
      "step": 14490
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.027099091559648514,
      "learning_rate": 7.68016e-06,
      "loss": 0.2019,
      "step": 14500
    },
    {
      "epoch": 0.46432,
      "grad_norm": 0.024592410773038864,
      "learning_rate": 7.67856e-06,
      "loss": 0.1994,
      "step": 14510
    },
    {
      "epoch": 0.46464,
      "grad_norm": 0.02381596527993679,
      "learning_rate": 7.676960000000002e-06,
      "loss": 0.1995,
      "step": 14520
    },
    {
      "epoch": 0.46496,
      "grad_norm": 0.02734932117164135,
      "learning_rate": 7.67536e-06,
      "loss": 0.2001,
      "step": 14530
    },
    {
      "epoch": 0.46528,
      "grad_norm": 0.016302043572068214,
      "learning_rate": 7.67376e-06,
      "loss": 0.2037,
      "step": 14540
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.01684231124818325,
      "learning_rate": 7.672160000000001e-06,
      "loss": 0.1994,
      "step": 14550
    },
    {
      "epoch": 0.46592,
      "grad_norm": 0.013919499702751637,
      "learning_rate": 7.67056e-06,
      "loss": 0.1994,
      "step": 14560
    },
    {
      "epoch": 0.46624,
      "grad_norm": 0.022057030349969864,
      "learning_rate": 7.668960000000001e-06,
      "loss": 0.2187,
      "step": 14570
    },
    {
      "epoch": 0.46656,
      "grad_norm": 0.023701107129454613,
      "learning_rate": 7.667360000000001e-06,
      "loss": 0.1994,
      "step": 14580
    },
    {
      "epoch": 0.46688,
      "grad_norm": 0.015978582203388214,
      "learning_rate": 7.665760000000001e-06,
      "loss": 0.2128,
      "step": 14590
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.008527141995728016,
      "learning_rate": 7.66416e-06,
      "loss": 0.1991,
      "step": 14600
    },
    {
      "epoch": 0.46752,
      "grad_norm": 0.06486712396144867,
      "learning_rate": 7.66256e-06,
      "loss": 0.1996,
      "step": 14610
    },
    {
      "epoch": 0.46784,
      "grad_norm": 0.012752083130180836,
      "learning_rate": 7.66096e-06,
      "loss": 0.1995,
      "step": 14620
    },
    {
      "epoch": 0.46816,
      "grad_norm": 0.027749240398406982,
      "learning_rate": 7.65936e-06,
      "loss": 0.2001,
      "step": 14630
    },
    {
      "epoch": 0.46848,
      "grad_norm": 0.07134487479925156,
      "learning_rate": 7.65776e-06,
      "loss": 0.1992,
      "step": 14640
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.030721105635166168,
      "learning_rate": 7.65616e-06,
      "loss": 0.1993,
      "step": 14650
    },
    {
      "epoch": 0.46912,
      "grad_norm": 0.029291227459907532,
      "learning_rate": 7.65456e-06,
      "loss": 0.2083,
      "step": 14660
    },
    {
      "epoch": 0.46944,
      "grad_norm": 0.03642166405916214,
      "learning_rate": 7.65296e-06,
      "loss": 0.2145,
      "step": 14670
    },
    {
      "epoch": 0.46976,
      "grad_norm": 0.028011856600642204,
      "learning_rate": 7.651360000000002e-06,
      "loss": 0.1992,
      "step": 14680
    },
    {
      "epoch": 0.47008,
      "grad_norm": 0.01509191282093525,
      "learning_rate": 7.64976e-06,
      "loss": 0.1997,
      "step": 14690
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.03199540823698044,
      "learning_rate": 7.648160000000001e-06,
      "loss": 0.1992,
      "step": 14700
    },
    {
      "epoch": 0.47072,
      "grad_norm": 0.018297482281923294,
      "learning_rate": 7.646560000000001e-06,
      "loss": 0.1995,
      "step": 14710
    },
    {
      "epoch": 0.47104,
      "grad_norm": 0.043543267995119095,
      "learning_rate": 7.64496e-06,
      "loss": 0.1995,
      "step": 14720
    },
    {
      "epoch": 0.47136,
      "grad_norm": 0.02587776444852352,
      "learning_rate": 7.643360000000001e-06,
      "loss": 0.1993,
      "step": 14730
    },
    {
      "epoch": 0.47168,
      "grad_norm": 0.032991088926792145,
      "learning_rate": 7.64176e-06,
      "loss": 0.1995,
      "step": 14740
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.03726313263177872,
      "learning_rate": 7.64016e-06,
      "loss": 0.1995,
      "step": 14750
    },
    {
      "epoch": 0.47232,
      "grad_norm": 0.02268589474260807,
      "learning_rate": 7.63856e-06,
      "loss": 0.1992,
      "step": 14760
    },
    {
      "epoch": 0.47264,
      "grad_norm": 0.012256626039743423,
      "learning_rate": 7.63696e-06,
      "loss": 0.213,
      "step": 14770
    },
    {
      "epoch": 0.47296,
      "grad_norm": 0.09243176132440567,
      "learning_rate": 7.63536e-06,
      "loss": 0.2007,
      "step": 14780
    },
    {
      "epoch": 0.47328,
      "grad_norm": 0.05125318467617035,
      "learning_rate": 7.633760000000002e-06,
      "loss": 0.2125,
      "step": 14790
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.04600970074534416,
      "learning_rate": 7.63216e-06,
      "loss": 0.1993,
      "step": 14800
    },
    {
      "epoch": 0.47392,
      "grad_norm": 0.029210878536105156,
      "learning_rate": 7.63056e-06,
      "loss": 0.1995,
      "step": 14810
    },
    {
      "epoch": 0.47424,
      "grad_norm": 0.020315783098340034,
      "learning_rate": 7.628960000000001e-06,
      "loss": 0.2103,
      "step": 14820
    },
    {
      "epoch": 0.47456,
      "grad_norm": 0.037934280931949615,
      "learning_rate": 7.627360000000001e-06,
      "loss": 0.2299,
      "step": 14830
    },
    {
      "epoch": 0.47488,
      "grad_norm": 0.022038768976926804,
      "learning_rate": 7.625760000000001e-06,
      "loss": 0.2117,
      "step": 14840
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.013825244270265102,
      "learning_rate": 7.62416e-06,
      "loss": 0.2003,
      "step": 14850
    },
    {
      "epoch": 0.47552,
      "grad_norm": 0.017566898837685585,
      "learning_rate": 7.622560000000001e-06,
      "loss": 0.1993,
      "step": 14860
    },
    {
      "epoch": 0.47584,
      "grad_norm": 0.032238174229860306,
      "learning_rate": 7.620960000000001e-06,
      "loss": 0.2007,
      "step": 14870
    },
    {
      "epoch": 0.47616,
      "grad_norm": 0.18684594333171844,
      "learning_rate": 7.61936e-06,
      "loss": 0.1999,
      "step": 14880
    },
    {
      "epoch": 0.47648,
      "grad_norm": 0.02840227447450161,
      "learning_rate": 7.617760000000001e-06,
      "loss": 0.1992,
      "step": 14890
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.16621139645576477,
      "learning_rate": 7.616160000000001e-06,
      "loss": 0.2138,
      "step": 14900
    },
    {
      "epoch": 0.47712,
      "grad_norm": 0.020779749378561974,
      "learning_rate": 7.6145600000000005e-06,
      "loss": 0.1991,
      "step": 14910
    },
    {
      "epoch": 0.47744,
      "grad_norm": 0.035608962178230286,
      "learning_rate": 7.61296e-06,
      "loss": 0.1991,
      "step": 14920
    },
    {
      "epoch": 0.47776,
      "grad_norm": 0.02045912854373455,
      "learning_rate": 7.611360000000001e-06,
      "loss": 0.1995,
      "step": 14930
    },
    {
      "epoch": 0.47808,
      "grad_norm": 0.04782412201166153,
      "learning_rate": 7.60976e-06,
      "loss": 0.1992,
      "step": 14940
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.03613680228590965,
      "learning_rate": 7.608160000000001e-06,
      "loss": 0.2003,
      "step": 14950
    },
    {
      "epoch": 0.47872,
      "grad_norm": 0.11373197287321091,
      "learning_rate": 7.606560000000001e-06,
      "loss": 0.2033,
      "step": 14960
    },
    {
      "epoch": 0.47904,
      "grad_norm": 0.042644865810871124,
      "learning_rate": 7.60496e-06,
      "loss": 0.1993,
      "step": 14970
    },
    {
      "epoch": 0.47936,
      "grad_norm": 0.030146727338433266,
      "learning_rate": 7.6033600000000005e-06,
      "loss": 0.2138,
      "step": 14980
    },
    {
      "epoch": 0.47968,
      "grad_norm": 0.020862996578216553,
      "learning_rate": 7.60176e-06,
      "loss": 0.2125,
      "step": 14990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.04469393193721771,
      "learning_rate": 7.600160000000001e-06,
      "loss": 0.1997,
      "step": 15000
    },
    {
      "epoch": 0.48,
      "eval_runtime": 49.1748,
      "eval_samples_per_second": 203.356,
      "eval_steps_per_second": 12.71,
      "step": 15000
    },
    {
      "epoch": 0.48032,
      "grad_norm": 0.022466493770480156,
      "learning_rate": 7.59856e-06,
      "loss": 0.1994,
      "step": 15010
    },
    {
      "epoch": 0.48064,
      "grad_norm": 0.040837060660123825,
      "learning_rate": 7.596960000000001e-06,
      "loss": 0.2022,
      "step": 15020
    },
    {
      "epoch": 0.48096,
      "grad_norm": 0.03131343424320221,
      "learning_rate": 7.595360000000001e-06,
      "loss": 0.1992,
      "step": 15030
    },
    {
      "epoch": 0.48128,
      "grad_norm": 0.026538211852312088,
      "learning_rate": 7.5937600000000015e-06,
      "loss": 0.2025,
      "step": 15040
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.025056876242160797,
      "learning_rate": 7.5921600000000005e-06,
      "loss": 0.1993,
      "step": 15050
    },
    {
      "epoch": 0.48192,
      "grad_norm": 0.02042754553258419,
      "learning_rate": 7.59056e-06,
      "loss": 0.1993,
      "step": 15060
    },
    {
      "epoch": 0.48224,
      "grad_norm": 0.0368448905646801,
      "learning_rate": 7.588960000000001e-06,
      "loss": 0.1994,
      "step": 15070
    },
    {
      "epoch": 0.48256,
      "grad_norm": 0.01458732783794403,
      "learning_rate": 7.58736e-06,
      "loss": 0.1995,
      "step": 15080
    },
    {
      "epoch": 0.48288,
      "grad_norm": 0.015762217342853546,
      "learning_rate": 7.585760000000001e-06,
      "loss": 0.201,
      "step": 15090
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.038364868611097336,
      "learning_rate": 7.584160000000001e-06,
      "loss": 0.2118,
      "step": 15100
    },
    {
      "epoch": 0.48352,
      "grad_norm": 0.018318792805075645,
      "learning_rate": 7.582560000000001e-06,
      "loss": 0.1992,
      "step": 15110
    },
    {
      "epoch": 0.48384,
      "grad_norm": 0.011885616928339005,
      "learning_rate": 7.5809600000000005e-06,
      "loss": 0.1993,
      "step": 15120
    },
    {
      "epoch": 0.48416,
      "grad_norm": 0.013345194980502129,
      "learning_rate": 7.579360000000001e-06,
      "loss": 0.1991,
      "step": 15130
    },
    {
      "epoch": 0.48448,
      "grad_norm": 0.010721337981522083,
      "learning_rate": 7.57776e-06,
      "loss": 0.2156,
      "step": 15140
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.05480040982365608,
      "learning_rate": 7.57616e-06,
      "loss": 0.1993,
      "step": 15150
    },
    {
      "epoch": 0.48512,
      "grad_norm": 0.7532787322998047,
      "learning_rate": 7.574560000000001e-06,
      "loss": 0.2166,
      "step": 15160
    },
    {
      "epoch": 0.48544,
      "grad_norm": 0.03794081509113312,
      "learning_rate": 7.57296e-06,
      "loss": 0.1992,
      "step": 15170
    },
    {
      "epoch": 0.48576,
      "grad_norm": 0.028894608840346336,
      "learning_rate": 7.571360000000001e-06,
      "loss": 0.1991,
      "step": 15180
    },
    {
      "epoch": 0.48608,
      "grad_norm": 0.013847745023667812,
      "learning_rate": 7.5697600000000005e-06,
      "loss": 0.1995,
      "step": 15190
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.0237156730145216,
      "learning_rate": 7.568160000000001e-06,
      "loss": 0.2053,
      "step": 15200
    },
    {
      "epoch": 0.48672,
      "grad_norm": 0.04169372096657753,
      "learning_rate": 7.56656e-06,
      "loss": 0.1993,
      "step": 15210
    },
    {
      "epoch": 0.48704,
      "grad_norm": 0.04923500120639801,
      "learning_rate": 7.56496e-06,
      "loss": 0.2287,
      "step": 15220
    },
    {
      "epoch": 0.48736,
      "grad_norm": 0.04326361417770386,
      "learning_rate": 7.563360000000001e-06,
      "loss": 0.2001,
      "step": 15230
    },
    {
      "epoch": 0.48768,
      "grad_norm": 0.017455685883760452,
      "learning_rate": 7.56176e-06,
      "loss": 0.1993,
      "step": 15240
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.028142163529992104,
      "learning_rate": 7.560160000000001e-06,
      "loss": 0.1996,
      "step": 15250
    },
    {
      "epoch": 0.48832,
      "grad_norm": 0.05791870504617691,
      "learning_rate": 7.5585600000000005e-06,
      "loss": 0.1994,
      "step": 15260
    },
    {
      "epoch": 0.48864,
      "grad_norm": 0.5482786893844604,
      "learning_rate": 7.556960000000001e-06,
      "loss": 0.2163,
      "step": 15270
    },
    {
      "epoch": 0.48896,
      "grad_norm": 0.03522588685154915,
      "learning_rate": 7.55536e-06,
      "loss": 0.2069,
      "step": 15280
    },
    {
      "epoch": 0.48928,
      "grad_norm": 0.0246666818857193,
      "learning_rate": 7.553760000000001e-06,
      "loss": 0.1993,
      "step": 15290
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.007419358007609844,
      "learning_rate": 7.552160000000001e-06,
      "loss": 0.2026,
      "step": 15300
    },
    {
      "epoch": 0.48992,
      "grad_norm": 0.02254674769937992,
      "learning_rate": 7.55056e-06,
      "loss": 0.1994,
      "step": 15310
    },
    {
      "epoch": 0.49024,
      "grad_norm": 0.0209958516061306,
      "learning_rate": 7.548960000000001e-06,
      "loss": 0.2113,
      "step": 15320
    },
    {
      "epoch": 0.49056,
      "grad_norm": 0.022374816238880157,
      "learning_rate": 7.5473600000000005e-06,
      "loss": 0.2145,
      "step": 15330
    },
    {
      "epoch": 0.49088,
      "grad_norm": 0.015825040638446808,
      "learning_rate": 7.545760000000001e-06,
      "loss": 0.1994,
      "step": 15340
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.028256699442863464,
      "learning_rate": 7.54416e-06,
      "loss": 0.1993,
      "step": 15350
    },
    {
      "epoch": 0.49152,
      "grad_norm": 0.05488499999046326,
      "learning_rate": 7.542560000000001e-06,
      "loss": 0.1997,
      "step": 15360
    },
    {
      "epoch": 0.49184,
      "grad_norm": 0.029435565695166588,
      "learning_rate": 7.540960000000001e-06,
      "loss": 0.2282,
      "step": 15370
    },
    {
      "epoch": 0.49216,
      "grad_norm": 0.030846796929836273,
      "learning_rate": 7.539360000000001e-06,
      "loss": 0.1993,
      "step": 15380
    },
    {
      "epoch": 0.49248,
      "grad_norm": 0.024292446672916412,
      "learning_rate": 7.537760000000001e-06,
      "loss": 0.1994,
      "step": 15390
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.017744846642017365,
      "learning_rate": 7.5361600000000005e-06,
      "loss": 0.1991,
      "step": 15400
    },
    {
      "epoch": 0.49312,
      "grad_norm": 0.021935658529400826,
      "learning_rate": 7.53456e-06,
      "loss": 0.1994,
      "step": 15410
    },
    {
      "epoch": 0.49344,
      "grad_norm": 0.010426707565784454,
      "learning_rate": 7.53296e-06,
      "loss": 0.2004,
      "step": 15420
    },
    {
      "epoch": 0.49376,
      "grad_norm": 0.03194137662649155,
      "learning_rate": 7.531360000000001e-06,
      "loss": 0.2001,
      "step": 15430
    },
    {
      "epoch": 0.49408,
      "grad_norm": 0.05573258176445961,
      "learning_rate": 7.52976e-06,
      "loss": 0.2166,
      "step": 15440
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.026463810354471207,
      "learning_rate": 7.528160000000001e-06,
      "loss": 0.1995,
      "step": 15450
    },
    {
      "epoch": 0.49472,
      "grad_norm": 0.03570558875799179,
      "learning_rate": 7.526560000000001e-06,
      "loss": 0.1991,
      "step": 15460
    },
    {
      "epoch": 0.49504,
      "grad_norm": 0.02930394560098648,
      "learning_rate": 7.524960000000001e-06,
      "loss": 0.1995,
      "step": 15470
    },
    {
      "epoch": 0.49536,
      "grad_norm": 0.032202959060668945,
      "learning_rate": 7.52336e-06,
      "loss": 0.1992,
      "step": 15480
    },
    {
      "epoch": 0.49568,
      "grad_norm": 0.016393881291151047,
      "learning_rate": 7.52176e-06,
      "loss": 0.2197,
      "step": 15490
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.03775796666741371,
      "learning_rate": 7.520160000000001e-06,
      "loss": 0.1994,
      "step": 15500
    },
    {
      "epoch": 0.49632,
      "grad_norm": 0.017898431047797203,
      "learning_rate": 7.51856e-06,
      "loss": 0.2201,
      "step": 15510
    },
    {
      "epoch": 0.49664,
      "grad_norm": 0.016779856756329536,
      "learning_rate": 7.516960000000001e-06,
      "loss": 0.2103,
      "step": 15520
    },
    {
      "epoch": 0.49696,
      "grad_norm": 0.024532856419682503,
      "learning_rate": 7.515360000000001e-06,
      "loss": 0.2033,
      "step": 15530
    },
    {
      "epoch": 0.49728,
      "grad_norm": 0.023931371048092842,
      "learning_rate": 7.513760000000001e-06,
      "loss": 0.1998,
      "step": 15540
    },
    {
      "epoch": 0.4976,
      "grad_norm": 2.473888397216797,
      "learning_rate": 7.5121600000000004e-06,
      "loss": 0.2198,
      "step": 15550
    },
    {
      "epoch": 0.49792,
      "grad_norm": 0.03762471675872803,
      "learning_rate": 7.51056e-06,
      "loss": 0.1994,
      "step": 15560
    },
    {
      "epoch": 0.49824,
      "grad_norm": 0.020578207448124886,
      "learning_rate": 7.508960000000001e-06,
      "loss": 0.1992,
      "step": 15570
    },
    {
      "epoch": 0.49856,
      "grad_norm": 0.041738077998161316,
      "learning_rate": 7.50736e-06,
      "loss": 0.1993,
      "step": 15580
    },
    {
      "epoch": 0.49888,
      "grad_norm": 0.017141183838248253,
      "learning_rate": 7.505760000000001e-06,
      "loss": 0.2167,
      "step": 15590
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.011316768825054169,
      "learning_rate": 7.504160000000001e-06,
      "loss": 0.1992,
      "step": 15600
    },
    {
      "epoch": 0.49952,
      "grad_norm": 0.018903421238064766,
      "learning_rate": 7.5025600000000006e-06,
      "loss": 0.1993,
      "step": 15610
    },
    {
      "epoch": 0.49984,
      "grad_norm": 0.030727844685316086,
      "learning_rate": 7.5009600000000004e-06,
      "loss": 0.1998,
      "step": 15620
    },
    {
      "epoch": 0.50016,
      "grad_norm": 0.026495063677430153,
      "learning_rate": 7.499360000000001e-06,
      "loss": 0.214,
      "step": 15630
    },
    {
      "epoch": 0.50048,
      "grad_norm": 0.01824444904923439,
      "learning_rate": 7.49776e-06,
      "loss": 0.1997,
      "step": 15640
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.0175044983625412,
      "learning_rate": 7.49616e-06,
      "loss": 0.1993,
      "step": 15650
    },
    {
      "epoch": 0.50112,
      "grad_norm": 0.04261785000562668,
      "learning_rate": 7.494560000000001e-06,
      "loss": 0.2147,
      "step": 15660
    },
    {
      "epoch": 0.50144,
      "grad_norm": 0.016945870593190193,
      "learning_rate": 7.49296e-06,
      "loss": 0.1996,
      "step": 15670
    },
    {
      "epoch": 0.50176,
      "grad_norm": 0.03321607783436775,
      "learning_rate": 7.4913600000000006e-06,
      "loss": 0.1992,
      "step": 15680
    },
    {
      "epoch": 0.50208,
      "grad_norm": 0.013110075145959854,
      "learning_rate": 7.4897600000000004e-06,
      "loss": 0.1994,
      "step": 15690
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.022519707679748535,
      "learning_rate": 7.488160000000001e-06,
      "loss": 0.2259,
      "step": 15700
    },
    {
      "epoch": 0.50272,
      "grad_norm": 0.020620763301849365,
      "learning_rate": 7.48656e-06,
      "loss": 0.201,
      "step": 15710
    },
    {
      "epoch": 0.50304,
      "grad_norm": 0.06630461663007736,
      "learning_rate": 7.484960000000001e-06,
      "loss": 0.2006,
      "step": 15720
    },
    {
      "epoch": 0.50336,
      "grad_norm": 0.011564741842448711,
      "learning_rate": 7.483360000000001e-06,
      "loss": 0.2119,
      "step": 15730
    },
    {
      "epoch": 0.50368,
      "grad_norm": 0.01500649657100439,
      "learning_rate": 7.48176e-06,
      "loss": 0.2145,
      "step": 15740
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.05799955502152443,
      "learning_rate": 7.4801600000000006e-06,
      "loss": 0.1994,
      "step": 15750
    },
    {
      "epoch": 0.50432,
      "grad_norm": 0.03567713871598244,
      "learning_rate": 7.4785600000000004e-06,
      "loss": 0.2149,
      "step": 15760
    },
    {
      "epoch": 0.50464,
      "grad_norm": 0.025145551189780235,
      "learning_rate": 7.476960000000001e-06,
      "loss": 0.1996,
      "step": 15770
    },
    {
      "epoch": 0.50496,
      "grad_norm": 0.7745233774185181,
      "learning_rate": 7.47536e-06,
      "loss": 0.2203,
      "step": 15780
    },
    {
      "epoch": 0.50528,
      "grad_norm": 0.015553130768239498,
      "learning_rate": 7.473760000000001e-06,
      "loss": 0.1995,
      "step": 15790
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.03529000282287598,
      "learning_rate": 7.472160000000001e-06,
      "loss": 0.1995,
      "step": 15800
    },
    {
      "epoch": 0.50592,
      "grad_norm": 0.02358645759522915,
      "learning_rate": 7.4705600000000015e-06,
      "loss": 0.1993,
      "step": 15810
    },
    {
      "epoch": 0.50624,
      "grad_norm": 0.027102237567305565,
      "learning_rate": 7.4689600000000006e-06,
      "loss": 0.2137,
      "step": 15820
    },
    {
      "epoch": 0.50656,
      "grad_norm": 0.02112453244626522,
      "learning_rate": 7.4673600000000004e-06,
      "loss": 0.1994,
      "step": 15830
    },
    {
      "epoch": 0.50688,
      "grad_norm": 0.015161029994487762,
      "learning_rate": 7.465760000000001e-06,
      "loss": 0.1993,
      "step": 15840
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.13784122467041016,
      "learning_rate": 7.46416e-06,
      "loss": 0.1997,
      "step": 15850
    },
    {
      "epoch": 0.50752,
      "grad_norm": 0.046581149101257324,
      "learning_rate": 7.462560000000001e-06,
      "loss": 0.1992,
      "step": 15860
    },
    {
      "epoch": 0.50784,
      "grad_norm": 0.05053713545203209,
      "learning_rate": 7.460960000000001e-06,
      "loss": 0.1993,
      "step": 15870
    },
    {
      "epoch": 0.50816,
      "grad_norm": 0.01287807710468769,
      "learning_rate": 7.459360000000001e-06,
      "loss": 0.1995,
      "step": 15880
    },
    {
      "epoch": 0.50848,
      "grad_norm": 0.017995992675423622,
      "learning_rate": 7.4577600000000006e-06,
      "loss": 0.2167,
      "step": 15890
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.016367215663194656,
      "learning_rate": 7.4561600000000005e-06,
      "loss": 0.2143,
      "step": 15900
    },
    {
      "epoch": 0.50912,
      "grad_norm": 0.057431191205978394,
      "learning_rate": 7.45456e-06,
      "loss": 0.1995,
      "step": 15910
    },
    {
      "epoch": 0.50944,
      "grad_norm": 0.015007088892161846,
      "learning_rate": 7.45296e-06,
      "loss": 0.2003,
      "step": 15920
    },
    {
      "epoch": 0.50976,
      "grad_norm": 0.020784031599760056,
      "learning_rate": 7.451360000000001e-06,
      "loss": 0.2051,
      "step": 15930
    },
    {
      "epoch": 0.51008,
      "grad_norm": 0.01861136592924595,
      "learning_rate": 7.44976e-06,
      "loss": 0.2006,
      "step": 15940
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.048222050070762634,
      "learning_rate": 7.448160000000001e-06,
      "loss": 0.2111,
      "step": 15950
    },
    {
      "epoch": 0.51072,
      "grad_norm": 0.0673404335975647,
      "learning_rate": 7.446560000000001e-06,
      "loss": 0.2433,
      "step": 15960
    },
    {
      "epoch": 0.51104,
      "grad_norm": 0.06499499827623367,
      "learning_rate": 7.444960000000001e-06,
      "loss": 0.2027,
      "step": 15970
    },
    {
      "epoch": 0.51136,
      "grad_norm": 0.06485508382320404,
      "learning_rate": 7.44336e-06,
      "loss": 0.1994,
      "step": 15980
    },
    {
      "epoch": 0.51168,
      "grad_norm": 0.08958182483911514,
      "learning_rate": 7.44176e-06,
      "loss": 0.1996,
      "step": 15990
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.024348828941583633,
      "learning_rate": 7.440160000000001e-06,
      "loss": 0.1995,
      "step": 16000
    },
    {
      "epoch": 0.512,
      "eval_runtime": 50.703,
      "eval_samples_per_second": 197.227,
      "eval_steps_per_second": 12.327,
      "step": 16000
    },
    {
      "epoch": 0.51232,
      "grad_norm": 0.04078389331698418,
      "learning_rate": 7.43856e-06,
      "loss": 0.2155,
      "step": 16010
    },
    {
      "epoch": 0.51264,
      "grad_norm": 0.020053835585713387,
      "learning_rate": 7.436960000000001e-06,
      "loss": 0.1993,
      "step": 16020
    },
    {
      "epoch": 0.51296,
      "grad_norm": 0.03955478221178055,
      "learning_rate": 7.435360000000001e-06,
      "loss": 0.1993,
      "step": 16030
    },
    {
      "epoch": 0.51328,
      "grad_norm": 0.0456218421459198,
      "learning_rate": 7.433760000000001e-06,
      "loss": 0.1993,
      "step": 16040
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.05484455078840256,
      "learning_rate": 7.43216e-06,
      "loss": 0.1992,
      "step": 16050
    },
    {
      "epoch": 0.51392,
      "grad_norm": 0.006745809689164162,
      "learning_rate": 7.430560000000001e-06,
      "loss": 0.204,
      "step": 16060
    },
    {
      "epoch": 0.51424,
      "grad_norm": 0.021113581955432892,
      "learning_rate": 7.428960000000001e-06,
      "loss": 0.1991,
      "step": 16070
    },
    {
      "epoch": 0.51456,
      "grad_norm": 0.028794683516025543,
      "learning_rate": 7.42736e-06,
      "loss": 0.2103,
      "step": 16080
    },
    {
      "epoch": 0.51488,
      "grad_norm": 0.01561671495437622,
      "learning_rate": 7.425760000000001e-06,
      "loss": 0.1991,
      "step": 16090
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.020298879593610764,
      "learning_rate": 7.424160000000001e-06,
      "loss": 0.2093,
      "step": 16100
    },
    {
      "epoch": 0.51552,
      "grad_norm": 0.04755142703652382,
      "learning_rate": 7.4225600000000005e-06,
      "loss": 0.1995,
      "step": 16110
    },
    {
      "epoch": 0.51584,
      "grad_norm": 0.013454506173729897,
      "learning_rate": 7.42096e-06,
      "loss": 0.2001,
      "step": 16120
    },
    {
      "epoch": 0.51616,
      "grad_norm": 0.024577176198363304,
      "learning_rate": 7.419360000000001e-06,
      "loss": 0.1993,
      "step": 16130
    },
    {
      "epoch": 0.51648,
      "grad_norm": 0.013596106320619583,
      "learning_rate": 7.41776e-06,
      "loss": 0.2146,
      "step": 16140
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.14303144812583923,
      "learning_rate": 7.416160000000001e-06,
      "loss": 0.2007,
      "step": 16150
    },
    {
      "epoch": 0.51712,
      "grad_norm": 0.013867589645087719,
      "learning_rate": 7.414560000000001e-06,
      "loss": 0.1992,
      "step": 16160
    },
    {
      "epoch": 0.51744,
      "grad_norm": 0.039922747761011124,
      "learning_rate": 7.41296e-06,
      "loss": 0.1997,
      "step": 16170
    },
    {
      "epoch": 0.51776,
      "grad_norm": 0.012312364764511585,
      "learning_rate": 7.4113600000000005e-06,
      "loss": 0.1993,
      "step": 16180
    },
    {
      "epoch": 0.51808,
      "grad_norm": 0.04745158180594444,
      "learning_rate": 7.40976e-06,
      "loss": 0.2313,
      "step": 16190
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.04893076419830322,
      "learning_rate": 7.408160000000001e-06,
      "loss": 0.1992,
      "step": 16200
    },
    {
      "epoch": 0.51872,
      "grad_norm": 0.05859638378024101,
      "learning_rate": 7.40656e-06,
      "loss": 0.2156,
      "step": 16210
    },
    {
      "epoch": 0.51904,
      "grad_norm": 0.06357496231794357,
      "learning_rate": 7.404960000000001e-06,
      "loss": 0.1994,
      "step": 16220
    },
    {
      "epoch": 0.51936,
      "grad_norm": 0.07969935238361359,
      "learning_rate": 7.403360000000001e-06,
      "loss": 0.2288,
      "step": 16230
    },
    {
      "epoch": 0.51968,
      "grad_norm": 0.020776329562067986,
      "learning_rate": 7.40176e-06,
      "loss": 0.2141,
      "step": 16240
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.14608341455459595,
      "learning_rate": 7.4001600000000005e-06,
      "loss": 0.1995,
      "step": 16250
    },
    {
      "epoch": 0.52032,
      "grad_norm": 0.03348102793097496,
      "learning_rate": 7.39856e-06,
      "loss": 0.1996,
      "step": 16260
    },
    {
      "epoch": 0.52064,
      "grad_norm": 0.05733085051178932,
      "learning_rate": 7.396960000000001e-06,
      "loss": 0.1991,
      "step": 16270
    },
    {
      "epoch": 0.52096,
      "grad_norm": 0.049263983964920044,
      "learning_rate": 7.39536e-06,
      "loss": 0.1993,
      "step": 16280
    },
    {
      "epoch": 0.52128,
      "grad_norm": 0.011203295551240444,
      "learning_rate": 7.393760000000001e-06,
      "loss": 0.1997,
      "step": 16290
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.018879685550928116,
      "learning_rate": 7.392160000000001e-06,
      "loss": 0.1994,
      "step": 16300
    },
    {
      "epoch": 0.52192,
      "grad_norm": 0.0669504776597023,
      "learning_rate": 7.3905600000000015e-06,
      "loss": 0.2126,
      "step": 16310
    },
    {
      "epoch": 0.52224,
      "grad_norm": 0.03226601704955101,
      "learning_rate": 7.3889600000000005e-06,
      "loss": 0.1993,
      "step": 16320
    },
    {
      "epoch": 0.52256,
      "grad_norm": 0.025089792907238007,
      "learning_rate": 7.38736e-06,
      "loss": 0.1994,
      "step": 16330
    },
    {
      "epoch": 0.52288,
      "grad_norm": 0.06944314390420914,
      "learning_rate": 7.385760000000001e-06,
      "loss": 0.212,
      "step": 16340
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.02125006914138794,
      "learning_rate": 7.38416e-06,
      "loss": 0.1993,
      "step": 16350
    },
    {
      "epoch": 0.52352,
      "grad_norm": 0.9321761131286621,
      "learning_rate": 7.382560000000001e-06,
      "loss": 0.2007,
      "step": 16360
    },
    {
      "epoch": 0.52384,
      "grad_norm": 0.030532555654644966,
      "learning_rate": 7.380960000000001e-06,
      "loss": 0.1992,
      "step": 16370
    },
    {
      "epoch": 0.52416,
      "grad_norm": 0.025246409699320793,
      "learning_rate": 7.379360000000001e-06,
      "loss": 0.2307,
      "step": 16380
    },
    {
      "epoch": 0.52448,
      "grad_norm": 0.11662987619638443,
      "learning_rate": 7.3777600000000005e-06,
      "loss": 0.1998,
      "step": 16390
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.013280600309371948,
      "learning_rate": 7.376160000000001e-06,
      "loss": 0.1992,
      "step": 16400
    },
    {
      "epoch": 0.52512,
      "grad_norm": 0.039017848670482635,
      "learning_rate": 7.37456e-06,
      "loss": 0.2136,
      "step": 16410
    },
    {
      "epoch": 0.52544,
      "grad_norm": 0.0382266640663147,
      "learning_rate": 7.37296e-06,
      "loss": 0.2006,
      "step": 16420
    },
    {
      "epoch": 0.52576,
      "grad_norm": 0.05368884280323982,
      "learning_rate": 7.371360000000001e-06,
      "loss": 0.2146,
      "step": 16430
    },
    {
      "epoch": 0.52608,
      "grad_norm": 0.02209658920764923,
      "learning_rate": 7.36976e-06,
      "loss": 0.2173,
      "step": 16440
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.06006930023431778,
      "learning_rate": 7.368160000000001e-06,
      "loss": 0.1994,
      "step": 16450
    },
    {
      "epoch": 0.52672,
      "grad_norm": 0.038403622806072235,
      "learning_rate": 7.3665600000000005e-06,
      "loss": 0.1995,
      "step": 16460
    },
    {
      "epoch": 0.52704,
      "grad_norm": 0.038734424859285355,
      "learning_rate": 7.364960000000001e-06,
      "loss": 0.1994,
      "step": 16470
    },
    {
      "epoch": 0.52736,
      "grad_norm": 0.023645224049687386,
      "learning_rate": 7.36336e-06,
      "loss": 0.1995,
      "step": 16480
    },
    {
      "epoch": 0.52768,
      "grad_norm": 0.06242474541068077,
      "learning_rate": 7.361760000000001e-06,
      "loss": 0.1999,
      "step": 16490
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.017669642344117165,
      "learning_rate": 7.360160000000001e-06,
      "loss": 0.2143,
      "step": 16500
    },
    {
      "epoch": 0.52832,
      "grad_norm": 0.01471796166151762,
      "learning_rate": 7.35856e-06,
      "loss": 0.2,
      "step": 16510
    },
    {
      "epoch": 0.52864,
      "grad_norm": 0.01567438431084156,
      "learning_rate": 7.356960000000001e-06,
      "loss": 0.2264,
      "step": 16520
    },
    {
      "epoch": 0.52896,
      "grad_norm": 0.03957616537809372,
      "learning_rate": 7.3553600000000005e-06,
      "loss": 0.1994,
      "step": 16530
    },
    {
      "epoch": 0.52928,
      "grad_norm": 0.01841552183032036,
      "learning_rate": 7.353760000000001e-06,
      "loss": 0.1994,
      "step": 16540
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.04404677450656891,
      "learning_rate": 7.35216e-06,
      "loss": 0.1993,
      "step": 16550
    },
    {
      "epoch": 0.52992,
      "grad_norm": 0.05566227063536644,
      "learning_rate": 7.350560000000001e-06,
      "loss": 0.2167,
      "step": 16560
    },
    {
      "epoch": 0.53024,
      "grad_norm": 0.03023582696914673,
      "learning_rate": 7.348960000000001e-06,
      "loss": 0.1998,
      "step": 16570
    },
    {
      "epoch": 0.53056,
      "grad_norm": 0.06285106390714645,
      "learning_rate": 7.34736e-06,
      "loss": 0.1993,
      "step": 16580
    },
    {
      "epoch": 0.53088,
      "grad_norm": 0.028526106849312782,
      "learning_rate": 7.345760000000001e-06,
      "loss": 0.2062,
      "step": 16590
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.03878827765583992,
      "learning_rate": 7.3441600000000005e-06,
      "loss": 0.1993,
      "step": 16600
    },
    {
      "epoch": 0.53152,
      "grad_norm": 0.05862438678741455,
      "learning_rate": 7.34256e-06,
      "loss": 0.1994,
      "step": 16610
    },
    {
      "epoch": 0.53184,
      "grad_norm": 0.01466136984527111,
      "learning_rate": 7.34096e-06,
      "loss": 0.1991,
      "step": 16620
    },
    {
      "epoch": 0.53216,
      "grad_norm": 0.0607670433819294,
      "learning_rate": 7.339360000000001e-06,
      "loss": 0.1995,
      "step": 16630
    },
    {
      "epoch": 0.53248,
      "grad_norm": 0.025192562490701675,
      "learning_rate": 7.33776e-06,
      "loss": 0.21,
      "step": 16640
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.02219388447701931,
      "learning_rate": 7.336160000000001e-06,
      "loss": 0.1991,
      "step": 16650
    },
    {
      "epoch": 0.53312,
      "grad_norm": 0.05408656597137451,
      "learning_rate": 7.334560000000001e-06,
      "loss": 0.1997,
      "step": 16660
    },
    {
      "epoch": 0.53344,
      "grad_norm": 0.042402233928442,
      "learning_rate": 7.33296e-06,
      "loss": 0.1999,
      "step": 16670
    },
    {
      "epoch": 0.53376,
      "grad_norm": 0.018045924603939056,
      "learning_rate": 7.33136e-06,
      "loss": 0.1992,
      "step": 16680
    },
    {
      "epoch": 0.53408,
      "grad_norm": 0.7029064893722534,
      "learning_rate": 7.32976e-06,
      "loss": 0.2139,
      "step": 16690
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.014661160297691822,
      "learning_rate": 7.328160000000001e-06,
      "loss": 0.2073,
      "step": 16700
    },
    {
      "epoch": 0.53472,
      "grad_norm": 0.03458946943283081,
      "learning_rate": 7.32656e-06,
      "loss": 0.1994,
      "step": 16710
    },
    {
      "epoch": 0.53504,
      "grad_norm": 1.072261929512024,
      "learning_rate": 7.324960000000001e-06,
      "loss": 0.2163,
      "step": 16720
    },
    {
      "epoch": 0.53536,
      "grad_norm": 0.0374845452606678,
      "learning_rate": 7.323360000000001e-06,
      "loss": 0.2168,
      "step": 16730
    },
    {
      "epoch": 0.53568,
      "grad_norm": 0.027300609275698662,
      "learning_rate": 7.321760000000001e-06,
      "loss": 0.1994,
      "step": 16740
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.03619838133454323,
      "learning_rate": 7.32016e-06,
      "loss": 0.1992,
      "step": 16750
    },
    {
      "epoch": 0.53632,
      "grad_norm": 0.0659690722823143,
      "learning_rate": 7.31856e-06,
      "loss": 0.1994,
      "step": 16760
    },
    {
      "epoch": 0.53664,
      "grad_norm": 0.030166558921337128,
      "learning_rate": 7.316960000000001e-06,
      "loss": 0.2229,
      "step": 16770
    },
    {
      "epoch": 0.53696,
      "grad_norm": 0.013694753870368004,
      "learning_rate": 7.31536e-06,
      "loss": 0.1993,
      "step": 16780
    },
    {
      "epoch": 0.53728,
      "grad_norm": 0.019057810306549072,
      "learning_rate": 7.313760000000001e-06,
      "loss": 0.1993,
      "step": 16790
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.010260091163218021,
      "learning_rate": 7.312160000000001e-06,
      "loss": 0.2009,
      "step": 16800
    },
    {
      "epoch": 0.53792,
      "grad_norm": 0.014884685166180134,
      "learning_rate": 7.310560000000001e-06,
      "loss": 0.1993,
      "step": 16810
    },
    {
      "epoch": 0.53824,
      "grad_norm": 0.06623409688472748,
      "learning_rate": 7.30896e-06,
      "loss": 0.2163,
      "step": 16820
    },
    {
      "epoch": 0.53856,
      "grad_norm": 0.016375601291656494,
      "learning_rate": 7.307360000000001e-06,
      "loss": 0.2078,
      "step": 16830
    },
    {
      "epoch": 0.53888,
      "grad_norm": 0.018794717267155647,
      "learning_rate": 7.305760000000001e-06,
      "loss": 0.1991,
      "step": 16840
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.01653706654906273,
      "learning_rate": 7.30416e-06,
      "loss": 0.1996,
      "step": 16850
    },
    {
      "epoch": 0.53952,
      "grad_norm": 0.035067856311798096,
      "learning_rate": 7.302560000000001e-06,
      "loss": 0.1993,
      "step": 16860
    },
    {
      "epoch": 0.53984,
      "grad_norm": 0.033907435834407806,
      "learning_rate": 7.300960000000001e-06,
      "loss": 0.2094,
      "step": 16870
    },
    {
      "epoch": 0.54016,
      "grad_norm": 0.03062594309449196,
      "learning_rate": 7.2993600000000005e-06,
      "loss": 0.1994,
      "step": 16880
    },
    {
      "epoch": 0.54048,
      "grad_norm": 0.04352512210607529,
      "learning_rate": 7.29776e-06,
      "loss": 0.1994,
      "step": 16890
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.03734075278043747,
      "learning_rate": 7.296160000000001e-06,
      "loss": 0.1995,
      "step": 16900
    },
    {
      "epoch": 0.54112,
      "grad_norm": 0.06362448632717133,
      "learning_rate": 7.29456e-06,
      "loss": 0.2001,
      "step": 16910
    },
    {
      "epoch": 0.54144,
      "grad_norm": 0.01668745093047619,
      "learning_rate": 7.292960000000001e-06,
      "loss": 0.2122,
      "step": 16920
    },
    {
      "epoch": 0.54176,
      "grad_norm": 0.02434111386537552,
      "learning_rate": 7.291360000000001e-06,
      "loss": 0.1996,
      "step": 16930
    },
    {
      "epoch": 0.54208,
      "grad_norm": 0.015982910990715027,
      "learning_rate": 7.28976e-06,
      "loss": 0.1993,
      "step": 16940
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.01767636463046074,
      "learning_rate": 7.2881600000000005e-06,
      "loss": 0.2009,
      "step": 16950
    },
    {
      "epoch": 0.54272,
      "grad_norm": 0.01687854342162609,
      "learning_rate": 7.28656e-06,
      "loss": 0.1996,
      "step": 16960
    },
    {
      "epoch": 0.54304,
      "grad_norm": 0.034073445945978165,
      "learning_rate": 7.284960000000001e-06,
      "loss": 0.1994,
      "step": 16970
    },
    {
      "epoch": 0.54336,
      "grad_norm": 0.027996668592095375,
      "learning_rate": 7.28336e-06,
      "loss": 0.1992,
      "step": 16980
    },
    {
      "epoch": 0.54368,
      "grad_norm": 0.014870663173496723,
      "learning_rate": 7.281760000000001e-06,
      "loss": 0.2118,
      "step": 16990
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.018431076779961586,
      "learning_rate": 7.280160000000001e-06,
      "loss": 0.199,
      "step": 17000
    },
    {
      "epoch": 0.544,
      "eval_runtime": 52.7553,
      "eval_samples_per_second": 189.555,
      "eval_steps_per_second": 11.847,
      "step": 17000
    },
    {
      "epoch": 0.54432,
      "grad_norm": 0.017279120162129402,
      "learning_rate": 7.27856e-06,
      "loss": 0.2072,
      "step": 17010
    },
    {
      "epoch": 0.54464,
      "grad_norm": 0.018350517377257347,
      "learning_rate": 7.2769600000000005e-06,
      "loss": 0.1993,
      "step": 17020
    },
    {
      "epoch": 0.54496,
      "grad_norm": 0.021868854761123657,
      "learning_rate": 7.27536e-06,
      "loss": 0.1993,
      "step": 17030
    },
    {
      "epoch": 0.54528,
      "grad_norm": 0.019653629511594772,
      "learning_rate": 7.273760000000001e-06,
      "loss": 0.1996,
      "step": 17040
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.19053329527378082,
      "learning_rate": 7.27216e-06,
      "loss": 0.1996,
      "step": 17050
    },
    {
      "epoch": 0.54592,
      "grad_norm": 0.017600439488887787,
      "learning_rate": 7.270560000000001e-06,
      "loss": 0.2171,
      "step": 17060
    },
    {
      "epoch": 0.54624,
      "grad_norm": 0.04314646124839783,
      "learning_rate": 7.268960000000001e-06,
      "loss": 0.2073,
      "step": 17070
    },
    {
      "epoch": 0.54656,
      "grad_norm": 0.01739349029958248,
      "learning_rate": 7.267360000000001e-06,
      "loss": 0.1993,
      "step": 17080
    },
    {
      "epoch": 0.54688,
      "grad_norm": 0.025662703439593315,
      "learning_rate": 7.2657600000000005e-06,
      "loss": 0.2163,
      "step": 17090
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.04076423868536949,
      "learning_rate": 7.2641600000000004e-06,
      "loss": 0.1995,
      "step": 17100
    },
    {
      "epoch": 0.54752,
      "grad_norm": 0.02850339561700821,
      "learning_rate": 7.26256e-06,
      "loss": 0.2153,
      "step": 17110
    },
    {
      "epoch": 0.54784,
      "grad_norm": 0.0633879005908966,
      "learning_rate": 7.26096e-06,
      "loss": 0.1992,
      "step": 17120
    },
    {
      "epoch": 0.54816,
      "grad_norm": 0.02257184125483036,
      "learning_rate": 7.259360000000001e-06,
      "loss": 0.213,
      "step": 17130
    },
    {
      "epoch": 0.54848,
      "grad_norm": 0.16562068462371826,
      "learning_rate": 7.25776e-06,
      "loss": 0.2103,
      "step": 17140
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.03704053536057472,
      "learning_rate": 7.256160000000001e-06,
      "loss": 0.1993,
      "step": 17150
    },
    {
      "epoch": 0.54912,
      "grad_norm": 0.03297704458236694,
      "learning_rate": 7.2545600000000006e-06,
      "loss": 0.2149,
      "step": 17160
    },
    {
      "epoch": 0.54944,
      "grad_norm": 0.023528719320893288,
      "learning_rate": 7.252960000000001e-06,
      "loss": 0.1994,
      "step": 17170
    },
    {
      "epoch": 0.54976,
      "grad_norm": 0.03473744913935661,
      "learning_rate": 7.25136e-06,
      "loss": 0.1993,
      "step": 17180
    },
    {
      "epoch": 0.55008,
      "grad_norm": 0.026735549792647362,
      "learning_rate": 7.24976e-06,
      "loss": 0.1992,
      "step": 17190
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.019821476191282272,
      "learning_rate": 7.248160000000001e-06,
      "loss": 0.1996,
      "step": 17200
    },
    {
      "epoch": 0.55072,
      "grad_norm": 0.023922132328152657,
      "learning_rate": 7.24656e-06,
      "loss": 0.1992,
      "step": 17210
    },
    {
      "epoch": 0.55104,
      "grad_norm": 0.04034871980547905,
      "learning_rate": 7.244960000000001e-06,
      "loss": 0.2074,
      "step": 17220
    },
    {
      "epoch": 0.55136,
      "grad_norm": 1.1477117538452148,
      "learning_rate": 7.2433600000000006e-06,
      "loss": 0.2284,
      "step": 17230
    },
    {
      "epoch": 0.55168,
      "grad_norm": 0.025574559345841408,
      "learning_rate": 7.241760000000001e-06,
      "loss": 0.1993,
      "step": 17240
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.02405761182308197,
      "learning_rate": 7.24016e-06,
      "loss": 0.1991,
      "step": 17250
    },
    {
      "epoch": 0.55232,
      "grad_norm": 0.015664974227547646,
      "learning_rate": 7.238560000000001e-06,
      "loss": 0.1994,
      "step": 17260
    },
    {
      "epoch": 0.55264,
      "grad_norm": 0.026972662657499313,
      "learning_rate": 7.236960000000001e-06,
      "loss": 0.1992,
      "step": 17270
    },
    {
      "epoch": 0.55296,
      "grad_norm": 0.02454638108611107,
      "learning_rate": 7.23536e-06,
      "loss": 0.1992,
      "step": 17280
    },
    {
      "epoch": 0.55328,
      "grad_norm": 0.06270062178373337,
      "learning_rate": 7.233760000000001e-06,
      "loss": 0.1996,
      "step": 17290
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.016322707757353783,
      "learning_rate": 7.2321600000000006e-06,
      "loss": 0.1997,
      "step": 17300
    },
    {
      "epoch": 0.55392,
      "grad_norm": 0.02777341939508915,
      "learning_rate": 7.230560000000001e-06,
      "loss": 0.2165,
      "step": 17310
    },
    {
      "epoch": 0.55424,
      "grad_norm": 0.020479874685406685,
      "learning_rate": 7.22896e-06,
      "loss": 0.1991,
      "step": 17320
    },
    {
      "epoch": 0.55456,
      "grad_norm": 0.023845935240387917,
      "learning_rate": 7.227360000000001e-06,
      "loss": 0.2115,
      "step": 17330
    },
    {
      "epoch": 0.55488,
      "grad_norm": 0.0289432592689991,
      "learning_rate": 7.225760000000001e-06,
      "loss": 0.1995,
      "step": 17340
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.008986669592559338,
      "learning_rate": 7.22416e-06,
      "loss": 0.213,
      "step": 17350
    },
    {
      "epoch": 0.55552,
      "grad_norm": 0.027271194383502007,
      "learning_rate": 7.222560000000001e-06,
      "loss": 0.1994,
      "step": 17360
    },
    {
      "epoch": 0.55584,
      "grad_norm": 0.02182016149163246,
      "learning_rate": 7.2209600000000006e-06,
      "loss": 0.1993,
      "step": 17370
    },
    {
      "epoch": 0.55616,
      "grad_norm": 0.017198288813233376,
      "learning_rate": 7.2193600000000005e-06,
      "loss": 0.203,
      "step": 17380
    },
    {
      "epoch": 0.55648,
      "grad_norm": 0.05140785500407219,
      "learning_rate": 7.21776e-06,
      "loss": 0.1997,
      "step": 17390
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.02294788882136345,
      "learning_rate": 7.216160000000001e-06,
      "loss": 0.2161,
      "step": 17400
    },
    {
      "epoch": 0.55712,
      "grad_norm": 0.017269855365157127,
      "learning_rate": 7.21456e-06,
      "loss": 0.2023,
      "step": 17410
    },
    {
      "epoch": 0.55744,
      "grad_norm": 0.7148480415344238,
      "learning_rate": 7.212960000000001e-06,
      "loss": 0.229,
      "step": 17420
    },
    {
      "epoch": 0.55776,
      "grad_norm": 0.05796007812023163,
      "learning_rate": 7.211360000000001e-06,
      "loss": 0.2075,
      "step": 17430
    },
    {
      "epoch": 0.55808,
      "grad_norm": 0.03465365245938301,
      "learning_rate": 7.20976e-06,
      "loss": 0.2102,
      "step": 17440
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.38678139448165894,
      "learning_rate": 7.2081600000000005e-06,
      "loss": 0.2271,
      "step": 17450
    },
    {
      "epoch": 0.55872,
      "grad_norm": 0.02950156107544899,
      "learning_rate": 7.20656e-06,
      "loss": 0.1995,
      "step": 17460
    },
    {
      "epoch": 0.55904,
      "grad_norm": 0.06632258743047714,
      "learning_rate": 7.204960000000001e-06,
      "loss": 0.2003,
      "step": 17470
    },
    {
      "epoch": 0.55936,
      "grad_norm": 0.03983854502439499,
      "learning_rate": 7.20336e-06,
      "loss": 0.2287,
      "step": 17480
    },
    {
      "epoch": 0.55968,
      "grad_norm": 0.02272181399166584,
      "learning_rate": 7.201760000000001e-06,
      "loss": 0.2052,
      "step": 17490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.05527614802122116,
      "learning_rate": 7.200160000000001e-06,
      "loss": 0.1994,
      "step": 17500
    },
    {
      "epoch": 0.56032,
      "grad_norm": 0.03355012834072113,
      "learning_rate": 7.1985600000000014e-06,
      "loss": 0.1992,
      "step": 17510
    },
    {
      "epoch": 0.56064,
      "grad_norm": 0.00786520354449749,
      "learning_rate": 7.1969600000000005e-06,
      "loss": 0.1995,
      "step": 17520
    },
    {
      "epoch": 0.56096,
      "grad_norm": 0.06435499340295792,
      "learning_rate": 7.19536e-06,
      "loss": 0.2112,
      "step": 17530
    },
    {
      "epoch": 0.56128,
      "grad_norm": 0.035775844007730484,
      "learning_rate": 7.193760000000001e-06,
      "loss": 0.2137,
      "step": 17540
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.016986265778541565,
      "learning_rate": 7.19216e-06,
      "loss": 0.1992,
      "step": 17550
    },
    {
      "epoch": 0.56192,
      "grad_norm": 0.04647894948720932,
      "learning_rate": 7.190560000000001e-06,
      "loss": 0.1994,
      "step": 17560
    },
    {
      "epoch": 0.56224,
      "grad_norm": 0.0359891876578331,
      "learning_rate": 7.188960000000001e-06,
      "loss": 0.1996,
      "step": 17570
    },
    {
      "epoch": 0.56256,
      "grad_norm": 0.02276211977005005,
      "learning_rate": 7.187360000000001e-06,
      "loss": 0.2127,
      "step": 17580
    },
    {
      "epoch": 0.56288,
      "grad_norm": 0.014894774183630943,
      "learning_rate": 7.1857600000000005e-06,
      "loss": 0.1995,
      "step": 17590
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.04990468919277191,
      "learning_rate": 7.184160000000001e-06,
      "loss": 0.1993,
      "step": 17600
    },
    {
      "epoch": 0.56352,
      "grad_norm": 0.05284887179732323,
      "learning_rate": 7.18256e-06,
      "loss": 0.2155,
      "step": 17610
    },
    {
      "epoch": 0.56384,
      "grad_norm": 0.02641318552196026,
      "learning_rate": 7.18096e-06,
      "loss": 0.2131,
      "step": 17620
    },
    {
      "epoch": 0.56416,
      "grad_norm": 0.049010809510946274,
      "learning_rate": 7.179360000000001e-06,
      "loss": 0.2029,
      "step": 17630
    },
    {
      "epoch": 0.56448,
      "grad_norm": 0.04993230104446411,
      "learning_rate": 7.17776e-06,
      "loss": 0.226,
      "step": 17640
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.23966912925243378,
      "learning_rate": 7.176160000000001e-06,
      "loss": 0.1996,
      "step": 17650
    },
    {
      "epoch": 0.56512,
      "grad_norm": 0.03299739584326744,
      "learning_rate": 7.1745600000000005e-06,
      "loss": 0.1995,
      "step": 17660
    },
    {
      "epoch": 0.56544,
      "grad_norm": 0.02120986394584179,
      "learning_rate": 7.172960000000001e-06,
      "loss": 0.2156,
      "step": 17670
    },
    {
      "epoch": 0.56576,
      "grad_norm": 0.0465519055724144,
      "learning_rate": 7.17136e-06,
      "loss": 0.207,
      "step": 17680
    },
    {
      "epoch": 0.56608,
      "grad_norm": 0.018152106553316116,
      "learning_rate": 7.16976e-06,
      "loss": 0.1993,
      "step": 17690
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.017616666853427887,
      "learning_rate": 7.168160000000001e-06,
      "loss": 0.1993,
      "step": 17700
    },
    {
      "epoch": 0.56672,
      "grad_norm": 0.02003788948059082,
      "learning_rate": 7.16656e-06,
      "loss": 0.1992,
      "step": 17710
    },
    {
      "epoch": 0.56704,
      "grad_norm": 0.02708159014582634,
      "learning_rate": 7.164960000000001e-06,
      "loss": 0.1992,
      "step": 17720
    },
    {
      "epoch": 0.56736,
      "grad_norm": 0.01355752069503069,
      "learning_rate": 7.1633600000000005e-06,
      "loss": 0.1992,
      "step": 17730
    },
    {
      "epoch": 0.56768,
      "grad_norm": 0.03117786906659603,
      "learning_rate": 7.161760000000001e-06,
      "loss": 0.1992,
      "step": 17740
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.020956704393029213,
      "learning_rate": 7.16016e-06,
      "loss": 0.2005,
      "step": 17750
    },
    {
      "epoch": 0.56832,
      "grad_norm": 0.03112957440316677,
      "learning_rate": 7.158560000000001e-06,
      "loss": 0.1993,
      "step": 17760
    },
    {
      "epoch": 0.56864,
      "grad_norm": 0.01371847279369831,
      "learning_rate": 7.156960000000001e-06,
      "loss": 0.2147,
      "step": 17770
    },
    {
      "epoch": 0.56896,
      "grad_norm": 0.029009224846959114,
      "learning_rate": 7.15536e-06,
      "loss": 0.1999,
      "step": 17780
    },
    {
      "epoch": 0.56928,
      "grad_norm": 0.02653229609131813,
      "learning_rate": 7.153760000000001e-06,
      "loss": 0.1993,
      "step": 17790
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.024598166346549988,
      "learning_rate": 7.1521600000000005e-06,
      "loss": 0.2003,
      "step": 17800
    },
    {
      "epoch": 0.56992,
      "grad_norm": 0.013289745897054672,
      "learning_rate": 7.150560000000001e-06,
      "loss": 0.1992,
      "step": 17810
    },
    {
      "epoch": 0.57024,
      "grad_norm": 0.03407066687941551,
      "learning_rate": 7.14896e-06,
      "loss": 0.205,
      "step": 17820
    },
    {
      "epoch": 0.57056,
      "grad_norm": 0.021968655288219452,
      "learning_rate": 7.147360000000001e-06,
      "loss": 0.2036,
      "step": 17830
    },
    {
      "epoch": 0.57088,
      "grad_norm": 0.024195633828639984,
      "learning_rate": 7.145760000000001e-06,
      "loss": 0.1994,
      "step": 17840
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.026157621294260025,
      "learning_rate": 7.144160000000001e-06,
      "loss": 0.1994,
      "step": 17850
    },
    {
      "epoch": 0.57152,
      "grad_norm": 0.02372756600379944,
      "learning_rate": 7.142560000000001e-06,
      "loss": 0.1992,
      "step": 17860
    },
    {
      "epoch": 0.57184,
      "grad_norm": 0.021448882296681404,
      "learning_rate": 7.1409600000000005e-06,
      "loss": 0.2144,
      "step": 17870
    },
    {
      "epoch": 0.57216,
      "grad_norm": 0.04052410647273064,
      "learning_rate": 7.13936e-06,
      "loss": 0.1997,
      "step": 17880
    },
    {
      "epoch": 0.57248,
      "grad_norm": 0.023554684594273567,
      "learning_rate": 7.13776e-06,
      "loss": 0.2148,
      "step": 17890
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.01362877432256937,
      "learning_rate": 7.136160000000001e-06,
      "loss": 0.1993,
      "step": 17900
    },
    {
      "epoch": 0.57312,
      "grad_norm": 0.02014600858092308,
      "learning_rate": 7.13456e-06,
      "loss": 0.1991,
      "step": 17910
    },
    {
      "epoch": 0.57344,
      "grad_norm": 0.031573425978422165,
      "learning_rate": 7.132960000000001e-06,
      "loss": 0.2088,
      "step": 17920
    },
    {
      "epoch": 0.57376,
      "grad_norm": 0.03308039531111717,
      "learning_rate": 7.131360000000001e-06,
      "loss": 0.1994,
      "step": 17930
    },
    {
      "epoch": 0.57408,
      "grad_norm": 0.007440696470439434,
      "learning_rate": 7.129760000000001e-06,
      "loss": 0.2132,
      "step": 17940
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.025266947224736214,
      "learning_rate": 7.12816e-06,
      "loss": 0.2031,
      "step": 17950
    },
    {
      "epoch": 0.57472,
      "grad_norm": 0.014737124554812908,
      "learning_rate": 7.12656e-06,
      "loss": 0.2005,
      "step": 17960
    },
    {
      "epoch": 0.57504,
      "grad_norm": 0.04845109581947327,
      "learning_rate": 7.124960000000001e-06,
      "loss": 0.2132,
      "step": 17970
    },
    {
      "epoch": 0.57536,
      "grad_norm": 0.06952515989542007,
      "learning_rate": 7.12336e-06,
      "loss": 0.2004,
      "step": 17980
    },
    {
      "epoch": 0.57568,
      "grad_norm": 0.03592598810791969,
      "learning_rate": 7.121760000000001e-06,
      "loss": 0.2151,
      "step": 17990
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.022244010120630264,
      "learning_rate": 7.120160000000001e-06,
      "loss": 0.1997,
      "step": 18000
    },
    {
      "epoch": 0.576,
      "eval_runtime": 50.438,
      "eval_samples_per_second": 198.263,
      "eval_steps_per_second": 12.391,
      "step": 18000
    },
    {
      "epoch": 0.57632,
      "grad_norm": 0.2016066610813141,
      "learning_rate": 7.118560000000001e-06,
      "loss": 0.1995,
      "step": 18010
    },
    {
      "epoch": 0.57664,
      "grad_norm": 0.051316458731889725,
      "learning_rate": 7.11696e-06,
      "loss": 0.1994,
      "step": 18020
    },
    {
      "epoch": 0.57696,
      "grad_norm": 0.046435657888650894,
      "learning_rate": 7.11536e-06,
      "loss": 0.2159,
      "step": 18030
    },
    {
      "epoch": 0.57728,
      "grad_norm": 0.01592089980840683,
      "learning_rate": 7.113760000000001e-06,
      "loss": 0.1993,
      "step": 18040
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.019662564620375633,
      "learning_rate": 7.11216e-06,
      "loss": 0.1991,
      "step": 18050
    },
    {
      "epoch": 0.57792,
      "grad_norm": 0.04710644483566284,
      "learning_rate": 7.110560000000001e-06,
      "loss": 0.1995,
      "step": 18060
    },
    {
      "epoch": 0.57824,
      "grad_norm": 0.029432393610477448,
      "learning_rate": 7.108960000000001e-06,
      "loss": 0.1993,
      "step": 18070
    },
    {
      "epoch": 0.57856,
      "grad_norm": 0.030440162867307663,
      "learning_rate": 7.1073600000000005e-06,
      "loss": 0.1991,
      "step": 18080
    },
    {
      "epoch": 0.57888,
      "grad_norm": 0.028869586065411568,
      "learning_rate": 7.10576e-06,
      "loss": 0.1995,
      "step": 18090
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.018409661948680878,
      "learning_rate": 7.104160000000001e-06,
      "loss": 0.1993,
      "step": 18100
    },
    {
      "epoch": 0.57952,
      "grad_norm": 0.0231157299131155,
      "learning_rate": 7.10256e-06,
      "loss": 0.1993,
      "step": 18110
    },
    {
      "epoch": 0.57984,
      "grad_norm": 1.0840188264846802,
      "learning_rate": 7.10096e-06,
      "loss": 0.2218,
      "step": 18120
    },
    {
      "epoch": 0.58016,
      "grad_norm": 0.00987593550235033,
      "learning_rate": 7.099360000000001e-06,
      "loss": 0.1993,
      "step": 18130
    },
    {
      "epoch": 0.58048,
      "grad_norm": 0.03596990928053856,
      "learning_rate": 7.09776e-06,
      "loss": 0.1992,
      "step": 18140
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.0244574174284935,
      "learning_rate": 7.0961600000000005e-06,
      "loss": 0.1993,
      "step": 18150
    },
    {
      "epoch": 0.58112,
      "grad_norm": 0.01406993716955185,
      "learning_rate": 7.09456e-06,
      "loss": 0.1992,
      "step": 18160
    },
    {
      "epoch": 0.58144,
      "grad_norm": 0.7856054902076721,
      "learning_rate": 7.092960000000001e-06,
      "loss": 0.2008,
      "step": 18170
    },
    {
      "epoch": 0.58176,
      "grad_norm": 0.3114652931690216,
      "learning_rate": 7.09136e-06,
      "loss": 0.1995,
      "step": 18180
    },
    {
      "epoch": 0.58208,
      "grad_norm": 0.028040539473295212,
      "learning_rate": 7.089760000000001e-06,
      "loss": 0.1993,
      "step": 18190
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.013222360983490944,
      "learning_rate": 7.088160000000001e-06,
      "loss": 0.2025,
      "step": 18200
    },
    {
      "epoch": 0.58272,
      "grad_norm": 0.025687502697110176,
      "learning_rate": 7.08656e-06,
      "loss": 0.2152,
      "step": 18210
    },
    {
      "epoch": 0.58304,
      "grad_norm": 0.03518030047416687,
      "learning_rate": 7.0849600000000005e-06,
      "loss": 0.2141,
      "step": 18220
    },
    {
      "epoch": 0.58336,
      "grad_norm": 0.055655308067798615,
      "learning_rate": 7.08336e-06,
      "loss": 0.2011,
      "step": 18230
    },
    {
      "epoch": 0.58368,
      "grad_norm": 0.016812268644571304,
      "learning_rate": 7.081760000000001e-06,
      "loss": 0.1994,
      "step": 18240
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.029875606298446655,
      "learning_rate": 7.08016e-06,
      "loss": 0.2021,
      "step": 18250
    },
    {
      "epoch": 0.58432,
      "grad_norm": 0.037739526480436325,
      "learning_rate": 7.078560000000001e-06,
      "loss": 0.1993,
      "step": 18260
    },
    {
      "epoch": 0.58464,
      "grad_norm": 0.01297967042773962,
      "learning_rate": 7.076960000000001e-06,
      "loss": 0.1994,
      "step": 18270
    },
    {
      "epoch": 0.58496,
      "grad_norm": 0.7263765335083008,
      "learning_rate": 7.0753600000000015e-06,
      "loss": 0.2159,
      "step": 18280
    },
    {
      "epoch": 0.58528,
      "grad_norm": 0.011135303415358067,
      "learning_rate": 7.0737600000000005e-06,
      "loss": 0.2155,
      "step": 18290
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.09372207522392273,
      "learning_rate": 7.07216e-06,
      "loss": 0.1994,
      "step": 18300
    },
    {
      "epoch": 0.58592,
      "grad_norm": 0.032342635095119476,
      "learning_rate": 7.070560000000001e-06,
      "loss": 0.1995,
      "step": 18310
    },
    {
      "epoch": 0.58624,
      "grad_norm": 0.020066985860466957,
      "learning_rate": 7.06896e-06,
      "loss": 0.1993,
      "step": 18320
    },
    {
      "epoch": 0.58656,
      "grad_norm": 0.723434329032898,
      "learning_rate": 7.067360000000001e-06,
      "loss": 0.2124,
      "step": 18330
    },
    {
      "epoch": 0.58688,
      "grad_norm": 0.021303730085492134,
      "learning_rate": 7.065760000000001e-06,
      "loss": 0.1993,
      "step": 18340
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.025768784806132317,
      "learning_rate": 7.064160000000001e-06,
      "loss": 0.1992,
      "step": 18350
    },
    {
      "epoch": 0.58752,
      "grad_norm": 0.052180033177137375,
      "learning_rate": 7.0625600000000005e-06,
      "loss": 0.1993,
      "step": 18360
    },
    {
      "epoch": 0.58784,
      "grad_norm": 0.02596735954284668,
      "learning_rate": 7.06096e-06,
      "loss": 0.2151,
      "step": 18370
    },
    {
      "epoch": 0.58816,
      "grad_norm": 0.021251419559121132,
      "learning_rate": 7.05936e-06,
      "loss": 0.1992,
      "step": 18380
    },
    {
      "epoch": 0.58848,
      "grad_norm": 0.029210567474365234,
      "learning_rate": 7.05776e-06,
      "loss": 0.1996,
      "step": 18390
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.03922135382890701,
      "learning_rate": 7.056160000000001e-06,
      "loss": 0.1992,
      "step": 18400
    },
    {
      "epoch": 0.58912,
      "grad_norm": 0.01533256471157074,
      "learning_rate": 7.05456e-06,
      "loss": 0.1994,
      "step": 18410
    },
    {
      "epoch": 0.58944,
      "grad_norm": 0.01605665497481823,
      "learning_rate": 7.052960000000001e-06,
      "loss": 0.2029,
      "step": 18420
    },
    {
      "epoch": 0.58976,
      "grad_norm": 0.03443681076169014,
      "learning_rate": 7.0513600000000005e-06,
      "loss": 0.2109,
      "step": 18430
    },
    {
      "epoch": 0.59008,
      "grad_norm": 0.02193390764296055,
      "learning_rate": 7.049760000000001e-06,
      "loss": 0.1993,
      "step": 18440
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.01646939292550087,
      "learning_rate": 7.04816e-06,
      "loss": 0.1996,
      "step": 18450
    },
    {
      "epoch": 0.59072,
      "grad_norm": 0.02662121318280697,
      "learning_rate": 7.04656e-06,
      "loss": 0.1991,
      "step": 18460
    },
    {
      "epoch": 0.59104,
      "grad_norm": 0.03614179044961929,
      "learning_rate": 7.044960000000001e-06,
      "loss": 0.2143,
      "step": 18470
    },
    {
      "epoch": 0.59136,
      "grad_norm": 0.027334636077284813,
      "learning_rate": 7.04336e-06,
      "loss": 0.1994,
      "step": 18480
    },
    {
      "epoch": 0.59168,
      "grad_norm": 0.03309591859579086,
      "learning_rate": 7.041760000000001e-06,
      "loss": 0.2128,
      "step": 18490
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.022716309875249863,
      "learning_rate": 7.0401600000000005e-06,
      "loss": 0.2127,
      "step": 18500
    },
    {
      "epoch": 0.59232,
      "grad_norm": 0.017694789916276932,
      "learning_rate": 7.038560000000001e-06,
      "loss": 0.2,
      "step": 18510
    },
    {
      "epoch": 0.59264,
      "grad_norm": 0.012692307122051716,
      "learning_rate": 7.03696e-06,
      "loss": 0.1991,
      "step": 18520
    },
    {
      "epoch": 0.59296,
      "grad_norm": 0.018563678488135338,
      "learning_rate": 7.035360000000001e-06,
      "loss": 0.1993,
      "step": 18530
    },
    {
      "epoch": 0.59328,
      "grad_norm": 0.013584094122052193,
      "learning_rate": 7.033760000000001e-06,
      "loss": 0.2134,
      "step": 18540
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.01343416329473257,
      "learning_rate": 7.03216e-06,
      "loss": 0.214,
      "step": 18550
    },
    {
      "epoch": 0.59392,
      "grad_norm": 0.03097420372068882,
      "learning_rate": 7.030560000000001e-06,
      "loss": 0.2084,
      "step": 18560
    },
    {
      "epoch": 0.59424,
      "grad_norm": 0.024032801389694214,
      "learning_rate": 7.0289600000000005e-06,
      "loss": 0.1995,
      "step": 18570
    },
    {
      "epoch": 0.59456,
      "grad_norm": 0.062347568571567535,
      "learning_rate": 7.0273600000000004e-06,
      "loss": 0.2125,
      "step": 18580
    },
    {
      "epoch": 0.59488,
      "grad_norm": 0.013069618493318558,
      "learning_rate": 7.02576e-06,
      "loss": 0.2264,
      "step": 18590
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.02955782227218151,
      "learning_rate": 7.024160000000001e-06,
      "loss": 0.1997,
      "step": 18600
    },
    {
      "epoch": 0.59552,
      "grad_norm": 0.049740780144929886,
      "learning_rate": 7.02256e-06,
      "loss": 0.1994,
      "step": 18610
    },
    {
      "epoch": 0.59584,
      "grad_norm": 0.02005535177886486,
      "learning_rate": 7.020960000000001e-06,
      "loss": 0.1993,
      "step": 18620
    },
    {
      "epoch": 0.59616,
      "grad_norm": 0.0804901123046875,
      "learning_rate": 7.019360000000001e-06,
      "loss": 0.2,
      "step": 18630
    },
    {
      "epoch": 0.59648,
      "grad_norm": 0.16214898228645325,
      "learning_rate": 7.01776e-06,
      "loss": 0.2047,
      "step": 18640
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.027255160734057426,
      "learning_rate": 7.0161600000000004e-06,
      "loss": 0.1992,
      "step": 18650
    },
    {
      "epoch": 0.59712,
      "grad_norm": 0.011696134693920612,
      "learning_rate": 7.01456e-06,
      "loss": 0.1993,
      "step": 18660
    },
    {
      "epoch": 0.59744,
      "grad_norm": 0.05292537063360214,
      "learning_rate": 7.012960000000001e-06,
      "loss": 0.1993,
      "step": 18670
    },
    {
      "epoch": 0.59776,
      "grad_norm": 0.13054612278938293,
      "learning_rate": 7.01136e-06,
      "loss": 0.2067,
      "step": 18680
    },
    {
      "epoch": 0.59808,
      "grad_norm": 0.05956513062119484,
      "learning_rate": 7.009760000000001e-06,
      "loss": 0.2312,
      "step": 18690
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.03602913022041321,
      "learning_rate": 7.008160000000001e-06,
      "loss": 0.1993,
      "step": 18700
    },
    {
      "epoch": 0.59872,
      "grad_norm": 0.0453663133084774,
      "learning_rate": 7.00656e-06,
      "loss": 0.2163,
      "step": 18710
    },
    {
      "epoch": 0.59904,
      "grad_norm": 0.03768674656748772,
      "learning_rate": 7.0049600000000004e-06,
      "loss": 0.1994,
      "step": 18720
    },
    {
      "epoch": 0.59936,
      "grad_norm": 0.251981258392334,
      "learning_rate": 7.00336e-06,
      "loss": 0.1997,
      "step": 18730
    },
    {
      "epoch": 0.59968,
      "grad_norm": 0.008509742096066475,
      "learning_rate": 7.001760000000001e-06,
      "loss": 0.216,
      "step": 18740
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.05231664329767227,
      "learning_rate": 7.00016e-06,
      "loss": 0.1998,
      "step": 18750
    },
    {
      "epoch": 0.60032,
      "grad_norm": 0.03559679165482521,
      "learning_rate": 6.998560000000001e-06,
      "loss": 0.1994,
      "step": 18760
    },
    {
      "epoch": 0.60064,
      "grad_norm": 0.030627375468611717,
      "learning_rate": 6.996960000000001e-06,
      "loss": 0.1993,
      "step": 18770
    },
    {
      "epoch": 0.60096,
      "grad_norm": 0.028541473671793938,
      "learning_rate": 6.995360000000001e-06,
      "loss": 0.2002,
      "step": 18780
    },
    {
      "epoch": 0.60128,
      "grad_norm": 0.019267700612545013,
      "learning_rate": 6.9937600000000004e-06,
      "loss": 0.1992,
      "step": 18790
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.7334179282188416,
      "learning_rate": 6.99216e-06,
      "loss": 0.2137,
      "step": 18800
    },
    {
      "epoch": 0.60192,
      "grad_norm": 0.11608855426311493,
      "learning_rate": 6.990560000000001e-06,
      "loss": 0.1993,
      "step": 18810
    },
    {
      "epoch": 0.60224,
      "grad_norm": 0.04434986412525177,
      "learning_rate": 6.98896e-06,
      "loss": 0.2019,
      "step": 18820
    },
    {
      "epoch": 0.60256,
      "grad_norm": 1.9002346992492676,
      "learning_rate": 6.987360000000001e-06,
      "loss": 0.2098,
      "step": 18830
    },
    {
      "epoch": 0.60288,
      "grad_norm": 0.01448048185557127,
      "learning_rate": 6.985760000000001e-06,
      "loss": 0.1992,
      "step": 18840
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.038703277707099915,
      "learning_rate": 6.9841600000000006e-06,
      "loss": 0.2138,
      "step": 18850
    },
    {
      "epoch": 0.60352,
      "grad_norm": 0.04944322258234024,
      "learning_rate": 6.9825600000000004e-06,
      "loss": 0.1991,
      "step": 18860
    },
    {
      "epoch": 0.60384,
      "grad_norm": 1.2513313293457031,
      "learning_rate": 6.980960000000001e-06,
      "loss": 0.2064,
      "step": 18870
    },
    {
      "epoch": 0.60416,
      "grad_norm": 0.012058723717927933,
      "learning_rate": 6.97936e-06,
      "loss": 0.2024,
      "step": 18880
    },
    {
      "epoch": 0.60448,
      "grad_norm": 0.01395835354924202,
      "learning_rate": 6.97776e-06,
      "loss": 0.1992,
      "step": 18890
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.721123456954956,
      "learning_rate": 6.976160000000001e-06,
      "loss": 0.2144,
      "step": 18900
    },
    {
      "epoch": 0.60512,
      "grad_norm": 0.02492205984890461,
      "learning_rate": 6.97456e-06,
      "loss": 0.2136,
      "step": 18910
    },
    {
      "epoch": 0.60544,
      "grad_norm": 0.04698880389332771,
      "learning_rate": 6.9729600000000006e-06,
      "loss": 0.2014,
      "step": 18920
    },
    {
      "epoch": 0.60576,
      "grad_norm": 0.03304769843816757,
      "learning_rate": 6.9713600000000005e-06,
      "loss": 0.1992,
      "step": 18930
    },
    {
      "epoch": 0.60608,
      "grad_norm": 0.04704203084111214,
      "learning_rate": 6.969760000000001e-06,
      "loss": 0.2044,
      "step": 18940
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.03188784793019295,
      "learning_rate": 6.96816e-06,
      "loss": 0.2152,
      "step": 18950
    },
    {
      "epoch": 0.60672,
      "grad_norm": 0.025446156039834023,
      "learning_rate": 6.966560000000001e-06,
      "loss": 0.1993,
      "step": 18960
    },
    {
      "epoch": 0.60704,
      "grad_norm": 0.05246962234377861,
      "learning_rate": 6.964960000000001e-06,
      "loss": 0.2014,
      "step": 18970
    },
    {
      "epoch": 0.60736,
      "grad_norm": 0.015791313722729683,
      "learning_rate": 6.96336e-06,
      "loss": 0.1995,
      "step": 18980
    },
    {
      "epoch": 0.60768,
      "grad_norm": 0.04837658628821373,
      "learning_rate": 6.961760000000001e-06,
      "loss": 0.2008,
      "step": 18990
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.021372195333242416,
      "learning_rate": 6.9601600000000005e-06,
      "loss": 0.1992,
      "step": 19000
    },
    {
      "epoch": 0.608,
      "eval_runtime": 50.1536,
      "eval_samples_per_second": 199.387,
      "eval_steps_per_second": 12.462,
      "step": 19000
    },
    {
      "epoch": 0.60832,
      "grad_norm": 0.08736789226531982,
      "learning_rate": 6.958560000000001e-06,
      "loss": 0.2,
      "step": 19010
    },
    {
      "epoch": 0.60864,
      "grad_norm": 0.03164396062493324,
      "learning_rate": 6.95696e-06,
      "loss": 0.1992,
      "step": 19020
    },
    {
      "epoch": 0.60896,
      "grad_norm": 0.027332277968525887,
      "learning_rate": 6.955360000000001e-06,
      "loss": 0.1993,
      "step": 19030
    },
    {
      "epoch": 0.60928,
      "grad_norm": 0.01872299425303936,
      "learning_rate": 6.953760000000001e-06,
      "loss": 0.201,
      "step": 19040
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.09012877941131592,
      "learning_rate": 6.95216e-06,
      "loss": 0.1994,
      "step": 19050
    },
    {
      "epoch": 0.60992,
      "grad_norm": 0.01520173903554678,
      "learning_rate": 6.950560000000001e-06,
      "loss": 0.2018,
      "step": 19060
    },
    {
      "epoch": 0.61024,
      "grad_norm": 0.034785427153110504,
      "learning_rate": 6.9489600000000005e-06,
      "loss": 0.1992,
      "step": 19070
    },
    {
      "epoch": 0.61056,
      "grad_norm": 0.013771358877420425,
      "learning_rate": 6.947360000000001e-06,
      "loss": 0.1993,
      "step": 19080
    },
    {
      "epoch": 0.61088,
      "grad_norm": 0.015226633287966251,
      "learning_rate": 6.94576e-06,
      "loss": 0.1994,
      "step": 19090
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.04240473732352257,
      "learning_rate": 6.944160000000001e-06,
      "loss": 0.2172,
      "step": 19100
    },
    {
      "epoch": 0.61152,
      "grad_norm": 0.03497469797730446,
      "learning_rate": 6.942560000000001e-06,
      "loss": 0.2029,
      "step": 19110
    },
    {
      "epoch": 0.61184,
      "grad_norm": 0.05014987662434578,
      "learning_rate": 6.940960000000001e-06,
      "loss": 0.215,
      "step": 19120
    },
    {
      "epoch": 0.61216,
      "grad_norm": 0.014676093123853207,
      "learning_rate": 6.939360000000001e-06,
      "loss": 0.2147,
      "step": 19130
    },
    {
      "epoch": 0.61248,
      "grad_norm": 0.029632851481437683,
      "learning_rate": 6.9377600000000005e-06,
      "loss": 0.215,
      "step": 19140
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.030684616416692734,
      "learning_rate": 6.93616e-06,
      "loss": 0.1991,
      "step": 19150
    },
    {
      "epoch": 0.61312,
      "grad_norm": 0.041518937796354294,
      "learning_rate": 6.93456e-06,
      "loss": 0.1992,
      "step": 19160
    },
    {
      "epoch": 0.61344,
      "grad_norm": 0.08152293413877487,
      "learning_rate": 6.932960000000001e-06,
      "loss": 0.1995,
      "step": 19170
    },
    {
      "epoch": 0.61376,
      "grad_norm": 0.045504141598939896,
      "learning_rate": 6.93136e-06,
      "loss": 0.2307,
      "step": 19180
    },
    {
      "epoch": 0.61408,
      "grad_norm": 0.04999179393053055,
      "learning_rate": 6.929760000000001e-06,
      "loss": 0.1999,
      "step": 19190
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.02135160192847252,
      "learning_rate": 6.928160000000001e-06,
      "loss": 0.2123,
      "step": 19200
    },
    {
      "epoch": 0.61472,
      "grad_norm": 0.04298080876469612,
      "learning_rate": 6.926560000000001e-06,
      "loss": 0.2,
      "step": 19210
    },
    {
      "epoch": 0.61504,
      "grad_norm": 0.03041170910000801,
      "learning_rate": 6.92496e-06,
      "loss": 0.1994,
      "step": 19220
    },
    {
      "epoch": 0.61536,
      "grad_norm": 0.010594342835247517,
      "learning_rate": 6.92336e-06,
      "loss": 0.2147,
      "step": 19230
    },
    {
      "epoch": 0.61568,
      "grad_norm": 0.01433481927961111,
      "learning_rate": 6.921760000000001e-06,
      "loss": 0.1994,
      "step": 19240
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.028186969459056854,
      "learning_rate": 6.92016e-06,
      "loss": 0.2138,
      "step": 19250
    },
    {
      "epoch": 0.61632,
      "grad_norm": 0.03986641392111778,
      "learning_rate": 6.918560000000001e-06,
      "loss": 0.203,
      "step": 19260
    },
    {
      "epoch": 0.61664,
      "grad_norm": 0.018768656998872757,
      "learning_rate": 6.916960000000001e-06,
      "loss": 0.1992,
      "step": 19270
    },
    {
      "epoch": 0.61696,
      "grad_norm": 0.03197697550058365,
      "learning_rate": 6.915360000000001e-06,
      "loss": 0.1992,
      "step": 19280
    },
    {
      "epoch": 0.61728,
      "grad_norm": 0.08394727855920792,
      "learning_rate": 6.91376e-06,
      "loss": 0.1999,
      "step": 19290
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.027165118604898453,
      "learning_rate": 6.912160000000001e-06,
      "loss": 0.1992,
      "step": 19300
    },
    {
      "epoch": 0.61792,
      "grad_norm": 0.01264919713139534,
      "learning_rate": 6.910560000000001e-06,
      "loss": 0.1992,
      "step": 19310
    },
    {
      "epoch": 0.61824,
      "grad_norm": 0.03342777490615845,
      "learning_rate": 6.90896e-06,
      "loss": 0.1994,
      "step": 19320
    },
    {
      "epoch": 0.61856,
      "grad_norm": 0.019195562228560448,
      "learning_rate": 6.907360000000001e-06,
      "loss": 0.2155,
      "step": 19330
    },
    {
      "epoch": 0.61888,
      "grad_norm": 0.04042503610253334,
      "learning_rate": 6.905760000000001e-06,
      "loss": 0.1996,
      "step": 19340
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.03860914334654808,
      "learning_rate": 6.9041600000000005e-06,
      "loss": 0.1993,
      "step": 19350
    },
    {
      "epoch": 0.61952,
      "grad_norm": 0.022379813715815544,
      "learning_rate": 6.90256e-06,
      "loss": 0.2113,
      "step": 19360
    },
    {
      "epoch": 0.61984,
      "grad_norm": 0.02918218821287155,
      "learning_rate": 6.900960000000001e-06,
      "loss": 0.1997,
      "step": 19370
    },
    {
      "epoch": 0.62016,
      "grad_norm": 0.0158937219530344,
      "learning_rate": 6.89936e-06,
      "loss": 0.222,
      "step": 19380
    },
    {
      "epoch": 0.62048,
      "grad_norm": 0.04054924100637436,
      "learning_rate": 6.89776e-06,
      "loss": 0.1993,
      "step": 19390
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.008316528052091599,
      "learning_rate": 6.896160000000001e-06,
      "loss": 0.205,
      "step": 19400
    },
    {
      "epoch": 0.62112,
      "grad_norm": 0.011796506121754646,
      "learning_rate": 6.89456e-06,
      "loss": 0.1992,
      "step": 19410
    },
    {
      "epoch": 0.62144,
      "grad_norm": 0.05205506086349487,
      "learning_rate": 6.8929600000000005e-06,
      "loss": 0.1993,
      "step": 19420
    },
    {
      "epoch": 0.62176,
      "grad_norm": 0.020139697939157486,
      "learning_rate": 6.89136e-06,
      "loss": 0.1995,
      "step": 19430
    },
    {
      "epoch": 0.62208,
      "grad_norm": 0.011269892565906048,
      "learning_rate": 6.889760000000001e-06,
      "loss": 0.2001,
      "step": 19440
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.022784793749451637,
      "learning_rate": 6.88816e-06,
      "loss": 0.2051,
      "step": 19450
    },
    {
      "epoch": 0.62272,
      "grad_norm": 0.02419440634548664,
      "learning_rate": 6.886560000000001e-06,
      "loss": 0.199,
      "step": 19460
    },
    {
      "epoch": 0.62304,
      "grad_norm": 0.02985769510269165,
      "learning_rate": 6.884960000000001e-06,
      "loss": 0.2111,
      "step": 19470
    },
    {
      "epoch": 0.62336,
      "grad_norm": 0.014802823774516582,
      "learning_rate": 6.88336e-06,
      "loss": 0.231,
      "step": 19480
    },
    {
      "epoch": 0.62368,
      "grad_norm": 0.023731335997581482,
      "learning_rate": 6.8817600000000005e-06,
      "loss": 0.2135,
      "step": 19490
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.04675354063510895,
      "learning_rate": 6.88016e-06,
      "loss": 0.2142,
      "step": 19500
    },
    {
      "epoch": 0.62432,
      "grad_norm": 0.057258397340774536,
      "learning_rate": 6.878560000000001e-06,
      "loss": 0.1994,
      "step": 19510
    },
    {
      "epoch": 0.62464,
      "grad_norm": 0.09759332984685898,
      "learning_rate": 6.87696e-06,
      "loss": 0.2005,
      "step": 19520
    },
    {
      "epoch": 0.62496,
      "grad_norm": 0.02117101475596428,
      "learning_rate": 6.875360000000001e-06,
      "loss": 0.2,
      "step": 19530
    },
    {
      "epoch": 0.62528,
      "grad_norm": 0.03314893692731857,
      "learning_rate": 6.873760000000001e-06,
      "loss": 0.1996,
      "step": 19540
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.049313999712467194,
      "learning_rate": 6.8721600000000015e-06,
      "loss": 0.1993,
      "step": 19550
    },
    {
      "epoch": 0.62592,
      "grad_norm": 0.05571833625435829,
      "learning_rate": 6.8705600000000005e-06,
      "loss": 0.2142,
      "step": 19560
    },
    {
      "epoch": 0.62624,
      "grad_norm": 0.024224381893873215,
      "learning_rate": 6.86896e-06,
      "loss": 0.2196,
      "step": 19570
    },
    {
      "epoch": 0.62656,
      "grad_norm": 0.028986835852265358,
      "learning_rate": 6.867360000000001e-06,
      "loss": 0.2159,
      "step": 19580
    },
    {
      "epoch": 0.62688,
      "grad_norm": 0.2253144234418869,
      "learning_rate": 6.86576e-06,
      "loss": 0.2052,
      "step": 19590
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.23368386924266815,
      "learning_rate": 6.864160000000001e-06,
      "loss": 0.1995,
      "step": 19600
    },
    {
      "epoch": 0.62752,
      "grad_norm": 0.030828841030597687,
      "learning_rate": 6.862560000000001e-06,
      "loss": 0.1992,
      "step": 19610
    },
    {
      "epoch": 0.62784,
      "grad_norm": 0.010126820765435696,
      "learning_rate": 6.860960000000001e-06,
      "loss": 0.1994,
      "step": 19620
    },
    {
      "epoch": 0.62816,
      "grad_norm": 0.03502211347222328,
      "learning_rate": 6.8593600000000005e-06,
      "loss": 0.1995,
      "step": 19630
    },
    {
      "epoch": 0.62848,
      "grad_norm": 0.027929773554205894,
      "learning_rate": 6.857760000000001e-06,
      "loss": 0.2012,
      "step": 19640
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.039756182581186295,
      "learning_rate": 6.85616e-06,
      "loss": 0.1992,
      "step": 19650
    },
    {
      "epoch": 0.62912,
      "grad_norm": 0.01860072836279869,
      "learning_rate": 6.85456e-06,
      "loss": 0.1995,
      "step": 19660
    },
    {
      "epoch": 0.62944,
      "grad_norm": 0.019993405789136887,
      "learning_rate": 6.852960000000001e-06,
      "loss": 0.1991,
      "step": 19670
    },
    {
      "epoch": 0.62976,
      "grad_norm": 0.022205965593457222,
      "learning_rate": 6.85136e-06,
      "loss": 0.1994,
      "step": 19680
    },
    {
      "epoch": 0.63008,
      "grad_norm": 0.667243242263794,
      "learning_rate": 6.849760000000001e-06,
      "loss": 0.1998,
      "step": 19690
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.24426308274269104,
      "learning_rate": 6.8481600000000005e-06,
      "loss": 0.2034,
      "step": 19700
    },
    {
      "epoch": 0.63072,
      "grad_norm": 0.029274240136146545,
      "learning_rate": 6.846560000000001e-06,
      "loss": 0.1992,
      "step": 19710
    },
    {
      "epoch": 0.63104,
      "grad_norm": 0.03292378783226013,
      "learning_rate": 6.84496e-06,
      "loss": 0.2006,
      "step": 19720
    },
    {
      "epoch": 0.63136,
      "grad_norm": 0.03286297991871834,
      "learning_rate": 6.843360000000001e-06,
      "loss": 0.1995,
      "step": 19730
    },
    {
      "epoch": 0.63168,
      "grad_norm": 0.011178084649145603,
      "learning_rate": 6.841760000000001e-06,
      "loss": 0.1992,
      "step": 19740
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.043525293469429016,
      "learning_rate": 6.84016e-06,
      "loss": 0.2151,
      "step": 19750
    },
    {
      "epoch": 0.63232,
      "grad_norm": 0.7641414999961853,
      "learning_rate": 6.838560000000001e-06,
      "loss": 0.2315,
      "step": 19760
    },
    {
      "epoch": 0.63264,
      "grad_norm": 0.013260832987725735,
      "learning_rate": 6.8369600000000005e-06,
      "loss": 0.2164,
      "step": 19770
    },
    {
      "epoch": 0.63296,
      "grad_norm": 0.7041549682617188,
      "learning_rate": 6.835360000000001e-06,
      "loss": 0.202,
      "step": 19780
    },
    {
      "epoch": 0.63328,
      "grad_norm": 0.6769521832466125,
      "learning_rate": 6.83376e-06,
      "loss": 0.2131,
      "step": 19790
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.0359291136264801,
      "learning_rate": 6.832160000000001e-06,
      "loss": 0.1994,
      "step": 19800
    },
    {
      "epoch": 0.63392,
      "grad_norm": 0.03963223844766617,
      "learning_rate": 6.830560000000001e-06,
      "loss": 0.2171,
      "step": 19810
    },
    {
      "epoch": 0.63424,
      "grad_norm": 0.05377890542149544,
      "learning_rate": 6.82896e-06,
      "loss": 0.2138,
      "step": 19820
    },
    {
      "epoch": 0.63456,
      "grad_norm": 0.043462079018354416,
      "learning_rate": 6.827360000000001e-06,
      "loss": 0.2304,
      "step": 19830
    },
    {
      "epoch": 0.63488,
      "grad_norm": 0.03223972022533417,
      "learning_rate": 6.8257600000000005e-06,
      "loss": 0.1994,
      "step": 19840
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.037538811564445496,
      "learning_rate": 6.82416e-06,
      "loss": 0.2174,
      "step": 19850
    },
    {
      "epoch": 0.63552,
      "grad_norm": 0.03221080079674721,
      "learning_rate": 6.82256e-06,
      "loss": 0.1997,
      "step": 19860
    },
    {
      "epoch": 0.63584,
      "grad_norm": 0.025628304108977318,
      "learning_rate": 6.820960000000001e-06,
      "loss": 0.1993,
      "step": 19870
    },
    {
      "epoch": 0.63616,
      "grad_norm": 0.04268968850374222,
      "learning_rate": 6.81936e-06,
      "loss": 0.1992,
      "step": 19880
    },
    {
      "epoch": 0.63648,
      "grad_norm": 0.04787888750433922,
      "learning_rate": 6.817760000000001e-06,
      "loss": 0.1994,
      "step": 19890
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.03880828991532326,
      "learning_rate": 6.816160000000001e-06,
      "loss": 0.2119,
      "step": 19900
    },
    {
      "epoch": 0.63712,
      "grad_norm": 0.03405645862221718,
      "learning_rate": 6.81456e-06,
      "loss": 0.1992,
      "step": 19910
    },
    {
      "epoch": 0.63744,
      "grad_norm": 0.035450588911771774,
      "learning_rate": 6.81296e-06,
      "loss": 0.1992,
      "step": 19920
    },
    {
      "epoch": 0.63776,
      "grad_norm": 0.020799050107598305,
      "learning_rate": 6.81136e-06,
      "loss": 0.1993,
      "step": 19930
    },
    {
      "epoch": 0.63808,
      "grad_norm": 0.051629312336444855,
      "learning_rate": 6.809760000000001e-06,
      "loss": 0.2126,
      "step": 19940
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.06755948811769485,
      "learning_rate": 6.80816e-06,
      "loss": 0.1993,
      "step": 19950
    },
    {
      "epoch": 0.63872,
      "grad_norm": 0.04077824205160141,
      "learning_rate": 6.806560000000001e-06,
      "loss": 0.1992,
      "step": 19960
    },
    {
      "epoch": 0.63904,
      "grad_norm": 0.026167195290327072,
      "learning_rate": 6.804960000000001e-06,
      "loss": 0.1992,
      "step": 19970
    },
    {
      "epoch": 0.63936,
      "grad_norm": 0.013694977387785912,
      "learning_rate": 6.803360000000001e-06,
      "loss": 0.1991,
      "step": 19980
    },
    {
      "epoch": 0.63968,
      "grad_norm": 0.04531096667051315,
      "learning_rate": 6.80176e-06,
      "loss": 0.2211,
      "step": 19990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.04317048192024231,
      "learning_rate": 6.80016e-06,
      "loss": 0.1994,
      "step": 20000
    },
    {
      "epoch": 0.64,
      "eval_runtime": 52.0406,
      "eval_samples_per_second": 192.158,
      "eval_steps_per_second": 12.01,
      "step": 20000
    },
    {
      "epoch": 0.64032,
      "grad_norm": 0.04513170197606087,
      "learning_rate": 6.798560000000001e-06,
      "loss": 0.2159,
      "step": 20010
    },
    {
      "epoch": 0.64064,
      "grad_norm": 0.8504871726036072,
      "learning_rate": 6.79696e-06,
      "loss": 0.2156,
      "step": 20020
    },
    {
      "epoch": 0.64096,
      "grad_norm": 0.037691522389650345,
      "learning_rate": 6.795360000000001e-06,
      "loss": 0.2092,
      "step": 20030
    },
    {
      "epoch": 0.64128,
      "grad_norm": 0.0726672112941742,
      "learning_rate": 6.793760000000001e-06,
      "loss": 0.1995,
      "step": 20040
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.01862187311053276,
      "learning_rate": 6.792160000000001e-06,
      "loss": 0.1995,
      "step": 20050
    },
    {
      "epoch": 0.64192,
      "grad_norm": 0.029422292485833168,
      "learning_rate": 6.79056e-06,
      "loss": 0.1992,
      "step": 20060
    },
    {
      "epoch": 0.64224,
      "grad_norm": 0.04750804975628853,
      "learning_rate": 6.788960000000001e-06,
      "loss": 0.1992,
      "step": 20070
    },
    {
      "epoch": 0.64256,
      "grad_norm": 0.03211643919348717,
      "learning_rate": 6.787360000000001e-06,
      "loss": 0.1992,
      "step": 20080
    },
    {
      "epoch": 0.64288,
      "grad_norm": 0.019120700657367706,
      "learning_rate": 6.78576e-06,
      "loss": 0.2011,
      "step": 20090
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.019422313198447227,
      "learning_rate": 6.784160000000001e-06,
      "loss": 0.1992,
      "step": 20100
    },
    {
      "epoch": 0.64352,
      "grad_norm": 0.019072795286774635,
      "learning_rate": 6.782560000000001e-06,
      "loss": 0.1996,
      "step": 20110
    },
    {
      "epoch": 0.64384,
      "grad_norm": 0.041186001151800156,
      "learning_rate": 6.7809600000000005e-06,
      "loss": 0.2161,
      "step": 20120
    },
    {
      "epoch": 0.64416,
      "grad_norm": 0.019770247861742973,
      "learning_rate": 6.7793600000000004e-06,
      "loss": 0.2011,
      "step": 20130
    },
    {
      "epoch": 0.64448,
      "grad_norm": 0.02838786132633686,
      "learning_rate": 6.777760000000001e-06,
      "loss": 0.2022,
      "step": 20140
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.06988254189491272,
      "learning_rate": 6.77616e-06,
      "loss": 0.2001,
      "step": 20150
    },
    {
      "epoch": 0.64512,
      "grad_norm": 0.05215303599834442,
      "learning_rate": 6.77456e-06,
      "loss": 0.2,
      "step": 20160
    },
    {
      "epoch": 0.64544,
      "grad_norm": 0.017586272209882736,
      "learning_rate": 6.772960000000001e-06,
      "loss": 0.2125,
      "step": 20170
    },
    {
      "epoch": 0.64576,
      "grad_norm": 0.024640163406729698,
      "learning_rate": 6.77136e-06,
      "loss": 0.1993,
      "step": 20180
    },
    {
      "epoch": 0.64608,
      "grad_norm": 0.06857491284608841,
      "learning_rate": 6.7697600000000006e-06,
      "loss": 0.2139,
      "step": 20190
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.02336716093122959,
      "learning_rate": 6.7681600000000004e-06,
      "loss": 0.2132,
      "step": 20200
    },
    {
      "epoch": 0.64672,
      "grad_norm": 0.015398369170725346,
      "learning_rate": 6.766560000000001e-06,
      "loss": 0.1992,
      "step": 20210
    },
    {
      "epoch": 0.64704,
      "grad_norm": 0.07332859933376312,
      "learning_rate": 6.76496e-06,
      "loss": 0.2159,
      "step": 20220
    },
    {
      "epoch": 0.64736,
      "grad_norm": 0.025003619492053986,
      "learning_rate": 6.763360000000001e-06,
      "loss": 0.2025,
      "step": 20230
    },
    {
      "epoch": 0.64768,
      "grad_norm": 0.02564900927245617,
      "learning_rate": 6.761760000000001e-06,
      "loss": 0.1993,
      "step": 20240
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.04490057751536369,
      "learning_rate": 6.76016e-06,
      "loss": 0.1994,
      "step": 20250
    },
    {
      "epoch": 0.64832,
      "grad_norm": 0.029410140588879585,
      "learning_rate": 6.7585600000000006e-06,
      "loss": 0.2166,
      "step": 20260
    },
    {
      "epoch": 0.64864,
      "grad_norm": 0.0400807224214077,
      "learning_rate": 6.7569600000000004e-06,
      "loss": 0.1995,
      "step": 20270
    },
    {
      "epoch": 0.64896,
      "grad_norm": 0.0185984056442976,
      "learning_rate": 6.755360000000001e-06,
      "loss": 0.1995,
      "step": 20280
    },
    {
      "epoch": 0.64928,
      "grad_norm": 0.04395406320691109,
      "learning_rate": 6.75376e-06,
      "loss": 0.1993,
      "step": 20290
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.02507966198027134,
      "learning_rate": 6.752160000000001e-06,
      "loss": 0.1993,
      "step": 20300
    },
    {
      "epoch": 0.64992,
      "grad_norm": 0.026344023644924164,
      "learning_rate": 6.750560000000001e-06,
      "loss": 0.2161,
      "step": 20310
    },
    {
      "epoch": 0.65024,
      "grad_norm": 0.025944534689188004,
      "learning_rate": 6.748960000000001e-06,
      "loss": 0.1997,
      "step": 20320
    },
    {
      "epoch": 0.65056,
      "grad_norm": 0.055509667843580246,
      "learning_rate": 6.7473600000000006e-06,
      "loss": 0.1992,
      "step": 20330
    },
    {
      "epoch": 0.65088,
      "grad_norm": 0.012653214856982231,
      "learning_rate": 6.7457600000000004e-06,
      "loss": 0.2165,
      "step": 20340
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.03160005435347557,
      "learning_rate": 6.74416e-06,
      "loss": 0.1994,
      "step": 20350
    },
    {
      "epoch": 0.65152,
      "grad_norm": 0.020287109538912773,
      "learning_rate": 6.74256e-06,
      "loss": 0.1995,
      "step": 20360
    },
    {
      "epoch": 0.65184,
      "grad_norm": 0.027692677453160286,
      "learning_rate": 6.740960000000001e-06,
      "loss": 0.1993,
      "step": 20370
    },
    {
      "epoch": 0.65216,
      "grad_norm": 0.776154100894928,
      "learning_rate": 6.73936e-06,
      "loss": 0.2125,
      "step": 20380
    },
    {
      "epoch": 0.65248,
      "grad_norm": 0.05779072642326355,
      "learning_rate": 6.737760000000001e-06,
      "loss": 0.2108,
      "step": 20390
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.027053678408265114,
      "learning_rate": 6.7361600000000006e-06,
      "loss": 0.2128,
      "step": 20400
    },
    {
      "epoch": 0.65312,
      "grad_norm": 0.0681774765253067,
      "learning_rate": 6.734560000000001e-06,
      "loss": 0.1996,
      "step": 20410
    },
    {
      "epoch": 0.65344,
      "grad_norm": 0.01722763665020466,
      "learning_rate": 6.73296e-06,
      "loss": 0.2124,
      "step": 20420
    },
    {
      "epoch": 0.65376,
      "grad_norm": 0.043553318828344345,
      "learning_rate": 6.73136e-06,
      "loss": 0.2109,
      "step": 20430
    },
    {
      "epoch": 0.65408,
      "grad_norm": 0.03845635801553726,
      "learning_rate": 6.729760000000001e-06,
      "loss": 0.1993,
      "step": 20440
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.040966786444187164,
      "learning_rate": 6.72816e-06,
      "loss": 0.1992,
      "step": 20450
    },
    {
      "epoch": 0.65472,
      "grad_norm": 0.06425859779119492,
      "learning_rate": 6.726560000000001e-06,
      "loss": 0.1993,
      "step": 20460
    },
    {
      "epoch": 0.65504,
      "grad_norm": 0.010992632247507572,
      "learning_rate": 6.724960000000001e-06,
      "loss": 0.2037,
      "step": 20470
    },
    {
      "epoch": 0.65536,
      "grad_norm": 0.01865202747285366,
      "learning_rate": 6.723360000000001e-06,
      "loss": 0.1994,
      "step": 20480
    },
    {
      "epoch": 0.65568,
      "grad_norm": 0.013247386552393436,
      "learning_rate": 6.72176e-06,
      "loss": 0.1995,
      "step": 20490
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.05283700302243233,
      "learning_rate": 6.72016e-06,
      "loss": 0.2144,
      "step": 20500
    },
    {
      "epoch": 0.65632,
      "grad_norm": 0.03045688569545746,
      "learning_rate": 6.718560000000001e-06,
      "loss": 0.201,
      "step": 20510
    },
    {
      "epoch": 0.65664,
      "grad_norm": 0.025965122506022453,
      "learning_rate": 6.71696e-06,
      "loss": 0.1992,
      "step": 20520
    },
    {
      "epoch": 0.65696,
      "grad_norm": 0.018438134342432022,
      "learning_rate": 6.715360000000001e-06,
      "loss": 0.1991,
      "step": 20530
    },
    {
      "epoch": 0.65728,
      "grad_norm": 0.09010917693376541,
      "learning_rate": 6.713760000000001e-06,
      "loss": 0.1994,
      "step": 20540
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.025876738131046295,
      "learning_rate": 6.712160000000001e-06,
      "loss": 0.2045,
      "step": 20550
    },
    {
      "epoch": 0.65792,
      "grad_norm": 0.02672971785068512,
      "learning_rate": 6.71056e-06,
      "loss": 0.1992,
      "step": 20560
    },
    {
      "epoch": 0.65824,
      "grad_norm": 0.019053207710385323,
      "learning_rate": 6.708960000000001e-06,
      "loss": 0.2067,
      "step": 20570
    },
    {
      "epoch": 0.65856,
      "grad_norm": 0.02787272445857525,
      "learning_rate": 6.707360000000001e-06,
      "loss": 0.199,
      "step": 20580
    },
    {
      "epoch": 0.65888,
      "grad_norm": 0.019517693668603897,
      "learning_rate": 6.70576e-06,
      "loss": 0.2175,
      "step": 20590
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.011585932224988937,
      "learning_rate": 6.704160000000001e-06,
      "loss": 0.199,
      "step": 20600
    },
    {
      "epoch": 0.65952,
      "grad_norm": 0.04396156594157219,
      "learning_rate": 6.702560000000001e-06,
      "loss": 0.1991,
      "step": 20610
    },
    {
      "epoch": 0.65984,
      "grad_norm": 0.01631075143814087,
      "learning_rate": 6.7009600000000005e-06,
      "loss": 0.1992,
      "step": 20620
    },
    {
      "epoch": 0.66016,
      "grad_norm": 0.039784662425518036,
      "learning_rate": 6.69936e-06,
      "loss": 0.1999,
      "step": 20630
    },
    {
      "epoch": 0.66048,
      "grad_norm": 0.028311893343925476,
      "learning_rate": 6.697760000000001e-06,
      "loss": 0.1992,
      "step": 20640
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.016809338703751564,
      "learning_rate": 6.69616e-06,
      "loss": 0.1992,
      "step": 20650
    },
    {
      "epoch": 0.66112,
      "grad_norm": 0.06276476383209229,
      "learning_rate": 6.694560000000001e-06,
      "loss": 0.1992,
      "step": 20660
    },
    {
      "epoch": 0.66144,
      "grad_norm": 0.012747755274176598,
      "learning_rate": 6.692960000000001e-06,
      "loss": 0.1991,
      "step": 20670
    },
    {
      "epoch": 0.66176,
      "grad_norm": 0.03401045501232147,
      "learning_rate": 6.69136e-06,
      "loss": 0.1997,
      "step": 20680
    },
    {
      "epoch": 0.66208,
      "grad_norm": 0.024171866476535797,
      "learning_rate": 6.6897600000000005e-06,
      "loss": 0.1993,
      "step": 20690
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.026101453229784966,
      "learning_rate": 6.68816e-06,
      "loss": 0.1993,
      "step": 20700
    },
    {
      "epoch": 0.66272,
      "grad_norm": 0.03642532601952553,
      "learning_rate": 6.686560000000001e-06,
      "loss": 0.2108,
      "step": 20710
    },
    {
      "epoch": 0.66304,
      "grad_norm": 0.10147249698638916,
      "learning_rate": 6.68496e-06,
      "loss": 0.2015,
      "step": 20720
    },
    {
      "epoch": 0.66336,
      "grad_norm": 0.021547099575400352,
      "learning_rate": 6.683360000000001e-06,
      "loss": 0.2156,
      "step": 20730
    },
    {
      "epoch": 0.66368,
      "grad_norm": 0.0279712975025177,
      "learning_rate": 6.681760000000001e-06,
      "loss": 0.2107,
      "step": 20740
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.03752543777227402,
      "learning_rate": 6.6801600000000014e-06,
      "loss": 0.1992,
      "step": 20750
    },
    {
      "epoch": 0.66432,
      "grad_norm": 0.020906494930386543,
      "learning_rate": 6.6785600000000005e-06,
      "loss": 0.1991,
      "step": 20760
    },
    {
      "epoch": 0.66464,
      "grad_norm": 0.037367623299360275,
      "learning_rate": 6.67696e-06,
      "loss": 0.1992,
      "step": 20770
    },
    {
      "epoch": 0.66496,
      "grad_norm": 0.02018660306930542,
      "learning_rate": 6.675360000000001e-06,
      "loss": 0.2285,
      "step": 20780
    },
    {
      "epoch": 0.66528,
      "grad_norm": 0.021718185395002365,
      "learning_rate": 6.67376e-06,
      "loss": 0.1993,
      "step": 20790
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.02611556276679039,
      "learning_rate": 6.672160000000001e-06,
      "loss": 0.215,
      "step": 20800
    },
    {
      "epoch": 0.66592,
      "grad_norm": 0.08978859335184097,
      "learning_rate": 6.670560000000001e-06,
      "loss": 0.2142,
      "step": 20810
    },
    {
      "epoch": 0.66624,
      "grad_norm": 1.1390966176986694,
      "learning_rate": 6.668960000000001e-06,
      "loss": 0.2161,
      "step": 20820
    },
    {
      "epoch": 0.66656,
      "grad_norm": 0.01764845848083496,
      "learning_rate": 6.6673600000000005e-06,
      "loss": 0.1994,
      "step": 20830
    },
    {
      "epoch": 0.66688,
      "grad_norm": 0.05027703940868378,
      "learning_rate": 6.66576e-06,
      "loss": 0.1991,
      "step": 20840
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.01566450297832489,
      "learning_rate": 6.66416e-06,
      "loss": 0.1992,
      "step": 20850
    },
    {
      "epoch": 0.66752,
      "grad_norm": 0.05711248517036438,
      "learning_rate": 6.66256e-06,
      "loss": 0.1992,
      "step": 20860
    },
    {
      "epoch": 0.66784,
      "grad_norm": 0.016035230830311775,
      "learning_rate": 6.660960000000001e-06,
      "loss": 0.1991,
      "step": 20870
    },
    {
      "epoch": 0.66816,
      "grad_norm": 0.03706362843513489,
      "learning_rate": 6.65936e-06,
      "loss": 0.2198,
      "step": 20880
    },
    {
      "epoch": 0.66848,
      "grad_norm": 0.023309437558054924,
      "learning_rate": 6.657760000000001e-06,
      "loss": 0.2145,
      "step": 20890
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.05142103135585785,
      "learning_rate": 6.6561600000000005e-06,
      "loss": 0.1991,
      "step": 20900
    },
    {
      "epoch": 0.66912,
      "grad_norm": 0.025919320061802864,
      "learning_rate": 6.654560000000001e-06,
      "loss": 0.2157,
      "step": 20910
    },
    {
      "epoch": 0.66944,
      "grad_norm": 0.04912611097097397,
      "learning_rate": 6.65296e-06,
      "loss": 0.1994,
      "step": 20920
    },
    {
      "epoch": 0.66976,
      "grad_norm": 0.041464656591415405,
      "learning_rate": 6.65136e-06,
      "loss": 0.1992,
      "step": 20930
    },
    {
      "epoch": 0.67008,
      "grad_norm": 0.6704124212265015,
      "learning_rate": 6.649760000000001e-06,
      "loss": 0.212,
      "step": 20940
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.04156191274523735,
      "learning_rate": 6.64816e-06,
      "loss": 0.1994,
      "step": 20950
    },
    {
      "epoch": 0.67072,
      "grad_norm": 0.01350423414260149,
      "learning_rate": 6.646560000000001e-06,
      "loss": 0.1993,
      "step": 20960
    },
    {
      "epoch": 0.67104,
      "grad_norm": 0.013997108675539494,
      "learning_rate": 6.6449600000000005e-06,
      "loss": 0.2317,
      "step": 20970
    },
    {
      "epoch": 0.67136,
      "grad_norm": 0.024995625019073486,
      "learning_rate": 6.643360000000001e-06,
      "loss": 0.1997,
      "step": 20980
    },
    {
      "epoch": 0.67168,
      "grad_norm": 1.5675557851791382,
      "learning_rate": 6.64176e-06,
      "loss": 0.2237,
      "step": 20990
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.07436127960681915,
      "learning_rate": 6.640160000000001e-06,
      "loss": 0.1994,
      "step": 21000
    },
    {
      "epoch": 0.672,
      "eval_runtime": 50.2189,
      "eval_samples_per_second": 199.128,
      "eval_steps_per_second": 12.446,
      "step": 21000
    },
    {
      "epoch": 0.67232,
      "grad_norm": 0.02195940725505352,
      "learning_rate": 6.638560000000001e-06,
      "loss": 0.1994,
      "step": 21010
    },
    {
      "epoch": 0.67264,
      "grad_norm": 0.0219136793166399,
      "learning_rate": 6.63696e-06,
      "loss": 0.2157,
      "step": 21020
    },
    {
      "epoch": 0.67296,
      "grad_norm": 0.06817707419395447,
      "learning_rate": 6.635360000000001e-06,
      "loss": 0.1992,
      "step": 21030
    },
    {
      "epoch": 0.67328,
      "grad_norm": 0.24782466888427734,
      "learning_rate": 6.6337600000000005e-06,
      "loss": 0.2042,
      "step": 21040
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.021447237581014633,
      "learning_rate": 6.632160000000001e-06,
      "loss": 0.1993,
      "step": 21050
    },
    {
      "epoch": 0.67392,
      "grad_norm": 0.0250626802444458,
      "learning_rate": 6.63056e-06,
      "loss": 0.1993,
      "step": 21060
    },
    {
      "epoch": 0.67424,
      "grad_norm": 0.035455889999866486,
      "learning_rate": 6.628960000000001e-06,
      "loss": 0.1994,
      "step": 21070
    },
    {
      "epoch": 0.67456,
      "grad_norm": 0.9945878982543945,
      "learning_rate": 6.627360000000001e-06,
      "loss": 0.2185,
      "step": 21080
    },
    {
      "epoch": 0.67488,
      "grad_norm": 0.025796175003051758,
      "learning_rate": 6.625760000000001e-06,
      "loss": 0.1995,
      "step": 21090
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.022942863404750824,
      "learning_rate": 6.624160000000001e-06,
      "loss": 0.1995,
      "step": 21100
    },
    {
      "epoch": 0.67552,
      "grad_norm": 0.03771166503429413,
      "learning_rate": 6.6225600000000005e-06,
      "loss": 0.1992,
      "step": 21110
    },
    {
      "epoch": 0.67584,
      "grad_norm": 0.03399938717484474,
      "learning_rate": 6.62096e-06,
      "loss": 0.2457,
      "step": 21120
    },
    {
      "epoch": 0.67616,
      "grad_norm": 0.04459055885672569,
      "learning_rate": 6.61936e-06,
      "loss": 0.2141,
      "step": 21130
    },
    {
      "epoch": 0.67648,
      "grad_norm": 0.0577397346496582,
      "learning_rate": 6.617760000000001e-06,
      "loss": 0.1996,
      "step": 21140
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.05876709520816803,
      "learning_rate": 6.61616e-06,
      "loss": 0.1995,
      "step": 21150
    },
    {
      "epoch": 0.67712,
      "grad_norm": 0.0749875009059906,
      "learning_rate": 6.614560000000001e-06,
      "loss": 0.1997,
      "step": 21160
    },
    {
      "epoch": 0.67744,
      "grad_norm": 0.0207066647708416,
      "learning_rate": 6.612960000000001e-06,
      "loss": 0.1992,
      "step": 21170
    },
    {
      "epoch": 0.67776,
      "grad_norm": 0.04003988951444626,
      "learning_rate": 6.61136e-06,
      "loss": 0.2163,
      "step": 21180
    },
    {
      "epoch": 0.67808,
      "grad_norm": 0.02846279740333557,
      "learning_rate": 6.60976e-06,
      "loss": 0.2063,
      "step": 21190
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.02527555264532566,
      "learning_rate": 6.60816e-06,
      "loss": 0.1993,
      "step": 21200
    },
    {
      "epoch": 0.67872,
      "grad_norm": 0.05013548210263252,
      "learning_rate": 6.606560000000001e-06,
      "loss": 0.1993,
      "step": 21210
    },
    {
      "epoch": 0.67904,
      "grad_norm": 0.037994515150785446,
      "learning_rate": 6.60496e-06,
      "loss": 0.2119,
      "step": 21220
    },
    {
      "epoch": 0.67936,
      "grad_norm": 0.019289417192339897,
      "learning_rate": 6.603360000000001e-06,
      "loss": 0.2001,
      "step": 21230
    },
    {
      "epoch": 0.67968,
      "grad_norm": 0.02944612130522728,
      "learning_rate": 6.601760000000001e-06,
      "loss": 0.2135,
      "step": 21240
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.305801123380661,
      "learning_rate": 6.600160000000001e-06,
      "loss": 0.1999,
      "step": 21250
    },
    {
      "epoch": 0.68032,
      "grad_norm": 0.027426013723015785,
      "learning_rate": 6.59856e-06,
      "loss": 0.2001,
      "step": 21260
    },
    {
      "epoch": 0.68064,
      "grad_norm": 0.02939894050359726,
      "learning_rate": 6.59696e-06,
      "loss": 0.2012,
      "step": 21270
    },
    {
      "epoch": 0.68096,
      "grad_norm": 0.047206323593854904,
      "learning_rate": 6.595360000000001e-06,
      "loss": 0.2033,
      "step": 21280
    },
    {
      "epoch": 0.68128,
      "grad_norm": 0.023886248469352722,
      "learning_rate": 6.59376e-06,
      "loss": 0.2126,
      "step": 21290
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.02826240099966526,
      "learning_rate": 6.592160000000001e-06,
      "loss": 0.1992,
      "step": 21300
    },
    {
      "epoch": 0.68192,
      "grad_norm": 0.024377366527915,
      "learning_rate": 6.590560000000001e-06,
      "loss": 0.1993,
      "step": 21310
    },
    {
      "epoch": 0.68224,
      "grad_norm": 0.039063479751348495,
      "learning_rate": 6.5889600000000005e-06,
      "loss": 0.1995,
      "step": 21320
    },
    {
      "epoch": 0.68256,
      "grad_norm": 0.029987715184688568,
      "learning_rate": 6.58736e-06,
      "loss": 0.1998,
      "step": 21330
    },
    {
      "epoch": 0.68288,
      "grad_norm": 0.02929232083261013,
      "learning_rate": 6.585760000000001e-06,
      "loss": 0.2306,
      "step": 21340
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.06296952813863754,
      "learning_rate": 6.58416e-06,
      "loss": 0.2122,
      "step": 21350
    },
    {
      "epoch": 0.68352,
      "grad_norm": 0.01352250948548317,
      "learning_rate": 6.58256e-06,
      "loss": 0.1995,
      "step": 21360
    },
    {
      "epoch": 0.68384,
      "grad_norm": 0.017978155985474586,
      "learning_rate": 6.580960000000001e-06,
      "loss": 0.215,
      "step": 21370
    },
    {
      "epoch": 0.68416,
      "grad_norm": 0.02128291130065918,
      "learning_rate": 6.57936e-06,
      "loss": 0.2054,
      "step": 21380
    },
    {
      "epoch": 0.68448,
      "grad_norm": 0.055236056447029114,
      "learning_rate": 6.5777600000000005e-06,
      "loss": 0.2007,
      "step": 21390
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.0536576583981514,
      "learning_rate": 6.57616e-06,
      "loss": 0.1993,
      "step": 21400
    },
    {
      "epoch": 0.68512,
      "grad_norm": 0.019419051706790924,
      "learning_rate": 6.574560000000001e-06,
      "loss": 0.1994,
      "step": 21410
    },
    {
      "epoch": 0.68544,
      "grad_norm": 0.9973331093788147,
      "learning_rate": 6.57296e-06,
      "loss": 0.2024,
      "step": 21420
    },
    {
      "epoch": 0.68576,
      "grad_norm": 0.03056451678276062,
      "learning_rate": 6.571360000000001e-06,
      "loss": 0.2047,
      "step": 21430
    },
    {
      "epoch": 0.68608,
      "grad_norm": 0.01794171892106533,
      "learning_rate": 6.569760000000001e-06,
      "loss": 0.21,
      "step": 21440
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.03430439904332161,
      "learning_rate": 6.56816e-06,
      "loss": 0.1992,
      "step": 21450
    },
    {
      "epoch": 0.68672,
      "grad_norm": 0.03284567594528198,
      "learning_rate": 6.5665600000000005e-06,
      "loss": 0.2164,
      "step": 21460
    },
    {
      "epoch": 0.68704,
      "grad_norm": 0.043882161378860474,
      "learning_rate": 6.56496e-06,
      "loss": 0.1991,
      "step": 21470
    },
    {
      "epoch": 0.68736,
      "grad_norm": 0.022930964827537537,
      "learning_rate": 6.563360000000001e-06,
      "loss": 0.2039,
      "step": 21480
    },
    {
      "epoch": 0.68768,
      "grad_norm": 0.014979175291955471,
      "learning_rate": 6.56176e-06,
      "loss": 0.1992,
      "step": 21490
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.014498880133032799,
      "learning_rate": 6.560160000000001e-06,
      "loss": 0.2003,
      "step": 21500
    },
    {
      "epoch": 0.68832,
      "grad_norm": 0.02632630243897438,
      "learning_rate": 6.558560000000001e-06,
      "loss": 0.1992,
      "step": 21510
    },
    {
      "epoch": 0.68864,
      "grad_norm": 0.03934938460588455,
      "learning_rate": 6.55696e-06,
      "loss": 0.201,
      "step": 21520
    },
    {
      "epoch": 0.68896,
      "grad_norm": 0.03422747179865837,
      "learning_rate": 6.5553600000000005e-06,
      "loss": 0.1993,
      "step": 21530
    },
    {
      "epoch": 0.68928,
      "grad_norm": 0.04222685471177101,
      "learning_rate": 6.55376e-06,
      "loss": 0.1994,
      "step": 21540
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.02056403085589409,
      "learning_rate": 6.552160000000001e-06,
      "loss": 0.1991,
      "step": 21550
    },
    {
      "epoch": 0.68992,
      "grad_norm": 0.021811047568917274,
      "learning_rate": 6.55056e-06,
      "loss": 0.1992,
      "step": 21560
    },
    {
      "epoch": 0.69024,
      "grad_norm": 0.016264421865344048,
      "learning_rate": 6.548960000000001e-06,
      "loss": 0.1991,
      "step": 21570
    },
    {
      "epoch": 0.69056,
      "grad_norm": 0.03935089334845543,
      "learning_rate": 6.547360000000001e-06,
      "loss": 0.1992,
      "step": 21580
    },
    {
      "epoch": 0.69088,
      "grad_norm": 0.022064050659537315,
      "learning_rate": 6.545760000000001e-06,
      "loss": 0.1991,
      "step": 21590
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.028124293312430382,
      "learning_rate": 6.5441600000000005e-06,
      "loss": 0.1995,
      "step": 21600
    },
    {
      "epoch": 0.69152,
      "grad_norm": 0.01062938291579485,
      "learning_rate": 6.5425600000000004e-06,
      "loss": 0.216,
      "step": 21610
    },
    {
      "epoch": 0.69184,
      "grad_norm": 0.04458513483405113,
      "learning_rate": 6.54096e-06,
      "loss": 0.1993,
      "step": 21620
    },
    {
      "epoch": 0.69216,
      "grad_norm": 0.03439320996403694,
      "learning_rate": 6.53936e-06,
      "loss": 0.1994,
      "step": 21630
    },
    {
      "epoch": 0.69248,
      "grad_norm": 0.01957116462290287,
      "learning_rate": 6.537760000000001e-06,
      "loss": 0.1991,
      "step": 21640
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.03700239583849907,
      "learning_rate": 6.53616e-06,
      "loss": 0.199,
      "step": 21650
    },
    {
      "epoch": 0.69312,
      "grad_norm": 0.013606409542262554,
      "learning_rate": 6.534560000000001e-06,
      "loss": 0.1992,
      "step": 21660
    },
    {
      "epoch": 0.69344,
      "grad_norm": 0.019469423219561577,
      "learning_rate": 6.5329600000000006e-06,
      "loss": 0.1993,
      "step": 21670
    },
    {
      "epoch": 0.69376,
      "grad_norm": 0.015694867819547653,
      "learning_rate": 6.531360000000001e-06,
      "loss": 0.2162,
      "step": 21680
    },
    {
      "epoch": 0.69408,
      "grad_norm": 0.02828112058341503,
      "learning_rate": 6.52976e-06,
      "loss": 0.1995,
      "step": 21690
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.03816187009215355,
      "learning_rate": 6.52816e-06,
      "loss": 0.1991,
      "step": 21700
    },
    {
      "epoch": 0.69472,
      "grad_norm": 0.05023946613073349,
      "learning_rate": 6.526560000000001e-06,
      "loss": 0.2131,
      "step": 21710
    },
    {
      "epoch": 0.69504,
      "grad_norm": 0.017794089391827583,
      "learning_rate": 6.52496e-06,
      "loss": 0.1991,
      "step": 21720
    },
    {
      "epoch": 0.69536,
      "grad_norm": 0.0749359130859375,
      "learning_rate": 6.523360000000001e-06,
      "loss": 0.1994,
      "step": 21730
    },
    {
      "epoch": 0.69568,
      "grad_norm": 0.028548920527100563,
      "learning_rate": 6.5217600000000006e-06,
      "loss": 0.2153,
      "step": 21740
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.6261512637138367,
      "learning_rate": 6.520160000000001e-06,
      "loss": 0.2271,
      "step": 21750
    },
    {
      "epoch": 0.69632,
      "grad_norm": 0.021261410787701607,
      "learning_rate": 6.51856e-06,
      "loss": 0.21,
      "step": 21760
    },
    {
      "epoch": 0.69664,
      "grad_norm": 0.011556021869182587,
      "learning_rate": 6.516960000000001e-06,
      "loss": 0.1992,
      "step": 21770
    },
    {
      "epoch": 0.69696,
      "grad_norm": 0.03266258165240288,
      "learning_rate": 6.515360000000001e-06,
      "loss": 0.1994,
      "step": 21780
    },
    {
      "epoch": 0.69728,
      "grad_norm": 0.028456924483180046,
      "learning_rate": 6.51376e-06,
      "loss": 0.1993,
      "step": 21790
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.024917861446738243,
      "learning_rate": 6.512160000000001e-06,
      "loss": 0.1991,
      "step": 21800
    },
    {
      "epoch": 0.69792,
      "grad_norm": 0.027065560221672058,
      "learning_rate": 6.5105600000000006e-06,
      "loss": 0.1992,
      "step": 21810
    },
    {
      "epoch": 0.69824,
      "grad_norm": 0.027215631678700447,
      "learning_rate": 6.5089600000000004e-06,
      "loss": 0.1992,
      "step": 21820
    },
    {
      "epoch": 0.69856,
      "grad_norm": 0.031142203137278557,
      "learning_rate": 6.50736e-06,
      "loss": 0.214,
      "step": 21830
    },
    {
      "epoch": 0.69888,
      "grad_norm": 0.02647176757454872,
      "learning_rate": 6.505760000000001e-06,
      "loss": 0.2279,
      "step": 21840
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.06282754242420197,
      "learning_rate": 6.50416e-06,
      "loss": 0.1995,
      "step": 21850
    },
    {
      "epoch": 0.69952,
      "grad_norm": 0.02410631626844406,
      "learning_rate": 6.50256e-06,
      "loss": 0.1996,
      "step": 21860
    },
    {
      "epoch": 0.69984,
      "grad_norm": 0.03489910066127777,
      "learning_rate": 6.500960000000001e-06,
      "loss": 0.1993,
      "step": 21870
    },
    {
      "epoch": 0.70016,
      "grad_norm": 0.11192047595977783,
      "learning_rate": 6.49936e-06,
      "loss": 0.2126,
      "step": 21880
    },
    {
      "epoch": 0.70048,
      "grad_norm": 0.019857347011566162,
      "learning_rate": 6.4977600000000004e-06,
      "loss": 0.2142,
      "step": 21890
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.015013321302831173,
      "learning_rate": 6.49616e-06,
      "loss": 0.1995,
      "step": 21900
    },
    {
      "epoch": 0.70112,
      "grad_norm": 0.02565942518413067,
      "learning_rate": 6.494560000000001e-06,
      "loss": 0.1993,
      "step": 21910
    },
    {
      "epoch": 0.70144,
      "grad_norm": 0.03386157378554344,
      "learning_rate": 6.49296e-06,
      "loss": 0.1991,
      "step": 21920
    },
    {
      "epoch": 0.70176,
      "grad_norm": 0.02998926490545273,
      "learning_rate": 6.491360000000001e-06,
      "loss": 0.2043,
      "step": 21930
    },
    {
      "epoch": 0.70208,
      "grad_norm": 0.011338569223880768,
      "learning_rate": 6.489760000000001e-06,
      "loss": 0.1993,
      "step": 21940
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.025686588138341904,
      "learning_rate": 6.48816e-06,
      "loss": 0.1991,
      "step": 21950
    },
    {
      "epoch": 0.70272,
      "grad_norm": 0.024736670777201653,
      "learning_rate": 6.4865600000000005e-06,
      "loss": 0.1993,
      "step": 21960
    },
    {
      "epoch": 0.70304,
      "grad_norm": 0.021728157997131348,
      "learning_rate": 6.48496e-06,
      "loss": 0.2003,
      "step": 21970
    },
    {
      "epoch": 0.70336,
      "grad_norm": 0.0539323091506958,
      "learning_rate": 6.483360000000001e-06,
      "loss": 0.2134,
      "step": 21980
    },
    {
      "epoch": 0.70368,
      "grad_norm": 0.02334253303706646,
      "learning_rate": 6.48176e-06,
      "loss": 0.1992,
      "step": 21990
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.030267784371972084,
      "learning_rate": 6.480160000000001e-06,
      "loss": 0.1992,
      "step": 22000
    },
    {
      "epoch": 0.704,
      "eval_runtime": 49.5217,
      "eval_samples_per_second": 201.932,
      "eval_steps_per_second": 12.621,
      "step": 22000
    },
    {
      "epoch": 0.70432,
      "grad_norm": 0.01737908646464348,
      "learning_rate": 6.478560000000001e-06,
      "loss": 0.1995,
      "step": 22010
    },
    {
      "epoch": 0.70464,
      "grad_norm": 0.03709955886006355,
      "learning_rate": 6.4769600000000014e-06,
      "loss": 0.1994,
      "step": 22020
    },
    {
      "epoch": 0.70496,
      "grad_norm": 0.022033249959349632,
      "learning_rate": 6.4753600000000005e-06,
      "loss": 0.1992,
      "step": 22030
    },
    {
      "epoch": 0.70528,
      "grad_norm": 0.016776124015450478,
      "learning_rate": 6.47376e-06,
      "loss": 0.1993,
      "step": 22040
    },
    {
      "epoch": 0.7056,
      "grad_norm": 1.6291937828063965,
      "learning_rate": 6.472160000000001e-06,
      "loss": 0.2214,
      "step": 22050
    },
    {
      "epoch": 0.70592,
      "grad_norm": 0.7204111814498901,
      "learning_rate": 6.47056e-06,
      "loss": 0.2461,
      "step": 22060
    },
    {
      "epoch": 0.70624,
      "grad_norm": 0.03493327274918556,
      "learning_rate": 6.468960000000001e-06,
      "loss": 0.1992,
      "step": 22070
    },
    {
      "epoch": 0.70656,
      "grad_norm": 0.03828699141740799,
      "learning_rate": 6.467360000000001e-06,
      "loss": 0.2012,
      "step": 22080
    },
    {
      "epoch": 0.70688,
      "grad_norm": 0.025663185864686966,
      "learning_rate": 6.465760000000001e-06,
      "loss": 0.215,
      "step": 22090
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.03591648489236832,
      "learning_rate": 6.4641600000000005e-06,
      "loss": 0.1993,
      "step": 22100
    },
    {
      "epoch": 0.70752,
      "grad_norm": 0.02406132221221924,
      "learning_rate": 6.462560000000001e-06,
      "loss": 0.1995,
      "step": 22110
    },
    {
      "epoch": 0.70784,
      "grad_norm": 0.0854969471693039,
      "learning_rate": 6.46096e-06,
      "loss": 0.2118,
      "step": 22120
    },
    {
      "epoch": 0.70816,
      "grad_norm": 0.07596462219953537,
      "learning_rate": 6.45936e-06,
      "loss": 0.1995,
      "step": 22130
    },
    {
      "epoch": 0.70848,
      "grad_norm": 0.0316203348338604,
      "learning_rate": 6.457760000000001e-06,
      "loss": 0.2039,
      "step": 22140
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.012485950253903866,
      "learning_rate": 6.45616e-06,
      "loss": 0.1993,
      "step": 22150
    },
    {
      "epoch": 0.70912,
      "grad_norm": 0.07548508048057556,
      "learning_rate": 6.454560000000001e-06,
      "loss": 0.2168,
      "step": 22160
    },
    {
      "epoch": 0.70944,
      "grad_norm": 0.012437981553375721,
      "learning_rate": 6.4529600000000005e-06,
      "loss": 0.1994,
      "step": 22170
    },
    {
      "epoch": 0.70976,
      "grad_norm": 0.01006949134171009,
      "learning_rate": 6.451360000000001e-06,
      "loss": 0.2254,
      "step": 22180
    },
    {
      "epoch": 0.71008,
      "grad_norm": 0.011550134979188442,
      "learning_rate": 6.44976e-06,
      "loss": 0.1999,
      "step": 22190
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.050160981714725494,
      "learning_rate": 6.44816e-06,
      "loss": 0.1994,
      "step": 22200
    },
    {
      "epoch": 0.71072,
      "grad_norm": 0.044244322925806046,
      "learning_rate": 6.446560000000001e-06,
      "loss": 0.1991,
      "step": 22210
    },
    {
      "epoch": 0.71104,
      "grad_norm": 0.03538963571190834,
      "learning_rate": 6.44496e-06,
      "loss": 0.2123,
      "step": 22220
    },
    {
      "epoch": 0.71136,
      "grad_norm": 0.10187437385320663,
      "learning_rate": 6.443360000000001e-06,
      "loss": 0.2131,
      "step": 22230
    },
    {
      "epoch": 0.71168,
      "grad_norm": 0.8477052450180054,
      "learning_rate": 6.4417600000000005e-06,
      "loss": 0.2111,
      "step": 22240
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.02415931038558483,
      "learning_rate": 6.440160000000001e-06,
      "loss": 0.2272,
      "step": 22250
    },
    {
      "epoch": 0.71232,
      "grad_norm": 0.03955429047346115,
      "learning_rate": 6.43856e-06,
      "loss": 0.1993,
      "step": 22260
    },
    {
      "epoch": 0.71264,
      "grad_norm": 0.037597835063934326,
      "learning_rate": 6.436960000000001e-06,
      "loss": 0.2102,
      "step": 22270
    },
    {
      "epoch": 0.71296,
      "grad_norm": 0.04773259907960892,
      "learning_rate": 6.435360000000001e-06,
      "loss": 0.1993,
      "step": 22280
    },
    {
      "epoch": 0.71328,
      "grad_norm": 0.0301640834659338,
      "learning_rate": 6.43376e-06,
      "loss": 0.2074,
      "step": 22290
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.03675096854567528,
      "learning_rate": 6.432160000000001e-06,
      "loss": 0.226,
      "step": 22300
    },
    {
      "epoch": 0.71392,
      "grad_norm": 0.023488342761993408,
      "learning_rate": 6.4305600000000005e-06,
      "loss": 0.1994,
      "step": 22310
    },
    {
      "epoch": 0.71424,
      "grad_norm": 0.01698678731918335,
      "learning_rate": 6.42896e-06,
      "loss": 0.1993,
      "step": 22320
    },
    {
      "epoch": 0.71456,
      "grad_norm": 0.013912173919379711,
      "learning_rate": 6.42736e-06,
      "loss": 0.2266,
      "step": 22330
    },
    {
      "epoch": 0.71488,
      "grad_norm": 0.04378924518823624,
      "learning_rate": 6.425760000000001e-06,
      "loss": 0.1993,
      "step": 22340
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.04484511539340019,
      "learning_rate": 6.42416e-06,
      "loss": 0.1995,
      "step": 22350
    },
    {
      "epoch": 0.71552,
      "grad_norm": 0.050990354269742966,
      "learning_rate": 6.422560000000001e-06,
      "loss": 0.1992,
      "step": 22360
    },
    {
      "epoch": 0.71584,
      "grad_norm": 0.043848197907209396,
      "learning_rate": 6.420960000000001e-06,
      "loss": 0.2074,
      "step": 22370
    },
    {
      "epoch": 0.71616,
      "grad_norm": 0.03924231231212616,
      "learning_rate": 6.41936e-06,
      "loss": 0.1996,
      "step": 22380
    },
    {
      "epoch": 0.71648,
      "grad_norm": 0.023259654641151428,
      "learning_rate": 6.41776e-06,
      "loss": 0.1991,
      "step": 22390
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.03253069519996643,
      "learning_rate": 6.41616e-06,
      "loss": 0.2163,
      "step": 22400
    },
    {
      "epoch": 0.71712,
      "grad_norm": 0.01207309402525425,
      "learning_rate": 6.414560000000001e-06,
      "loss": 0.1993,
      "step": 22410
    },
    {
      "epoch": 0.71744,
      "grad_norm": 0.01657138392329216,
      "learning_rate": 6.41296e-06,
      "loss": 0.1994,
      "step": 22420
    },
    {
      "epoch": 0.71776,
      "grad_norm": 0.021483540534973145,
      "learning_rate": 6.411360000000001e-06,
      "loss": 0.1993,
      "step": 22430
    },
    {
      "epoch": 0.71808,
      "grad_norm": 0.0575624518096447,
      "learning_rate": 6.409760000000001e-06,
      "loss": 0.2191,
      "step": 22440
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.04660705476999283,
      "learning_rate": 6.408160000000001e-06,
      "loss": 0.1994,
      "step": 22450
    },
    {
      "epoch": 0.71872,
      "grad_norm": 0.01824834756553173,
      "learning_rate": 6.40656e-06,
      "loss": 0.1993,
      "step": 22460
    },
    {
      "epoch": 0.71904,
      "grad_norm": 0.0164264515042305,
      "learning_rate": 6.40496e-06,
      "loss": 0.1991,
      "step": 22470
    },
    {
      "epoch": 0.71936,
      "grad_norm": 0.030692366883158684,
      "learning_rate": 6.403360000000001e-06,
      "loss": 0.2131,
      "step": 22480
    },
    {
      "epoch": 0.71968,
      "grad_norm": 0.02371152676641941,
      "learning_rate": 6.40176e-06,
      "loss": 0.2169,
      "step": 22490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.0137950973585248,
      "learning_rate": 6.400160000000001e-06,
      "loss": 0.1991,
      "step": 22500
    },
    {
      "epoch": 0.72032,
      "grad_norm": 0.04664398729801178,
      "learning_rate": 6.398560000000001e-06,
      "loss": 0.1992,
      "step": 22510
    },
    {
      "epoch": 0.72064,
      "grad_norm": 0.01975088007748127,
      "learning_rate": 6.396960000000001e-06,
      "loss": 0.2087,
      "step": 22520
    },
    {
      "epoch": 0.72096,
      "grad_norm": 0.09815336018800735,
      "learning_rate": 6.39536e-06,
      "loss": 0.1994,
      "step": 22530
    },
    {
      "epoch": 0.72128,
      "grad_norm": 0.02253451943397522,
      "learning_rate": 6.393760000000001e-06,
      "loss": 0.1998,
      "step": 22540
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.013779315166175365,
      "learning_rate": 6.392160000000001e-06,
      "loss": 0.1995,
      "step": 22550
    },
    {
      "epoch": 0.72192,
      "grad_norm": 0.032189130783081055,
      "learning_rate": 6.39056e-06,
      "loss": 0.2,
      "step": 22560
    },
    {
      "epoch": 0.72224,
      "grad_norm": 0.06496728956699371,
      "learning_rate": 6.388960000000001e-06,
      "loss": 0.1996,
      "step": 22570
    },
    {
      "epoch": 0.72256,
      "grad_norm": 0.03032945841550827,
      "learning_rate": 6.387360000000001e-06,
      "loss": 0.1991,
      "step": 22580
    },
    {
      "epoch": 0.72288,
      "grad_norm": 0.04243256524205208,
      "learning_rate": 6.3857600000000005e-06,
      "loss": 0.1995,
      "step": 22590
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.030510319396853447,
      "learning_rate": 6.38416e-06,
      "loss": 0.1993,
      "step": 22600
    },
    {
      "epoch": 0.72352,
      "grad_norm": 0.018796171993017197,
      "learning_rate": 6.382560000000001e-06,
      "loss": 0.1996,
      "step": 22610
    },
    {
      "epoch": 0.72384,
      "grad_norm": 0.017071209847927094,
      "learning_rate": 6.38096e-06,
      "loss": 0.1992,
      "step": 22620
    },
    {
      "epoch": 0.72416,
      "grad_norm": 0.02567124180495739,
      "learning_rate": 6.37936e-06,
      "loss": 0.1993,
      "step": 22630
    },
    {
      "epoch": 0.72448,
      "grad_norm": 0.028764884918928146,
      "learning_rate": 6.377760000000001e-06,
      "loss": 0.1994,
      "step": 22640
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.017991801723837852,
      "learning_rate": 6.37616e-06,
      "loss": 0.201,
      "step": 22650
    },
    {
      "epoch": 0.72512,
      "grad_norm": 0.010032875463366508,
      "learning_rate": 6.3745600000000005e-06,
      "loss": 0.2067,
      "step": 22660
    },
    {
      "epoch": 0.72544,
      "grad_norm": 0.033453162759542465,
      "learning_rate": 6.37296e-06,
      "loss": 0.2142,
      "step": 22670
    },
    {
      "epoch": 0.72576,
      "grad_norm": 0.021773135289549828,
      "learning_rate": 6.371360000000001e-06,
      "loss": 0.1992,
      "step": 22680
    },
    {
      "epoch": 0.72608,
      "grad_norm": 0.015702662989497185,
      "learning_rate": 6.36976e-06,
      "loss": 0.2068,
      "step": 22690
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.050256453454494476,
      "learning_rate": 6.368160000000001e-06,
      "loss": 0.1994,
      "step": 22700
    },
    {
      "epoch": 0.72672,
      "grad_norm": 0.5250985622406006,
      "learning_rate": 6.366560000000001e-06,
      "loss": 0.2167,
      "step": 22710
    },
    {
      "epoch": 0.72704,
      "grad_norm": 1.1534706354141235,
      "learning_rate": 6.36496e-06,
      "loss": 0.2012,
      "step": 22720
    },
    {
      "epoch": 0.72736,
      "grad_norm": 0.048605237156152725,
      "learning_rate": 6.3633600000000005e-06,
      "loss": 0.1993,
      "step": 22730
    },
    {
      "epoch": 0.72768,
      "grad_norm": 0.010295533575117588,
      "learning_rate": 6.36176e-06,
      "loss": 0.2088,
      "step": 22740
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.02099665254354477,
      "learning_rate": 6.360160000000001e-06,
      "loss": 0.1991,
      "step": 22750
    },
    {
      "epoch": 0.72832,
      "grad_norm": 0.025054745376110077,
      "learning_rate": 6.35856e-06,
      "loss": 0.2123,
      "step": 22760
    },
    {
      "epoch": 0.72864,
      "grad_norm": 0.03838204964995384,
      "learning_rate": 6.356960000000001e-06,
      "loss": 0.1992,
      "step": 22770
    },
    {
      "epoch": 0.72896,
      "grad_norm": 0.010596062988042831,
      "learning_rate": 6.355360000000001e-06,
      "loss": 0.1994,
      "step": 22780
    },
    {
      "epoch": 0.72928,
      "grad_norm": 0.014815162867307663,
      "learning_rate": 6.353760000000001e-06,
      "loss": 0.2001,
      "step": 22790
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.04150175303220749,
      "learning_rate": 6.3521600000000005e-06,
      "loss": 0.1993,
      "step": 22800
    },
    {
      "epoch": 0.72992,
      "grad_norm": 0.020180849358439445,
      "learning_rate": 6.35056e-06,
      "loss": 0.1993,
      "step": 22810
    },
    {
      "epoch": 0.73024,
      "grad_norm": 0.0342656634747982,
      "learning_rate": 6.34896e-06,
      "loss": 0.1992,
      "step": 22820
    },
    {
      "epoch": 0.73056,
      "grad_norm": 0.02740894816815853,
      "learning_rate": 6.34736e-06,
      "loss": 0.1993,
      "step": 22830
    },
    {
      "epoch": 0.73088,
      "grad_norm": 0.015219048596918583,
      "learning_rate": 6.345760000000001e-06,
      "loss": 0.1992,
      "step": 22840
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.01747043803334236,
      "learning_rate": 6.34416e-06,
      "loss": 0.2051,
      "step": 22850
    },
    {
      "epoch": 0.73152,
      "grad_norm": 0.022453362122178078,
      "learning_rate": 6.342560000000001e-06,
      "loss": 0.1991,
      "step": 22860
    },
    {
      "epoch": 0.73184,
      "grad_norm": 0.032208267599344254,
      "learning_rate": 6.3409600000000005e-06,
      "loss": 0.2137,
      "step": 22870
    },
    {
      "epoch": 0.73216,
      "grad_norm": 0.027697652578353882,
      "learning_rate": 6.339360000000001e-06,
      "loss": 0.2162,
      "step": 22880
    },
    {
      "epoch": 0.73248,
      "grad_norm": 0.012909297831356525,
      "learning_rate": 6.33776e-06,
      "loss": 0.1999,
      "step": 22890
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.019889850169420242,
      "learning_rate": 6.33616e-06,
      "loss": 0.2,
      "step": 22900
    },
    {
      "epoch": 0.73312,
      "grad_norm": 0.03741535171866417,
      "learning_rate": 6.334560000000001e-06,
      "loss": 0.1993,
      "step": 22910
    },
    {
      "epoch": 0.73344,
      "grad_norm": 0.011357474140822887,
      "learning_rate": 6.33296e-06,
      "loss": 0.1996,
      "step": 22920
    },
    {
      "epoch": 0.73376,
      "grad_norm": 0.027162112295627594,
      "learning_rate": 6.331360000000001e-06,
      "loss": 0.2014,
      "step": 22930
    },
    {
      "epoch": 0.73408,
      "grad_norm": 0.016673920676112175,
      "learning_rate": 6.3297600000000005e-06,
      "loss": 0.1994,
      "step": 22940
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.02070091851055622,
      "learning_rate": 6.328160000000001e-06,
      "loss": 0.2148,
      "step": 22950
    },
    {
      "epoch": 0.73472,
      "grad_norm": 0.01121611800044775,
      "learning_rate": 6.32656e-06,
      "loss": 0.1991,
      "step": 22960
    },
    {
      "epoch": 0.73504,
      "grad_norm": 0.023649046197533607,
      "learning_rate": 6.32496e-06,
      "loss": 0.2015,
      "step": 22970
    },
    {
      "epoch": 0.73536,
      "grad_norm": 0.8320293426513672,
      "learning_rate": 6.323360000000001e-06,
      "loss": 0.2126,
      "step": 22980
    },
    {
      "epoch": 0.73568,
      "grad_norm": 0.05500984936952591,
      "learning_rate": 6.32176e-06,
      "loss": 0.1992,
      "step": 22990
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.04768539220094681,
      "learning_rate": 6.320160000000001e-06,
      "loss": 0.1994,
      "step": 23000
    },
    {
      "epoch": 0.736,
      "eval_runtime": 50.8157,
      "eval_samples_per_second": 196.79,
      "eval_steps_per_second": 12.299,
      "step": 23000
    },
    {
      "epoch": 0.73632,
      "grad_norm": 0.021368004381656647,
      "learning_rate": 6.3185600000000005e-06,
      "loss": 0.1993,
      "step": 23010
    },
    {
      "epoch": 0.73664,
      "grad_norm": 1.5452957153320312,
      "learning_rate": 6.316960000000001e-06,
      "loss": 0.2014,
      "step": 23020
    },
    {
      "epoch": 0.73696,
      "grad_norm": 0.030657967552542686,
      "learning_rate": 6.31536e-06,
      "loss": 0.2122,
      "step": 23030
    },
    {
      "epoch": 0.73728,
      "grad_norm": 0.02784244529902935,
      "learning_rate": 6.313760000000001e-06,
      "loss": 0.2132,
      "step": 23040
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.03174840286374092,
      "learning_rate": 6.312160000000001e-06,
      "loss": 0.1994,
      "step": 23050
    },
    {
      "epoch": 0.73792,
      "grad_norm": 0.016161825507879257,
      "learning_rate": 6.31056e-06,
      "loss": 0.2013,
      "step": 23060
    },
    {
      "epoch": 0.73824,
      "grad_norm": 0.026970714330673218,
      "learning_rate": 6.308960000000001e-06,
      "loss": 0.1992,
      "step": 23070
    },
    {
      "epoch": 0.73856,
      "grad_norm": 0.010762929916381836,
      "learning_rate": 6.3073600000000005e-06,
      "loss": 0.1999,
      "step": 23080
    },
    {
      "epoch": 0.73888,
      "grad_norm": 0.013051021844148636,
      "learning_rate": 6.30576e-06,
      "loss": 0.1991,
      "step": 23090
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.02147102728486061,
      "learning_rate": 6.30416e-06,
      "loss": 0.1991,
      "step": 23100
    },
    {
      "epoch": 0.73952,
      "grad_norm": 0.021299010142683983,
      "learning_rate": 6.302560000000001e-06,
      "loss": 0.2002,
      "step": 23110
    },
    {
      "epoch": 0.73984,
      "grad_norm": 0.030812140554189682,
      "learning_rate": 6.30096e-06,
      "loss": 0.1991,
      "step": 23120
    },
    {
      "epoch": 0.74016,
      "grad_norm": 0.024430543184280396,
      "learning_rate": 6.299360000000001e-06,
      "loss": 0.1991,
      "step": 23130
    },
    {
      "epoch": 0.74048,
      "grad_norm": 0.031194569543004036,
      "learning_rate": 6.297760000000001e-06,
      "loss": 0.2115,
      "step": 23140
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.02556196041405201,
      "learning_rate": 6.29616e-06,
      "loss": 0.2024,
      "step": 23150
    },
    {
      "epoch": 0.74112,
      "grad_norm": 0.03219333291053772,
      "learning_rate": 6.2945600000000004e-06,
      "loss": 0.1995,
      "step": 23160
    },
    {
      "epoch": 0.74144,
      "grad_norm": 0.03196486085653305,
      "learning_rate": 6.29296e-06,
      "loss": 0.2138,
      "step": 23170
    },
    {
      "epoch": 0.74176,
      "grad_norm": 0.012601225636899471,
      "learning_rate": 6.291360000000001e-06,
      "loss": 0.1993,
      "step": 23180
    },
    {
      "epoch": 0.74208,
      "grad_norm": 0.03398501127958298,
      "learning_rate": 6.28976e-06,
      "loss": 0.1992,
      "step": 23190
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.04268420487642288,
      "learning_rate": 6.288160000000001e-06,
      "loss": 0.1994,
      "step": 23200
    },
    {
      "epoch": 0.74272,
      "grad_norm": 0.01266382820904255,
      "learning_rate": 6.286560000000001e-06,
      "loss": 0.2149,
      "step": 23210
    },
    {
      "epoch": 0.74304,
      "grad_norm": 0.03318746015429497,
      "learning_rate": 6.284960000000001e-06,
      "loss": 0.1993,
      "step": 23220
    },
    {
      "epoch": 0.74336,
      "grad_norm": 0.02111305296421051,
      "learning_rate": 6.2833600000000004e-06,
      "loss": 0.1993,
      "step": 23230
    },
    {
      "epoch": 0.74368,
      "grad_norm": 0.016745729371905327,
      "learning_rate": 6.28176e-06,
      "loss": 0.1993,
      "step": 23240
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.020145662128925323,
      "learning_rate": 6.280160000000001e-06,
      "loss": 0.2004,
      "step": 23250
    },
    {
      "epoch": 0.74432,
      "grad_norm": 0.03051742911338806,
      "learning_rate": 6.27856e-06,
      "loss": 0.1991,
      "step": 23260
    },
    {
      "epoch": 0.74464,
      "grad_norm": 0.022090481594204903,
      "learning_rate": 6.276960000000001e-06,
      "loss": 0.2005,
      "step": 23270
    },
    {
      "epoch": 0.74496,
      "grad_norm": 0.02636350318789482,
      "learning_rate": 6.275360000000001e-06,
      "loss": 0.1993,
      "step": 23280
    },
    {
      "epoch": 0.74528,
      "grad_norm": 1.0782361030578613,
      "learning_rate": 6.2737600000000006e-06,
      "loss": 0.2011,
      "step": 23290
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.6997036933898926,
      "learning_rate": 6.2721600000000004e-06,
      "loss": 0.2139,
      "step": 23300
    },
    {
      "epoch": 0.74592,
      "grad_norm": 0.013264915905892849,
      "learning_rate": 6.27056e-06,
      "loss": 0.1993,
      "step": 23310
    },
    {
      "epoch": 0.74624,
      "grad_norm": 0.01701759174466133,
      "learning_rate": 6.26896e-06,
      "loss": 0.2359,
      "step": 23320
    },
    {
      "epoch": 0.74656,
      "grad_norm": 0.04439308121800423,
      "learning_rate": 6.26736e-06,
      "loss": 0.1992,
      "step": 23330
    },
    {
      "epoch": 0.74688,
      "grad_norm": 0.04682237282395363,
      "learning_rate": 6.265760000000001e-06,
      "loss": 0.2313,
      "step": 23340
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.015403490513563156,
      "learning_rate": 6.26416e-06,
      "loss": 0.1996,
      "step": 23350
    },
    {
      "epoch": 0.74752,
      "grad_norm": 0.045039307326078415,
      "learning_rate": 6.2625600000000006e-06,
      "loss": 0.207,
      "step": 23360
    },
    {
      "epoch": 0.74784,
      "grad_norm": 0.06696268916130066,
      "learning_rate": 6.2609600000000004e-06,
      "loss": 0.2002,
      "step": 23370
    },
    {
      "epoch": 0.74816,
      "grad_norm": 0.7802053689956665,
      "learning_rate": 6.259360000000001e-06,
      "loss": 0.2317,
      "step": 23380
    },
    {
      "epoch": 0.74848,
      "grad_norm": 0.02175683155655861,
      "learning_rate": 6.25776e-06,
      "loss": 0.1992,
      "step": 23390
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.027228116989135742,
      "learning_rate": 6.25616e-06,
      "loss": 0.2118,
      "step": 23400
    },
    {
      "epoch": 0.74912,
      "grad_norm": 0.03132622689008713,
      "learning_rate": 6.254560000000001e-06,
      "loss": 0.1995,
      "step": 23410
    },
    {
      "epoch": 0.74944,
      "grad_norm": 0.020889006555080414,
      "learning_rate": 6.25296e-06,
      "loss": 0.1993,
      "step": 23420
    },
    {
      "epoch": 0.74976,
      "grad_norm": 0.051996078342199326,
      "learning_rate": 6.2513600000000006e-06,
      "loss": 0.2157,
      "step": 23430
    },
    {
      "epoch": 0.75008,
      "grad_norm": 0.03726482763886452,
      "learning_rate": 6.2497600000000005e-06,
      "loss": 0.1994,
      "step": 23440
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.050923630595207214,
      "learning_rate": 6.248160000000001e-06,
      "loss": 0.1992,
      "step": 23450
    },
    {
      "epoch": 0.75072,
      "grad_norm": 0.01265627983957529,
      "learning_rate": 6.24656e-06,
      "loss": 0.2006,
      "step": 23460
    },
    {
      "epoch": 0.75104,
      "grad_norm": 0.019724557176232338,
      "learning_rate": 6.244960000000001e-06,
      "loss": 0.2145,
      "step": 23470
    },
    {
      "epoch": 0.75136,
      "grad_norm": 0.04140091314911842,
      "learning_rate": 6.243360000000001e-06,
      "loss": 0.1993,
      "step": 23480
    },
    {
      "epoch": 0.75168,
      "grad_norm": 0.03359665721654892,
      "learning_rate": 6.24176e-06,
      "loss": 0.1992,
      "step": 23490
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.027744591236114502,
      "learning_rate": 6.240160000000001e-06,
      "loss": 0.2056,
      "step": 23500
    },
    {
      "epoch": 0.75232,
      "grad_norm": 0.019556736573576927,
      "learning_rate": 6.2385600000000005e-06,
      "loss": 0.2164,
      "step": 23510
    },
    {
      "epoch": 0.75264,
      "grad_norm": 0.03857812657952309,
      "learning_rate": 6.236960000000001e-06,
      "loss": 0.2021,
      "step": 23520
    },
    {
      "epoch": 0.75296,
      "grad_norm": 0.024509362876415253,
      "learning_rate": 6.23536e-06,
      "loss": 0.1992,
      "step": 23530
    },
    {
      "epoch": 0.75328,
      "grad_norm": 0.017942309379577637,
      "learning_rate": 6.233760000000001e-06,
      "loss": 0.1996,
      "step": 23540
    },
    {
      "epoch": 0.7536,
      "grad_norm": 1.87651526927948,
      "learning_rate": 6.232160000000001e-06,
      "loss": 0.2073,
      "step": 23550
    },
    {
      "epoch": 0.75392,
      "grad_norm": 0.011469988152384758,
      "learning_rate": 6.230560000000001e-06,
      "loss": 0.2126,
      "step": 23560
    },
    {
      "epoch": 0.75424,
      "grad_norm": 0.011949550360441208,
      "learning_rate": 6.228960000000001e-06,
      "loss": 0.1991,
      "step": 23570
    },
    {
      "epoch": 0.75456,
      "grad_norm": 0.023532496765255928,
      "learning_rate": 6.2273600000000005e-06,
      "loss": 0.1991,
      "step": 23580
    },
    {
      "epoch": 0.75488,
      "grad_norm": 0.049769334495067596,
      "learning_rate": 6.22576e-06,
      "loss": 0.213,
      "step": 23590
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.07971218228340149,
      "learning_rate": 6.22416e-06,
      "loss": 0.1993,
      "step": 23600
    },
    {
      "epoch": 0.75552,
      "grad_norm": 0.02411496825516224,
      "learning_rate": 6.222560000000001e-06,
      "loss": 0.1992,
      "step": 23610
    },
    {
      "epoch": 0.75584,
      "grad_norm": 0.02603725902736187,
      "learning_rate": 6.22096e-06,
      "loss": 0.1992,
      "step": 23620
    },
    {
      "epoch": 0.75616,
      "grad_norm": 0.05846244841814041,
      "learning_rate": 6.219360000000001e-06,
      "loss": 0.1991,
      "step": 23630
    },
    {
      "epoch": 0.75648,
      "grad_norm": 0.022508103400468826,
      "learning_rate": 6.217760000000001e-06,
      "loss": 0.1993,
      "step": 23640
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.06887926161289215,
      "learning_rate": 6.21616e-06,
      "loss": 0.2103,
      "step": 23650
    },
    {
      "epoch": 0.75712,
      "grad_norm": 0.05229565501213074,
      "learning_rate": 6.21456e-06,
      "loss": 0.2166,
      "step": 23660
    },
    {
      "epoch": 0.75744,
      "grad_norm": 0.014480150304734707,
      "learning_rate": 6.21296e-06,
      "loss": 0.1996,
      "step": 23670
    },
    {
      "epoch": 0.75776,
      "grad_norm": 0.009575014002621174,
      "learning_rate": 6.211360000000001e-06,
      "loss": 0.2092,
      "step": 23680
    },
    {
      "epoch": 0.75808,
      "grad_norm": 0.020507780835032463,
      "learning_rate": 6.20976e-06,
      "loss": 0.2077,
      "step": 23690
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.011502460576593876,
      "learning_rate": 6.208160000000001e-06,
      "loss": 0.1992,
      "step": 23700
    },
    {
      "epoch": 0.75872,
      "grad_norm": 0.027457335963845253,
      "learning_rate": 6.206560000000001e-06,
      "loss": 0.1994,
      "step": 23710
    },
    {
      "epoch": 0.75904,
      "grad_norm": 0.024684647098183632,
      "learning_rate": 6.204960000000001e-06,
      "loss": 0.2105,
      "step": 23720
    },
    {
      "epoch": 0.75936,
      "grad_norm": 0.019707856699824333,
      "learning_rate": 6.20336e-06,
      "loss": 0.1992,
      "step": 23730
    },
    {
      "epoch": 0.75968,
      "grad_norm": 0.016480160877108574,
      "learning_rate": 6.20176e-06,
      "loss": 0.2137,
      "step": 23740
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.010310656391084194,
      "learning_rate": 6.200160000000001e-06,
      "loss": 0.2074,
      "step": 23750
    },
    {
      "epoch": 0.76032,
      "grad_norm": 0.017667192965745926,
      "learning_rate": 6.19856e-06,
      "loss": 0.1997,
      "step": 23760
    },
    {
      "epoch": 0.76064,
      "grad_norm": 0.01727982610464096,
      "learning_rate": 6.196960000000001e-06,
      "loss": 0.213,
      "step": 23770
    },
    {
      "epoch": 0.76096,
      "grad_norm": 0.02392212115228176,
      "learning_rate": 6.195360000000001e-06,
      "loss": 0.2153,
      "step": 23780
    },
    {
      "epoch": 0.76128,
      "grad_norm": 0.055145904421806335,
      "learning_rate": 6.1937600000000005e-06,
      "loss": 0.1992,
      "step": 23790
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.02227373607456684,
      "learning_rate": 6.19216e-06,
      "loss": 0.1994,
      "step": 23800
    },
    {
      "epoch": 0.76192,
      "grad_norm": 0.023998843505978584,
      "learning_rate": 6.190560000000001e-06,
      "loss": 0.224,
      "step": 23810
    },
    {
      "epoch": 0.76224,
      "grad_norm": 0.02794261835515499,
      "learning_rate": 6.18896e-06,
      "loss": 0.1992,
      "step": 23820
    },
    {
      "epoch": 0.76256,
      "grad_norm": 0.014604005962610245,
      "learning_rate": 6.18736e-06,
      "loss": 0.1993,
      "step": 23830
    },
    {
      "epoch": 0.76288,
      "grad_norm": 0.03537541627883911,
      "learning_rate": 6.185760000000001e-06,
      "loss": 0.1991,
      "step": 23840
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.05216483399271965,
      "learning_rate": 6.18416e-06,
      "loss": 0.2101,
      "step": 23850
    },
    {
      "epoch": 0.76352,
      "grad_norm": 1.2028378248214722,
      "learning_rate": 6.1825600000000005e-06,
      "loss": 0.2102,
      "step": 23860
    },
    {
      "epoch": 0.76384,
      "grad_norm": 0.03982005640864372,
      "learning_rate": 6.18096e-06,
      "loss": 0.2001,
      "step": 23870
    },
    {
      "epoch": 0.76416,
      "grad_norm": 0.07743965834379196,
      "learning_rate": 6.179360000000001e-06,
      "loss": 0.1998,
      "step": 23880
    },
    {
      "epoch": 0.76448,
      "grad_norm": 0.04613964259624481,
      "learning_rate": 6.17776e-06,
      "loss": 0.1992,
      "step": 23890
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.013125643134117126,
      "learning_rate": 6.176160000000001e-06,
      "loss": 0.1995,
      "step": 23900
    },
    {
      "epoch": 0.76512,
      "grad_norm": 0.05074484646320343,
      "learning_rate": 6.174560000000001e-06,
      "loss": 0.24,
      "step": 23910
    },
    {
      "epoch": 0.76544,
      "grad_norm": 0.014117543585598469,
      "learning_rate": 6.17296e-06,
      "loss": 0.2135,
      "step": 23920
    },
    {
      "epoch": 0.76576,
      "grad_norm": 1.0147963762283325,
      "learning_rate": 6.1713600000000005e-06,
      "loss": 0.205,
      "step": 23930
    },
    {
      "epoch": 0.76608,
      "grad_norm": 0.0669371634721756,
      "learning_rate": 6.16976e-06,
      "loss": 0.1996,
      "step": 23940
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.009752857498824596,
      "learning_rate": 6.168160000000001e-06,
      "loss": 0.1993,
      "step": 23950
    },
    {
      "epoch": 0.76672,
      "grad_norm": 0.019000237807631493,
      "learning_rate": 6.16656e-06,
      "loss": 0.2046,
      "step": 23960
    },
    {
      "epoch": 0.76704,
      "grad_norm": 0.019608110189437866,
      "learning_rate": 6.164960000000001e-06,
      "loss": 0.1992,
      "step": 23970
    },
    {
      "epoch": 0.76736,
      "grad_norm": 0.04624621570110321,
      "learning_rate": 6.163360000000001e-06,
      "loss": 0.1992,
      "step": 23980
    },
    {
      "epoch": 0.76768,
      "grad_norm": 0.027006594464182854,
      "learning_rate": 6.16176e-06,
      "loss": 0.1991,
      "step": 23990
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.05723962560296059,
      "learning_rate": 6.1601600000000005e-06,
      "loss": 0.1994,
      "step": 24000
    },
    {
      "epoch": 0.768,
      "eval_runtime": 50.2048,
      "eval_samples_per_second": 199.184,
      "eval_steps_per_second": 12.449,
      "step": 24000
    },
    {
      "epoch": 0.76832,
      "grad_norm": 0.04283052310347557,
      "learning_rate": 6.15856e-06,
      "loss": 0.1992,
      "step": 24010
    },
    {
      "epoch": 0.76864,
      "grad_norm": 1.3756005764007568,
      "learning_rate": 6.156960000000001e-06,
      "loss": 0.2253,
      "step": 24020
    },
    {
      "epoch": 0.76896,
      "grad_norm": 0.03965939208865166,
      "learning_rate": 6.15536e-06,
      "loss": 0.1997,
      "step": 24030
    },
    {
      "epoch": 0.76928,
      "grad_norm": 0.7421031594276428,
      "learning_rate": 6.153760000000001e-06,
      "loss": 0.2286,
      "step": 24040
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.01699356734752655,
      "learning_rate": 6.152160000000001e-06,
      "loss": 0.1991,
      "step": 24050
    },
    {
      "epoch": 0.76992,
      "grad_norm": 0.05138517543673515,
      "learning_rate": 6.150560000000001e-06,
      "loss": 0.2011,
      "step": 24060
    },
    {
      "epoch": 0.77024,
      "grad_norm": 0.8142761588096619,
      "learning_rate": 6.1489600000000005e-06,
      "loss": 0.2123,
      "step": 24070
    },
    {
      "epoch": 0.77056,
      "grad_norm": 0.030913179740309715,
      "learning_rate": 6.14736e-06,
      "loss": 0.1993,
      "step": 24080
    },
    {
      "epoch": 0.77088,
      "grad_norm": 0.01990046538412571,
      "learning_rate": 6.14576e-06,
      "loss": 0.1992,
      "step": 24090
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.05326046422123909,
      "learning_rate": 6.14416e-06,
      "loss": 0.2138,
      "step": 24100
    },
    {
      "epoch": 0.77152,
      "grad_norm": 0.04409307986497879,
      "learning_rate": 6.142560000000001e-06,
      "loss": 0.1995,
      "step": 24110
    },
    {
      "epoch": 0.77184,
      "grad_norm": 0.028225846588611603,
      "learning_rate": 6.14096e-06,
      "loss": 0.1993,
      "step": 24120
    },
    {
      "epoch": 0.77216,
      "grad_norm": 0.015998857095837593,
      "learning_rate": 6.139360000000001e-06,
      "loss": 0.1994,
      "step": 24130
    },
    {
      "epoch": 0.77248,
      "grad_norm": 0.00824258103966713,
      "learning_rate": 6.1377600000000005e-06,
      "loss": 0.2131,
      "step": 24140
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.025216825306415558,
      "learning_rate": 6.136160000000001e-06,
      "loss": 0.216,
      "step": 24150
    },
    {
      "epoch": 0.77312,
      "grad_norm": 0.022045748308300972,
      "learning_rate": 6.13456e-06,
      "loss": 0.1995,
      "step": 24160
    },
    {
      "epoch": 0.77344,
      "grad_norm": 0.033113013952970505,
      "learning_rate": 6.13296e-06,
      "loss": 0.1996,
      "step": 24170
    },
    {
      "epoch": 0.77376,
      "grad_norm": 0.026996010914444923,
      "learning_rate": 6.131360000000001e-06,
      "loss": 0.2148,
      "step": 24180
    },
    {
      "epoch": 0.77408,
      "grad_norm": 0.10603849589824677,
      "learning_rate": 6.12976e-06,
      "loss": 0.2024,
      "step": 24190
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.03389185294508934,
      "learning_rate": 6.128160000000001e-06,
      "loss": 0.2056,
      "step": 24200
    },
    {
      "epoch": 0.77472,
      "grad_norm": 0.06122853234410286,
      "learning_rate": 6.1265600000000005e-06,
      "loss": 0.2017,
      "step": 24210
    },
    {
      "epoch": 0.77504,
      "grad_norm": 0.01951110176742077,
      "learning_rate": 6.124960000000001e-06,
      "loss": 0.1993,
      "step": 24220
    },
    {
      "epoch": 0.77536,
      "grad_norm": 1.874375343322754,
      "learning_rate": 6.12336e-06,
      "loss": 0.2334,
      "step": 24230
    },
    {
      "epoch": 0.77568,
      "grad_norm": 0.019361812621355057,
      "learning_rate": 6.121760000000001e-06,
      "loss": 0.1999,
      "step": 24240
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.04524606838822365,
      "learning_rate": 6.120160000000001e-06,
      "loss": 0.1993,
      "step": 24250
    },
    {
      "epoch": 0.77632,
      "grad_norm": 0.03661486878991127,
      "learning_rate": 6.11856e-06,
      "loss": 0.2016,
      "step": 24260
    },
    {
      "epoch": 0.77664,
      "grad_norm": 0.025901073589920998,
      "learning_rate": 6.116960000000001e-06,
      "loss": 0.1991,
      "step": 24270
    },
    {
      "epoch": 0.77696,
      "grad_norm": 0.027459440752863884,
      "learning_rate": 6.1153600000000005e-06,
      "loss": 0.1992,
      "step": 24280
    },
    {
      "epoch": 0.77728,
      "grad_norm": 0.03693321719765663,
      "learning_rate": 6.11376e-06,
      "loss": 0.1998,
      "step": 24290
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.009843149222433567,
      "learning_rate": 6.11216e-06,
      "loss": 0.1998,
      "step": 24300
    },
    {
      "epoch": 0.77792,
      "grad_norm": 0.04157179221510887,
      "learning_rate": 6.110560000000001e-06,
      "loss": 0.2005,
      "step": 24310
    },
    {
      "epoch": 0.77824,
      "grad_norm": 0.012354646809399128,
      "learning_rate": 6.10896e-06,
      "loss": 0.1993,
      "step": 24320
    },
    {
      "epoch": 0.77856,
      "grad_norm": 0.020234428346157074,
      "learning_rate": 6.10736e-06,
      "loss": 0.1993,
      "step": 24330
    },
    {
      "epoch": 0.77888,
      "grad_norm": 0.04334602877497673,
      "learning_rate": 6.105760000000001e-06,
      "loss": 0.2159,
      "step": 24340
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.011331113986670971,
      "learning_rate": 6.10416e-06,
      "loss": 0.1996,
      "step": 24350
    },
    {
      "epoch": 0.77952,
      "grad_norm": 0.051382023841142654,
      "learning_rate": 6.10256e-06,
      "loss": 0.1992,
      "step": 24360
    },
    {
      "epoch": 0.77984,
      "grad_norm": 0.03979543596506119,
      "learning_rate": 6.10096e-06,
      "loss": 0.1994,
      "step": 24370
    },
    {
      "epoch": 0.78016,
      "grad_norm": 0.026455041021108627,
      "learning_rate": 6.099360000000001e-06,
      "loss": 0.1993,
      "step": 24380
    },
    {
      "epoch": 0.78048,
      "grad_norm": 0.02068355306982994,
      "learning_rate": 6.09776e-06,
      "loss": 0.1991,
      "step": 24390
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.04338328167796135,
      "learning_rate": 6.096160000000001e-06,
      "loss": 0.1993,
      "step": 24400
    },
    {
      "epoch": 0.78112,
      "grad_norm": 0.019440729171037674,
      "learning_rate": 6.094560000000001e-06,
      "loss": 0.2012,
      "step": 24410
    },
    {
      "epoch": 0.78144,
      "grad_norm": 0.05307973176240921,
      "learning_rate": 6.09296e-06,
      "loss": 0.2144,
      "step": 24420
    },
    {
      "epoch": 0.78176,
      "grad_norm": 0.01414776872843504,
      "learning_rate": 6.09136e-06,
      "loss": 0.2217,
      "step": 24430
    },
    {
      "epoch": 0.78208,
      "grad_norm": 0.02405664324760437,
      "learning_rate": 6.08976e-06,
      "loss": 0.1995,
      "step": 24440
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.06611491739749908,
      "learning_rate": 6.088160000000001e-06,
      "loss": 0.1995,
      "step": 24450
    },
    {
      "epoch": 0.78272,
      "grad_norm": 0.01120374072343111,
      "learning_rate": 6.08656e-06,
      "loss": 0.1991,
      "step": 24460
    },
    {
      "epoch": 0.78304,
      "grad_norm": 0.03832310065627098,
      "learning_rate": 6.084960000000001e-06,
      "loss": 0.2325,
      "step": 24470
    },
    {
      "epoch": 0.78336,
      "grad_norm": 0.010464913211762905,
      "learning_rate": 6.083360000000001e-06,
      "loss": 0.2165,
      "step": 24480
    },
    {
      "epoch": 0.78368,
      "grad_norm": 0.04888947308063507,
      "learning_rate": 6.081760000000001e-06,
      "loss": 0.1994,
      "step": 24490
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.012533782981336117,
      "learning_rate": 6.08016e-06,
      "loss": 0.1993,
      "step": 24500
    },
    {
      "epoch": 0.78432,
      "grad_norm": 0.010545718483626842,
      "learning_rate": 6.07856e-06,
      "loss": 0.216,
      "step": 24510
    },
    {
      "epoch": 0.78464,
      "grad_norm": 0.044722817838191986,
      "learning_rate": 6.076960000000001e-06,
      "loss": 0.2124,
      "step": 24520
    },
    {
      "epoch": 0.78496,
      "grad_norm": 0.06537341326475143,
      "learning_rate": 6.07536e-06,
      "loss": 0.2126,
      "step": 24530
    },
    {
      "epoch": 0.78528,
      "grad_norm": 0.022800013422966003,
      "learning_rate": 6.073760000000001e-06,
      "loss": 0.2092,
      "step": 24540
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.028214598074555397,
      "learning_rate": 6.072160000000001e-06,
      "loss": 0.1994,
      "step": 24550
    },
    {
      "epoch": 0.78592,
      "grad_norm": 0.016161005944013596,
      "learning_rate": 6.0705600000000005e-06,
      "loss": 0.2028,
      "step": 24560
    },
    {
      "epoch": 0.78624,
      "grad_norm": 0.020241349935531616,
      "learning_rate": 6.06896e-06,
      "loss": 0.1993,
      "step": 24570
    },
    {
      "epoch": 0.78656,
      "grad_norm": 0.02143832854926586,
      "learning_rate": 6.067360000000001e-06,
      "loss": 0.231,
      "step": 24580
    },
    {
      "epoch": 0.78688,
      "grad_norm": 0.033150024712085724,
      "learning_rate": 6.06576e-06,
      "loss": 0.1994,
      "step": 24590
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.04737194627523422,
      "learning_rate": 6.06416e-06,
      "loss": 0.1993,
      "step": 24600
    },
    {
      "epoch": 0.78752,
      "grad_norm": 0.6841026544570923,
      "learning_rate": 6.062560000000001e-06,
      "loss": 0.2141,
      "step": 24610
    },
    {
      "epoch": 0.78784,
      "grad_norm": 0.03694359213113785,
      "learning_rate": 6.06096e-06,
      "loss": 0.1996,
      "step": 24620
    },
    {
      "epoch": 0.78816,
      "grad_norm": 0.04069911316037178,
      "learning_rate": 6.0593600000000005e-06,
      "loss": 0.1992,
      "step": 24630
    },
    {
      "epoch": 0.78848,
      "grad_norm": 0.012635556049644947,
      "learning_rate": 6.0577600000000004e-06,
      "loss": 0.1997,
      "step": 24640
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.011856211349368095,
      "learning_rate": 6.056160000000001e-06,
      "loss": 0.2082,
      "step": 24650
    },
    {
      "epoch": 0.78912,
      "grad_norm": 0.04085635393857956,
      "learning_rate": 6.05456e-06,
      "loss": 0.1991,
      "step": 24660
    },
    {
      "epoch": 0.78944,
      "grad_norm": 1.5966343879699707,
      "learning_rate": 6.05296e-06,
      "loss": 0.2061,
      "step": 24670
    },
    {
      "epoch": 0.78976,
      "grad_norm": 0.011684345081448555,
      "learning_rate": 6.051360000000001e-06,
      "loss": 0.1993,
      "step": 24680
    },
    {
      "epoch": 0.79008,
      "grad_norm": 0.022908270359039307,
      "learning_rate": 6.04976e-06,
      "loss": 0.1996,
      "step": 24690
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.02642076089978218,
      "learning_rate": 6.0481600000000006e-06,
      "loss": 0.1993,
      "step": 24700
    },
    {
      "epoch": 0.79072,
      "grad_norm": 0.009145922027528286,
      "learning_rate": 6.0465600000000004e-06,
      "loss": 0.2103,
      "step": 24710
    },
    {
      "epoch": 0.79104,
      "grad_norm": 0.04315659776329994,
      "learning_rate": 6.044960000000001e-06,
      "loss": 0.2014,
      "step": 24720
    },
    {
      "epoch": 0.79136,
      "grad_norm": 0.026266518980264664,
      "learning_rate": 6.04336e-06,
      "loss": 0.2009,
      "step": 24730
    },
    {
      "epoch": 0.79168,
      "grad_norm": 0.029156461358070374,
      "learning_rate": 6.041760000000001e-06,
      "loss": 0.208,
      "step": 24740
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.022424619644880295,
      "learning_rate": 6.040160000000001e-06,
      "loss": 0.1997,
      "step": 24750
    },
    {
      "epoch": 0.79232,
      "grad_norm": 0.676101803779602,
      "learning_rate": 6.03856e-06,
      "loss": 0.2001,
      "step": 24760
    },
    {
      "epoch": 0.79264,
      "grad_norm": 0.017768023535609245,
      "learning_rate": 6.0369600000000006e-06,
      "loss": 0.1992,
      "step": 24770
    },
    {
      "epoch": 0.79296,
      "grad_norm": 0.015168284066021442,
      "learning_rate": 6.0353600000000004e-06,
      "loss": 0.2001,
      "step": 24780
    },
    {
      "epoch": 0.79328,
      "grad_norm": 0.016996121034026146,
      "learning_rate": 6.03376e-06,
      "loss": 0.1991,
      "step": 24790
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.02730531431734562,
      "learning_rate": 6.03216e-06,
      "loss": 0.2097,
      "step": 24800
    },
    {
      "epoch": 0.79392,
      "grad_norm": 0.04779240861535072,
      "learning_rate": 6.030560000000001e-06,
      "loss": 0.2151,
      "step": 24810
    },
    {
      "epoch": 0.79424,
      "grad_norm": 0.02493458241224289,
      "learning_rate": 6.02896e-06,
      "loss": 0.1995,
      "step": 24820
    },
    {
      "epoch": 0.79456,
      "grad_norm": 0.028307002037763596,
      "learning_rate": 6.027360000000001e-06,
      "loss": 0.1996,
      "step": 24830
    },
    {
      "epoch": 0.79488,
      "grad_norm": 0.049580588936805725,
      "learning_rate": 6.0257600000000006e-06,
      "loss": 0.1993,
      "step": 24840
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.05200260505080223,
      "learning_rate": 6.02416e-06,
      "loss": 0.1992,
      "step": 24850
    },
    {
      "epoch": 0.79552,
      "grad_norm": 0.6388766765594482,
      "learning_rate": 6.02256e-06,
      "loss": 0.2178,
      "step": 24860
    },
    {
      "epoch": 0.79584,
      "grad_norm": 0.020188426598906517,
      "learning_rate": 6.02096e-06,
      "loss": 0.1994,
      "step": 24870
    },
    {
      "epoch": 0.79616,
      "grad_norm": 0.012055633589625359,
      "learning_rate": 6.019360000000001e-06,
      "loss": 0.1991,
      "step": 24880
    },
    {
      "epoch": 0.79648,
      "grad_norm": 0.4141101539134979,
      "learning_rate": 6.01776e-06,
      "loss": 0.2131,
      "step": 24890
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.5598257780075073,
      "learning_rate": 6.016160000000001e-06,
      "loss": 0.2135,
      "step": 24900
    },
    {
      "epoch": 0.79712,
      "grad_norm": 0.06273484230041504,
      "learning_rate": 6.0145600000000006e-06,
      "loss": 0.1995,
      "step": 24910
    },
    {
      "epoch": 0.79744,
      "grad_norm": 0.02335931546986103,
      "learning_rate": 6.012960000000001e-06,
      "loss": 0.217,
      "step": 24920
    },
    {
      "epoch": 0.79776,
      "grad_norm": 0.036469217389822006,
      "learning_rate": 6.01136e-06,
      "loss": 0.1993,
      "step": 24930
    },
    {
      "epoch": 0.79808,
      "grad_norm": 0.01912255585193634,
      "learning_rate": 6.00976e-06,
      "loss": 0.1995,
      "step": 24940
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.06457646936178207,
      "learning_rate": 6.008160000000001e-06,
      "loss": 0.2316,
      "step": 24950
    },
    {
      "epoch": 0.79872,
      "grad_norm": 0.0402502678334713,
      "learning_rate": 6.00656e-06,
      "loss": 0.2132,
      "step": 24960
    },
    {
      "epoch": 0.79904,
      "grad_norm": 0.08380398899316788,
      "learning_rate": 6.004960000000001e-06,
      "loss": 0.1997,
      "step": 24970
    },
    {
      "epoch": 0.79936,
      "grad_norm": 0.030614452436566353,
      "learning_rate": 6.0033600000000006e-06,
      "loss": 0.1992,
      "step": 24980
    },
    {
      "epoch": 0.79968,
      "grad_norm": 0.011324000544846058,
      "learning_rate": 6.001760000000001e-06,
      "loss": 0.2126,
      "step": 24990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.07105682045221329,
      "learning_rate": 6.00016e-06,
      "loss": 0.2167,
      "step": 25000
    },
    {
      "epoch": 0.8,
      "eval_runtime": 49.9117,
      "eval_samples_per_second": 200.354,
      "eval_steps_per_second": 12.522,
      "step": 25000
    },
    {
      "epoch": 0.80032,
      "grad_norm": 0.026073437184095383,
      "learning_rate": 5.99856e-06,
      "loss": 0.1993,
      "step": 25010
    },
    {
      "epoch": 0.80064,
      "grad_norm": 0.04111013188958168,
      "learning_rate": 5.996960000000001e-06,
      "loss": 0.1995,
      "step": 25020
    },
    {
      "epoch": 0.80096,
      "grad_norm": 0.0479830801486969,
      "learning_rate": 5.99536e-06,
      "loss": 0.1996,
      "step": 25030
    },
    {
      "epoch": 0.80128,
      "grad_norm": 0.47200140357017517,
      "learning_rate": 5.993760000000001e-06,
      "loss": 0.2013,
      "step": 25040
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.013921719044446945,
      "learning_rate": 5.992160000000001e-06,
      "loss": 0.1996,
      "step": 25050
    },
    {
      "epoch": 0.80192,
      "grad_norm": 0.014024686068296432,
      "learning_rate": 5.9905600000000005e-06,
      "loss": 0.1993,
      "step": 25060
    },
    {
      "epoch": 0.80224,
      "grad_norm": 0.01586836576461792,
      "learning_rate": 5.98896e-06,
      "loss": 0.2264,
      "step": 25070
    },
    {
      "epoch": 0.80256,
      "grad_norm": 0.051872074604034424,
      "learning_rate": 5.987360000000001e-06,
      "loss": 0.1994,
      "step": 25080
    },
    {
      "epoch": 0.80288,
      "grad_norm": 0.01860019937157631,
      "learning_rate": 5.98576e-06,
      "loss": 0.2,
      "step": 25090
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.017777221277356148,
      "learning_rate": 5.98416e-06,
      "loss": 0.1994,
      "step": 25100
    },
    {
      "epoch": 0.80352,
      "grad_norm": 0.04373824596405029,
      "learning_rate": 5.982560000000001e-06,
      "loss": 0.1993,
      "step": 25110
    },
    {
      "epoch": 0.80384,
      "grad_norm": 0.05073849484324455,
      "learning_rate": 5.98096e-06,
      "loss": 0.1995,
      "step": 25120
    },
    {
      "epoch": 0.80416,
      "grad_norm": 0.04054688662290573,
      "learning_rate": 5.9793600000000005e-06,
      "loss": 0.1995,
      "step": 25130
    },
    {
      "epoch": 0.80448,
      "grad_norm": 0.017254654318094254,
      "learning_rate": 5.97776e-06,
      "loss": 0.2,
      "step": 25140
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.03322811797261238,
      "learning_rate": 5.976160000000001e-06,
      "loss": 0.1995,
      "step": 25150
    },
    {
      "epoch": 0.80512,
      "grad_norm": 0.013109706342220306,
      "learning_rate": 5.97456e-06,
      "loss": 0.1994,
      "step": 25160
    },
    {
      "epoch": 0.80544,
      "grad_norm": 0.009142285212874413,
      "learning_rate": 5.972960000000001e-06,
      "loss": 0.1992,
      "step": 25170
    },
    {
      "epoch": 0.80576,
      "grad_norm": 0.048064518719911575,
      "learning_rate": 5.971360000000001e-06,
      "loss": 0.2001,
      "step": 25180
    },
    {
      "epoch": 0.80608,
      "grad_norm": 0.024855991825461388,
      "learning_rate": 5.96976e-06,
      "loss": 0.1996,
      "step": 25190
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.05991898104548454,
      "learning_rate": 5.9681600000000005e-06,
      "loss": 0.1992,
      "step": 25200
    },
    {
      "epoch": 0.80672,
      "grad_norm": 0.024948732927441597,
      "learning_rate": 5.96656e-06,
      "loss": 0.1993,
      "step": 25210
    },
    {
      "epoch": 0.80704,
      "grad_norm": 0.012737497687339783,
      "learning_rate": 5.964960000000001e-06,
      "loss": 0.2042,
      "step": 25220
    },
    {
      "epoch": 0.80736,
      "grad_norm": 0.0470997616648674,
      "learning_rate": 5.96336e-06,
      "loss": 0.1992,
      "step": 25230
    },
    {
      "epoch": 0.80768,
      "grad_norm": 0.016051681712269783,
      "learning_rate": 5.961760000000001e-06,
      "loss": 0.2102,
      "step": 25240
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.027037980034947395,
      "learning_rate": 5.960160000000001e-06,
      "loss": 0.2157,
      "step": 25250
    },
    {
      "epoch": 0.80832,
      "grad_norm": 0.01709335669875145,
      "learning_rate": 5.958560000000001e-06,
      "loss": 0.1994,
      "step": 25260
    },
    {
      "epoch": 0.80864,
      "grad_norm": 0.05504288524389267,
      "learning_rate": 5.9569600000000005e-06,
      "loss": 0.1993,
      "step": 25270
    },
    {
      "epoch": 0.80896,
      "grad_norm": 0.02446759305894375,
      "learning_rate": 5.95536e-06,
      "loss": 0.2035,
      "step": 25280
    },
    {
      "epoch": 0.80928,
      "grad_norm": 0.04519908130168915,
      "learning_rate": 5.95376e-06,
      "loss": 0.2152,
      "step": 25290
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.031753286719322205,
      "learning_rate": 5.95216e-06,
      "loss": 0.2065,
      "step": 25300
    },
    {
      "epoch": 0.80992,
      "grad_norm": 0.023303277790546417,
      "learning_rate": 5.950560000000001e-06,
      "loss": 0.2003,
      "step": 25310
    },
    {
      "epoch": 0.81024,
      "grad_norm": 0.03157145157456398,
      "learning_rate": 5.94896e-06,
      "loss": 0.1999,
      "step": 25320
    },
    {
      "epoch": 0.81056,
      "grad_norm": 0.018379345536231995,
      "learning_rate": 5.947360000000001e-06,
      "loss": 0.2008,
      "step": 25330
    },
    {
      "epoch": 0.81088,
      "grad_norm": 0.04053889587521553,
      "learning_rate": 5.9457600000000005e-06,
      "loss": 0.2007,
      "step": 25340
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.01589849404990673,
      "learning_rate": 5.944160000000001e-06,
      "loss": 0.199,
      "step": 25350
    },
    {
      "epoch": 0.81152,
      "grad_norm": 0.03540083393454552,
      "learning_rate": 5.94256e-06,
      "loss": 0.1991,
      "step": 25360
    },
    {
      "epoch": 0.81184,
      "grad_norm": 0.012850591912865639,
      "learning_rate": 5.94096e-06,
      "loss": 0.1993,
      "step": 25370
    },
    {
      "epoch": 0.81216,
      "grad_norm": 0.016489138826727867,
      "learning_rate": 5.939360000000001e-06,
      "loss": 0.1998,
      "step": 25380
    },
    {
      "epoch": 0.81248,
      "grad_norm": 0.02553754858672619,
      "learning_rate": 5.93776e-06,
      "loss": 0.1992,
      "step": 25390
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.029237626120448112,
      "learning_rate": 5.936160000000001e-06,
      "loss": 0.1993,
      "step": 25400
    },
    {
      "epoch": 0.81312,
      "grad_norm": 0.03841683268547058,
      "learning_rate": 5.9345600000000005e-06,
      "loss": 0.1994,
      "step": 25410
    },
    {
      "epoch": 0.81344,
      "grad_norm": 0.022870877757668495,
      "learning_rate": 5.932960000000001e-06,
      "loss": 0.1992,
      "step": 25420
    },
    {
      "epoch": 0.81376,
      "grad_norm": 0.021367577835917473,
      "learning_rate": 5.93136e-06,
      "loss": 0.1997,
      "step": 25430
    },
    {
      "epoch": 0.81408,
      "grad_norm": 0.06431378424167633,
      "learning_rate": 5.92976e-06,
      "loss": 0.2223,
      "step": 25440
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.014698433689773083,
      "learning_rate": 5.928160000000001e-06,
      "loss": 0.217,
      "step": 25450
    },
    {
      "epoch": 0.81472,
      "grad_norm": 0.03634694963693619,
      "learning_rate": 5.92656e-06,
      "loss": 0.201,
      "step": 25460
    },
    {
      "epoch": 0.81504,
      "grad_norm": 0.17897450923919678,
      "learning_rate": 5.924960000000001e-06,
      "loss": 0.1994,
      "step": 25470
    },
    {
      "epoch": 0.81536,
      "grad_norm": 0.0321994312107563,
      "learning_rate": 5.9233600000000005e-06,
      "loss": 0.2137,
      "step": 25480
    },
    {
      "epoch": 0.81568,
      "grad_norm": 0.023099837824702263,
      "learning_rate": 5.921760000000001e-06,
      "loss": 0.2018,
      "step": 25490
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.023721756413578987,
      "learning_rate": 5.92016e-06,
      "loss": 0.1992,
      "step": 25500
    },
    {
      "epoch": 0.81632,
      "grad_norm": 0.02215057797729969,
      "learning_rate": 5.918560000000001e-06,
      "loss": 0.1992,
      "step": 25510
    },
    {
      "epoch": 0.81664,
      "grad_norm": 0.02919132634997368,
      "learning_rate": 5.916960000000001e-06,
      "loss": 0.1998,
      "step": 25520
    },
    {
      "epoch": 0.81696,
      "grad_norm": 0.020589396357536316,
      "learning_rate": 5.91536e-06,
      "loss": 0.2028,
      "step": 25530
    },
    {
      "epoch": 0.81728,
      "grad_norm": 0.04991597682237625,
      "learning_rate": 5.913760000000001e-06,
      "loss": 0.2072,
      "step": 25540
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.023534294217824936,
      "learning_rate": 5.9121600000000005e-06,
      "loss": 0.1992,
      "step": 25550
    },
    {
      "epoch": 0.81792,
      "grad_norm": 0.1568569391965866,
      "learning_rate": 5.91056e-06,
      "loss": 0.1998,
      "step": 25560
    },
    {
      "epoch": 0.81824,
      "grad_norm": 0.011504839174449444,
      "learning_rate": 5.90896e-06,
      "loss": 0.1996,
      "step": 25570
    },
    {
      "epoch": 0.81856,
      "grad_norm": 0.026221750304102898,
      "learning_rate": 5.907360000000001e-06,
      "loss": 0.2134,
      "step": 25580
    },
    {
      "epoch": 0.81888,
      "grad_norm": 0.727674663066864,
      "learning_rate": 5.90576e-06,
      "loss": 0.1999,
      "step": 25590
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.030964460223913193,
      "learning_rate": 5.904160000000001e-06,
      "loss": 0.2148,
      "step": 25600
    },
    {
      "epoch": 0.81952,
      "grad_norm": 0.02106528729200363,
      "learning_rate": 5.902560000000001e-06,
      "loss": 0.1999,
      "step": 25610
    },
    {
      "epoch": 0.81984,
      "grad_norm": 0.018244879320263863,
      "learning_rate": 5.90096e-06,
      "loss": 0.2112,
      "step": 25620
    },
    {
      "epoch": 0.82016,
      "grad_norm": 0.13000766932964325,
      "learning_rate": 5.89936e-06,
      "loss": 0.2025,
      "step": 25630
    },
    {
      "epoch": 0.82048,
      "grad_norm": 0.0429348386824131,
      "learning_rate": 5.89776e-06,
      "loss": 0.223,
      "step": 25640
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.029738571494817734,
      "learning_rate": 5.896160000000001e-06,
      "loss": 0.2066,
      "step": 25650
    },
    {
      "epoch": 0.82112,
      "grad_norm": 0.3736254870891571,
      "learning_rate": 5.89456e-06,
      "loss": 0.1995,
      "step": 25660
    },
    {
      "epoch": 0.82144,
      "grad_norm": 0.031198961660265923,
      "learning_rate": 5.892960000000001e-06,
      "loss": 0.2003,
      "step": 25670
    },
    {
      "epoch": 0.82176,
      "grad_norm": 0.01925627514719963,
      "learning_rate": 5.891360000000001e-06,
      "loss": 0.1993,
      "step": 25680
    },
    {
      "epoch": 0.82208,
      "grad_norm": 0.027212131768465042,
      "learning_rate": 5.889760000000001e-06,
      "loss": 0.1996,
      "step": 25690
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.029354263097047806,
      "learning_rate": 5.88816e-06,
      "loss": 0.1993,
      "step": 25700
    },
    {
      "epoch": 0.82272,
      "grad_norm": 0.01957608014345169,
      "learning_rate": 5.88656e-06,
      "loss": 0.1997,
      "step": 25710
    },
    {
      "epoch": 0.82304,
      "grad_norm": 0.0187966488301754,
      "learning_rate": 5.884960000000001e-06,
      "loss": 0.1995,
      "step": 25720
    },
    {
      "epoch": 0.82336,
      "grad_norm": 0.01335268933326006,
      "learning_rate": 5.88336e-06,
      "loss": 0.1991,
      "step": 25730
    },
    {
      "epoch": 0.82368,
      "grad_norm": 0.06143292039632797,
      "learning_rate": 5.881760000000001e-06,
      "loss": 0.2006,
      "step": 25740
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.026277264580130577,
      "learning_rate": 5.880160000000001e-06,
      "loss": 0.1995,
      "step": 25750
    },
    {
      "epoch": 0.82432,
      "grad_norm": 0.02303987555205822,
      "learning_rate": 5.8785600000000005e-06,
      "loss": 0.2171,
      "step": 25760
    },
    {
      "epoch": 0.82464,
      "grad_norm": 0.019734298810362816,
      "learning_rate": 5.87696e-06,
      "loss": 0.1989,
      "step": 25770
    },
    {
      "epoch": 0.82496,
      "grad_norm": 0.026358872652053833,
      "learning_rate": 5.87536e-06,
      "loss": 0.2017,
      "step": 25780
    },
    {
      "epoch": 0.82528,
      "grad_norm": 0.022117624059319496,
      "learning_rate": 5.87376e-06,
      "loss": 0.2155,
      "step": 25790
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.025480138137936592,
      "learning_rate": 5.87216e-06,
      "loss": 0.1992,
      "step": 25800
    },
    {
      "epoch": 0.82592,
      "grad_norm": 0.013181757181882858,
      "learning_rate": 5.870560000000001e-06,
      "loss": 0.1993,
      "step": 25810
    },
    {
      "epoch": 0.82624,
      "grad_norm": 0.024766575545072556,
      "learning_rate": 5.86896e-06,
      "loss": 0.1992,
      "step": 25820
    },
    {
      "epoch": 0.82656,
      "grad_norm": 0.02859571948647499,
      "learning_rate": 5.8673600000000005e-06,
      "loss": 0.1996,
      "step": 25830
    },
    {
      "epoch": 0.82688,
      "grad_norm": 0.024525262415409088,
      "learning_rate": 5.86576e-06,
      "loss": 0.2136,
      "step": 25840
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.039096180349588394,
      "learning_rate": 5.864160000000001e-06,
      "loss": 0.1993,
      "step": 25850
    },
    {
      "epoch": 0.82752,
      "grad_norm": 0.038408659398555756,
      "learning_rate": 5.86256e-06,
      "loss": 0.2162,
      "step": 25860
    },
    {
      "epoch": 0.82784,
      "grad_norm": 0.021674126386642456,
      "learning_rate": 5.86096e-06,
      "loss": 0.1998,
      "step": 25870
    },
    {
      "epoch": 0.82816,
      "grad_norm": 0.028105009347200394,
      "learning_rate": 5.859360000000001e-06,
      "loss": 0.2002,
      "step": 25880
    },
    {
      "epoch": 0.82848,
      "grad_norm": 0.025379270315170288,
      "learning_rate": 5.85776e-06,
      "loss": 0.1994,
      "step": 25890
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.024355748668313026,
      "learning_rate": 5.8561600000000005e-06,
      "loss": 0.1993,
      "step": 25900
    },
    {
      "epoch": 0.82912,
      "grad_norm": 0.026689965277910233,
      "learning_rate": 5.85456e-06,
      "loss": 0.1993,
      "step": 25910
    },
    {
      "epoch": 0.82944,
      "grad_norm": 0.054430294781923294,
      "learning_rate": 5.852960000000001e-06,
      "loss": 0.1996,
      "step": 25920
    },
    {
      "epoch": 0.82976,
      "grad_norm": 0.01595698855817318,
      "learning_rate": 5.85136e-06,
      "loss": 0.2124,
      "step": 25930
    },
    {
      "epoch": 0.83008,
      "grad_norm": 0.023491928353905678,
      "learning_rate": 5.849760000000001e-06,
      "loss": 0.1994,
      "step": 25940
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.03244435414671898,
      "learning_rate": 5.848160000000001e-06,
      "loss": 0.2118,
      "step": 25950
    },
    {
      "epoch": 0.83072,
      "grad_norm": 0.025100015103816986,
      "learning_rate": 5.84656e-06,
      "loss": 0.1995,
      "step": 25960
    },
    {
      "epoch": 0.83104,
      "grad_norm": 0.03942715376615524,
      "learning_rate": 5.8449600000000005e-06,
      "loss": 0.1994,
      "step": 25970
    },
    {
      "epoch": 0.83136,
      "grad_norm": 0.05134452506899834,
      "learning_rate": 5.84336e-06,
      "loss": 0.2277,
      "step": 25980
    },
    {
      "epoch": 0.83168,
      "grad_norm": 0.07188861072063446,
      "learning_rate": 5.841760000000001e-06,
      "loss": 0.2161,
      "step": 25990
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.017835725098848343,
      "learning_rate": 5.84016e-06,
      "loss": 0.1993,
      "step": 26000
    },
    {
      "epoch": 0.832,
      "eval_runtime": 50.5209,
      "eval_samples_per_second": 197.938,
      "eval_steps_per_second": 12.371,
      "step": 26000
    },
    {
      "epoch": 0.83232,
      "grad_norm": 0.21457774937152863,
      "learning_rate": 5.838560000000001e-06,
      "loss": 0.1996,
      "step": 26010
    },
    {
      "epoch": 0.83264,
      "grad_norm": 0.022179681807756424,
      "learning_rate": 5.836960000000001e-06,
      "loss": 0.1992,
      "step": 26020
    },
    {
      "epoch": 0.83296,
      "grad_norm": 0.029666846618056297,
      "learning_rate": 5.835360000000001e-06,
      "loss": 0.1993,
      "step": 26030
    },
    {
      "epoch": 0.83328,
      "grad_norm": 0.055144499987363815,
      "learning_rate": 5.8337600000000005e-06,
      "loss": 0.1994,
      "step": 26040
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.015741653740406036,
      "learning_rate": 5.83216e-06,
      "loss": 0.2106,
      "step": 26050
    },
    {
      "epoch": 0.83392,
      "grad_norm": 0.026363348588347435,
      "learning_rate": 5.83056e-06,
      "loss": 0.1992,
      "step": 26060
    },
    {
      "epoch": 0.83424,
      "grad_norm": 0.02525666542351246,
      "learning_rate": 5.82896e-06,
      "loss": 0.2162,
      "step": 26070
    },
    {
      "epoch": 0.83456,
      "grad_norm": 0.01384937483817339,
      "learning_rate": 5.827360000000001e-06,
      "loss": 0.2148,
      "step": 26080
    },
    {
      "epoch": 0.83488,
      "grad_norm": 0.021164584904909134,
      "learning_rate": 5.82576e-06,
      "loss": 0.1993,
      "step": 26090
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.035076804459095,
      "learning_rate": 5.824160000000001e-06,
      "loss": 0.1995,
      "step": 26100
    },
    {
      "epoch": 0.83552,
      "grad_norm": 0.026433400809764862,
      "learning_rate": 5.8225600000000005e-06,
      "loss": 0.1992,
      "step": 26110
    },
    {
      "epoch": 0.83584,
      "grad_norm": 0.040290843695402145,
      "learning_rate": 5.8209599999999996e-06,
      "loss": 0.1993,
      "step": 26120
    },
    {
      "epoch": 0.83616,
      "grad_norm": 0.02184790000319481,
      "learning_rate": 5.81936e-06,
      "loss": 0.1991,
      "step": 26130
    },
    {
      "epoch": 0.83648,
      "grad_norm": 0.01978464610874653,
      "learning_rate": 5.81776e-06,
      "loss": 0.1994,
      "step": 26140
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.016615331172943115,
      "learning_rate": 5.816160000000001e-06,
      "loss": 0.1997,
      "step": 26150
    },
    {
      "epoch": 0.83712,
      "grad_norm": 0.013315306976437569,
      "learning_rate": 5.81456e-06,
      "loss": 0.2103,
      "step": 26160
    },
    {
      "epoch": 0.83744,
      "grad_norm": 0.012541679665446281,
      "learning_rate": 5.812960000000001e-06,
      "loss": 0.1998,
      "step": 26170
    },
    {
      "epoch": 0.83776,
      "grad_norm": 0.014541255310177803,
      "learning_rate": 5.8113600000000005e-06,
      "loss": 0.1996,
      "step": 26180
    },
    {
      "epoch": 0.83808,
      "grad_norm": 0.040384694933891296,
      "learning_rate": 5.809760000000001e-06,
      "loss": 0.1995,
      "step": 26190
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.021293293684720993,
      "learning_rate": 5.80816e-06,
      "loss": 0.2048,
      "step": 26200
    },
    {
      "epoch": 0.83872,
      "grad_norm": 0.024497542530298233,
      "learning_rate": 5.80656e-06,
      "loss": 0.2096,
      "step": 26210
    },
    {
      "epoch": 0.83904,
      "grad_norm": 0.044727716594934464,
      "learning_rate": 5.804960000000001e-06,
      "loss": 0.1993,
      "step": 26220
    },
    {
      "epoch": 0.83936,
      "grad_norm": 0.03479764983057976,
      "learning_rate": 5.80336e-06,
      "loss": 0.1991,
      "step": 26230
    },
    {
      "epoch": 0.83968,
      "grad_norm": 0.023461060598492622,
      "learning_rate": 5.801760000000001e-06,
      "loss": 0.1993,
      "step": 26240
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.047164615243673325,
      "learning_rate": 5.8001600000000006e-06,
      "loss": 0.232,
      "step": 26250
    },
    {
      "epoch": 0.84032,
      "grad_norm": 0.05772070214152336,
      "learning_rate": 5.7985600000000004e-06,
      "loss": 0.1994,
      "step": 26260
    },
    {
      "epoch": 0.84064,
      "grad_norm": 0.031339455395936966,
      "learning_rate": 5.79696e-06,
      "loss": 0.2001,
      "step": 26270
    },
    {
      "epoch": 0.84096,
      "grad_norm": 0.8559486865997314,
      "learning_rate": 5.795360000000001e-06,
      "loss": 0.2298,
      "step": 26280
    },
    {
      "epoch": 0.84128,
      "grad_norm": 0.014763694256544113,
      "learning_rate": 5.79376e-06,
      "loss": 0.1995,
      "step": 26290
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.012213614769279957,
      "learning_rate": 5.79216e-06,
      "loss": 0.2,
      "step": 26300
    },
    {
      "epoch": 0.84192,
      "grad_norm": 0.027294965460896492,
      "learning_rate": 5.790560000000001e-06,
      "loss": 0.2081,
      "step": 26310
    },
    {
      "epoch": 0.84224,
      "grad_norm": 0.03341715782880783,
      "learning_rate": 5.78896e-06,
      "loss": 0.1994,
      "step": 26320
    },
    {
      "epoch": 0.84256,
      "grad_norm": 0.030127745121717453,
      "learning_rate": 5.7873600000000004e-06,
      "loss": 0.2143,
      "step": 26330
    },
    {
      "epoch": 0.84288,
      "grad_norm": 0.030356474220752716,
      "learning_rate": 5.78576e-06,
      "loss": 0.2156,
      "step": 26340
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.06777174025774002,
      "learning_rate": 5.784160000000001e-06,
      "loss": 0.2119,
      "step": 26350
    },
    {
      "epoch": 0.84352,
      "grad_norm": 0.03924058750271797,
      "learning_rate": 5.78256e-06,
      "loss": 0.1999,
      "step": 26360
    },
    {
      "epoch": 0.84384,
      "grad_norm": 0.014640326611697674,
      "learning_rate": 5.780960000000001e-06,
      "loss": 0.2024,
      "step": 26370
    },
    {
      "epoch": 0.84416,
      "grad_norm": 0.016054928302764893,
      "learning_rate": 5.779360000000001e-06,
      "loss": 0.1992,
      "step": 26380
    },
    {
      "epoch": 0.84448,
      "grad_norm": 0.022703075781464577,
      "learning_rate": 5.77776e-06,
      "loss": 0.2138,
      "step": 26390
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.07641617953777313,
      "learning_rate": 5.7761600000000004e-06,
      "loss": 0.2236,
      "step": 26400
    },
    {
      "epoch": 0.84512,
      "grad_norm": 0.057184718549251556,
      "learning_rate": 5.77456e-06,
      "loss": 0.1997,
      "step": 26410
    },
    {
      "epoch": 0.84544,
      "grad_norm": 0.04509805142879486,
      "learning_rate": 5.772960000000001e-06,
      "loss": 0.1992,
      "step": 26420
    },
    {
      "epoch": 0.84576,
      "grad_norm": 0.05020484700798988,
      "learning_rate": 5.77136e-06,
      "loss": 0.2155,
      "step": 26430
    },
    {
      "epoch": 0.84608,
      "grad_norm": 0.03225897252559662,
      "learning_rate": 5.769760000000001e-06,
      "loss": 0.2044,
      "step": 26440
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.036778420209884644,
      "learning_rate": 5.768160000000001e-06,
      "loss": 0.2063,
      "step": 26450
    },
    {
      "epoch": 0.84672,
      "grad_norm": 0.03261736407876015,
      "learning_rate": 5.76656e-06,
      "loss": 0.1994,
      "step": 26460
    },
    {
      "epoch": 0.84704,
      "grad_norm": 0.01655031181871891,
      "learning_rate": 5.7649600000000005e-06,
      "loss": 0.1993,
      "step": 26470
    },
    {
      "epoch": 0.84736,
      "grad_norm": 0.05397549644112587,
      "learning_rate": 5.76336e-06,
      "loss": 0.219,
      "step": 26480
    },
    {
      "epoch": 0.84768,
      "grad_norm": 0.042830873280763626,
      "learning_rate": 5.761760000000001e-06,
      "loss": 0.2003,
      "step": 26490
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.06948991119861603,
      "learning_rate": 5.76016e-06,
      "loss": 0.2162,
      "step": 26500
    },
    {
      "epoch": 0.84832,
      "grad_norm": 0.022823045030236244,
      "learning_rate": 5.758560000000001e-06,
      "loss": 0.1997,
      "step": 26510
    },
    {
      "epoch": 0.84864,
      "grad_norm": 0.027056245133280754,
      "learning_rate": 5.756960000000001e-06,
      "loss": 0.1993,
      "step": 26520
    },
    {
      "epoch": 0.84896,
      "grad_norm": 0.03616419807076454,
      "learning_rate": 5.755360000000001e-06,
      "loss": 0.1993,
      "step": 26530
    },
    {
      "epoch": 0.84928,
      "grad_norm": 0.07125511765480042,
      "learning_rate": 5.7537600000000005e-06,
      "loss": 0.2019,
      "step": 26540
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.0364668071269989,
      "learning_rate": 5.75216e-06,
      "loss": 0.1993,
      "step": 26550
    },
    {
      "epoch": 0.84992,
      "grad_norm": 0.00936902780085802,
      "learning_rate": 5.75056e-06,
      "loss": 0.1996,
      "step": 26560
    },
    {
      "epoch": 0.85024,
      "grad_norm": 0.036685701459646225,
      "learning_rate": 5.74896e-06,
      "loss": 0.1993,
      "step": 26570
    },
    {
      "epoch": 0.85056,
      "grad_norm": 0.018668755888938904,
      "learning_rate": 5.747360000000001e-06,
      "loss": 0.1993,
      "step": 26580
    },
    {
      "epoch": 0.85088,
      "grad_norm": 0.058520395308732986,
      "learning_rate": 5.74576e-06,
      "loss": 0.1995,
      "step": 26590
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.19283446669578552,
      "learning_rate": 5.744160000000001e-06,
      "loss": 0.1995,
      "step": 26600
    },
    {
      "epoch": 0.85152,
      "grad_norm": 0.012047426775097847,
      "learning_rate": 5.7425600000000005e-06,
      "loss": 0.1992,
      "step": 26610
    },
    {
      "epoch": 0.85184,
      "grad_norm": 0.049285467714071274,
      "learning_rate": 5.740960000000001e-06,
      "loss": 0.2021,
      "step": 26620
    },
    {
      "epoch": 0.85216,
      "grad_norm": 0.08839858323335648,
      "learning_rate": 5.73936e-06,
      "loss": 0.1995,
      "step": 26630
    },
    {
      "epoch": 0.85248,
      "grad_norm": 0.03082031011581421,
      "learning_rate": 5.73776e-06,
      "loss": 0.2112,
      "step": 26640
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.033256951719522476,
      "learning_rate": 5.736160000000001e-06,
      "loss": 0.1997,
      "step": 26650
    },
    {
      "epoch": 0.85312,
      "grad_norm": 0.02436416782438755,
      "learning_rate": 5.73456e-06,
      "loss": 0.1993,
      "step": 26660
    },
    {
      "epoch": 0.85344,
      "grad_norm": 0.02282330021262169,
      "learning_rate": 5.732960000000001e-06,
      "loss": 0.2155,
      "step": 26670
    },
    {
      "epoch": 0.85376,
      "grad_norm": 0.023068848997354507,
      "learning_rate": 5.7313600000000005e-06,
      "loss": 0.1993,
      "step": 26680
    },
    {
      "epoch": 0.85408,
      "grad_norm": 0.03801153600215912,
      "learning_rate": 5.729760000000001e-06,
      "loss": 0.1994,
      "step": 26690
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.01704452559351921,
      "learning_rate": 5.72816e-06,
      "loss": 0.1995,
      "step": 26700
    },
    {
      "epoch": 0.85472,
      "grad_norm": 0.01835356093943119,
      "learning_rate": 5.726560000000001e-06,
      "loss": 0.1993,
      "step": 26710
    },
    {
      "epoch": 0.85504,
      "grad_norm": 0.0156375952064991,
      "learning_rate": 5.724960000000001e-06,
      "loss": 0.1992,
      "step": 26720
    },
    {
      "epoch": 0.85536,
      "grad_norm": 0.01688414439558983,
      "learning_rate": 5.72336e-06,
      "loss": 0.1993,
      "step": 26730
    },
    {
      "epoch": 0.85568,
      "grad_norm": 0.05205221474170685,
      "learning_rate": 5.721760000000001e-06,
      "loss": 0.1992,
      "step": 26740
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.040928613394498825,
      "learning_rate": 5.7201600000000005e-06,
      "loss": 0.1994,
      "step": 26750
    },
    {
      "epoch": 0.85632,
      "grad_norm": 0.020744703710079193,
      "learning_rate": 5.718560000000001e-06,
      "loss": 0.2044,
      "step": 26760
    },
    {
      "epoch": 0.85664,
      "grad_norm": 0.017637688666582108,
      "learning_rate": 5.71696e-06,
      "loss": 0.1994,
      "step": 26770
    },
    {
      "epoch": 0.85696,
      "grad_norm": 0.35710442066192627,
      "learning_rate": 5.715360000000001e-06,
      "loss": 0.1999,
      "step": 26780
    },
    {
      "epoch": 0.85728,
      "grad_norm": 0.02095979079604149,
      "learning_rate": 5.713760000000001e-06,
      "loss": 0.1992,
      "step": 26790
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.025897888466715813,
      "learning_rate": 5.71216e-06,
      "loss": 0.2079,
      "step": 26800
    },
    {
      "epoch": 0.85792,
      "grad_norm": 0.01747862808406353,
      "learning_rate": 5.710560000000001e-06,
      "loss": 0.212,
      "step": 26810
    },
    {
      "epoch": 0.85824,
      "grad_norm": 0.029114164412021637,
      "learning_rate": 5.7089600000000005e-06,
      "loss": 0.1992,
      "step": 26820
    },
    {
      "epoch": 0.85856,
      "grad_norm": 0.07687358558177948,
      "learning_rate": 5.70736e-06,
      "loss": 0.2,
      "step": 26830
    },
    {
      "epoch": 0.85888,
      "grad_norm": 0.04368952289223671,
      "learning_rate": 5.70576e-06,
      "loss": 0.1993,
      "step": 26840
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.29423782229423523,
      "learning_rate": 5.704160000000001e-06,
      "loss": 0.2125,
      "step": 26850
    },
    {
      "epoch": 0.85952,
      "grad_norm": 0.025550976395606995,
      "learning_rate": 5.70256e-06,
      "loss": 0.1992,
      "step": 26860
    },
    {
      "epoch": 0.85984,
      "grad_norm": 0.4349994361400604,
      "learning_rate": 5.700960000000001e-06,
      "loss": 0.1996,
      "step": 26870
    },
    {
      "epoch": 0.86016,
      "grad_norm": 0.03178931400179863,
      "learning_rate": 5.699360000000001e-06,
      "loss": 0.2147,
      "step": 26880
    },
    {
      "epoch": 0.86048,
      "grad_norm": 0.02136233076453209,
      "learning_rate": 5.69776e-06,
      "loss": 0.1992,
      "step": 26890
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.04737793654203415,
      "learning_rate": 5.69616e-06,
      "loss": 0.2164,
      "step": 26900
    },
    {
      "epoch": 0.86112,
      "grad_norm": 0.0248557198792696,
      "learning_rate": 5.69456e-06,
      "loss": 0.1991,
      "step": 26910
    },
    {
      "epoch": 0.86144,
      "grad_norm": 0.0629204586148262,
      "learning_rate": 5.692960000000001e-06,
      "loss": 0.2217,
      "step": 26920
    },
    {
      "epoch": 0.86176,
      "grad_norm": 0.16090475022792816,
      "learning_rate": 5.69136e-06,
      "loss": 0.1996,
      "step": 26930
    },
    {
      "epoch": 0.86208,
      "grad_norm": 0.01744239032268524,
      "learning_rate": 5.689760000000001e-06,
      "loss": 0.1992,
      "step": 26940
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.026159295812249184,
      "learning_rate": 5.688160000000001e-06,
      "loss": 0.2151,
      "step": 26950
    },
    {
      "epoch": 0.86272,
      "grad_norm": 0.014788401313126087,
      "learning_rate": 5.686560000000001e-06,
      "loss": 0.2142,
      "step": 26960
    },
    {
      "epoch": 0.86304,
      "grad_norm": 0.05474532023072243,
      "learning_rate": 5.68496e-06,
      "loss": 0.1996,
      "step": 26970
    },
    {
      "epoch": 0.86336,
      "grad_norm": 0.03317295014858246,
      "learning_rate": 5.68336e-06,
      "loss": 0.2132,
      "step": 26980
    },
    {
      "epoch": 0.86368,
      "grad_norm": 0.047078631818294525,
      "learning_rate": 5.681760000000001e-06,
      "loss": 0.1992,
      "step": 26990
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.023983633145689964,
      "learning_rate": 5.68016e-06,
      "loss": 0.1992,
      "step": 27000
    },
    {
      "epoch": 0.864,
      "eval_runtime": 50.2899,
      "eval_samples_per_second": 198.847,
      "eval_steps_per_second": 12.428,
      "step": 27000
    },
    {
      "epoch": 0.86432,
      "grad_norm": 0.046036068350076675,
      "learning_rate": 5.678560000000001e-06,
      "loss": 0.1994,
      "step": 27010
    },
    {
      "epoch": 0.86464,
      "grad_norm": 0.024352893233299255,
      "learning_rate": 5.676960000000001e-06,
      "loss": 0.2153,
      "step": 27020
    },
    {
      "epoch": 0.86496,
      "grad_norm": 0.02463478595018387,
      "learning_rate": 5.6753600000000005e-06,
      "loss": 0.2157,
      "step": 27030
    },
    {
      "epoch": 0.86528,
      "grad_norm": 0.04728403687477112,
      "learning_rate": 5.67376e-06,
      "loss": 0.1996,
      "step": 27040
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.02500303089618683,
      "learning_rate": 5.672160000000001e-06,
      "loss": 0.2095,
      "step": 27050
    },
    {
      "epoch": 0.86592,
      "grad_norm": 0.04001927375793457,
      "learning_rate": 5.67056e-06,
      "loss": 0.1993,
      "step": 27060
    },
    {
      "epoch": 0.86624,
      "grad_norm": 0.04307589307427406,
      "learning_rate": 5.66896e-06,
      "loss": 0.2151,
      "step": 27070
    },
    {
      "epoch": 0.86656,
      "grad_norm": 0.08562257885932922,
      "learning_rate": 5.667360000000001e-06,
      "loss": 0.1995,
      "step": 27080
    },
    {
      "epoch": 0.86688,
      "grad_norm": 0.03409423679113388,
      "learning_rate": 5.66576e-06,
      "loss": 0.1993,
      "step": 27090
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.025289323180913925,
      "learning_rate": 5.6641600000000005e-06,
      "loss": 0.1992,
      "step": 27100
    },
    {
      "epoch": 0.86752,
      "grad_norm": 0.018062938004732132,
      "learning_rate": 5.66256e-06,
      "loss": 0.2139,
      "step": 27110
    },
    {
      "epoch": 0.86784,
      "grad_norm": 0.04952477663755417,
      "learning_rate": 5.660960000000001e-06,
      "loss": 0.214,
      "step": 27120
    },
    {
      "epoch": 0.86816,
      "grad_norm": 0.031400155276060104,
      "learning_rate": 5.65936e-06,
      "loss": 0.1995,
      "step": 27130
    },
    {
      "epoch": 0.86848,
      "grad_norm": 0.02842467837035656,
      "learning_rate": 5.65776e-06,
      "loss": 0.199,
      "step": 27140
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.11271464079618454,
      "learning_rate": 5.656160000000001e-06,
      "loss": 0.1995,
      "step": 27150
    },
    {
      "epoch": 0.86912,
      "grad_norm": 0.03849015757441521,
      "learning_rate": 5.65456e-06,
      "loss": 0.1995,
      "step": 27160
    },
    {
      "epoch": 0.86944,
      "grad_norm": 0.016840463504195213,
      "learning_rate": 5.6529600000000005e-06,
      "loss": 0.1993,
      "step": 27170
    },
    {
      "epoch": 0.86976,
      "grad_norm": 0.026239853352308273,
      "learning_rate": 5.65136e-06,
      "loss": 0.1992,
      "step": 27180
    },
    {
      "epoch": 0.87008,
      "grad_norm": 0.015171995386481285,
      "learning_rate": 5.649760000000001e-06,
      "loss": 0.2013,
      "step": 27190
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.03630473464727402,
      "learning_rate": 5.64816e-06,
      "loss": 0.2001,
      "step": 27200
    },
    {
      "epoch": 0.87072,
      "grad_norm": 0.022208256646990776,
      "learning_rate": 5.646560000000001e-06,
      "loss": 0.1992,
      "step": 27210
    },
    {
      "epoch": 0.87104,
      "grad_norm": 0.029151098802685738,
      "learning_rate": 5.644960000000001e-06,
      "loss": 0.1995,
      "step": 27220
    },
    {
      "epoch": 0.87136,
      "grad_norm": 0.030190959572792053,
      "learning_rate": 5.64336e-06,
      "loss": 0.1994,
      "step": 27230
    },
    {
      "epoch": 0.87168,
      "grad_norm": 0.28181707859039307,
      "learning_rate": 5.6417600000000005e-06,
      "loss": 0.2019,
      "step": 27240
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.03233418986201286,
      "learning_rate": 5.64016e-06,
      "loss": 0.1991,
      "step": 27250
    },
    {
      "epoch": 0.87232,
      "grad_norm": 0.01796845532953739,
      "learning_rate": 5.638560000000001e-06,
      "loss": 0.2312,
      "step": 27260
    },
    {
      "epoch": 0.87264,
      "grad_norm": 0.018011018633842468,
      "learning_rate": 5.63696e-06,
      "loss": 0.1992,
      "step": 27270
    },
    {
      "epoch": 0.87296,
      "grad_norm": 0.043718189001083374,
      "learning_rate": 5.635360000000001e-06,
      "loss": 0.1994,
      "step": 27280
    },
    {
      "epoch": 0.87328,
      "grad_norm": 0.029360849410295486,
      "learning_rate": 5.633760000000001e-06,
      "loss": 0.199,
      "step": 27290
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.01851668767631054,
      "learning_rate": 5.632160000000001e-06,
      "loss": 0.2031,
      "step": 27300
    },
    {
      "epoch": 0.87392,
      "grad_norm": 0.019225893542170525,
      "learning_rate": 5.6305600000000005e-06,
      "loss": 0.1993,
      "step": 27310
    },
    {
      "epoch": 0.87424,
      "grad_norm": 0.02087082900106907,
      "learning_rate": 5.62896e-06,
      "loss": 0.1992,
      "step": 27320
    },
    {
      "epoch": 0.87456,
      "grad_norm": 0.048335637897253036,
      "learning_rate": 5.62736e-06,
      "loss": 0.2134,
      "step": 27330
    },
    {
      "epoch": 0.87488,
      "grad_norm": 0.04225748032331467,
      "learning_rate": 5.62576e-06,
      "loss": 0.2001,
      "step": 27340
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.024236874654889107,
      "learning_rate": 5.624160000000001e-06,
      "loss": 0.1993,
      "step": 27350
    },
    {
      "epoch": 0.87552,
      "grad_norm": 0.08108403533697128,
      "learning_rate": 5.62256e-06,
      "loss": 0.1995,
      "step": 27360
    },
    {
      "epoch": 0.87584,
      "grad_norm": 0.015743045136332512,
      "learning_rate": 5.620960000000001e-06,
      "loss": 0.2043,
      "step": 27370
    },
    {
      "epoch": 0.87616,
      "grad_norm": 0.029312357306480408,
      "learning_rate": 5.6193600000000005e-06,
      "loss": 0.2147,
      "step": 27380
    },
    {
      "epoch": 0.87648,
      "grad_norm": 0.03342451900243759,
      "learning_rate": 5.617760000000001e-06,
      "loss": 0.1993,
      "step": 27390
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.7758661508560181,
      "learning_rate": 5.61616e-06,
      "loss": 0.2125,
      "step": 27400
    },
    {
      "epoch": 0.87712,
      "grad_norm": 0.5833059549331665,
      "learning_rate": 5.61456e-06,
      "loss": 0.2003,
      "step": 27410
    },
    {
      "epoch": 0.87744,
      "grad_norm": 0.023223459720611572,
      "learning_rate": 5.612960000000001e-06,
      "loss": 0.1999,
      "step": 27420
    },
    {
      "epoch": 0.87776,
      "grad_norm": 0.02700134925544262,
      "learning_rate": 5.61136e-06,
      "loss": 0.1992,
      "step": 27430
    },
    {
      "epoch": 0.87808,
      "grad_norm": 0.03031204082071781,
      "learning_rate": 5.609760000000001e-06,
      "loss": 0.1994,
      "step": 27440
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.028062792494893074,
      "learning_rate": 5.6081600000000005e-06,
      "loss": 0.1992,
      "step": 27450
    },
    {
      "epoch": 0.87872,
      "grad_norm": 0.008024130016565323,
      "learning_rate": 5.606560000000001e-06,
      "loss": 0.2075,
      "step": 27460
    },
    {
      "epoch": 0.87904,
      "grad_norm": 0.021589411422610283,
      "learning_rate": 5.60496e-06,
      "loss": 0.1998,
      "step": 27470
    },
    {
      "epoch": 0.87936,
      "grad_norm": 0.020153822377324104,
      "learning_rate": 5.60336e-06,
      "loss": 0.207,
      "step": 27480
    },
    {
      "epoch": 0.87968,
      "grad_norm": 0.07334402948617935,
      "learning_rate": 5.601760000000001e-06,
      "loss": 0.1993,
      "step": 27490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.021288258954882622,
      "learning_rate": 5.60016e-06,
      "loss": 0.1992,
      "step": 27500
    },
    {
      "epoch": 0.88032,
      "grad_norm": 0.23713453114032745,
      "learning_rate": 5.598560000000001e-06,
      "loss": 0.216,
      "step": 27510
    },
    {
      "epoch": 0.88064,
      "grad_norm": 0.019234297797083855,
      "learning_rate": 5.5969600000000005e-06,
      "loss": 0.1996,
      "step": 27520
    },
    {
      "epoch": 0.88096,
      "grad_norm": 0.03273025527596474,
      "learning_rate": 5.59536e-06,
      "loss": 0.2123,
      "step": 27530
    },
    {
      "epoch": 0.88128,
      "grad_norm": 0.010866536758840084,
      "learning_rate": 5.59376e-06,
      "loss": 0.1992,
      "step": 27540
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.023502783849835396,
      "learning_rate": 5.592160000000001e-06,
      "loss": 0.2112,
      "step": 27550
    },
    {
      "epoch": 0.88192,
      "grad_norm": 0.01998910866677761,
      "learning_rate": 5.59056e-06,
      "loss": 0.2002,
      "step": 27560
    },
    {
      "epoch": 0.88224,
      "grad_norm": 0.016898471862077713,
      "learning_rate": 5.58896e-06,
      "loss": 0.1995,
      "step": 27570
    },
    {
      "epoch": 0.88256,
      "grad_norm": 0.030155669897794724,
      "learning_rate": 5.587360000000001e-06,
      "loss": 0.2116,
      "step": 27580
    },
    {
      "epoch": 0.88288,
      "grad_norm": 0.041464913636446,
      "learning_rate": 5.58576e-06,
      "loss": 0.1993,
      "step": 27590
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.06273393332958221,
      "learning_rate": 5.58416e-06,
      "loss": 0.1994,
      "step": 27600
    },
    {
      "epoch": 0.88352,
      "grad_norm": 0.033410314470529556,
      "learning_rate": 5.58256e-06,
      "loss": 0.1995,
      "step": 27610
    },
    {
      "epoch": 0.88384,
      "grad_norm": 0.02172429859638214,
      "learning_rate": 5.580960000000001e-06,
      "loss": 0.1992,
      "step": 27620
    },
    {
      "epoch": 0.88416,
      "grad_norm": 0.033049825578927994,
      "learning_rate": 5.57936e-06,
      "loss": 0.2155,
      "step": 27630
    },
    {
      "epoch": 0.88448,
      "grad_norm": 0.03514464199542999,
      "learning_rate": 5.577760000000001e-06,
      "loss": 0.1996,
      "step": 27640
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.03980003297328949,
      "learning_rate": 5.576160000000001e-06,
      "loss": 0.2314,
      "step": 27650
    },
    {
      "epoch": 0.88512,
      "grad_norm": 0.026211993768811226,
      "learning_rate": 5.57456e-06,
      "loss": 0.1991,
      "step": 27660
    },
    {
      "epoch": 0.88544,
      "grad_norm": 0.018187787383794785,
      "learning_rate": 5.5729600000000004e-06,
      "loss": 0.1992,
      "step": 27670
    },
    {
      "epoch": 0.88576,
      "grad_norm": 1.225382924079895,
      "learning_rate": 5.57136e-06,
      "loss": 0.2031,
      "step": 27680
    },
    {
      "epoch": 0.88608,
      "grad_norm": 0.8233300447463989,
      "learning_rate": 5.569760000000001e-06,
      "loss": 0.2131,
      "step": 27690
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.036938004195690155,
      "learning_rate": 5.56816e-06,
      "loss": 0.1993,
      "step": 27700
    },
    {
      "epoch": 0.88672,
      "grad_norm": 0.016750680282711983,
      "learning_rate": 5.566560000000001e-06,
      "loss": 0.2171,
      "step": 27710
    },
    {
      "epoch": 0.88704,
      "grad_norm": 0.023663824424147606,
      "learning_rate": 5.564960000000001e-06,
      "loss": 0.1993,
      "step": 27720
    },
    {
      "epoch": 0.88736,
      "grad_norm": 0.030123617500066757,
      "learning_rate": 5.563360000000001e-06,
      "loss": 0.2016,
      "step": 27730
    },
    {
      "epoch": 0.88768,
      "grad_norm": 0.02500774897634983,
      "learning_rate": 5.5617600000000004e-06,
      "loss": 0.2,
      "step": 27740
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.6790875792503357,
      "learning_rate": 5.56016e-06,
      "loss": 0.2153,
      "step": 27750
    },
    {
      "epoch": 0.88832,
      "grad_norm": 0.18040058016777039,
      "learning_rate": 5.558560000000001e-06,
      "loss": 0.1996,
      "step": 27760
    },
    {
      "epoch": 0.88864,
      "grad_norm": 0.010853578336536884,
      "learning_rate": 5.55696e-06,
      "loss": 0.1992,
      "step": 27770
    },
    {
      "epoch": 0.88896,
      "grad_norm": 0.026094749569892883,
      "learning_rate": 5.555360000000001e-06,
      "loss": 0.2169,
      "step": 27780
    },
    {
      "epoch": 0.88928,
      "grad_norm": 0.03485020622611046,
      "learning_rate": 5.553760000000001e-06,
      "loss": 0.1992,
      "step": 27790
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.027137257158756256,
      "learning_rate": 5.5521600000000006e-06,
      "loss": 0.1993,
      "step": 27800
    },
    {
      "epoch": 0.88992,
      "grad_norm": 0.021402468904852867,
      "learning_rate": 5.5505600000000004e-06,
      "loss": 0.1992,
      "step": 27810
    },
    {
      "epoch": 0.89024,
      "grad_norm": 0.02160216122865677,
      "learning_rate": 5.54896e-06,
      "loss": 0.1994,
      "step": 27820
    },
    {
      "epoch": 0.89056,
      "grad_norm": 0.024947864934802055,
      "learning_rate": 5.54736e-06,
      "loss": 0.1994,
      "step": 27830
    },
    {
      "epoch": 0.89088,
      "grad_norm": 0.04768243432044983,
      "learning_rate": 5.54576e-06,
      "loss": 0.1992,
      "step": 27840
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.035102736204862595,
      "learning_rate": 5.544160000000001e-06,
      "loss": 0.1993,
      "step": 27850
    },
    {
      "epoch": 0.89152,
      "grad_norm": 0.021514715626835823,
      "learning_rate": 5.54256e-06,
      "loss": 0.2295,
      "step": 27860
    },
    {
      "epoch": 0.89184,
      "grad_norm": 0.04217070713639259,
      "learning_rate": 5.5409600000000006e-06,
      "loss": 0.1991,
      "step": 27870
    },
    {
      "epoch": 0.89216,
      "grad_norm": 1.8035279512405396,
      "learning_rate": 5.5393600000000004e-06,
      "loss": 0.2191,
      "step": 27880
    },
    {
      "epoch": 0.89248,
      "grad_norm": 0.032194994390010834,
      "learning_rate": 5.537760000000001e-06,
      "loss": 0.1992,
      "step": 27890
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.039276160299777985,
      "learning_rate": 5.53616e-06,
      "loss": 0.1993,
      "step": 27900
    },
    {
      "epoch": 0.89312,
      "grad_norm": 0.044273924082517624,
      "learning_rate": 5.53456e-06,
      "loss": 0.1991,
      "step": 27910
    },
    {
      "epoch": 0.89344,
      "grad_norm": 0.01692986488342285,
      "learning_rate": 5.532960000000001e-06,
      "loss": 0.1993,
      "step": 27920
    },
    {
      "epoch": 0.89376,
      "grad_norm": 0.009608597494661808,
      "learning_rate": 5.53136e-06,
      "loss": 0.2138,
      "step": 27930
    },
    {
      "epoch": 0.89408,
      "grad_norm": 0.008616508916020393,
      "learning_rate": 5.5297600000000006e-06,
      "loss": 0.1991,
      "step": 27940
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.026343613862991333,
      "learning_rate": 5.5281600000000004e-06,
      "loss": 0.1992,
      "step": 27950
    },
    {
      "epoch": 0.89472,
      "grad_norm": 0.03665049746632576,
      "learning_rate": 5.526560000000001e-06,
      "loss": 0.1992,
      "step": 27960
    },
    {
      "epoch": 0.89504,
      "grad_norm": 0.02144308015704155,
      "learning_rate": 5.52496e-06,
      "loss": 0.202,
      "step": 27970
    },
    {
      "epoch": 0.89536,
      "grad_norm": 0.5301952362060547,
      "learning_rate": 5.523360000000001e-06,
      "loss": 0.2007,
      "step": 27980
    },
    {
      "epoch": 0.89568,
      "grad_norm": 0.013319732621312141,
      "learning_rate": 5.521760000000001e-06,
      "loss": 0.1992,
      "step": 27990
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.01233263872563839,
      "learning_rate": 5.52016e-06,
      "loss": 0.211,
      "step": 28000
    },
    {
      "epoch": 0.896,
      "eval_runtime": 52.2853,
      "eval_samples_per_second": 191.258,
      "eval_steps_per_second": 11.954,
      "step": 28000
    },
    {
      "epoch": 0.89632,
      "grad_norm": 0.05158397555351257,
      "learning_rate": 5.5185600000000006e-06,
      "loss": 0.2204,
      "step": 28010
    },
    {
      "epoch": 0.89664,
      "grad_norm": 0.04838842898607254,
      "learning_rate": 5.5169600000000005e-06,
      "loss": 0.1991,
      "step": 28020
    },
    {
      "epoch": 0.89696,
      "grad_norm": 0.02839275822043419,
      "learning_rate": 5.51536e-06,
      "loss": 0.2022,
      "step": 28030
    },
    {
      "epoch": 0.89728,
      "grad_norm": 0.8030250072479248,
      "learning_rate": 5.51376e-06,
      "loss": 0.2137,
      "step": 28040
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.05344372242689133,
      "learning_rate": 5.512160000000001e-06,
      "loss": 0.1997,
      "step": 28050
    },
    {
      "epoch": 0.89792,
      "grad_norm": 0.059368498623371124,
      "learning_rate": 5.51056e-06,
      "loss": 0.2111,
      "step": 28060
    },
    {
      "epoch": 0.89824,
      "grad_norm": 0.02549385093152523,
      "learning_rate": 5.508960000000001e-06,
      "loss": 0.1993,
      "step": 28070
    },
    {
      "epoch": 0.89856,
      "grad_norm": 1.6397690773010254,
      "learning_rate": 5.507360000000001e-06,
      "loss": 0.2017,
      "step": 28080
    },
    {
      "epoch": 0.89888,
      "grad_norm": 0.03687159717082977,
      "learning_rate": 5.50576e-06,
      "loss": 0.1994,
      "step": 28090
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.01904688961803913,
      "learning_rate": 5.50416e-06,
      "loss": 0.2016,
      "step": 28100
    },
    {
      "epoch": 0.89952,
      "grad_norm": 0.9531436562538147,
      "learning_rate": 5.50256e-06,
      "loss": 0.2106,
      "step": 28110
    },
    {
      "epoch": 0.89984,
      "grad_norm": 0.03059137426316738,
      "learning_rate": 5.500960000000001e-06,
      "loss": 0.2058,
      "step": 28120
    },
    {
      "epoch": 0.90016,
      "grad_norm": 0.06107896938920021,
      "learning_rate": 5.49936e-06,
      "loss": 0.2089,
      "step": 28130
    },
    {
      "epoch": 0.90048,
      "grad_norm": 0.06958557665348053,
      "learning_rate": 5.497760000000001e-06,
      "loss": 0.1994,
      "step": 28140
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.014956880360841751,
      "learning_rate": 5.496160000000001e-06,
      "loss": 0.2096,
      "step": 28150
    },
    {
      "epoch": 0.90112,
      "grad_norm": 0.05017326399683952,
      "learning_rate": 5.494560000000001e-06,
      "loss": 0.1992,
      "step": 28160
    },
    {
      "epoch": 0.90144,
      "grad_norm": 0.04687066748738289,
      "learning_rate": 5.49296e-06,
      "loss": 0.1992,
      "step": 28170
    },
    {
      "epoch": 0.90176,
      "grad_norm": 0.021565264090895653,
      "learning_rate": 5.49136e-06,
      "loss": 0.2081,
      "step": 28180
    },
    {
      "epoch": 0.90208,
      "grad_norm": 0.015214172191917896,
      "learning_rate": 5.489760000000001e-06,
      "loss": 0.1993,
      "step": 28190
    },
    {
      "epoch": 0.9024,
      "grad_norm": 1.7707070112228394,
      "learning_rate": 5.48816e-06,
      "loss": 0.2161,
      "step": 28200
    },
    {
      "epoch": 0.90272,
      "grad_norm": 0.04177689924836159,
      "learning_rate": 5.486560000000001e-06,
      "loss": 0.2004,
      "step": 28210
    },
    {
      "epoch": 0.90304,
      "grad_norm": 0.030475633218884468,
      "learning_rate": 5.484960000000001e-06,
      "loss": 0.1995,
      "step": 28220
    },
    {
      "epoch": 0.90336,
      "grad_norm": 0.031908366829156876,
      "learning_rate": 5.483360000000001e-06,
      "loss": 0.213,
      "step": 28230
    },
    {
      "epoch": 0.90368,
      "grad_norm": 1.1014771461486816,
      "learning_rate": 5.48176e-06,
      "loss": 0.2267,
      "step": 28240
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.01339664589613676,
      "learning_rate": 5.48016e-06,
      "loss": 0.1992,
      "step": 28250
    },
    {
      "epoch": 0.90432,
      "grad_norm": 0.0184727031737566,
      "learning_rate": 5.478560000000001e-06,
      "loss": 0.2038,
      "step": 28260
    },
    {
      "epoch": 0.90464,
      "grad_norm": 0.03967193141579628,
      "learning_rate": 5.47696e-06,
      "loss": 0.2027,
      "step": 28270
    },
    {
      "epoch": 0.90496,
      "grad_norm": 0.02000313252210617,
      "learning_rate": 5.475360000000001e-06,
      "loss": 0.2099,
      "step": 28280
    },
    {
      "epoch": 0.90528,
      "grad_norm": 0.03585099056363106,
      "learning_rate": 5.473760000000001e-06,
      "loss": 0.1993,
      "step": 28290
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.059472501277923584,
      "learning_rate": 5.4721600000000005e-06,
      "loss": 0.2081,
      "step": 28300
    },
    {
      "epoch": 0.90592,
      "grad_norm": 0.025526095181703568,
      "learning_rate": 5.47056e-06,
      "loss": 0.2088,
      "step": 28310
    },
    {
      "epoch": 0.90624,
      "grad_norm": 0.011529963463544846,
      "learning_rate": 5.468960000000001e-06,
      "loss": 0.1995,
      "step": 28320
    },
    {
      "epoch": 0.90656,
      "grad_norm": 0.019820956513285637,
      "learning_rate": 5.46736e-06,
      "loss": 0.1993,
      "step": 28330
    },
    {
      "epoch": 0.90688,
      "grad_norm": 0.014372825622558594,
      "learning_rate": 5.46576e-06,
      "loss": 0.2154,
      "step": 28340
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.009339124895632267,
      "learning_rate": 5.464160000000001e-06,
      "loss": 0.1992,
      "step": 28350
    },
    {
      "epoch": 0.90752,
      "grad_norm": 0.02451458014547825,
      "learning_rate": 5.46256e-06,
      "loss": 0.2148,
      "step": 28360
    },
    {
      "epoch": 0.90784,
      "grad_norm": 0.0185282863676548,
      "learning_rate": 5.4609600000000005e-06,
      "loss": 0.2154,
      "step": 28370
    },
    {
      "epoch": 0.90816,
      "grad_norm": 0.056825485080480576,
      "learning_rate": 5.45936e-06,
      "loss": 0.2168,
      "step": 28380
    },
    {
      "epoch": 0.90848,
      "grad_norm": 0.03477982059121132,
      "learning_rate": 5.457760000000001e-06,
      "loss": 0.1993,
      "step": 28390
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.02062351256608963,
      "learning_rate": 5.45616e-06,
      "loss": 0.1995,
      "step": 28400
    },
    {
      "epoch": 0.90912,
      "grad_norm": 0.017652243375778198,
      "learning_rate": 5.454560000000001e-06,
      "loss": 0.2038,
      "step": 28410
    },
    {
      "epoch": 0.90944,
      "grad_norm": 0.030296659097075462,
      "learning_rate": 5.452960000000001e-06,
      "loss": 0.1993,
      "step": 28420
    },
    {
      "epoch": 0.90976,
      "grad_norm": 1.487266182899475,
      "learning_rate": 5.45136e-06,
      "loss": 0.2083,
      "step": 28430
    },
    {
      "epoch": 0.91008,
      "grad_norm": 0.03571728616952896,
      "learning_rate": 5.4497600000000005e-06,
      "loss": 0.1992,
      "step": 28440
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.06032590940594673,
      "learning_rate": 5.44816e-06,
      "loss": 0.1994,
      "step": 28450
    },
    {
      "epoch": 0.91072,
      "grad_norm": 0.01968041993677616,
      "learning_rate": 5.446560000000001e-06,
      "loss": 0.1992,
      "step": 28460
    },
    {
      "epoch": 0.91104,
      "grad_norm": 0.020243793725967407,
      "learning_rate": 5.44496e-06,
      "loss": 0.2098,
      "step": 28470
    },
    {
      "epoch": 0.91136,
      "grad_norm": 0.022647514939308167,
      "learning_rate": 5.443360000000001e-06,
      "loss": 0.1993,
      "step": 28480
    },
    {
      "epoch": 0.91168,
      "grad_norm": 0.018173575401306152,
      "learning_rate": 5.441760000000001e-06,
      "loss": 0.1992,
      "step": 28490
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.015658462420105934,
      "learning_rate": 5.440160000000001e-06,
      "loss": 0.1992,
      "step": 28500
    },
    {
      "epoch": 0.91232,
      "grad_norm": 0.05454597249627113,
      "learning_rate": 5.4385600000000005e-06,
      "loss": 0.1994,
      "step": 28510
    },
    {
      "epoch": 0.91264,
      "grad_norm": 0.04019157588481903,
      "learning_rate": 5.43696e-06,
      "loss": 0.2231,
      "step": 28520
    },
    {
      "epoch": 0.91296,
      "grad_norm": 0.018864020705223083,
      "learning_rate": 5.43536e-06,
      "loss": 0.1997,
      "step": 28530
    },
    {
      "epoch": 0.91328,
      "grad_norm": 0.014840075746178627,
      "learning_rate": 5.43376e-06,
      "loss": 0.2,
      "step": 28540
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.03268108889460564,
      "learning_rate": 5.432160000000001e-06,
      "loss": 0.1999,
      "step": 28550
    },
    {
      "epoch": 0.91392,
      "grad_norm": 0.021014142781496048,
      "learning_rate": 5.43056e-06,
      "loss": 0.1992,
      "step": 28560
    },
    {
      "epoch": 0.91424,
      "grad_norm": 0.03771449252963066,
      "learning_rate": 5.428960000000001e-06,
      "loss": 0.1992,
      "step": 28570
    },
    {
      "epoch": 0.91456,
      "grad_norm": 0.19702284038066864,
      "learning_rate": 5.4273600000000005e-06,
      "loss": 0.2008,
      "step": 28580
    },
    {
      "epoch": 0.91488,
      "grad_norm": 0.018506336957216263,
      "learning_rate": 5.4257599999999995e-06,
      "loss": 0.1993,
      "step": 28590
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.03489895537495613,
      "learning_rate": 5.42416e-06,
      "loss": 0.1991,
      "step": 28600
    },
    {
      "epoch": 0.91552,
      "grad_norm": 0.0130586763843894,
      "learning_rate": 5.42256e-06,
      "loss": 0.2134,
      "step": 28610
    },
    {
      "epoch": 0.91584,
      "grad_norm": 0.015663601458072662,
      "learning_rate": 5.420960000000001e-06,
      "loss": 0.1992,
      "step": 28620
    },
    {
      "epoch": 0.91616,
      "grad_norm": 0.025023244321346283,
      "learning_rate": 5.41936e-06,
      "loss": 0.1992,
      "step": 28630
    },
    {
      "epoch": 0.91648,
      "grad_norm": 0.030938420444726944,
      "learning_rate": 5.417760000000001e-06,
      "loss": 0.1993,
      "step": 28640
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.014025170356035233,
      "learning_rate": 5.4161600000000005e-06,
      "loss": 0.1994,
      "step": 28650
    },
    {
      "epoch": 0.91712,
      "grad_norm": 0.034511078149080276,
      "learning_rate": 5.414560000000001e-06,
      "loss": 0.1993,
      "step": 28660
    },
    {
      "epoch": 0.91744,
      "grad_norm": 0.6990947127342224,
      "learning_rate": 5.41296e-06,
      "loss": 0.2155,
      "step": 28670
    },
    {
      "epoch": 0.91776,
      "grad_norm": 1.3038830757141113,
      "learning_rate": 5.41136e-06,
      "loss": 0.2031,
      "step": 28680
    },
    {
      "epoch": 0.91808,
      "grad_norm": 0.034531041979789734,
      "learning_rate": 5.409760000000001e-06,
      "loss": 0.1995,
      "step": 28690
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.035569410771131516,
      "learning_rate": 5.40816e-06,
      "loss": 0.1993,
      "step": 28700
    },
    {
      "epoch": 0.91872,
      "grad_norm": 0.01551544014364481,
      "learning_rate": 5.406560000000001e-06,
      "loss": 0.1992,
      "step": 28710
    },
    {
      "epoch": 0.91904,
      "grad_norm": 0.028523916378617287,
      "learning_rate": 5.4049600000000005e-06,
      "loss": 0.1992,
      "step": 28720
    },
    {
      "epoch": 0.91936,
      "grad_norm": 0.018031874671578407,
      "learning_rate": 5.403360000000001e-06,
      "loss": 0.1993,
      "step": 28730
    },
    {
      "epoch": 0.91968,
      "grad_norm": 0.0170375257730484,
      "learning_rate": 5.40176e-06,
      "loss": 0.1993,
      "step": 28740
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.037390921264886856,
      "learning_rate": 5.400160000000001e-06,
      "loss": 0.1999,
      "step": 28750
    },
    {
      "epoch": 0.92032,
      "grad_norm": 0.12283460050821304,
      "learning_rate": 5.398560000000001e-06,
      "loss": 0.1993,
      "step": 28760
    },
    {
      "epoch": 0.92064,
      "grad_norm": 0.03936515375971794,
      "learning_rate": 5.39696e-06,
      "loss": 0.1991,
      "step": 28770
    },
    {
      "epoch": 0.92096,
      "grad_norm": 0.03690120577812195,
      "learning_rate": 5.395360000000001e-06,
      "loss": 0.1995,
      "step": 28780
    },
    {
      "epoch": 0.92128,
      "grad_norm": 0.010037966072559357,
      "learning_rate": 5.3937600000000005e-06,
      "loss": 0.2266,
      "step": 28790
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.8581814765930176,
      "learning_rate": 5.39216e-06,
      "loss": 0.2116,
      "step": 28800
    },
    {
      "epoch": 0.92192,
      "grad_norm": 0.051139649003744125,
      "learning_rate": 5.39056e-06,
      "loss": 0.1994,
      "step": 28810
    },
    {
      "epoch": 0.92224,
      "grad_norm": 0.013902357779443264,
      "learning_rate": 5.388960000000001e-06,
      "loss": 0.1993,
      "step": 28820
    },
    {
      "epoch": 0.92256,
      "grad_norm": 0.020861349999904633,
      "learning_rate": 5.38736e-06,
      "loss": 0.2148,
      "step": 28830
    },
    {
      "epoch": 0.92288,
      "grad_norm": 0.03250942379236221,
      "learning_rate": 5.385760000000001e-06,
      "loss": 0.1992,
      "step": 28840
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.034455522894859314,
      "learning_rate": 5.384160000000001e-06,
      "loss": 0.2139,
      "step": 28850
    },
    {
      "epoch": 0.92352,
      "grad_norm": 0.09676393121480942,
      "learning_rate": 5.38256e-06,
      "loss": 0.1994,
      "step": 28860
    },
    {
      "epoch": 0.92384,
      "grad_norm": 0.01746293529868126,
      "learning_rate": 5.38096e-06,
      "loss": 0.1994,
      "step": 28870
    },
    {
      "epoch": 0.92416,
      "grad_norm": 0.039772916585206985,
      "learning_rate": 5.37936e-06,
      "loss": 0.2124,
      "step": 28880
    },
    {
      "epoch": 0.92448,
      "grad_norm": 0.031387265771627426,
      "learning_rate": 5.377760000000001e-06,
      "loss": 0.1994,
      "step": 28890
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.025058161467313766,
      "learning_rate": 5.37616e-06,
      "loss": 0.1997,
      "step": 28900
    },
    {
      "epoch": 0.92512,
      "grad_norm": 0.027596700936555862,
      "learning_rate": 5.374560000000001e-06,
      "loss": 0.1993,
      "step": 28910
    },
    {
      "epoch": 0.92544,
      "grad_norm": 0.019525669515132904,
      "learning_rate": 5.372960000000001e-06,
      "loss": 0.1997,
      "step": 28920
    },
    {
      "epoch": 0.92576,
      "grad_norm": 0.025535428896546364,
      "learning_rate": 5.37136e-06,
      "loss": 0.2049,
      "step": 28930
    },
    {
      "epoch": 0.92608,
      "grad_norm": 0.018271684646606445,
      "learning_rate": 5.36976e-06,
      "loss": 0.2145,
      "step": 28940
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.73907071352005,
      "learning_rate": 5.36816e-06,
      "loss": 0.2153,
      "step": 28950
    },
    {
      "epoch": 0.92672,
      "grad_norm": 0.018980329856276512,
      "learning_rate": 5.366560000000001e-06,
      "loss": 0.1993,
      "step": 28960
    },
    {
      "epoch": 0.92704,
      "grad_norm": 0.03231937065720558,
      "learning_rate": 5.36496e-06,
      "loss": 0.2157,
      "step": 28970
    },
    {
      "epoch": 0.92736,
      "grad_norm": 0.055586908012628555,
      "learning_rate": 5.363360000000001e-06,
      "loss": 0.1993,
      "step": 28980
    },
    {
      "epoch": 0.92768,
      "grad_norm": 0.015636449679732323,
      "learning_rate": 5.361760000000001e-06,
      "loss": 0.2304,
      "step": 28990
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.3947250843048096,
      "learning_rate": 5.3601600000000005e-06,
      "loss": 0.2063,
      "step": 29000
    },
    {
      "epoch": 0.928,
      "eval_runtime": 50.5492,
      "eval_samples_per_second": 197.827,
      "eval_steps_per_second": 12.364,
      "step": 29000
    },
    {
      "epoch": 0.92832,
      "grad_norm": 0.03185378760099411,
      "learning_rate": 5.35856e-06,
      "loss": 0.1992,
      "step": 29010
    },
    {
      "epoch": 0.92864,
      "grad_norm": 0.014572206884622574,
      "learning_rate": 5.35696e-06,
      "loss": 0.2311,
      "step": 29020
    },
    {
      "epoch": 0.92896,
      "grad_norm": 0.03818690404295921,
      "learning_rate": 5.35536e-06,
      "loss": 0.1993,
      "step": 29030
    },
    {
      "epoch": 0.92928,
      "grad_norm": 0.035611312836408615,
      "learning_rate": 5.35376e-06,
      "loss": 0.1996,
      "step": 29040
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.03180760145187378,
      "learning_rate": 5.352160000000001e-06,
      "loss": 0.1994,
      "step": 29050
    },
    {
      "epoch": 0.92992,
      "grad_norm": 0.03753872215747833,
      "learning_rate": 5.35056e-06,
      "loss": 0.1993,
      "step": 29060
    },
    {
      "epoch": 0.93024,
      "grad_norm": 0.02864876016974449,
      "learning_rate": 5.3489600000000005e-06,
      "loss": 0.1993,
      "step": 29070
    },
    {
      "epoch": 0.93056,
      "grad_norm": 0.03436799719929695,
      "learning_rate": 5.34736e-06,
      "loss": 0.1995,
      "step": 29080
    },
    {
      "epoch": 0.93088,
      "grad_norm": 0.7822578549385071,
      "learning_rate": 5.345760000000001e-06,
      "loss": 0.2142,
      "step": 29090
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.04595917835831642,
      "learning_rate": 5.34416e-06,
      "loss": 0.1998,
      "step": 29100
    },
    {
      "epoch": 0.93152,
      "grad_norm": 0.014584843069314957,
      "learning_rate": 5.34256e-06,
      "loss": 0.1994,
      "step": 29110
    },
    {
      "epoch": 0.93184,
      "grad_norm": 0.04486175999045372,
      "learning_rate": 5.340960000000001e-06,
      "loss": 0.1993,
      "step": 29120
    },
    {
      "epoch": 0.93216,
      "grad_norm": 0.5677792429924011,
      "learning_rate": 5.33936e-06,
      "loss": 0.202,
      "step": 29130
    },
    {
      "epoch": 0.93248,
      "grad_norm": 1.478621244430542,
      "learning_rate": 5.3377600000000005e-06,
      "loss": 0.2124,
      "step": 29140
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.022749368101358414,
      "learning_rate": 5.33616e-06,
      "loss": 0.1993,
      "step": 29150
    },
    {
      "epoch": 0.93312,
      "grad_norm": 0.024484798312187195,
      "learning_rate": 5.334560000000001e-06,
      "loss": 0.1992,
      "step": 29160
    },
    {
      "epoch": 0.93344,
      "grad_norm": 0.028771286830306053,
      "learning_rate": 5.33296e-06,
      "loss": 0.217,
      "step": 29170
    },
    {
      "epoch": 0.93376,
      "grad_norm": 0.05410662293434143,
      "learning_rate": 5.331360000000001e-06,
      "loss": 0.1996,
      "step": 29180
    },
    {
      "epoch": 0.93408,
      "grad_norm": 0.029585080221295357,
      "learning_rate": 5.329760000000001e-06,
      "loss": 0.1994,
      "step": 29190
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.0291566401720047,
      "learning_rate": 5.32816e-06,
      "loss": 0.1991,
      "step": 29200
    },
    {
      "epoch": 0.93472,
      "grad_norm": 0.033599670976400375,
      "learning_rate": 5.3265600000000005e-06,
      "loss": 0.1994,
      "step": 29210
    },
    {
      "epoch": 0.93504,
      "grad_norm": 0.0230246651917696,
      "learning_rate": 5.3249600000000004e-06,
      "loss": 0.1993,
      "step": 29220
    },
    {
      "epoch": 0.93536,
      "grad_norm": 0.236180379986763,
      "learning_rate": 5.323360000000001e-06,
      "loss": 0.1996,
      "step": 29230
    },
    {
      "epoch": 0.93568,
      "grad_norm": 0.27082812786102295,
      "learning_rate": 5.32176e-06,
      "loss": 0.2174,
      "step": 29240
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.014545529149472713,
      "learning_rate": 5.320160000000001e-06,
      "loss": 0.2136,
      "step": 29250
    },
    {
      "epoch": 0.93632,
      "grad_norm": 0.011152990162372589,
      "learning_rate": 5.318560000000001e-06,
      "loss": 0.2109,
      "step": 29260
    },
    {
      "epoch": 0.93664,
      "grad_norm": 0.015327122062444687,
      "learning_rate": 5.31696e-06,
      "loss": 0.1994,
      "step": 29270
    },
    {
      "epoch": 0.93696,
      "grad_norm": 0.04238360375165939,
      "learning_rate": 5.3153600000000006e-06,
      "loss": 0.2,
      "step": 29280
    },
    {
      "epoch": 0.93728,
      "grad_norm": 0.04244907200336456,
      "learning_rate": 5.3137600000000004e-06,
      "loss": 0.1991,
      "step": 29290
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.025039244443178177,
      "learning_rate": 5.31216e-06,
      "loss": 0.1992,
      "step": 29300
    },
    {
      "epoch": 0.93792,
      "grad_norm": 0.03456073999404907,
      "learning_rate": 5.31056e-06,
      "loss": 0.2057,
      "step": 29310
    },
    {
      "epoch": 0.93824,
      "grad_norm": 0.05047166347503662,
      "learning_rate": 5.308960000000001e-06,
      "loss": 0.1992,
      "step": 29320
    },
    {
      "epoch": 0.93856,
      "grad_norm": 0.23739662766456604,
      "learning_rate": 5.30736e-06,
      "loss": 0.1996,
      "step": 29330
    },
    {
      "epoch": 0.93888,
      "grad_norm": 0.025556456297636032,
      "learning_rate": 5.305760000000001e-06,
      "loss": 0.211,
      "step": 29340
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.029682131484150887,
      "learning_rate": 5.3041600000000006e-06,
      "loss": 0.2141,
      "step": 29350
    },
    {
      "epoch": 0.93952,
      "grad_norm": 0.0377284400165081,
      "learning_rate": 5.30256e-06,
      "loss": 0.1994,
      "step": 29360
    },
    {
      "epoch": 0.93984,
      "grad_norm": 0.026359982788562775,
      "learning_rate": 5.30096e-06,
      "loss": 0.1997,
      "step": 29370
    },
    {
      "epoch": 0.94016,
      "grad_norm": 0.11323186755180359,
      "learning_rate": 5.29936e-06,
      "loss": 0.204,
      "step": 29380
    },
    {
      "epoch": 0.94048,
      "grad_norm": 0.14913496375083923,
      "learning_rate": 5.297760000000001e-06,
      "loss": 0.2039,
      "step": 29390
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.13687042891979218,
      "learning_rate": 5.29616e-06,
      "loss": 0.1999,
      "step": 29400
    },
    {
      "epoch": 0.94112,
      "grad_norm": 0.008535937406122684,
      "learning_rate": 5.294560000000001e-06,
      "loss": 0.2126,
      "step": 29410
    },
    {
      "epoch": 0.94144,
      "grad_norm": 0.03304509073495865,
      "learning_rate": 5.2929600000000006e-06,
      "loss": 0.1994,
      "step": 29420
    },
    {
      "epoch": 0.94176,
      "grad_norm": 0.02030768431723118,
      "learning_rate": 5.291360000000001e-06,
      "loss": 0.1992,
      "step": 29430
    },
    {
      "epoch": 0.94208,
      "grad_norm": 0.028439803048968315,
      "learning_rate": 5.28976e-06,
      "loss": 0.2085,
      "step": 29440
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.014630173332989216,
      "learning_rate": 5.28816e-06,
      "loss": 0.2133,
      "step": 29450
    },
    {
      "epoch": 0.94272,
      "grad_norm": 0.13548079133033752,
      "learning_rate": 5.286560000000001e-06,
      "loss": 0.1993,
      "step": 29460
    },
    {
      "epoch": 0.94304,
      "grad_norm": 0.011207511648535728,
      "learning_rate": 5.28496e-06,
      "loss": 0.1991,
      "step": 29470
    },
    {
      "epoch": 0.94336,
      "grad_norm": 0.028709637001156807,
      "learning_rate": 5.283360000000001e-06,
      "loss": 0.1993,
      "step": 29480
    },
    {
      "epoch": 0.94368,
      "grad_norm": 0.03109709732234478,
      "learning_rate": 5.2817600000000006e-06,
      "loss": 0.2118,
      "step": 29490
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.0323730930685997,
      "learning_rate": 5.2801600000000005e-06,
      "loss": 0.1994,
      "step": 29500
    },
    {
      "epoch": 0.94432,
      "grad_norm": 0.053265172988176346,
      "learning_rate": 5.27856e-06,
      "loss": 0.2167,
      "step": 29510
    },
    {
      "epoch": 0.94464,
      "grad_norm": 0.02872473932802677,
      "learning_rate": 5.276960000000001e-06,
      "loss": 0.2004,
      "step": 29520
    },
    {
      "epoch": 0.94496,
      "grad_norm": 0.02606274001300335,
      "learning_rate": 5.27536e-06,
      "loss": 0.1991,
      "step": 29530
    },
    {
      "epoch": 0.94528,
      "grad_norm": 0.029989663511514664,
      "learning_rate": 5.27376e-06,
      "loss": 0.2211,
      "step": 29540
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.02161797322332859,
      "learning_rate": 5.272160000000001e-06,
      "loss": 0.1996,
      "step": 29550
    },
    {
      "epoch": 0.94592,
      "grad_norm": 0.05176861584186554,
      "learning_rate": 5.27056e-06,
      "loss": 0.2001,
      "step": 29560
    },
    {
      "epoch": 0.94624,
      "grad_norm": 0.008890548720955849,
      "learning_rate": 5.2689600000000005e-06,
      "loss": 0.2088,
      "step": 29570
    },
    {
      "epoch": 0.94656,
      "grad_norm": 0.020424405112862587,
      "learning_rate": 5.26736e-06,
      "loss": 0.1994,
      "step": 29580
    },
    {
      "epoch": 0.94688,
      "grad_norm": 0.020868519321084023,
      "learning_rate": 5.265760000000001e-06,
      "loss": 0.2017,
      "step": 29590
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.7925832867622375,
      "learning_rate": 5.26416e-06,
      "loss": 0.2128,
      "step": 29600
    },
    {
      "epoch": 0.94752,
      "grad_norm": 0.031741607934236526,
      "learning_rate": 5.26256e-06,
      "loss": 0.1991,
      "step": 29610
    },
    {
      "epoch": 0.94784,
      "grad_norm": 0.030167987570166588,
      "learning_rate": 5.260960000000001e-06,
      "loss": 0.2034,
      "step": 29620
    },
    {
      "epoch": 0.94816,
      "grad_norm": 0.6983122229576111,
      "learning_rate": 5.25936e-06,
      "loss": 0.214,
      "step": 29630
    },
    {
      "epoch": 0.94848,
      "grad_norm": 0.011907974258065224,
      "learning_rate": 5.2577600000000005e-06,
      "loss": 0.1994,
      "step": 29640
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.02294730581343174,
      "learning_rate": 5.25616e-06,
      "loss": 0.1994,
      "step": 29650
    },
    {
      "epoch": 0.94912,
      "grad_norm": 0.026118963956832886,
      "learning_rate": 5.254560000000001e-06,
      "loss": 0.2003,
      "step": 29660
    },
    {
      "epoch": 0.94944,
      "grad_norm": 0.01825338788330555,
      "learning_rate": 5.25296e-06,
      "loss": 0.2002,
      "step": 29670
    },
    {
      "epoch": 0.94976,
      "grad_norm": 0.051704417914152145,
      "learning_rate": 5.251360000000001e-06,
      "loss": 0.1996,
      "step": 29680
    },
    {
      "epoch": 0.95008,
      "grad_norm": 0.04241020232439041,
      "learning_rate": 5.249760000000001e-06,
      "loss": 0.2285,
      "step": 29690
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.03422509506344795,
      "learning_rate": 5.24816e-06,
      "loss": 0.2006,
      "step": 29700
    },
    {
      "epoch": 0.95072,
      "grad_norm": 0.1080460175871849,
      "learning_rate": 5.2465600000000005e-06,
      "loss": 0.1993,
      "step": 29710
    },
    {
      "epoch": 0.95104,
      "grad_norm": 0.025790240615606308,
      "learning_rate": 5.24496e-06,
      "loss": 0.2152,
      "step": 29720
    },
    {
      "epoch": 0.95136,
      "grad_norm": 0.04746447876095772,
      "learning_rate": 5.243360000000001e-06,
      "loss": 0.1993,
      "step": 29730
    },
    {
      "epoch": 0.95168,
      "grad_norm": 0.014486965723335743,
      "learning_rate": 5.24176e-06,
      "loss": 0.2009,
      "step": 29740
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.023127004504203796,
      "learning_rate": 5.240160000000001e-06,
      "loss": 0.2187,
      "step": 29750
    },
    {
      "epoch": 0.95232,
      "grad_norm": 0.03847441449761391,
      "learning_rate": 5.238560000000001e-06,
      "loss": 0.1995,
      "step": 29760
    },
    {
      "epoch": 0.95264,
      "grad_norm": 0.02157224901020527,
      "learning_rate": 5.236960000000001e-06,
      "loss": 0.2124,
      "step": 29770
    },
    {
      "epoch": 0.95296,
      "grad_norm": 0.23386721312999725,
      "learning_rate": 5.2353600000000005e-06,
      "loss": 0.1999,
      "step": 29780
    },
    {
      "epoch": 0.95328,
      "grad_norm": 0.023689832538366318,
      "learning_rate": 5.23376e-06,
      "loss": 0.199,
      "step": 29790
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.05113453418016434,
      "learning_rate": 5.23216e-06,
      "loss": 0.1993,
      "step": 29800
    },
    {
      "epoch": 0.95392,
      "grad_norm": 0.02802279405295849,
      "learning_rate": 5.23056e-06,
      "loss": 0.2154,
      "step": 29810
    },
    {
      "epoch": 0.95424,
      "grad_norm": 0.01246828306466341,
      "learning_rate": 5.228960000000001e-06,
      "loss": 0.1992,
      "step": 29820
    },
    {
      "epoch": 0.95456,
      "grad_norm": 0.01530037447810173,
      "learning_rate": 5.22736e-06,
      "loss": 0.1992,
      "step": 29830
    },
    {
      "epoch": 0.95488,
      "grad_norm": 0.01661420613527298,
      "learning_rate": 5.225760000000001e-06,
      "loss": 0.2003,
      "step": 29840
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.0832442045211792,
      "learning_rate": 5.2241600000000005e-06,
      "loss": 0.1994,
      "step": 29850
    },
    {
      "epoch": 0.95552,
      "grad_norm": 0.049912724643945694,
      "learning_rate": 5.222560000000001e-06,
      "loss": 0.2262,
      "step": 29860
    },
    {
      "epoch": 0.95584,
      "grad_norm": 0.037503331899642944,
      "learning_rate": 5.22096e-06,
      "loss": 0.2119,
      "step": 29870
    },
    {
      "epoch": 0.95616,
      "grad_norm": 0.02517799660563469,
      "learning_rate": 5.21936e-06,
      "loss": 0.1992,
      "step": 29880
    },
    {
      "epoch": 0.95648,
      "grad_norm": 0.08979611843824387,
      "learning_rate": 5.217760000000001e-06,
      "loss": 0.2021,
      "step": 29890
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.01754513569176197,
      "learning_rate": 5.21616e-06,
      "loss": 0.212,
      "step": 29900
    },
    {
      "epoch": 0.95712,
      "grad_norm": 0.0363038033246994,
      "learning_rate": 5.214560000000001e-06,
      "loss": 0.2144,
      "step": 29910
    },
    {
      "epoch": 0.95744,
      "grad_norm": 0.04331474378705025,
      "learning_rate": 5.2129600000000005e-06,
      "loss": 0.214,
      "step": 29920
    },
    {
      "epoch": 0.95776,
      "grad_norm": 0.0348191000521183,
      "learning_rate": 5.211360000000001e-06,
      "loss": 0.2201,
      "step": 29930
    },
    {
      "epoch": 0.95808,
      "grad_norm": 0.046342674642801285,
      "learning_rate": 5.20976e-06,
      "loss": 0.1993,
      "step": 29940
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.033809881657361984,
      "learning_rate": 5.20816e-06,
      "loss": 0.1993,
      "step": 29950
    },
    {
      "epoch": 0.95872,
      "grad_norm": 0.03052690625190735,
      "learning_rate": 5.206560000000001e-06,
      "loss": 0.1992,
      "step": 29960
    },
    {
      "epoch": 0.95904,
      "grad_norm": 0.02079525776207447,
      "learning_rate": 5.20496e-06,
      "loss": 0.2047,
      "step": 29970
    },
    {
      "epoch": 0.95936,
      "grad_norm": 0.017652876675128937,
      "learning_rate": 5.203360000000001e-06,
      "loss": 0.2137,
      "step": 29980
    },
    {
      "epoch": 0.95968,
      "grad_norm": 0.03575284406542778,
      "learning_rate": 5.2017600000000005e-06,
      "loss": 0.1993,
      "step": 29990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.008907372131943703,
      "learning_rate": 5.20016e-06,
      "loss": 0.1995,
      "step": 30000
    },
    {
      "epoch": 0.96,
      "eval_runtime": 52.0987,
      "eval_samples_per_second": 191.943,
      "eval_steps_per_second": 11.996,
      "step": 30000
    },
    {
      "epoch": 0.96032,
      "grad_norm": 0.024302484467625618,
      "learning_rate": 5.19856e-06,
      "loss": 0.1993,
      "step": 30010
    },
    {
      "epoch": 0.96064,
      "grad_norm": 0.023179788142442703,
      "learning_rate": 5.196960000000001e-06,
      "loss": 0.1996,
      "step": 30020
    },
    {
      "epoch": 0.96096,
      "grad_norm": 0.0246321689337492,
      "learning_rate": 5.19536e-06,
      "loss": 0.1991,
      "step": 30030
    },
    {
      "epoch": 0.96128,
      "grad_norm": 0.02207295037806034,
      "learning_rate": 5.19376e-06,
      "loss": 0.2123,
      "step": 30040
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.020052820444107056,
      "learning_rate": 5.192160000000001e-06,
      "loss": 0.2136,
      "step": 30050
    },
    {
      "epoch": 0.96192,
      "grad_norm": 0.015085085295140743,
      "learning_rate": 5.19056e-06,
      "loss": 0.2118,
      "step": 30060
    },
    {
      "epoch": 0.96224,
      "grad_norm": 0.024949131533503532,
      "learning_rate": 5.18896e-06,
      "loss": 0.1992,
      "step": 30070
    },
    {
      "epoch": 0.96256,
      "grad_norm": 0.08809888362884521,
      "learning_rate": 5.18736e-06,
      "loss": 0.1998,
      "step": 30080
    },
    {
      "epoch": 0.96288,
      "grad_norm": 0.055638812482357025,
      "learning_rate": 5.185760000000001e-06,
      "loss": 0.2009,
      "step": 30090
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.028255252167582512,
      "learning_rate": 5.18416e-06,
      "loss": 0.1993,
      "step": 30100
    },
    {
      "epoch": 0.96352,
      "grad_norm": 0.09185710549354553,
      "learning_rate": 5.182560000000001e-06,
      "loss": 0.1994,
      "step": 30110
    },
    {
      "epoch": 0.96384,
      "grad_norm": 0.015817077830433846,
      "learning_rate": 5.180960000000001e-06,
      "loss": 0.1992,
      "step": 30120
    },
    {
      "epoch": 0.96416,
      "grad_norm": 0.012496077455580235,
      "learning_rate": 5.17936e-06,
      "loss": 0.2227,
      "step": 30130
    },
    {
      "epoch": 0.96448,
      "grad_norm": 0.03848574683070183,
      "learning_rate": 5.17776e-06,
      "loss": 0.202,
      "step": 30140
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.020585063844919205,
      "learning_rate": 5.17616e-06,
      "loss": 0.2072,
      "step": 30150
    },
    {
      "epoch": 0.96512,
      "grad_norm": 0.10918453335762024,
      "learning_rate": 5.174560000000001e-06,
      "loss": 0.1993,
      "step": 30160
    },
    {
      "epoch": 0.96544,
      "grad_norm": 0.043211620301008224,
      "learning_rate": 5.17296e-06,
      "loss": 0.2031,
      "step": 30170
    },
    {
      "epoch": 0.96576,
      "grad_norm": 0.03674152120947838,
      "learning_rate": 5.171360000000001e-06,
      "loss": 0.1994,
      "step": 30180
    },
    {
      "epoch": 0.96608,
      "grad_norm": 0.03623561933636665,
      "learning_rate": 5.169760000000001e-06,
      "loss": 0.1994,
      "step": 30190
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.019863635301589966,
      "learning_rate": 5.168160000000001e-06,
      "loss": 0.1992,
      "step": 30200
    },
    {
      "epoch": 0.96672,
      "grad_norm": 0.019073033705353737,
      "learning_rate": 5.16656e-06,
      "loss": 0.2023,
      "step": 30210
    },
    {
      "epoch": 0.96704,
      "grad_norm": 0.04113045334815979,
      "learning_rate": 5.16496e-06,
      "loss": 0.1993,
      "step": 30220
    },
    {
      "epoch": 0.96736,
      "grad_norm": 0.08058803528547287,
      "learning_rate": 5.163360000000001e-06,
      "loss": 0.1995,
      "step": 30230
    },
    {
      "epoch": 0.96768,
      "grad_norm": 0.023234989494085312,
      "learning_rate": 5.16176e-06,
      "loss": 0.1992,
      "step": 30240
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.01583028957247734,
      "learning_rate": 5.160160000000001e-06,
      "loss": 0.1995,
      "step": 30250
    },
    {
      "epoch": 0.96832,
      "grad_norm": 0.022932009771466255,
      "learning_rate": 5.158560000000001e-06,
      "loss": 0.1992,
      "step": 30260
    },
    {
      "epoch": 0.96864,
      "grad_norm": 0.027406509965658188,
      "learning_rate": 5.1569600000000005e-06,
      "loss": 0.2166,
      "step": 30270
    },
    {
      "epoch": 0.96896,
      "grad_norm": 0.47271257638931274,
      "learning_rate": 5.15536e-06,
      "loss": 0.2022,
      "step": 30280
    },
    {
      "epoch": 0.96928,
      "grad_norm": 0.18100662529468536,
      "learning_rate": 5.15376e-06,
      "loss": 0.2039,
      "step": 30290
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.7513035535812378,
      "learning_rate": 5.15216e-06,
      "loss": 0.2158,
      "step": 30300
    },
    {
      "epoch": 0.96992,
      "grad_norm": 0.05158410593867302,
      "learning_rate": 5.15056e-06,
      "loss": 0.2207,
      "step": 30310
    },
    {
      "epoch": 0.97024,
      "grad_norm": 0.735889732837677,
      "learning_rate": 5.148960000000001e-06,
      "loss": 0.2033,
      "step": 30320
    },
    {
      "epoch": 0.97056,
      "grad_norm": 0.04074852913618088,
      "learning_rate": 5.14736e-06,
      "loss": 0.1991,
      "step": 30330
    },
    {
      "epoch": 0.97088,
      "grad_norm": 0.03352775424718857,
      "learning_rate": 5.1457600000000005e-06,
      "loss": 0.1992,
      "step": 30340
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.9349836111068726,
      "learning_rate": 5.14416e-06,
      "loss": 0.2087,
      "step": 30350
    },
    {
      "epoch": 0.97152,
      "grad_norm": 0.019951187074184418,
      "learning_rate": 5.142560000000001e-06,
      "loss": 0.2053,
      "step": 30360
    },
    {
      "epoch": 0.97184,
      "grad_norm": 0.010865798220038414,
      "learning_rate": 5.14096e-06,
      "loss": 0.1991,
      "step": 30370
    },
    {
      "epoch": 0.97216,
      "grad_norm": 0.025968892499804497,
      "learning_rate": 5.13936e-06,
      "loss": 0.1992,
      "step": 30380
    },
    {
      "epoch": 0.97248,
      "grad_norm": 0.028891056776046753,
      "learning_rate": 5.137760000000001e-06,
      "loss": 0.2011,
      "step": 30390
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.018558954820036888,
      "learning_rate": 5.13616e-06,
      "loss": 0.1994,
      "step": 30400
    },
    {
      "epoch": 0.97312,
      "grad_norm": 0.016358816996216774,
      "learning_rate": 5.1345600000000005e-06,
      "loss": 0.1996,
      "step": 30410
    },
    {
      "epoch": 0.97344,
      "grad_norm": 0.023772474378347397,
      "learning_rate": 5.13296e-06,
      "loss": 0.2143,
      "step": 30420
    },
    {
      "epoch": 0.97376,
      "grad_norm": 0.9837953448295593,
      "learning_rate": 5.131360000000001e-06,
      "loss": 0.2143,
      "step": 30430
    },
    {
      "epoch": 0.97408,
      "grad_norm": 0.03327558562159538,
      "learning_rate": 5.12976e-06,
      "loss": 0.1994,
      "step": 30440
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.025620941072702408,
      "learning_rate": 5.128160000000001e-06,
      "loss": 0.1999,
      "step": 30450
    },
    {
      "epoch": 0.97472,
      "grad_norm": 0.3252090811729431,
      "learning_rate": 5.126560000000001e-06,
      "loss": 0.2005,
      "step": 30460
    },
    {
      "epoch": 0.97504,
      "grad_norm": 0.012670744210481644,
      "learning_rate": 5.12496e-06,
      "loss": 0.1992,
      "step": 30470
    },
    {
      "epoch": 0.97536,
      "grad_norm": 0.03144469112157822,
      "learning_rate": 5.1233600000000005e-06,
      "loss": 0.1991,
      "step": 30480
    },
    {
      "epoch": 0.97568,
      "grad_norm": 0.13245052099227905,
      "learning_rate": 5.12176e-06,
      "loss": 0.1997,
      "step": 30490
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.03333957865834236,
      "learning_rate": 5.12016e-06,
      "loss": 0.1996,
      "step": 30500
    },
    {
      "epoch": 0.97632,
      "grad_norm": 0.03440805524587631,
      "learning_rate": 5.11856e-06,
      "loss": 0.2139,
      "step": 30510
    },
    {
      "epoch": 0.97664,
      "grad_norm": 0.02244017831981182,
      "learning_rate": 5.116960000000001e-06,
      "loss": 0.2172,
      "step": 30520
    },
    {
      "epoch": 0.97696,
      "grad_norm": 0.09667948633432388,
      "learning_rate": 5.11536e-06,
      "loss": 0.1998,
      "step": 30530
    },
    {
      "epoch": 0.97728,
      "grad_norm": 0.02258959226310253,
      "learning_rate": 5.113760000000001e-06,
      "loss": 0.1996,
      "step": 30540
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.030411019921302795,
      "learning_rate": 5.1121600000000005e-06,
      "loss": 0.2026,
      "step": 30550
    },
    {
      "epoch": 0.97792,
      "grad_norm": 0.07078303396701813,
      "learning_rate": 5.1105599999999996e-06,
      "loss": 0.214,
      "step": 30560
    },
    {
      "epoch": 0.97824,
      "grad_norm": 0.011831289157271385,
      "learning_rate": 5.10896e-06,
      "loss": 0.1994,
      "step": 30570
    },
    {
      "epoch": 0.97856,
      "grad_norm": 0.07143493741750717,
      "learning_rate": 5.10736e-06,
      "loss": 0.1994,
      "step": 30580
    },
    {
      "epoch": 0.97888,
      "grad_norm": 0.029937105253338814,
      "learning_rate": 5.105760000000001e-06,
      "loss": 0.2003,
      "step": 30590
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.019208602607250214,
      "learning_rate": 5.10416e-06,
      "loss": 0.1994,
      "step": 30600
    },
    {
      "epoch": 0.97952,
      "grad_norm": 0.023069145157933235,
      "learning_rate": 5.102560000000001e-06,
      "loss": 0.1993,
      "step": 30610
    },
    {
      "epoch": 0.97984,
      "grad_norm": 0.02742445282638073,
      "learning_rate": 5.1009600000000005e-06,
      "loss": 0.1995,
      "step": 30620
    },
    {
      "epoch": 0.98016,
      "grad_norm": 0.028643572703003883,
      "learning_rate": 5.0993599999999996e-06,
      "loss": 0.2158,
      "step": 30630
    },
    {
      "epoch": 0.98048,
      "grad_norm": 0.016015376895666122,
      "learning_rate": 5.09776e-06,
      "loss": 0.1994,
      "step": 30640
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.04541875794529915,
      "learning_rate": 5.09616e-06,
      "loss": 0.2011,
      "step": 30650
    },
    {
      "epoch": 0.98112,
      "grad_norm": 1.9668281078338623,
      "learning_rate": 5.094560000000001e-06,
      "loss": 0.2052,
      "step": 30660
    },
    {
      "epoch": 0.98144,
      "grad_norm": 0.011173469945788383,
      "learning_rate": 5.09296e-06,
      "loss": 0.1993,
      "step": 30670
    },
    {
      "epoch": 0.98176,
      "grad_norm": 0.036619432270526886,
      "learning_rate": 5.091360000000001e-06,
      "loss": 0.1991,
      "step": 30680
    },
    {
      "epoch": 0.98208,
      "grad_norm": 0.026014400646090508,
      "learning_rate": 5.0897600000000005e-06,
      "loss": 0.1991,
      "step": 30690
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.04821695759892464,
      "learning_rate": 5.088160000000001e-06,
      "loss": 0.1993,
      "step": 30700
    },
    {
      "epoch": 0.98272,
      "grad_norm": 0.05459804832935333,
      "learning_rate": 5.08656e-06,
      "loss": 0.1995,
      "step": 30710
    },
    {
      "epoch": 0.98304,
      "grad_norm": 0.024744704365730286,
      "learning_rate": 5.08496e-06,
      "loss": 0.2005,
      "step": 30720
    },
    {
      "epoch": 0.98336,
      "grad_norm": 0.017825180664658546,
      "learning_rate": 5.083360000000001e-06,
      "loss": 0.2068,
      "step": 30730
    },
    {
      "epoch": 0.98368,
      "grad_norm": 1.1999117136001587,
      "learning_rate": 5.08176e-06,
      "loss": 0.2322,
      "step": 30740
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.03104804828763008,
      "learning_rate": 5.080160000000001e-06,
      "loss": 0.199,
      "step": 30750
    },
    {
      "epoch": 0.98432,
      "grad_norm": 0.08460607379674911,
      "learning_rate": 5.0785600000000006e-06,
      "loss": 0.2038,
      "step": 30760
    },
    {
      "epoch": 0.98464,
      "grad_norm": 0.018693679943680763,
      "learning_rate": 5.0769600000000004e-06,
      "loss": 0.1992,
      "step": 30770
    },
    {
      "epoch": 0.98496,
      "grad_norm": 2.114356279373169,
      "learning_rate": 5.07536e-06,
      "loss": 0.2081,
      "step": 30780
    },
    {
      "epoch": 0.98528,
      "grad_norm": 0.01238960213959217,
      "learning_rate": 5.073760000000001e-06,
      "loss": 0.1992,
      "step": 30790
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.015018030069768429,
      "learning_rate": 5.07216e-06,
      "loss": 0.2008,
      "step": 30800
    },
    {
      "epoch": 0.98592,
      "grad_norm": 0.045958470553159714,
      "learning_rate": 5.07056e-06,
      "loss": 0.1991,
      "step": 30810
    },
    {
      "epoch": 0.98624,
      "grad_norm": 0.018760310485959053,
      "learning_rate": 5.068960000000001e-06,
      "loss": 0.2001,
      "step": 30820
    },
    {
      "epoch": 0.98656,
      "grad_norm": 0.1455559879541397,
      "learning_rate": 5.06736e-06,
      "loss": 0.2143,
      "step": 30830
    },
    {
      "epoch": 0.98688,
      "grad_norm": 0.019397636875510216,
      "learning_rate": 5.0657600000000004e-06,
      "loss": 0.1993,
      "step": 30840
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.03805452957749367,
      "learning_rate": 5.06416e-06,
      "loss": 0.2111,
      "step": 30850
    },
    {
      "epoch": 0.98752,
      "grad_norm": 0.014503126963973045,
      "learning_rate": 5.062560000000001e-06,
      "loss": 0.1993,
      "step": 30860
    },
    {
      "epoch": 0.98784,
      "grad_norm": 0.022374192252755165,
      "learning_rate": 5.06096e-06,
      "loss": 0.1993,
      "step": 30870
    },
    {
      "epoch": 0.98816,
      "grad_norm": 0.028503209352493286,
      "learning_rate": 5.059360000000001e-06,
      "loss": 0.2003,
      "step": 30880
    },
    {
      "epoch": 0.98848,
      "grad_norm": 0.7422168254852295,
      "learning_rate": 5.057760000000001e-06,
      "loss": 0.2156,
      "step": 30890
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.03364274650812149,
      "learning_rate": 5.05616e-06,
      "loss": 0.2001,
      "step": 30900
    },
    {
      "epoch": 0.98912,
      "grad_norm": 0.013522057794034481,
      "learning_rate": 5.0545600000000004e-06,
      "loss": 0.1993,
      "step": 30910
    },
    {
      "epoch": 0.98944,
      "grad_norm": 0.013358786702156067,
      "learning_rate": 5.05296e-06,
      "loss": 0.1993,
      "step": 30920
    },
    {
      "epoch": 0.98976,
      "grad_norm": 0.027910972014069557,
      "learning_rate": 5.051360000000001e-06,
      "loss": 0.1994,
      "step": 30930
    },
    {
      "epoch": 0.99008,
      "grad_norm": 0.050978172570466995,
      "learning_rate": 5.04976e-06,
      "loss": 0.2025,
      "step": 30940
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.044876549392938614,
      "learning_rate": 5.048160000000001e-06,
      "loss": 0.1993,
      "step": 30950
    },
    {
      "epoch": 0.99072,
      "grad_norm": 0.03664335235953331,
      "learning_rate": 5.046560000000001e-06,
      "loss": 0.1998,
      "step": 30960
    },
    {
      "epoch": 0.99104,
      "grad_norm": 0.047729212790727615,
      "learning_rate": 5.0449600000000006e-06,
      "loss": 0.2002,
      "step": 30970
    },
    {
      "epoch": 0.99136,
      "grad_norm": 0.03410085290670395,
      "learning_rate": 5.0433600000000004e-06,
      "loss": 0.2001,
      "step": 30980
    },
    {
      "epoch": 0.99168,
      "grad_norm": 0.027900727465748787,
      "learning_rate": 5.04176e-06,
      "loss": 0.1993,
      "step": 30990
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.015633387491106987,
      "learning_rate": 5.04016e-06,
      "loss": 0.2094,
      "step": 31000
    },
    {
      "epoch": 0.992,
      "eval_runtime": 50.5947,
      "eval_samples_per_second": 197.649,
      "eval_steps_per_second": 12.353,
      "step": 31000
    },
    {
      "epoch": 0.99232,
      "grad_norm": 0.019098149612545967,
      "learning_rate": 5.03856e-06,
      "loss": 0.1992,
      "step": 31010
    },
    {
      "epoch": 0.99264,
      "grad_norm": 0.02087913081049919,
      "learning_rate": 5.036960000000001e-06,
      "loss": 0.1992,
      "step": 31020
    },
    {
      "epoch": 0.99296,
      "grad_norm": 0.020794125273823738,
      "learning_rate": 5.03536e-06,
      "loss": 0.1993,
      "step": 31030
    },
    {
      "epoch": 0.99328,
      "grad_norm": 0.02605079859495163,
      "learning_rate": 5.0337600000000006e-06,
      "loss": 0.2153,
      "step": 31040
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.01589018478989601,
      "learning_rate": 5.0321600000000005e-06,
      "loss": 0.199,
      "step": 31050
    },
    {
      "epoch": 0.99392,
      "grad_norm": 0.2042463719844818,
      "learning_rate": 5.0305599999999995e-06,
      "loss": 0.2001,
      "step": 31060
    },
    {
      "epoch": 0.99424,
      "grad_norm": 0.024571722373366356,
      "learning_rate": 5.02896e-06,
      "loss": 0.1991,
      "step": 31070
    },
    {
      "epoch": 0.99456,
      "grad_norm": 0.029017439112067223,
      "learning_rate": 5.02736e-06,
      "loss": 0.1997,
      "step": 31080
    },
    {
      "epoch": 0.99488,
      "grad_norm": 0.02787867747247219,
      "learning_rate": 5.025760000000001e-06,
      "loss": 0.207,
      "step": 31090
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.03552524000406265,
      "learning_rate": 5.02416e-06,
      "loss": 0.1993,
      "step": 31100
    },
    {
      "epoch": 0.99552,
      "grad_norm": 0.033240046352148056,
      "learning_rate": 5.022560000000001e-06,
      "loss": 0.1991,
      "step": 31110
    },
    {
      "epoch": 0.99584,
      "grad_norm": 0.024778097867965698,
      "learning_rate": 5.0209600000000005e-06,
      "loss": 0.2164,
      "step": 31120
    },
    {
      "epoch": 0.99616,
      "grad_norm": 0.043454937636852264,
      "learning_rate": 5.019360000000001e-06,
      "loss": 0.1998,
      "step": 31130
    },
    {
      "epoch": 0.99648,
      "grad_norm": 0.011904018931090832,
      "learning_rate": 5.01776e-06,
      "loss": 0.1991,
      "step": 31140
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.028498144820332527,
      "learning_rate": 5.01616e-06,
      "loss": 0.232,
      "step": 31150
    },
    {
      "epoch": 0.99712,
      "grad_norm": 0.055069420486688614,
      "learning_rate": 5.014560000000001e-06,
      "loss": 0.2088,
      "step": 31160
    },
    {
      "epoch": 0.99744,
      "grad_norm": 0.04503294825553894,
      "learning_rate": 5.01296e-06,
      "loss": 0.1994,
      "step": 31170
    },
    {
      "epoch": 0.99776,
      "grad_norm": 0.019504167139530182,
      "learning_rate": 5.011360000000001e-06,
      "loss": 0.2149,
      "step": 31180
    },
    {
      "epoch": 0.99808,
      "grad_norm": 0.027514491230249405,
      "learning_rate": 5.0097600000000005e-06,
      "loss": 0.1991,
      "step": 31190
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.04313771054148674,
      "learning_rate": 5.008160000000001e-06,
      "loss": 0.206,
      "step": 31200
    },
    {
      "epoch": 0.99872,
      "grad_norm": 0.04336336627602577,
      "learning_rate": 5.00656e-06,
      "loss": 0.2009,
      "step": 31210
    },
    {
      "epoch": 0.99904,
      "grad_norm": 0.016744768247008324,
      "learning_rate": 5.004960000000001e-06,
      "loss": 0.1993,
      "step": 31220
    },
    {
      "epoch": 0.99936,
      "grad_norm": 0.7810483574867249,
      "learning_rate": 5.003360000000001e-06,
      "loss": 0.2121,
      "step": 31230
    },
    {
      "epoch": 0.99968,
      "grad_norm": 0.05872904881834984,
      "learning_rate": 5.00176e-06,
      "loss": 0.1992,
      "step": 31240
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.01753871887922287,
      "learning_rate": 5.000160000000001e-06,
      "loss": 0.1993,
      "step": 31250
    },
    {
      "epoch": 1.00032,
      "grad_norm": 0.014070611447095871,
      "learning_rate": 4.9985600000000005e-06,
      "loss": 0.1992,
      "step": 31260
    },
    {
      "epoch": 1.00064,
      "grad_norm": 0.0830230638384819,
      "learning_rate": 4.99696e-06,
      "loss": 0.1995,
      "step": 31270
    },
    {
      "epoch": 1.00096,
      "grad_norm": 0.011586900800466537,
      "learning_rate": 4.99536e-06,
      "loss": 0.2121,
      "step": 31280
    },
    {
      "epoch": 1.00128,
      "grad_norm": 0.026879552751779556,
      "learning_rate": 4.99376e-06,
      "loss": 0.1994,
      "step": 31290
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.026479212567210197,
      "learning_rate": 4.99216e-06,
      "loss": 0.2118,
      "step": 31300
    },
    {
      "epoch": 1.00192,
      "grad_norm": 0.03528815880417824,
      "learning_rate": 4.990560000000001e-06,
      "loss": 0.1993,
      "step": 31310
    },
    {
      "epoch": 1.00224,
      "grad_norm": 0.021502558141946793,
      "learning_rate": 4.988960000000001e-06,
      "loss": 0.1996,
      "step": 31320
    },
    {
      "epoch": 1.00256,
      "grad_norm": 0.024136189371347427,
      "learning_rate": 4.9873600000000005e-06,
      "loss": 0.1995,
      "step": 31330
    },
    {
      "epoch": 1.00288,
      "grad_norm": 0.021644072607159615,
      "learning_rate": 4.98576e-06,
      "loss": 0.1994,
      "step": 31340
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.02952386811375618,
      "learning_rate": 4.984160000000001e-06,
      "loss": 0.2142,
      "step": 31350
    },
    {
      "epoch": 1.00352,
      "grad_norm": 0.05977969989180565,
      "learning_rate": 4.98256e-06,
      "loss": 0.1993,
      "step": 31360
    },
    {
      "epoch": 1.00384,
      "grad_norm": 0.06396596878767014,
      "learning_rate": 4.98096e-06,
      "loss": 0.1995,
      "step": 31370
    },
    {
      "epoch": 1.00416,
      "grad_norm": 1.1734741926193237,
      "learning_rate": 4.979360000000001e-06,
      "loss": 0.2114,
      "step": 31380
    },
    {
      "epoch": 1.00448,
      "grad_norm": 0.031116370111703873,
      "learning_rate": 4.977760000000001e-06,
      "loss": 0.1995,
      "step": 31390
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.028174126520752907,
      "learning_rate": 4.9761600000000005e-06,
      "loss": 0.1996,
      "step": 31400
    },
    {
      "epoch": 1.00512,
      "grad_norm": 0.04516126960515976,
      "learning_rate": 4.97456e-06,
      "loss": 0.1995,
      "step": 31410
    },
    {
      "epoch": 1.0054400000000001,
      "grad_norm": 0.04021988809108734,
      "learning_rate": 4.97296e-06,
      "loss": 0.208,
      "step": 31420
    },
    {
      "epoch": 1.00576,
      "grad_norm": 0.079427570104599,
      "learning_rate": 4.971360000000001e-06,
      "loss": 0.1993,
      "step": 31430
    },
    {
      "epoch": 1.00608,
      "grad_norm": 0.025789430364966393,
      "learning_rate": 4.96976e-06,
      "loss": 0.1993,
      "step": 31440
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.05687466263771057,
      "learning_rate": 4.96816e-06,
      "loss": 0.1996,
      "step": 31450
    },
    {
      "epoch": 1.00672,
      "grad_norm": 0.010882113128900528,
      "learning_rate": 4.966560000000001e-06,
      "loss": 0.1992,
      "step": 31460
    },
    {
      "epoch": 1.00704,
      "grad_norm": 0.020216431468725204,
      "learning_rate": 4.9649600000000005e-06,
      "loss": 0.1994,
      "step": 31470
    },
    {
      "epoch": 1.00736,
      "grad_norm": 0.025060512125492096,
      "learning_rate": 4.96336e-06,
      "loss": 0.1992,
      "step": 31480
    },
    {
      "epoch": 1.00768,
      "grad_norm": 0.028892580419778824,
      "learning_rate": 4.96176e-06,
      "loss": 0.214,
      "step": 31490
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.02520902454853058,
      "learning_rate": 4.96016e-06,
      "loss": 0.1994,
      "step": 31500
    },
    {
      "epoch": 1.00832,
      "grad_norm": 0.03504753112792969,
      "learning_rate": 4.958560000000001e-06,
      "loss": 0.1993,
      "step": 31510
    },
    {
      "epoch": 1.00864,
      "grad_norm": 0.03426429256796837,
      "learning_rate": 4.956960000000001e-06,
      "loss": 0.1993,
      "step": 31520
    },
    {
      "epoch": 1.00896,
      "grad_norm": 0.014909759163856506,
      "learning_rate": 4.95536e-06,
      "loss": 0.1994,
      "step": 31530
    },
    {
      "epoch": 1.00928,
      "grad_norm": 0.013122535310685635,
      "learning_rate": 4.9537600000000005e-06,
      "loss": 0.1993,
      "step": 31540
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.022993076592683792,
      "learning_rate": 4.95216e-06,
      "loss": 0.1992,
      "step": 31550
    },
    {
      "epoch": 1.00992,
      "grad_norm": 0.03326750919222832,
      "learning_rate": 4.95056e-06,
      "loss": 0.1993,
      "step": 31560
    },
    {
      "epoch": 1.01024,
      "grad_norm": 0.0154007812961936,
      "learning_rate": 4.94896e-06,
      "loss": 0.2075,
      "step": 31570
    },
    {
      "epoch": 1.01056,
      "grad_norm": 0.03316027671098709,
      "learning_rate": 4.947360000000001e-06,
      "loss": 0.1992,
      "step": 31580
    },
    {
      "epoch": 1.01088,
      "grad_norm": 0.02710777521133423,
      "learning_rate": 4.945760000000001e-06,
      "loss": 0.1991,
      "step": 31590
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.015439522452652454,
      "learning_rate": 4.944160000000001e-06,
      "loss": 0.1992,
      "step": 31600
    },
    {
      "epoch": 1.01152,
      "grad_norm": 0.02199770323932171,
      "learning_rate": 4.9425600000000005e-06,
      "loss": 0.1995,
      "step": 31610
    },
    {
      "epoch": 1.01184,
      "grad_norm": 0.01979878358542919,
      "learning_rate": 4.94096e-06,
      "loss": 0.1994,
      "step": 31620
    },
    {
      "epoch": 1.01216,
      "grad_norm": 0.020864231511950493,
      "learning_rate": 4.93936e-06,
      "loss": 0.2023,
      "step": 31630
    },
    {
      "epoch": 1.01248,
      "grad_norm": 0.02648662030696869,
      "learning_rate": 4.93776e-06,
      "loss": 0.2134,
      "step": 31640
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.027373919263482094,
      "learning_rate": 4.93616e-06,
      "loss": 0.1992,
      "step": 31650
    },
    {
      "epoch": 1.01312,
      "grad_norm": 0.014495695009827614,
      "learning_rate": 4.934560000000001e-06,
      "loss": 0.1993,
      "step": 31660
    },
    {
      "epoch": 1.01344,
      "grad_norm": 0.013625442050397396,
      "learning_rate": 4.932960000000001e-06,
      "loss": 0.1995,
      "step": 31670
    },
    {
      "epoch": 1.01376,
      "grad_norm": 0.018991323187947273,
      "learning_rate": 4.9313600000000005e-06,
      "loss": 0.1992,
      "step": 31680
    },
    {
      "epoch": 1.01408,
      "grad_norm": 0.013378320261836052,
      "learning_rate": 4.92976e-06,
      "loss": 0.2156,
      "step": 31690
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.30575263500213623,
      "learning_rate": 4.92816e-06,
      "loss": 0.1999,
      "step": 31700
    },
    {
      "epoch": 1.01472,
      "grad_norm": 0.03404068574309349,
      "learning_rate": 4.92656e-06,
      "loss": 0.1991,
      "step": 31710
    },
    {
      "epoch": 1.01504,
      "grad_norm": 0.024190660566091537,
      "learning_rate": 4.92496e-06,
      "loss": 0.2012,
      "step": 31720
    },
    {
      "epoch": 1.01536,
      "grad_norm": 0.14099755883216858,
      "learning_rate": 4.923360000000001e-06,
      "loss": 0.2135,
      "step": 31730
    },
    {
      "epoch": 1.01568,
      "grad_norm": 0.03709862753748894,
      "learning_rate": 4.921760000000001e-06,
      "loss": 0.2152,
      "step": 31740
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.015039882622659206,
      "learning_rate": 4.9201600000000005e-06,
      "loss": 0.1996,
      "step": 31750
    },
    {
      "epoch": 1.01632,
      "grad_norm": 0.06518881767988205,
      "learning_rate": 4.91856e-06,
      "loss": 0.1998,
      "step": 31760
    },
    {
      "epoch": 1.01664,
      "grad_norm": 0.0337810292840004,
      "learning_rate": 4.91696e-06,
      "loss": 0.2139,
      "step": 31770
    },
    {
      "epoch": 1.01696,
      "grad_norm": 0.03806628659367561,
      "learning_rate": 4.91536e-06,
      "loss": 0.1993,
      "step": 31780
    },
    {
      "epoch": 1.01728,
      "grad_norm": 0.03552806377410889,
      "learning_rate": 4.91376e-06,
      "loss": 0.2001,
      "step": 31790
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.028307167813181877,
      "learning_rate": 4.91216e-06,
      "loss": 0.1992,
      "step": 31800
    },
    {
      "epoch": 1.01792,
      "grad_norm": 0.04725207760930061,
      "learning_rate": 4.910560000000001e-06,
      "loss": 0.1991,
      "step": 31810
    },
    {
      "epoch": 1.01824,
      "grad_norm": 0.04542355611920357,
      "learning_rate": 4.9089600000000005e-06,
      "loss": 0.1991,
      "step": 31820
    },
    {
      "epoch": 1.01856,
      "grad_norm": 0.4893956184387207,
      "learning_rate": 4.90736e-06,
      "loss": 0.2142,
      "step": 31830
    },
    {
      "epoch": 1.01888,
      "grad_norm": 0.02579386718571186,
      "learning_rate": 4.90576e-06,
      "loss": 0.1993,
      "step": 31840
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.4434340000152588,
      "learning_rate": 4.904160000000001e-06,
      "loss": 0.2005,
      "step": 31850
    },
    {
      "epoch": 1.01952,
      "grad_norm": 0.04383592680096626,
      "learning_rate": 4.902560000000001e-06,
      "loss": 0.2246,
      "step": 31860
    },
    {
      "epoch": 1.01984,
      "grad_norm": 0.018718024715781212,
      "learning_rate": 4.90096e-06,
      "loss": 0.1994,
      "step": 31870
    },
    {
      "epoch": 1.02016,
      "grad_norm": 0.01594310626387596,
      "learning_rate": 4.899360000000001e-06,
      "loss": 0.2251,
      "step": 31880
    },
    {
      "epoch": 1.02048,
      "grad_norm": 0.029177682474255562,
      "learning_rate": 4.8977600000000005e-06,
      "loss": 0.205,
      "step": 31890
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.019427459686994553,
      "learning_rate": 4.89616e-06,
      "loss": 0.1998,
      "step": 31900
    },
    {
      "epoch": 1.02112,
      "grad_norm": 0.8588519096374512,
      "learning_rate": 4.89456e-06,
      "loss": 0.2122,
      "step": 31910
    },
    {
      "epoch": 1.02144,
      "grad_norm": 0.012798539362847805,
      "learning_rate": 4.89296e-06,
      "loss": 0.1994,
      "step": 31920
    },
    {
      "epoch": 1.02176,
      "grad_norm": 0.7808054089546204,
      "learning_rate": 4.891360000000001e-06,
      "loss": 0.2157,
      "step": 31930
    },
    {
      "epoch": 1.02208,
      "grad_norm": 0.01936386711895466,
      "learning_rate": 4.889760000000001e-06,
      "loss": 0.1994,
      "step": 31940
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.06647516787052155,
      "learning_rate": 4.88816e-06,
      "loss": 0.1993,
      "step": 31950
    },
    {
      "epoch": 1.02272,
      "grad_norm": 0.031620170921087265,
      "learning_rate": 4.8865600000000005e-06,
      "loss": 0.2136,
      "step": 31960
    },
    {
      "epoch": 1.02304,
      "grad_norm": 0.012839707545936108,
      "learning_rate": 4.88496e-06,
      "loss": 0.1992,
      "step": 31970
    },
    {
      "epoch": 1.02336,
      "grad_norm": 0.036175262182950974,
      "learning_rate": 4.88336e-06,
      "loss": 0.1991,
      "step": 31980
    },
    {
      "epoch": 1.02368,
      "grad_norm": 0.01938280276954174,
      "learning_rate": 4.88176e-06,
      "loss": 0.2004,
      "step": 31990
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.020497050136327744,
      "learning_rate": 4.88016e-06,
      "loss": 0.2188,
      "step": 32000
    },
    {
      "epoch": 1.024,
      "eval_runtime": 51.2352,
      "eval_samples_per_second": 195.178,
      "eval_steps_per_second": 12.199,
      "step": 32000
    },
    {
      "epoch": 1.02432,
      "grad_norm": 0.06108364462852478,
      "learning_rate": 4.878560000000001e-06,
      "loss": 0.1993,
      "step": 32010
    },
    {
      "epoch": 1.02464,
      "grad_norm": 0.039112068712711334,
      "learning_rate": 4.876960000000001e-06,
      "loss": 0.1992,
      "step": 32020
    },
    {
      "epoch": 1.02496,
      "grad_norm": 0.020043829455971718,
      "learning_rate": 4.8753600000000005e-06,
      "loss": 0.1991,
      "step": 32030
    },
    {
      "epoch": 1.02528,
      "grad_norm": 0.05886805057525635,
      "learning_rate": 4.87376e-06,
      "loss": 0.1997,
      "step": 32040
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.04053008556365967,
      "learning_rate": 4.87216e-06,
      "loss": 0.1992,
      "step": 32050
    },
    {
      "epoch": 1.02592,
      "grad_norm": 0.013123122043907642,
      "learning_rate": 4.87056e-06,
      "loss": 0.1998,
      "step": 32060
    },
    {
      "epoch": 1.02624,
      "grad_norm": 0.036774661391973495,
      "learning_rate": 4.86896e-06,
      "loss": 0.1993,
      "step": 32070
    },
    {
      "epoch": 1.02656,
      "grad_norm": 0.034815531224012375,
      "learning_rate": 4.867360000000001e-06,
      "loss": 0.1994,
      "step": 32080
    },
    {
      "epoch": 1.02688,
      "grad_norm": 0.018213532865047455,
      "learning_rate": 4.865760000000001e-06,
      "loss": 0.1991,
      "step": 32090
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.0331612229347229,
      "learning_rate": 4.8641600000000005e-06,
      "loss": 0.1993,
      "step": 32100
    },
    {
      "epoch": 1.02752,
      "grad_norm": 0.24768395721912384,
      "learning_rate": 4.86256e-06,
      "loss": 0.1996,
      "step": 32110
    },
    {
      "epoch": 1.02784,
      "grad_norm": 0.01589188724756241,
      "learning_rate": 4.86096e-06,
      "loss": 0.2266,
      "step": 32120
    },
    {
      "epoch": 1.02816,
      "grad_norm": 0.041767481714487076,
      "learning_rate": 4.85936e-06,
      "loss": 0.1991,
      "step": 32130
    },
    {
      "epoch": 1.02848,
      "grad_norm": 0.015288639813661575,
      "learning_rate": 4.85776e-06,
      "loss": 0.21,
      "step": 32140
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.013657297007739544,
      "learning_rate": 4.85616e-06,
      "loss": 0.1997,
      "step": 32150
    },
    {
      "epoch": 1.02912,
      "grad_norm": 0.7574141025543213,
      "learning_rate": 4.854560000000001e-06,
      "loss": 0.2163,
      "step": 32160
    },
    {
      "epoch": 1.02944,
      "grad_norm": 0.4998486638069153,
      "learning_rate": 4.8529600000000005e-06,
      "loss": 0.2009,
      "step": 32170
    },
    {
      "epoch": 1.02976,
      "grad_norm": 0.051256049424409866,
      "learning_rate": 4.85136e-06,
      "loss": 0.2101,
      "step": 32180
    },
    {
      "epoch": 1.03008,
      "grad_norm": 0.012220063246786594,
      "learning_rate": 4.84976e-06,
      "loss": 0.1991,
      "step": 32190
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.05183008685708046,
      "learning_rate": 4.848160000000001e-06,
      "loss": 0.2148,
      "step": 32200
    },
    {
      "epoch": 1.03072,
      "grad_norm": 0.035011518746614456,
      "learning_rate": 4.84656e-06,
      "loss": 0.1999,
      "step": 32210
    },
    {
      "epoch": 1.03104,
      "grad_norm": 0.039139050990343094,
      "learning_rate": 4.84496e-06,
      "loss": 0.2232,
      "step": 32220
    },
    {
      "epoch": 1.03136,
      "grad_norm": 0.02182900346815586,
      "learning_rate": 4.843360000000001e-06,
      "loss": 0.1993,
      "step": 32230
    },
    {
      "epoch": 1.03168,
      "grad_norm": 0.040486566722393036,
      "learning_rate": 4.8417600000000005e-06,
      "loss": 0.1993,
      "step": 32240
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.03886573761701584,
      "learning_rate": 4.8401600000000004e-06,
      "loss": 0.1998,
      "step": 32250
    },
    {
      "epoch": 1.03232,
      "grad_norm": 0.03389926254749298,
      "learning_rate": 4.83856e-06,
      "loss": 0.1995,
      "step": 32260
    },
    {
      "epoch": 1.03264,
      "grad_norm": 0.02122286520898342,
      "learning_rate": 4.83696e-06,
      "loss": 0.1993,
      "step": 32270
    },
    {
      "epoch": 1.03296,
      "grad_norm": 0.018584633246064186,
      "learning_rate": 4.835360000000001e-06,
      "loss": 0.1992,
      "step": 32280
    },
    {
      "epoch": 1.03328,
      "grad_norm": 0.03537820279598236,
      "learning_rate": 4.83376e-06,
      "loss": 0.1996,
      "step": 32290
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.032183822244405746,
      "learning_rate": 4.83216e-06,
      "loss": 0.2001,
      "step": 32300
    },
    {
      "epoch": 1.03392,
      "grad_norm": 0.03427018225193024,
      "learning_rate": 4.8305600000000006e-06,
      "loss": 0.1993,
      "step": 32310
    },
    {
      "epoch": 1.03424,
      "grad_norm": 0.013934711925685406,
      "learning_rate": 4.8289600000000004e-06,
      "loss": 0.1991,
      "step": 32320
    },
    {
      "epoch": 1.03456,
      "grad_norm": 0.03366946429014206,
      "learning_rate": 4.82736e-06,
      "loss": 0.2153,
      "step": 32330
    },
    {
      "epoch": 1.03488,
      "grad_norm": 0.02834559604525566,
      "learning_rate": 4.82576e-06,
      "loss": 0.2027,
      "step": 32340
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.0303729847073555,
      "learning_rate": 4.824160000000001e-06,
      "loss": 0.2125,
      "step": 32350
    },
    {
      "epoch": 1.03552,
      "grad_norm": 0.04130072891712189,
      "learning_rate": 4.822560000000001e-06,
      "loss": 0.1993,
      "step": 32360
    },
    {
      "epoch": 1.03584,
      "grad_norm": 0.03721422702074051,
      "learning_rate": 4.820960000000001e-06,
      "loss": 0.1992,
      "step": 32370
    },
    {
      "epoch": 1.03616,
      "grad_norm": 0.024287335574626923,
      "learning_rate": 4.8193600000000006e-06,
      "loss": 0.215,
      "step": 32380
    },
    {
      "epoch": 1.03648,
      "grad_norm": 0.04741040989756584,
      "learning_rate": 4.8177600000000004e-06,
      "loss": 0.2116,
      "step": 32390
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.0375499427318573,
      "learning_rate": 4.81616e-06,
      "loss": 0.1994,
      "step": 32400
    },
    {
      "epoch": 1.03712,
      "grad_norm": 0.026906097307801247,
      "learning_rate": 4.81456e-06,
      "loss": 0.2003,
      "step": 32410
    },
    {
      "epoch": 1.03744,
      "grad_norm": 0.013142489828169346,
      "learning_rate": 4.81296e-06,
      "loss": 0.1991,
      "step": 32420
    },
    {
      "epoch": 1.03776,
      "grad_norm": 0.021398363634943962,
      "learning_rate": 4.811360000000001e-06,
      "loss": 0.1991,
      "step": 32430
    },
    {
      "epoch": 1.03808,
      "grad_norm": 0.15950101613998413,
      "learning_rate": 4.809760000000001e-06,
      "loss": 0.1996,
      "step": 32440
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.037059150636196136,
      "learning_rate": 4.8081600000000006e-06,
      "loss": 0.1991,
      "step": 32450
    },
    {
      "epoch": 1.03872,
      "grad_norm": 0.03363920375704765,
      "learning_rate": 4.8065600000000004e-06,
      "loss": 0.1992,
      "step": 32460
    },
    {
      "epoch": 1.03904,
      "grad_norm": 0.01967969350516796,
      "learning_rate": 4.80496e-06,
      "loss": 0.1991,
      "step": 32470
    },
    {
      "epoch": 1.03936,
      "grad_norm": 0.029652604833245277,
      "learning_rate": 4.80336e-06,
      "loss": 0.2071,
      "step": 32480
    },
    {
      "epoch": 1.03968,
      "grad_norm": 0.01111447624862194,
      "learning_rate": 4.80176e-06,
      "loss": 0.2114,
      "step": 32490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.03466162458062172,
      "learning_rate": 4.80016e-06,
      "loss": 0.1993,
      "step": 32500
    },
    {
      "epoch": 1.04032,
      "grad_norm": 0.02376893162727356,
      "learning_rate": 4.798560000000001e-06,
      "loss": 0.2002,
      "step": 32510
    },
    {
      "epoch": 1.04064,
      "grad_norm": 0.048600200563669205,
      "learning_rate": 4.7969600000000006e-06,
      "loss": 0.2149,
      "step": 32520
    },
    {
      "epoch": 1.04096,
      "grad_norm": 0.008899932727217674,
      "learning_rate": 4.7953600000000005e-06,
      "loss": 0.1992,
      "step": 32530
    },
    {
      "epoch": 1.04128,
      "grad_norm": 0.02879578247666359,
      "learning_rate": 4.79376e-06,
      "loss": 0.2025,
      "step": 32540
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.029547078534960747,
      "learning_rate": 4.79216e-06,
      "loss": 0.2,
      "step": 32550
    },
    {
      "epoch": 1.04192,
      "grad_norm": 0.014861473813652992,
      "learning_rate": 4.79056e-06,
      "loss": 0.1992,
      "step": 32560
    },
    {
      "epoch": 1.04224,
      "grad_norm": 0.020466985180974007,
      "learning_rate": 4.78896e-06,
      "loss": 0.2053,
      "step": 32570
    },
    {
      "epoch": 1.04256,
      "grad_norm": 0.02261357381939888,
      "learning_rate": 4.787360000000001e-06,
      "loss": 0.2159,
      "step": 32580
    },
    {
      "epoch": 1.04288,
      "grad_norm": 0.010060399770736694,
      "learning_rate": 4.7857600000000006e-06,
      "loss": 0.199,
      "step": 32590
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.018254557624459267,
      "learning_rate": 4.7841600000000005e-06,
      "loss": 0.1996,
      "step": 32600
    },
    {
      "epoch": 1.04352,
      "grad_norm": 0.07548688352108002,
      "learning_rate": 4.78256e-06,
      "loss": 0.1993,
      "step": 32610
    },
    {
      "epoch": 1.04384,
      "grad_norm": 0.012933749705553055,
      "learning_rate": 4.78096e-06,
      "loss": 0.1992,
      "step": 32620
    },
    {
      "epoch": 1.04416,
      "grad_norm": 0.023987779393792152,
      "learning_rate": 4.779360000000001e-06,
      "loss": 0.1992,
      "step": 32630
    },
    {
      "epoch": 1.04448,
      "grad_norm": 0.013336315751075745,
      "learning_rate": 4.77776e-06,
      "loss": 0.2111,
      "step": 32640
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.04120166227221489,
      "learning_rate": 4.77616e-06,
      "loss": 0.2013,
      "step": 32650
    },
    {
      "epoch": 1.04512,
      "grad_norm": 0.027832627296447754,
      "learning_rate": 4.774560000000001e-06,
      "loss": 0.1991,
      "step": 32660
    },
    {
      "epoch": 1.04544,
      "grad_norm": 0.023598620668053627,
      "learning_rate": 4.7729600000000005e-06,
      "loss": 0.1994,
      "step": 32670
    },
    {
      "epoch": 1.04576,
      "grad_norm": 0.03487933427095413,
      "learning_rate": 4.77136e-06,
      "loss": 0.2153,
      "step": 32680
    },
    {
      "epoch": 1.04608,
      "grad_norm": 0.01797386258840561,
      "learning_rate": 4.76976e-06,
      "loss": 0.199,
      "step": 32690
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.04588507488369942,
      "learning_rate": 4.768160000000001e-06,
      "loss": 0.2212,
      "step": 32700
    },
    {
      "epoch": 1.04672,
      "grad_norm": 0.017979726195335388,
      "learning_rate": 4.766560000000001e-06,
      "loss": 0.1999,
      "step": 32710
    },
    {
      "epoch": 1.04704,
      "grad_norm": 0.048802584409713745,
      "learning_rate": 4.76496e-06,
      "loss": 0.1991,
      "step": 32720
    },
    {
      "epoch": 1.04736,
      "grad_norm": 0.03259841725230217,
      "learning_rate": 4.763360000000001e-06,
      "loss": 0.1992,
      "step": 32730
    },
    {
      "epoch": 1.04768,
      "grad_norm": 0.0394061841070652,
      "learning_rate": 4.7617600000000005e-06,
      "loss": 0.1996,
      "step": 32740
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.05084461718797684,
      "learning_rate": 4.76016e-06,
      "loss": 0.2238,
      "step": 32750
    },
    {
      "epoch": 1.04832,
      "grad_norm": 0.05575885996222496,
      "learning_rate": 4.75856e-06,
      "loss": 0.1994,
      "step": 32760
    },
    {
      "epoch": 1.04864,
      "grad_norm": 0.04281768575310707,
      "learning_rate": 4.75696e-06,
      "loss": 0.1992,
      "step": 32770
    },
    {
      "epoch": 1.04896,
      "grad_norm": 0.03245614469051361,
      "learning_rate": 4.755360000000001e-06,
      "loss": 0.2156,
      "step": 32780
    },
    {
      "epoch": 1.04928,
      "grad_norm": 0.048143014311790466,
      "learning_rate": 4.753760000000001e-06,
      "loss": 0.1994,
      "step": 32790
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.08008193224668503,
      "learning_rate": 4.752160000000001e-06,
      "loss": 0.1993,
      "step": 32800
    },
    {
      "epoch": 1.04992,
      "grad_norm": 0.026975838467478752,
      "learning_rate": 4.7505600000000005e-06,
      "loss": 0.1992,
      "step": 32810
    },
    {
      "epoch": 1.05024,
      "grad_norm": 0.03143089637160301,
      "learning_rate": 4.74896e-06,
      "loss": 0.2056,
      "step": 32820
    },
    {
      "epoch": 1.05056,
      "grad_norm": 0.020360587164759636,
      "learning_rate": 4.74736e-06,
      "loss": 0.1993,
      "step": 32830
    },
    {
      "epoch": 1.05088,
      "grad_norm": 0.014613554812967777,
      "learning_rate": 4.74576e-06,
      "loss": 0.2112,
      "step": 32840
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.01740410551428795,
      "learning_rate": 4.744160000000001e-06,
      "loss": 0.2006,
      "step": 32850
    },
    {
      "epoch": 1.05152,
      "grad_norm": 0.02976141683757305,
      "learning_rate": 4.742560000000001e-06,
      "loss": 0.2148,
      "step": 32860
    },
    {
      "epoch": 1.0518399999999999,
      "grad_norm": 0.02360934391617775,
      "learning_rate": 4.740960000000001e-06,
      "loss": 0.1992,
      "step": 32870
    },
    {
      "epoch": 1.05216,
      "grad_norm": 0.048037223517894745,
      "learning_rate": 4.7393600000000005e-06,
      "loss": 0.1993,
      "step": 32880
    },
    {
      "epoch": 1.05248,
      "grad_norm": 0.0321466438472271,
      "learning_rate": 4.73776e-06,
      "loss": 0.2247,
      "step": 32890
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.06584421545267105,
      "learning_rate": 4.73616e-06,
      "loss": 0.2123,
      "step": 32900
    },
    {
      "epoch": 1.05312,
      "grad_norm": 0.0456349216401577,
      "learning_rate": 4.73456e-06,
      "loss": 0.2126,
      "step": 32910
    },
    {
      "epoch": 1.05344,
      "grad_norm": 0.050702035427093506,
      "learning_rate": 4.73296e-06,
      "loss": 0.1997,
      "step": 32920
    },
    {
      "epoch": 1.05376,
      "grad_norm": 0.01553135272115469,
      "learning_rate": 4.731360000000001e-06,
      "loss": 0.199,
      "step": 32930
    },
    {
      "epoch": 1.05408,
      "grad_norm": 0.16797441244125366,
      "learning_rate": 4.729760000000001e-06,
      "loss": 0.1995,
      "step": 32940
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.05480509623885155,
      "learning_rate": 4.7281600000000005e-06,
      "loss": 0.2118,
      "step": 32950
    },
    {
      "epoch": 1.05472,
      "grad_norm": 0.03201976418495178,
      "learning_rate": 4.72656e-06,
      "loss": 0.1995,
      "step": 32960
    },
    {
      "epoch": 1.05504,
      "grad_norm": 0.02570940926671028,
      "learning_rate": 4.72496e-06,
      "loss": 0.1992,
      "step": 32970
    },
    {
      "epoch": 1.05536,
      "grad_norm": 0.17215575277805328,
      "learning_rate": 4.72336e-06,
      "loss": 0.1998,
      "step": 32980
    },
    {
      "epoch": 1.05568,
      "grad_norm": 0.04024467617273331,
      "learning_rate": 4.72176e-06,
      "loss": 0.1999,
      "step": 32990
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.024188917130231857,
      "learning_rate": 4.72016e-06,
      "loss": 0.1994,
      "step": 33000
    },
    {
      "epoch": 1.056,
      "eval_runtime": 50.3717,
      "eval_samples_per_second": 198.524,
      "eval_steps_per_second": 12.408,
      "step": 33000
    },
    {
      "epoch": 1.05632,
      "grad_norm": 0.017014015465974808,
      "learning_rate": 4.718560000000001e-06,
      "loss": 0.1995,
      "step": 33010
    },
    {
      "epoch": 1.05664,
      "grad_norm": 0.016602488234639168,
      "learning_rate": 4.7169600000000005e-06,
      "loss": 0.1994,
      "step": 33020
    },
    {
      "epoch": 1.05696,
      "grad_norm": 0.026733258739113808,
      "learning_rate": 4.71536e-06,
      "loss": 0.1993,
      "step": 33030
    },
    {
      "epoch": 1.05728,
      "grad_norm": 0.034115660935640335,
      "learning_rate": 4.71376e-06,
      "loss": 0.2017,
      "step": 33040
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.0262395478785038,
      "learning_rate": 4.712160000000001e-06,
      "loss": 0.2011,
      "step": 33050
    },
    {
      "epoch": 1.05792,
      "grad_norm": 0.04454268887639046,
      "learning_rate": 4.71056e-06,
      "loss": 0.1994,
      "step": 33060
    },
    {
      "epoch": 1.05824,
      "grad_norm": 0.042285703122615814,
      "learning_rate": 4.70896e-06,
      "loss": 0.2083,
      "step": 33070
    },
    {
      "epoch": 1.05856,
      "grad_norm": 1.1481070518493652,
      "learning_rate": 4.707360000000001e-06,
      "loss": 0.208,
      "step": 33080
    },
    {
      "epoch": 1.05888,
      "grad_norm": 0.022255705669522285,
      "learning_rate": 4.7057600000000005e-06,
      "loss": 0.1997,
      "step": 33090
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.026055756956338882,
      "learning_rate": 4.70416e-06,
      "loss": 0.2112,
      "step": 33100
    },
    {
      "epoch": 1.05952,
      "grad_norm": 0.02435915730893612,
      "learning_rate": 4.70256e-06,
      "loss": 0.1993,
      "step": 33110
    },
    {
      "epoch": 1.05984,
      "grad_norm": 0.017062660306692123,
      "learning_rate": 4.70096e-06,
      "loss": 0.2018,
      "step": 33120
    },
    {
      "epoch": 1.06016,
      "grad_norm": 0.01933608390390873,
      "learning_rate": 4.699360000000001e-06,
      "loss": 0.1993,
      "step": 33130
    },
    {
      "epoch": 1.06048,
      "grad_norm": 0.028852658346295357,
      "learning_rate": 4.697760000000001e-06,
      "loss": 0.1995,
      "step": 33140
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.019391268491744995,
      "learning_rate": 4.69616e-06,
      "loss": 0.2,
      "step": 33150
    },
    {
      "epoch": 1.06112,
      "grad_norm": 0.01309173833578825,
      "learning_rate": 4.6945600000000005e-06,
      "loss": 0.1997,
      "step": 33160
    },
    {
      "epoch": 1.06144,
      "grad_norm": 0.023977836593985558,
      "learning_rate": 4.69296e-06,
      "loss": 0.2141,
      "step": 33170
    },
    {
      "epoch": 1.06176,
      "grad_norm": 0.015342587605118752,
      "learning_rate": 4.69136e-06,
      "loss": 0.1992,
      "step": 33180
    },
    {
      "epoch": 1.06208,
      "grad_norm": 0.011690821498632431,
      "learning_rate": 4.68976e-06,
      "loss": 0.1994,
      "step": 33190
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.015469293110072613,
      "learning_rate": 4.688160000000001e-06,
      "loss": 0.2075,
      "step": 33200
    },
    {
      "epoch": 1.06272,
      "grad_norm": 0.021473998203873634,
      "learning_rate": 4.686560000000001e-06,
      "loss": 0.199,
      "step": 33210
    },
    {
      "epoch": 1.06304,
      "grad_norm": 0.0377337709069252,
      "learning_rate": 4.684960000000001e-06,
      "loss": 0.1997,
      "step": 33220
    },
    {
      "epoch": 1.06336,
      "grad_norm": 0.016019606962800026,
      "learning_rate": 4.6833600000000005e-06,
      "loss": 0.1992,
      "step": 33230
    },
    {
      "epoch": 1.06368,
      "grad_norm": 0.013030420057475567,
      "learning_rate": 4.68176e-06,
      "loss": 0.1992,
      "step": 33240
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.010481390170753002,
      "learning_rate": 4.68016e-06,
      "loss": 0.1995,
      "step": 33250
    },
    {
      "epoch": 1.06432,
      "grad_norm": 0.013519211672246456,
      "learning_rate": 4.67856e-06,
      "loss": 0.1992,
      "step": 33260
    },
    {
      "epoch": 1.06464,
      "grad_norm": 0.03351758420467377,
      "learning_rate": 4.67696e-06,
      "loss": 0.215,
      "step": 33270
    },
    {
      "epoch": 1.06496,
      "grad_norm": 0.028256071731448174,
      "learning_rate": 4.675360000000001e-06,
      "loss": 0.2142,
      "step": 33280
    },
    {
      "epoch": 1.06528,
      "grad_norm": 0.031081339344382286,
      "learning_rate": 4.673760000000001e-06,
      "loss": 0.1997,
      "step": 33290
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.02850634790956974,
      "learning_rate": 4.6721600000000005e-06,
      "loss": 0.1994,
      "step": 33300
    },
    {
      "epoch": 1.06592,
      "grad_norm": 0.016430482268333435,
      "learning_rate": 4.67056e-06,
      "loss": 0.1991,
      "step": 33310
    },
    {
      "epoch": 1.06624,
      "grad_norm": 0.017336774617433548,
      "learning_rate": 4.66896e-06,
      "loss": 0.1993,
      "step": 33320
    },
    {
      "epoch": 1.06656,
      "grad_norm": 0.041687123477458954,
      "learning_rate": 4.66736e-06,
      "loss": 0.1991,
      "step": 33330
    },
    {
      "epoch": 1.06688,
      "grad_norm": 0.029972130432724953,
      "learning_rate": 4.66576e-06,
      "loss": 0.2065,
      "step": 33340
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.0656588152050972,
      "learning_rate": 4.664160000000001e-06,
      "loss": 0.2134,
      "step": 33350
    },
    {
      "epoch": 1.06752,
      "grad_norm": 0.03426539897918701,
      "learning_rate": 4.662560000000001e-06,
      "loss": 0.1992,
      "step": 33360
    },
    {
      "epoch": 1.06784,
      "grad_norm": 0.01813720352947712,
      "learning_rate": 4.6609600000000005e-06,
      "loss": 0.2113,
      "step": 33370
    },
    {
      "epoch": 1.06816,
      "grad_norm": 0.010367704555392265,
      "learning_rate": 4.65936e-06,
      "loss": 0.1991,
      "step": 33380
    },
    {
      "epoch": 1.06848,
      "grad_norm": 0.03180820494890213,
      "learning_rate": 4.65776e-06,
      "loss": 0.208,
      "step": 33390
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.25913065671920776,
      "learning_rate": 4.65616e-06,
      "loss": 0.2094,
      "step": 33400
    },
    {
      "epoch": 1.06912,
      "grad_norm": 0.03893933817744255,
      "learning_rate": 4.65456e-06,
      "loss": 0.2162,
      "step": 33410
    },
    {
      "epoch": 1.06944,
      "grad_norm": 0.041671253740787506,
      "learning_rate": 4.65296e-06,
      "loss": 0.2057,
      "step": 33420
    },
    {
      "epoch": 1.06976,
      "grad_norm": 0.056029148399829865,
      "learning_rate": 4.651360000000001e-06,
      "loss": 0.1993,
      "step": 33430
    },
    {
      "epoch": 1.07008,
      "grad_norm": 0.05069413781166077,
      "learning_rate": 4.6497600000000005e-06,
      "loss": 0.1997,
      "step": 33440
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.015943605452775955,
      "learning_rate": 4.64816e-06,
      "loss": 0.199,
      "step": 33450
    },
    {
      "epoch": 1.0707200000000001,
      "grad_norm": 0.03374697268009186,
      "learning_rate": 4.64656e-06,
      "loss": 0.1991,
      "step": 33460
    },
    {
      "epoch": 1.07104,
      "grad_norm": 0.013758617453277111,
      "learning_rate": 4.64496e-06,
      "loss": 0.2026,
      "step": 33470
    },
    {
      "epoch": 1.07136,
      "grad_norm": 0.012325144372880459,
      "learning_rate": 4.643360000000001e-06,
      "loss": 0.1992,
      "step": 33480
    },
    {
      "epoch": 1.07168,
      "grad_norm": 0.087899349629879,
      "learning_rate": 4.64176e-06,
      "loss": 0.2133,
      "step": 33490
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.0220149215310812,
      "learning_rate": 4.64016e-06,
      "loss": 0.222,
      "step": 33500
    },
    {
      "epoch": 1.07232,
      "grad_norm": 0.04967626556754112,
      "learning_rate": 4.6385600000000005e-06,
      "loss": 0.2002,
      "step": 33510
    },
    {
      "epoch": 1.07264,
      "grad_norm": 0.012956504710018635,
      "learning_rate": 4.63696e-06,
      "loss": 0.1993,
      "step": 33520
    },
    {
      "epoch": 1.07296,
      "grad_norm": 0.018982822075486183,
      "learning_rate": 4.63536e-06,
      "loss": 0.1992,
      "step": 33530
    },
    {
      "epoch": 1.07328,
      "grad_norm": 0.014831487089395523,
      "learning_rate": 4.63376e-06,
      "loss": 0.1993,
      "step": 33540
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.10952280461788177,
      "learning_rate": 4.632160000000001e-06,
      "loss": 0.2041,
      "step": 33550
    },
    {
      "epoch": 1.07392,
      "grad_norm": 0.7919520139694214,
      "learning_rate": 4.630560000000001e-06,
      "loss": 0.2157,
      "step": 33560
    },
    {
      "epoch": 1.07424,
      "grad_norm": 0.5044266581535339,
      "learning_rate": 4.62896e-06,
      "loss": 0.1998,
      "step": 33570
    },
    {
      "epoch": 1.07456,
      "grad_norm": 0.015977634117007256,
      "learning_rate": 4.6273600000000005e-06,
      "loss": 0.1997,
      "step": 33580
    },
    {
      "epoch": 1.07488,
      "grad_norm": 0.2307710349559784,
      "learning_rate": 4.62576e-06,
      "loss": 0.1994,
      "step": 33590
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.07609494775533676,
      "learning_rate": 4.62416e-06,
      "loss": 0.1993,
      "step": 33600
    },
    {
      "epoch": 1.07552,
      "grad_norm": 0.05107395350933075,
      "learning_rate": 4.62256e-06,
      "loss": 0.1993,
      "step": 33610
    },
    {
      "epoch": 1.07584,
      "grad_norm": 0.02231534570455551,
      "learning_rate": 4.62096e-06,
      "loss": 0.2311,
      "step": 33620
    },
    {
      "epoch": 1.07616,
      "grad_norm": 0.02354893460869789,
      "learning_rate": 4.619360000000001e-06,
      "loss": 0.1991,
      "step": 33630
    },
    {
      "epoch": 1.07648,
      "grad_norm": 0.04099135845899582,
      "learning_rate": 4.617760000000001e-06,
      "loss": 0.1992,
      "step": 33640
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.04384347423911095,
      "learning_rate": 4.6161600000000005e-06,
      "loss": 0.1994,
      "step": 33650
    },
    {
      "epoch": 1.07712,
      "grad_norm": 0.044028740376234055,
      "learning_rate": 4.61456e-06,
      "loss": 0.1993,
      "step": 33660
    },
    {
      "epoch": 1.07744,
      "grad_norm": 1.3785921335220337,
      "learning_rate": 4.61296e-06,
      "loss": 0.2099,
      "step": 33670
    },
    {
      "epoch": 1.07776,
      "grad_norm": 0.026094788685441017,
      "learning_rate": 4.61136e-06,
      "loss": 0.1996,
      "step": 33680
    },
    {
      "epoch": 1.07808,
      "grad_norm": 0.023111611604690552,
      "learning_rate": 4.60976e-06,
      "loss": 0.1993,
      "step": 33690
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.02611681818962097,
      "learning_rate": 4.608160000000001e-06,
      "loss": 0.1991,
      "step": 33700
    },
    {
      "epoch": 1.07872,
      "grad_norm": 0.01710207760334015,
      "learning_rate": 4.606560000000001e-06,
      "loss": 0.2004,
      "step": 33710
    },
    {
      "epoch": 1.07904,
      "grad_norm": 0.018522005528211594,
      "learning_rate": 4.6049600000000005e-06,
      "loss": 0.2009,
      "step": 33720
    },
    {
      "epoch": 1.07936,
      "grad_norm": 0.03660508617758751,
      "learning_rate": 4.6033600000000004e-06,
      "loss": 0.2134,
      "step": 33730
    },
    {
      "epoch": 1.07968,
      "grad_norm": 0.04394083842635155,
      "learning_rate": 4.60176e-06,
      "loss": 0.1992,
      "step": 33740
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.03477957844734192,
      "learning_rate": 4.60016e-06,
      "loss": 0.1992,
      "step": 33750
    },
    {
      "epoch": 1.08032,
      "grad_norm": 0.011083121411502361,
      "learning_rate": 4.59856e-06,
      "loss": 0.21,
      "step": 33760
    },
    {
      "epoch": 1.08064,
      "grad_norm": 0.02676447108387947,
      "learning_rate": 4.59696e-06,
      "loss": 0.234,
      "step": 33770
    },
    {
      "epoch": 1.08096,
      "grad_norm": 0.18573202192783356,
      "learning_rate": 4.595360000000001e-06,
      "loss": 0.1994,
      "step": 33780
    },
    {
      "epoch": 1.08128,
      "grad_norm": 0.04825025051832199,
      "learning_rate": 4.5937600000000006e-06,
      "loss": 0.1992,
      "step": 33790
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.016937559470534325,
      "learning_rate": 4.5921600000000004e-06,
      "loss": 0.1996,
      "step": 33800
    },
    {
      "epoch": 1.08192,
      "grad_norm": 0.04189375787973404,
      "learning_rate": 4.59056e-06,
      "loss": 0.1991,
      "step": 33810
    },
    {
      "epoch": 1.08224,
      "grad_norm": 0.03379682078957558,
      "learning_rate": 4.588960000000001e-06,
      "loss": 0.1992,
      "step": 33820
    },
    {
      "epoch": 1.08256,
      "grad_norm": 0.04877859354019165,
      "learning_rate": 4.58736e-06,
      "loss": 0.2013,
      "step": 33830
    },
    {
      "epoch": 1.08288,
      "grad_norm": 0.016941625624895096,
      "learning_rate": 4.58576e-06,
      "loss": 0.199,
      "step": 33840
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.07994159311056137,
      "learning_rate": 4.584160000000001e-06,
      "loss": 0.1996,
      "step": 33850
    },
    {
      "epoch": 1.08352,
      "grad_norm": 0.013790280558168888,
      "learning_rate": 4.5825600000000006e-06,
      "loss": 0.1991,
      "step": 33860
    },
    {
      "epoch": 1.08384,
      "grad_norm": 0.017514431849122047,
      "learning_rate": 4.5809600000000004e-06,
      "loss": 0.2,
      "step": 33870
    },
    {
      "epoch": 1.08416,
      "grad_norm": 0.017926398664712906,
      "learning_rate": 4.57936e-06,
      "loss": 0.1993,
      "step": 33880
    },
    {
      "epoch": 1.08448,
      "grad_norm": 0.020348723977804184,
      "learning_rate": 4.57776e-06,
      "loss": 0.2098,
      "step": 33890
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.014754352159798145,
      "learning_rate": 4.576160000000001e-06,
      "loss": 0.1993,
      "step": 33900
    },
    {
      "epoch": 1.08512,
      "grad_norm": 0.01883201114833355,
      "learning_rate": 4.57456e-06,
      "loss": 0.1998,
      "step": 33910
    },
    {
      "epoch": 1.08544,
      "grad_norm": 1.1349889039993286,
      "learning_rate": 4.57296e-06,
      "loss": 0.2072,
      "step": 33920
    },
    {
      "epoch": 1.08576,
      "grad_norm": 0.015750370919704437,
      "learning_rate": 4.5713600000000006e-06,
      "loss": 0.2159,
      "step": 33930
    },
    {
      "epoch": 1.08608,
      "grad_norm": 0.014232170768082142,
      "learning_rate": 4.5697600000000004e-06,
      "loss": 0.1993,
      "step": 33940
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.08601915836334229,
      "learning_rate": 4.56816e-06,
      "loss": 0.1992,
      "step": 33950
    },
    {
      "epoch": 1.08672,
      "grad_norm": 0.009069996885955334,
      "learning_rate": 4.56656e-06,
      "loss": 0.1992,
      "step": 33960
    },
    {
      "epoch": 1.08704,
      "grad_norm": 0.0217702928930521,
      "learning_rate": 4.56496e-06,
      "loss": 0.1992,
      "step": 33970
    },
    {
      "epoch": 1.0873599999999999,
      "grad_norm": 0.020750481635332108,
      "learning_rate": 4.563360000000001e-06,
      "loss": 0.2,
      "step": 33980
    },
    {
      "epoch": 1.08768,
      "grad_norm": 0.015177126042544842,
      "learning_rate": 4.561760000000001e-06,
      "loss": 0.1993,
      "step": 33990
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.024319307878613472,
      "learning_rate": 4.56016e-06,
      "loss": 0.1991,
      "step": 34000
    },
    {
      "epoch": 1.088,
      "eval_runtime": 49.7103,
      "eval_samples_per_second": 201.166,
      "eval_steps_per_second": 12.573,
      "step": 34000
    },
    {
      "epoch": 1.08832,
      "grad_norm": 0.023076482117176056,
      "learning_rate": 4.5585600000000004e-06,
      "loss": 0.1992,
      "step": 34010
    },
    {
      "epoch": 1.08864,
      "grad_norm": 0.024249130859971046,
      "learning_rate": 4.55696e-06,
      "loss": 0.199,
      "step": 34020
    },
    {
      "epoch": 1.08896,
      "grad_norm": 0.014547123573720455,
      "learning_rate": 4.55536e-06,
      "loss": 0.2019,
      "step": 34030
    },
    {
      "epoch": 1.08928,
      "grad_norm": 0.015550654381513596,
      "learning_rate": 4.55376e-06,
      "loss": 0.2111,
      "step": 34040
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.023108957335352898,
      "learning_rate": 4.552160000000001e-06,
      "loss": 0.1992,
      "step": 34050
    },
    {
      "epoch": 1.08992,
      "grad_norm": 0.022401034832000732,
      "learning_rate": 4.550560000000001e-06,
      "loss": 0.2093,
      "step": 34060
    },
    {
      "epoch": 1.09024,
      "grad_norm": 0.0425758883357048,
      "learning_rate": 4.5489600000000006e-06,
      "loss": 0.1995,
      "step": 34070
    },
    {
      "epoch": 1.09056,
      "grad_norm": 0.05779929459095001,
      "learning_rate": 4.5473600000000005e-06,
      "loss": 0.2104,
      "step": 34080
    },
    {
      "epoch": 1.09088,
      "grad_norm": 0.035969171673059464,
      "learning_rate": 4.54576e-06,
      "loss": 0.2328,
      "step": 34090
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.047093551605939865,
      "learning_rate": 4.54416e-06,
      "loss": 0.1995,
      "step": 34100
    },
    {
      "epoch": 1.09152,
      "grad_norm": 0.04114575311541557,
      "learning_rate": 4.54256e-06,
      "loss": 0.1995,
      "step": 34110
    },
    {
      "epoch": 1.09184,
      "grad_norm": 0.03645465150475502,
      "learning_rate": 4.54096e-06,
      "loss": 0.2015,
      "step": 34120
    },
    {
      "epoch": 1.09216,
      "grad_norm": 0.1111496165394783,
      "learning_rate": 4.539360000000001e-06,
      "loss": 0.2032,
      "step": 34130
    },
    {
      "epoch": 1.0924800000000001,
      "grad_norm": 0.02195817045867443,
      "learning_rate": 4.537760000000001e-06,
      "loss": 0.1994,
      "step": 34140
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.0313335657119751,
      "learning_rate": 4.5361600000000005e-06,
      "loss": 0.2119,
      "step": 34150
    },
    {
      "epoch": 1.09312,
      "grad_norm": 0.015239431522786617,
      "learning_rate": 4.53456e-06,
      "loss": 0.1992,
      "step": 34160
    },
    {
      "epoch": 1.09344,
      "grad_norm": 0.024784140288829803,
      "learning_rate": 4.53296e-06,
      "loss": 0.2134,
      "step": 34170
    },
    {
      "epoch": 1.09376,
      "grad_norm": 0.010159609839320183,
      "learning_rate": 4.53136e-06,
      "loss": 0.1991,
      "step": 34180
    },
    {
      "epoch": 1.09408,
      "grad_norm": 0.018855078145861626,
      "learning_rate": 4.52976e-06,
      "loss": 0.1997,
      "step": 34190
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.024304110556840897,
      "learning_rate": 4.528160000000001e-06,
      "loss": 0.2018,
      "step": 34200
    },
    {
      "epoch": 1.09472,
      "grad_norm": 0.021339109167456627,
      "learning_rate": 4.526560000000001e-06,
      "loss": 0.1994,
      "step": 34210
    },
    {
      "epoch": 1.09504,
      "grad_norm": 0.04253033176064491,
      "learning_rate": 4.5249600000000005e-06,
      "loss": 0.2142,
      "step": 34220
    },
    {
      "epoch": 1.09536,
      "grad_norm": 0.02249705232679844,
      "learning_rate": 4.52336e-06,
      "loss": 0.1992,
      "step": 34230
    },
    {
      "epoch": 1.09568,
      "grad_norm": 0.025385523214936256,
      "learning_rate": 4.52176e-06,
      "loss": 0.2137,
      "step": 34240
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.01956315152347088,
      "learning_rate": 4.52016e-06,
      "loss": 0.2022,
      "step": 34250
    },
    {
      "epoch": 1.09632,
      "grad_norm": 0.01457245834171772,
      "learning_rate": 4.51856e-06,
      "loss": 0.1994,
      "step": 34260
    },
    {
      "epoch": 1.09664,
      "grad_norm": 0.011935209855437279,
      "learning_rate": 4.51696e-06,
      "loss": 0.1993,
      "step": 34270
    },
    {
      "epoch": 1.09696,
      "grad_norm": 0.05679770186543465,
      "learning_rate": 4.515360000000001e-06,
      "loss": 0.1994,
      "step": 34280
    },
    {
      "epoch": 1.09728,
      "grad_norm": 0.02919391728937626,
      "learning_rate": 4.5137600000000005e-06,
      "loss": 0.1992,
      "step": 34290
    },
    {
      "epoch": 1.0976,
      "grad_norm": 1.0732371807098389,
      "learning_rate": 4.51216e-06,
      "loss": 0.2233,
      "step": 34300
    },
    {
      "epoch": 1.09792,
      "grad_norm": 1.3377795219421387,
      "learning_rate": 4.51056e-06,
      "loss": 0.2066,
      "step": 34310
    },
    {
      "epoch": 1.09824,
      "grad_norm": 0.01665337011218071,
      "learning_rate": 4.508960000000001e-06,
      "loss": 0.1992,
      "step": 34320
    },
    {
      "epoch": 1.09856,
      "grad_norm": 0.03555087372660637,
      "learning_rate": 4.507360000000001e-06,
      "loss": 0.1991,
      "step": 34330
    },
    {
      "epoch": 1.09888,
      "grad_norm": 0.02226853370666504,
      "learning_rate": 4.50576e-06,
      "loss": 0.199,
      "step": 34340
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.020805710926651955,
      "learning_rate": 4.504160000000001e-06,
      "loss": 0.1998,
      "step": 34350
    },
    {
      "epoch": 1.09952,
      "grad_norm": 0.03043549880385399,
      "learning_rate": 4.5025600000000005e-06,
      "loss": 0.1992,
      "step": 34360
    },
    {
      "epoch": 1.09984,
      "grad_norm": 0.02916238270699978,
      "learning_rate": 4.50096e-06,
      "loss": 0.1994,
      "step": 34370
    },
    {
      "epoch": 1.10016,
      "grad_norm": 0.02562509849667549,
      "learning_rate": 4.49936e-06,
      "loss": 0.1996,
      "step": 34380
    },
    {
      "epoch": 1.10048,
      "grad_norm": 0.017697056755423546,
      "learning_rate": 4.49776e-06,
      "loss": 0.1991,
      "step": 34390
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.010102096945047379,
      "learning_rate": 4.496160000000001e-06,
      "loss": 0.2011,
      "step": 34400
    },
    {
      "epoch": 1.10112,
      "grad_norm": 0.8141464591026306,
      "learning_rate": 4.494560000000001e-06,
      "loss": 0.2153,
      "step": 34410
    },
    {
      "epoch": 1.10144,
      "grad_norm": 0.03197477385401726,
      "learning_rate": 4.49296e-06,
      "loss": 0.207,
      "step": 34420
    },
    {
      "epoch": 1.10176,
      "grad_norm": 0.01793602854013443,
      "learning_rate": 4.4913600000000005e-06,
      "loss": 0.2127,
      "step": 34430
    },
    {
      "epoch": 1.10208,
      "grad_norm": 0.02438972517848015,
      "learning_rate": 4.48976e-06,
      "loss": 0.2,
      "step": 34440
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.028217190876603127,
      "learning_rate": 4.48816e-06,
      "loss": 0.2049,
      "step": 34450
    },
    {
      "epoch": 1.10272,
      "grad_norm": 0.02725934237241745,
      "learning_rate": 4.48656e-06,
      "loss": 0.1999,
      "step": 34460
    },
    {
      "epoch": 1.10304,
      "grad_norm": 0.027975257486104965,
      "learning_rate": 4.484960000000001e-06,
      "loss": 0.2087,
      "step": 34470
    },
    {
      "epoch": 1.10336,
      "grad_norm": 0.043127261102199554,
      "learning_rate": 4.483360000000001e-06,
      "loss": 0.2006,
      "step": 34480
    },
    {
      "epoch": 1.10368,
      "grad_norm": 0.012757165357470512,
      "learning_rate": 4.481760000000001e-06,
      "loss": 0.201,
      "step": 34490
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.2531239986419678,
      "learning_rate": 4.4801600000000005e-06,
      "loss": 0.2001,
      "step": 34500
    },
    {
      "epoch": 1.10432,
      "grad_norm": 0.06396421045064926,
      "learning_rate": 4.47856e-06,
      "loss": 0.2001,
      "step": 34510
    },
    {
      "epoch": 1.10464,
      "grad_norm": 0.015915649011731148,
      "learning_rate": 4.47696e-06,
      "loss": 0.2192,
      "step": 34520
    },
    {
      "epoch": 1.10496,
      "grad_norm": 0.03547881916165352,
      "learning_rate": 4.47536e-06,
      "loss": 0.2291,
      "step": 34530
    },
    {
      "epoch": 1.10528,
      "grad_norm": 0.03766961395740509,
      "learning_rate": 4.47376e-06,
      "loss": 0.2127,
      "step": 34540
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.04475410282611847,
      "learning_rate": 4.472160000000001e-06,
      "loss": 0.2213,
      "step": 34550
    },
    {
      "epoch": 1.10592,
      "grad_norm": 0.021877866238355637,
      "learning_rate": 4.470560000000001e-06,
      "loss": 0.2012,
      "step": 34560
    },
    {
      "epoch": 1.1062400000000001,
      "grad_norm": 0.035157449543476105,
      "learning_rate": 4.4689600000000005e-06,
      "loss": 0.1992,
      "step": 34570
    },
    {
      "epoch": 1.10656,
      "grad_norm": 0.013589954935014248,
      "learning_rate": 4.46736e-06,
      "loss": 0.2005,
      "step": 34580
    },
    {
      "epoch": 1.10688,
      "grad_norm": 0.023655567318201065,
      "learning_rate": 4.46576e-06,
      "loss": 0.1998,
      "step": 34590
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.025712251663208008,
      "learning_rate": 4.46416e-06,
      "loss": 0.2005,
      "step": 34600
    },
    {
      "epoch": 1.10752,
      "grad_norm": 0.07354656606912613,
      "learning_rate": 4.46256e-06,
      "loss": 0.1992,
      "step": 34610
    },
    {
      "epoch": 1.10784,
      "grad_norm": 0.021616635844111443,
      "learning_rate": 4.46096e-06,
      "loss": 0.1997,
      "step": 34620
    },
    {
      "epoch": 1.10816,
      "grad_norm": 0.029686016961932182,
      "learning_rate": 4.459360000000001e-06,
      "loss": 0.2083,
      "step": 34630
    },
    {
      "epoch": 1.10848,
      "grad_norm": 0.01920499838888645,
      "learning_rate": 4.4577600000000005e-06,
      "loss": 0.1991,
      "step": 34640
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.011803585104644299,
      "learning_rate": 4.45616e-06,
      "loss": 0.1991,
      "step": 34650
    },
    {
      "epoch": 1.1091199999999999,
      "grad_norm": 0.01496005617082119,
      "learning_rate": 4.45456e-06,
      "loss": 0.2172,
      "step": 34660
    },
    {
      "epoch": 1.10944,
      "grad_norm": 0.01568390429019928,
      "learning_rate": 4.452960000000001e-06,
      "loss": 0.1992,
      "step": 34670
    },
    {
      "epoch": 1.10976,
      "grad_norm": 0.043483853340148926,
      "learning_rate": 4.45136e-06,
      "loss": 0.1995,
      "step": 34680
    },
    {
      "epoch": 1.11008,
      "grad_norm": 0.02612403593957424,
      "learning_rate": 4.44976e-06,
      "loss": 0.2181,
      "step": 34690
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.019128087908029556,
      "learning_rate": 4.448160000000001e-06,
      "loss": 0.1993,
      "step": 34700
    },
    {
      "epoch": 1.11072,
      "grad_norm": 0.10372436046600342,
      "learning_rate": 4.4465600000000005e-06,
      "loss": 0.2003,
      "step": 34710
    },
    {
      "epoch": 1.11104,
      "grad_norm": 0.05893769860267639,
      "learning_rate": 4.44496e-06,
      "loss": 0.2156,
      "step": 34720
    },
    {
      "epoch": 1.11136,
      "grad_norm": 0.0312662236392498,
      "learning_rate": 4.44336e-06,
      "loss": 0.1994,
      "step": 34730
    },
    {
      "epoch": 1.11168,
      "grad_norm": 0.041658543050289154,
      "learning_rate": 4.44176e-06,
      "loss": 0.2012,
      "step": 34740
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.03676808625459671,
      "learning_rate": 4.440160000000001e-06,
      "loss": 0.1991,
      "step": 34750
    },
    {
      "epoch": 1.11232,
      "grad_norm": 0.020768873393535614,
      "learning_rate": 4.43856e-06,
      "loss": 0.1993,
      "step": 34760
    },
    {
      "epoch": 1.11264,
      "grad_norm": 0.011234400793910027,
      "learning_rate": 4.43696e-06,
      "loss": 0.2151,
      "step": 34770
    },
    {
      "epoch": 1.11296,
      "grad_norm": 0.015201167203485966,
      "learning_rate": 4.4353600000000005e-06,
      "loss": 0.2005,
      "step": 34780
    },
    {
      "epoch": 1.11328,
      "grad_norm": 0.01128673367202282,
      "learning_rate": 4.43376e-06,
      "loss": 0.1992,
      "step": 34790
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.013871741481125355,
      "learning_rate": 4.43216e-06,
      "loss": 0.1992,
      "step": 34800
    },
    {
      "epoch": 1.11392,
      "grad_norm": 0.03443583846092224,
      "learning_rate": 4.43056e-06,
      "loss": 0.2004,
      "step": 34810
    },
    {
      "epoch": 1.11424,
      "grad_norm": 0.026591114699840546,
      "learning_rate": 4.428960000000001e-06,
      "loss": 0.1996,
      "step": 34820
    },
    {
      "epoch": 1.11456,
      "grad_norm": 0.0383586548268795,
      "learning_rate": 4.427360000000001e-06,
      "loss": 0.2016,
      "step": 34830
    },
    {
      "epoch": 1.11488,
      "grad_norm": 0.03693288564682007,
      "learning_rate": 4.425760000000001e-06,
      "loss": 0.1992,
      "step": 34840
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.039527323096990585,
      "learning_rate": 4.4241600000000005e-06,
      "loss": 0.2142,
      "step": 34850
    },
    {
      "epoch": 1.11552,
      "grad_norm": 0.02054917998611927,
      "learning_rate": 4.42256e-06,
      "loss": 0.2123,
      "step": 34860
    },
    {
      "epoch": 1.11584,
      "grad_norm": 0.021676287055015564,
      "learning_rate": 4.42096e-06,
      "loss": 0.2004,
      "step": 34870
    },
    {
      "epoch": 1.11616,
      "grad_norm": 0.22103577852249146,
      "learning_rate": 4.41936e-06,
      "loss": 0.2,
      "step": 34880
    },
    {
      "epoch": 1.11648,
      "grad_norm": 0.021809473633766174,
      "learning_rate": 4.41776e-06,
      "loss": 0.1991,
      "step": 34890
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.026520803570747375,
      "learning_rate": 4.416160000000001e-06,
      "loss": 0.1997,
      "step": 34900
    },
    {
      "epoch": 1.11712,
      "grad_norm": 0.057417772710323334,
      "learning_rate": 4.414560000000001e-06,
      "loss": 0.1992,
      "step": 34910
    },
    {
      "epoch": 1.11744,
      "grad_norm": 0.022253496572375298,
      "learning_rate": 4.4129600000000005e-06,
      "loss": 0.1991,
      "step": 34920
    },
    {
      "epoch": 1.11776,
      "grad_norm": 0.02743745781481266,
      "learning_rate": 4.41136e-06,
      "loss": 0.1997,
      "step": 34930
    },
    {
      "epoch": 1.11808,
      "grad_norm": 0.029774675145745277,
      "learning_rate": 4.40976e-06,
      "loss": 0.1996,
      "step": 34940
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.046594973653554916,
      "learning_rate": 4.40816e-06,
      "loss": 0.1991,
      "step": 34950
    },
    {
      "epoch": 1.11872,
      "grad_norm": 0.023923899978399277,
      "learning_rate": 4.40656e-06,
      "loss": 0.1994,
      "step": 34960
    },
    {
      "epoch": 1.11904,
      "grad_norm": 0.04761229455471039,
      "learning_rate": 4.404960000000001e-06,
      "loss": 0.1995,
      "step": 34970
    },
    {
      "epoch": 1.11936,
      "grad_norm": 0.020600970834493637,
      "learning_rate": 4.403360000000001e-06,
      "loss": 0.1992,
      "step": 34980
    },
    {
      "epoch": 1.11968,
      "grad_norm": 0.023268047720193863,
      "learning_rate": 4.4017600000000005e-06,
      "loss": 0.1994,
      "step": 34990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.06884562969207764,
      "learning_rate": 4.40016e-06,
      "loss": 0.1992,
      "step": 35000
    },
    {
      "epoch": 1.12,
      "eval_runtime": 51.6773,
      "eval_samples_per_second": 193.509,
      "eval_steps_per_second": 12.094,
      "step": 35000
    },
    {
      "epoch": 1.12032,
      "grad_norm": 0.022689584642648697,
      "learning_rate": 4.39856e-06,
      "loss": 0.1992,
      "step": 35010
    },
    {
      "epoch": 1.12064,
      "grad_norm": 0.024181345477700233,
      "learning_rate": 4.39696e-06,
      "loss": 0.1993,
      "step": 35020
    },
    {
      "epoch": 1.12096,
      "grad_norm": 0.015930600464344025,
      "learning_rate": 4.39536e-06,
      "loss": 0.1991,
      "step": 35030
    },
    {
      "epoch": 1.12128,
      "grad_norm": 0.1857568621635437,
      "learning_rate": 4.39376e-06,
      "loss": 0.2143,
      "step": 35040
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.035218141973018646,
      "learning_rate": 4.392160000000001e-06,
      "loss": 0.1992,
      "step": 35050
    },
    {
      "epoch": 1.12192,
      "grad_norm": 0.021993618458509445,
      "learning_rate": 4.3905600000000005e-06,
      "loss": 0.2017,
      "step": 35060
    },
    {
      "epoch": 1.12224,
      "grad_norm": 0.018760012462735176,
      "learning_rate": 4.38896e-06,
      "loss": 0.1992,
      "step": 35070
    },
    {
      "epoch": 1.12256,
      "grad_norm": 0.018158653751015663,
      "learning_rate": 4.38736e-06,
      "loss": 0.1993,
      "step": 35080
    },
    {
      "epoch": 1.12288,
      "grad_norm": 0.014295860193669796,
      "learning_rate": 4.38576e-06,
      "loss": 0.2015,
      "step": 35090
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.03022952191531658,
      "learning_rate": 4.38416e-06,
      "loss": 0.1992,
      "step": 35100
    },
    {
      "epoch": 1.12352,
      "grad_norm": 0.025464238598942757,
      "learning_rate": 4.38256e-06,
      "loss": 0.2027,
      "step": 35110
    },
    {
      "epoch": 1.12384,
      "grad_norm": 0.008900186978280544,
      "learning_rate": 4.38096e-06,
      "loss": 0.2134,
      "step": 35120
    },
    {
      "epoch": 1.12416,
      "grad_norm": 0.06134546548128128,
      "learning_rate": 4.3793600000000005e-06,
      "loss": 0.1997,
      "step": 35130
    },
    {
      "epoch": 1.12448,
      "grad_norm": 0.033068060874938965,
      "learning_rate": 4.37776e-06,
      "loss": 0.1993,
      "step": 35140
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.02405722625553608,
      "learning_rate": 4.37616e-06,
      "loss": 0.1994,
      "step": 35150
    },
    {
      "epoch": 1.12512,
      "grad_norm": 0.019074685871601105,
      "learning_rate": 4.37456e-06,
      "loss": 0.1994,
      "step": 35160
    },
    {
      "epoch": 1.12544,
      "grad_norm": 0.011106844991445541,
      "learning_rate": 4.372960000000001e-06,
      "loss": 0.1998,
      "step": 35170
    },
    {
      "epoch": 1.12576,
      "grad_norm": 0.17804013192653656,
      "learning_rate": 4.371360000000001e-06,
      "loss": 0.1995,
      "step": 35180
    },
    {
      "epoch": 1.12608,
      "grad_norm": 0.04419073089957237,
      "learning_rate": 4.36976e-06,
      "loss": 0.1992,
      "step": 35190
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.022051280364394188,
      "learning_rate": 4.3681600000000005e-06,
      "loss": 0.2097,
      "step": 35200
    },
    {
      "epoch": 1.12672,
      "grad_norm": 0.033570315688848495,
      "learning_rate": 4.36656e-06,
      "loss": 0.2148,
      "step": 35210
    },
    {
      "epoch": 1.12704,
      "grad_norm": 0.028268801048398018,
      "learning_rate": 4.36496e-06,
      "loss": 0.1994,
      "step": 35220
    },
    {
      "epoch": 1.12736,
      "grad_norm": 0.021680690348148346,
      "learning_rate": 4.36336e-06,
      "loss": 0.1992,
      "step": 35230
    },
    {
      "epoch": 1.12768,
      "grad_norm": 0.02048817090690136,
      "learning_rate": 4.36176e-06,
      "loss": 0.2002,
      "step": 35240
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.027442920953035355,
      "learning_rate": 4.360160000000001e-06,
      "loss": 0.1992,
      "step": 35250
    },
    {
      "epoch": 1.12832,
      "grad_norm": 0.015029353089630604,
      "learning_rate": 4.358560000000001e-06,
      "loss": 0.212,
      "step": 35260
    },
    {
      "epoch": 1.12864,
      "grad_norm": 0.01985647715628147,
      "learning_rate": 4.3569600000000005e-06,
      "loss": 0.2136,
      "step": 35270
    },
    {
      "epoch": 1.12896,
      "grad_norm": 0.01220412366092205,
      "learning_rate": 4.3553600000000004e-06,
      "loss": 0.1994,
      "step": 35280
    },
    {
      "epoch": 1.12928,
      "grad_norm": 0.03146061301231384,
      "learning_rate": 4.35376e-06,
      "loss": 0.2021,
      "step": 35290
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.018076006323099136,
      "learning_rate": 4.35216e-06,
      "loss": 0.2121,
      "step": 35300
    },
    {
      "epoch": 1.12992,
      "grad_norm": 0.05099499598145485,
      "learning_rate": 4.35056e-06,
      "loss": 0.1995,
      "step": 35310
    },
    {
      "epoch": 1.13024,
      "grad_norm": 0.21026897430419922,
      "learning_rate": 4.348960000000001e-06,
      "loss": 0.2142,
      "step": 35320
    },
    {
      "epoch": 1.13056,
      "grad_norm": 0.03400832787156105,
      "learning_rate": 4.347360000000001e-06,
      "loss": 0.2016,
      "step": 35330
    },
    {
      "epoch": 1.1308799999999999,
      "grad_norm": 0.03205357491970062,
      "learning_rate": 4.3457600000000006e-06,
      "loss": 0.1991,
      "step": 35340
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.10492635518312454,
      "learning_rate": 4.3441600000000004e-06,
      "loss": 0.1992,
      "step": 35350
    },
    {
      "epoch": 1.13152,
      "grad_norm": 0.022720523178577423,
      "learning_rate": 4.34256e-06,
      "loss": 0.1991,
      "step": 35360
    },
    {
      "epoch": 1.13184,
      "grad_norm": 0.020907049998641014,
      "learning_rate": 4.34096e-06,
      "loss": 0.1992,
      "step": 35370
    },
    {
      "epoch": 1.13216,
      "grad_norm": 0.02802053466439247,
      "learning_rate": 4.33936e-06,
      "loss": 0.2148,
      "step": 35380
    },
    {
      "epoch": 1.13248,
      "grad_norm": 0.028242263942956924,
      "learning_rate": 4.33776e-06,
      "loss": 0.1992,
      "step": 35390
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.024981962516903877,
      "learning_rate": 4.336160000000001e-06,
      "loss": 0.2216,
      "step": 35400
    },
    {
      "epoch": 1.13312,
      "grad_norm": 0.023677105084061623,
      "learning_rate": 4.3345600000000006e-06,
      "loss": 0.2018,
      "step": 35410
    },
    {
      "epoch": 1.13344,
      "grad_norm": 0.016133002936840057,
      "learning_rate": 4.3329600000000004e-06,
      "loss": 0.1991,
      "step": 35420
    },
    {
      "epoch": 1.13376,
      "grad_norm": 0.03284711390733719,
      "learning_rate": 4.33136e-06,
      "loss": 0.1994,
      "step": 35430
    },
    {
      "epoch": 1.13408,
      "grad_norm": 0.029797999188303947,
      "learning_rate": 4.329760000000001e-06,
      "loss": 0.1992,
      "step": 35440
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.028663238510489464,
      "learning_rate": 4.32816e-06,
      "loss": 0.1992,
      "step": 35450
    },
    {
      "epoch": 1.13472,
      "grad_norm": 0.802069365978241,
      "learning_rate": 4.32656e-06,
      "loss": 0.2121,
      "step": 35460
    },
    {
      "epoch": 1.13504,
      "grad_norm": 0.04574506729841232,
      "learning_rate": 4.324960000000001e-06,
      "loss": 0.1994,
      "step": 35470
    },
    {
      "epoch": 1.13536,
      "grad_norm": 0.02289961464703083,
      "learning_rate": 4.3233600000000006e-06,
      "loss": 0.1992,
      "step": 35480
    },
    {
      "epoch": 1.13568,
      "grad_norm": 0.0451958067715168,
      "learning_rate": 4.3217600000000004e-06,
      "loss": 0.213,
      "step": 35490
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.030865946784615517,
      "learning_rate": 4.32016e-06,
      "loss": 0.1993,
      "step": 35500
    },
    {
      "epoch": 1.13632,
      "grad_norm": 0.05230044946074486,
      "learning_rate": 4.31856e-06,
      "loss": 0.1994,
      "step": 35510
    },
    {
      "epoch": 1.13664,
      "grad_norm": 0.03796401992440224,
      "learning_rate": 4.316960000000001e-06,
      "loss": 0.1993,
      "step": 35520
    },
    {
      "epoch": 1.13696,
      "grad_norm": 0.029565051198005676,
      "learning_rate": 4.31536e-06,
      "loss": 0.2227,
      "step": 35530
    },
    {
      "epoch": 1.13728,
      "grad_norm": 0.04488333687186241,
      "learning_rate": 4.31376e-06,
      "loss": 0.1993,
      "step": 35540
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.6433065533638,
      "learning_rate": 4.3121600000000006e-06,
      "loss": 0.2015,
      "step": 35550
    },
    {
      "epoch": 1.13792,
      "grad_norm": 0.023735972121357918,
      "learning_rate": 4.3105600000000005e-06,
      "loss": 0.1992,
      "step": 35560
    },
    {
      "epoch": 1.13824,
      "grad_norm": 0.022951120510697365,
      "learning_rate": 4.30896e-06,
      "loss": 0.2054,
      "step": 35570
    },
    {
      "epoch": 1.13856,
      "grad_norm": 0.013826769776642323,
      "learning_rate": 4.30736e-06,
      "loss": 0.2167,
      "step": 35580
    },
    {
      "epoch": 1.13888,
      "grad_norm": 0.01521439477801323,
      "learning_rate": 4.30576e-06,
      "loss": 0.1994,
      "step": 35590
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.034256599843502045,
      "learning_rate": 4.304160000000001e-06,
      "loss": 0.208,
      "step": 35600
    },
    {
      "epoch": 1.13952,
      "grad_norm": 0.04481827840209007,
      "learning_rate": 4.302560000000001e-06,
      "loss": 0.2076,
      "step": 35610
    },
    {
      "epoch": 1.13984,
      "grad_norm": 0.042194612324237823,
      "learning_rate": 4.30096e-06,
      "loss": 0.2161,
      "step": 35620
    },
    {
      "epoch": 1.14016,
      "grad_norm": 0.020672280341386795,
      "learning_rate": 4.2993600000000005e-06,
      "loss": 0.1991,
      "step": 35630
    },
    {
      "epoch": 1.14048,
      "grad_norm": 0.013661243952810764,
      "learning_rate": 4.29776e-06,
      "loss": 0.1991,
      "step": 35640
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.020962435752153397,
      "learning_rate": 4.29616e-06,
      "loss": 0.1993,
      "step": 35650
    },
    {
      "epoch": 1.14112,
      "grad_norm": 0.04676695913076401,
      "learning_rate": 4.29456e-06,
      "loss": 0.1995,
      "step": 35660
    },
    {
      "epoch": 1.14144,
      "grad_norm": 0.021353231742978096,
      "learning_rate": 4.292960000000001e-06,
      "loss": 0.1992,
      "step": 35670
    },
    {
      "epoch": 1.14176,
      "grad_norm": 0.016365664079785347,
      "learning_rate": 4.291360000000001e-06,
      "loss": 0.2027,
      "step": 35680
    },
    {
      "epoch": 1.14208,
      "grad_norm": 0.016852229833602905,
      "learning_rate": 4.289760000000001e-06,
      "loss": 0.2121,
      "step": 35690
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.4047907292842865,
      "learning_rate": 4.2881600000000005e-06,
      "loss": 0.2018,
      "step": 35700
    },
    {
      "epoch": 1.14272,
      "grad_norm": 0.024104518815875053,
      "learning_rate": 4.28656e-06,
      "loss": 0.1992,
      "step": 35710
    },
    {
      "epoch": 1.14304,
      "grad_norm": 0.012370554730296135,
      "learning_rate": 4.28496e-06,
      "loss": 0.1992,
      "step": 35720
    },
    {
      "epoch": 1.14336,
      "grad_norm": 0.04414106160402298,
      "learning_rate": 4.28336e-06,
      "loss": 0.1999,
      "step": 35730
    },
    {
      "epoch": 1.14368,
      "grad_norm": 0.030081067234277725,
      "learning_rate": 4.28176e-06,
      "loss": 0.2075,
      "step": 35740
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.023585915565490723,
      "learning_rate": 4.280160000000001e-06,
      "loss": 0.2102,
      "step": 35750
    },
    {
      "epoch": 1.14432,
      "grad_norm": 0.02326139807701111,
      "learning_rate": 4.278560000000001e-06,
      "loss": 0.2158,
      "step": 35760
    },
    {
      "epoch": 1.1446399999999999,
      "grad_norm": 0.02127016708254814,
      "learning_rate": 4.2769600000000005e-06,
      "loss": 0.1994,
      "step": 35770
    },
    {
      "epoch": 1.14496,
      "grad_norm": 0.03050016053020954,
      "learning_rate": 4.27536e-06,
      "loss": 0.1993,
      "step": 35780
    },
    {
      "epoch": 1.14528,
      "grad_norm": 1.2135255336761475,
      "learning_rate": 4.27376e-06,
      "loss": 0.2043,
      "step": 35790
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.013195612467825413,
      "learning_rate": 4.27216e-06,
      "loss": 0.199,
      "step": 35800
    },
    {
      "epoch": 1.14592,
      "grad_norm": 0.046935539692640305,
      "learning_rate": 4.27056e-06,
      "loss": 0.1991,
      "step": 35810
    },
    {
      "epoch": 1.14624,
      "grad_norm": 0.06298017501831055,
      "learning_rate": 4.268960000000001e-06,
      "loss": 0.2118,
      "step": 35820
    },
    {
      "epoch": 1.14656,
      "grad_norm": 0.016900651156902313,
      "learning_rate": 4.267360000000001e-06,
      "loss": 0.1993,
      "step": 35830
    },
    {
      "epoch": 1.14688,
      "grad_norm": 0.023691711947321892,
      "learning_rate": 4.2657600000000005e-06,
      "loss": 0.1996,
      "step": 35840
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.0336274690926075,
      "learning_rate": 4.26416e-06,
      "loss": 0.2005,
      "step": 35850
    },
    {
      "epoch": 1.14752,
      "grad_norm": 0.028088007122278214,
      "learning_rate": 4.26256e-06,
      "loss": 0.1991,
      "step": 35860
    },
    {
      "epoch": 1.14784,
      "grad_norm": 0.0706152692437172,
      "learning_rate": 4.26096e-06,
      "loss": 0.1994,
      "step": 35870
    },
    {
      "epoch": 1.14816,
      "grad_norm": 0.04372848570346832,
      "learning_rate": 4.25936e-06,
      "loss": 0.2014,
      "step": 35880
    },
    {
      "epoch": 1.14848,
      "grad_norm": 0.03196630999445915,
      "learning_rate": 4.25776e-06,
      "loss": 0.1992,
      "step": 35890
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.039548084139823914,
      "learning_rate": 4.256160000000001e-06,
      "loss": 0.216,
      "step": 35900
    },
    {
      "epoch": 1.14912,
      "grad_norm": 0.012204821221530437,
      "learning_rate": 4.2545600000000005e-06,
      "loss": 0.1993,
      "step": 35910
    },
    {
      "epoch": 1.14944,
      "grad_norm": 0.019051721319556236,
      "learning_rate": 4.25296e-06,
      "loss": 0.1994,
      "step": 35920
    },
    {
      "epoch": 1.1497600000000001,
      "grad_norm": 0.03564194217324257,
      "learning_rate": 4.25136e-06,
      "loss": 0.1992,
      "step": 35930
    },
    {
      "epoch": 1.15008,
      "grad_norm": 0.012414776720106602,
      "learning_rate": 4.249760000000001e-06,
      "loss": 0.1993,
      "step": 35940
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.013193727470934391,
      "learning_rate": 4.248160000000001e-06,
      "loss": 0.1992,
      "step": 35950
    },
    {
      "epoch": 1.15072,
      "grad_norm": 0.03873307257890701,
      "learning_rate": 4.24656e-06,
      "loss": 0.2133,
      "step": 35960
    },
    {
      "epoch": 1.15104,
      "grad_norm": 0.031117260456085205,
      "learning_rate": 4.244960000000001e-06,
      "loss": 0.2067,
      "step": 35970
    },
    {
      "epoch": 1.15136,
      "grad_norm": 0.023423820734024048,
      "learning_rate": 4.2433600000000005e-06,
      "loss": 0.1992,
      "step": 35980
    },
    {
      "epoch": 1.15168,
      "grad_norm": 0.02228308655321598,
      "learning_rate": 4.24176e-06,
      "loss": 0.1991,
      "step": 35990
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.017259377986192703,
      "learning_rate": 4.24016e-06,
      "loss": 0.2182,
      "step": 36000
    },
    {
      "epoch": 1.152,
      "eval_runtime": 51.906,
      "eval_samples_per_second": 192.656,
      "eval_steps_per_second": 12.041,
      "step": 36000
    },
    {
      "epoch": 1.15232,
      "grad_norm": 0.15386657416820526,
      "learning_rate": 4.23856e-06,
      "loss": 0.1996,
      "step": 36010
    },
    {
      "epoch": 1.1526399999999999,
      "grad_norm": 0.023569611832499504,
      "learning_rate": 4.236960000000001e-06,
      "loss": 0.1994,
      "step": 36020
    },
    {
      "epoch": 1.15296,
      "grad_norm": 0.02834327705204487,
      "learning_rate": 4.235360000000001e-06,
      "loss": 0.1992,
      "step": 36030
    },
    {
      "epoch": 1.15328,
      "grad_norm": 0.02282731793820858,
      "learning_rate": 4.23376e-06,
      "loss": 0.2051,
      "step": 36040
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.013062465004622936,
      "learning_rate": 4.2321600000000005e-06,
      "loss": 0.1994,
      "step": 36050
    },
    {
      "epoch": 1.15392,
      "grad_norm": 1.1100081205368042,
      "learning_rate": 4.23056e-06,
      "loss": 0.2233,
      "step": 36060
    },
    {
      "epoch": 1.15424,
      "grad_norm": 0.015163823030889034,
      "learning_rate": 4.22896e-06,
      "loss": 0.1991,
      "step": 36070
    },
    {
      "epoch": 1.15456,
      "grad_norm": 0.049174632877111435,
      "learning_rate": 4.22736e-06,
      "loss": 0.1993,
      "step": 36080
    },
    {
      "epoch": 1.15488,
      "grad_norm": 0.06843392550945282,
      "learning_rate": 4.22576e-06,
      "loss": 0.2052,
      "step": 36090
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.05673626437783241,
      "learning_rate": 4.224160000000001e-06,
      "loss": 0.1997,
      "step": 36100
    },
    {
      "epoch": 1.15552,
      "grad_norm": 0.06047623232007027,
      "learning_rate": 4.222560000000001e-06,
      "loss": 0.2004,
      "step": 36110
    },
    {
      "epoch": 1.15584,
      "grad_norm": 0.010771258734166622,
      "learning_rate": 4.2209600000000005e-06,
      "loss": 0.1994,
      "step": 36120
    },
    {
      "epoch": 1.15616,
      "grad_norm": 0.1886429786682129,
      "learning_rate": 4.21936e-06,
      "loss": 0.2003,
      "step": 36130
    },
    {
      "epoch": 1.15648,
      "grad_norm": 0.018288223072886467,
      "learning_rate": 4.21776e-06,
      "loss": 0.2034,
      "step": 36140
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.047096945345401764,
      "learning_rate": 4.21616e-06,
      "loss": 0.2046,
      "step": 36150
    },
    {
      "epoch": 1.15712,
      "grad_norm": 0.025892967358231544,
      "learning_rate": 4.21456e-06,
      "loss": 0.1997,
      "step": 36160
    },
    {
      "epoch": 1.15744,
      "grad_norm": 0.020769493654370308,
      "learning_rate": 4.212960000000001e-06,
      "loss": 0.2074,
      "step": 36170
    },
    {
      "epoch": 1.1577600000000001,
      "grad_norm": 0.0357622392475605,
      "learning_rate": 4.211360000000001e-06,
      "loss": 0.1991,
      "step": 36180
    },
    {
      "epoch": 1.15808,
      "grad_norm": 0.01181185431778431,
      "learning_rate": 4.2097600000000005e-06,
      "loss": 0.1993,
      "step": 36190
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.01634918712079525,
      "learning_rate": 4.20816e-06,
      "loss": 0.2156,
      "step": 36200
    },
    {
      "epoch": 1.15872,
      "grad_norm": 0.03799799829721451,
      "learning_rate": 4.20656e-06,
      "loss": 0.1993,
      "step": 36210
    },
    {
      "epoch": 1.15904,
      "grad_norm": 0.060962460935115814,
      "learning_rate": 4.20496e-06,
      "loss": 0.1993,
      "step": 36220
    },
    {
      "epoch": 1.15936,
      "grad_norm": 0.04595402628183365,
      "learning_rate": 4.20336e-06,
      "loss": 0.2146,
      "step": 36230
    },
    {
      "epoch": 1.15968,
      "grad_norm": 0.028947746381163597,
      "learning_rate": 4.20176e-06,
      "loss": 0.1993,
      "step": 36240
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.03677836433053017,
      "learning_rate": 4.200160000000001e-06,
      "loss": 0.2163,
      "step": 36250
    },
    {
      "epoch": 1.16032,
      "grad_norm": 0.03701554611325264,
      "learning_rate": 4.1985600000000005e-06,
      "loss": 0.2031,
      "step": 36260
    },
    {
      "epoch": 1.16064,
      "grad_norm": 0.03016185574233532,
      "learning_rate": 4.19696e-06,
      "loss": 0.2001,
      "step": 36270
    },
    {
      "epoch": 1.16096,
      "grad_norm": 0.0440426804125309,
      "learning_rate": 4.19536e-06,
      "loss": 0.1992,
      "step": 36280
    },
    {
      "epoch": 1.16128,
      "grad_norm": 0.046662550419569016,
      "learning_rate": 4.193760000000001e-06,
      "loss": 0.2008,
      "step": 36290
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.016685765236616135,
      "learning_rate": 4.19216e-06,
      "loss": 0.2146,
      "step": 36300
    },
    {
      "epoch": 1.16192,
      "grad_norm": 0.035918816924095154,
      "learning_rate": 4.19056e-06,
      "loss": 0.2195,
      "step": 36310
    },
    {
      "epoch": 1.16224,
      "grad_norm": 0.023871969431638718,
      "learning_rate": 4.188960000000001e-06,
      "loss": 0.1991,
      "step": 36320
    },
    {
      "epoch": 1.16256,
      "grad_norm": 0.04890642687678337,
      "learning_rate": 4.1873600000000005e-06,
      "loss": 0.2054,
      "step": 36330
    },
    {
      "epoch": 1.16288,
      "grad_norm": 0.020060114562511444,
      "learning_rate": 4.18576e-06,
      "loss": 0.2009,
      "step": 36340
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.04332372918725014,
      "learning_rate": 4.18416e-06,
      "loss": 0.1999,
      "step": 36350
    },
    {
      "epoch": 1.16352,
      "grad_norm": 1.1297776699066162,
      "learning_rate": 4.18256e-06,
      "loss": 0.2017,
      "step": 36360
    },
    {
      "epoch": 1.16384,
      "grad_norm": 0.04292742908000946,
      "learning_rate": 4.180960000000001e-06,
      "loss": 0.1991,
      "step": 36370
    },
    {
      "epoch": 1.16416,
      "grad_norm": 0.017228661105036736,
      "learning_rate": 4.17936e-06,
      "loss": 0.2221,
      "step": 36380
    },
    {
      "epoch": 1.16448,
      "grad_norm": 0.024500541388988495,
      "learning_rate": 4.17776e-06,
      "loss": 0.2091,
      "step": 36390
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.033891793340444565,
      "learning_rate": 4.1761600000000005e-06,
      "loss": 0.201,
      "step": 36400
    },
    {
      "epoch": 1.16512,
      "grad_norm": 0.026714667677879333,
      "learning_rate": 4.17456e-06,
      "loss": 0.1994,
      "step": 36410
    },
    {
      "epoch": 1.16544,
      "grad_norm": 0.02177557907998562,
      "learning_rate": 4.17296e-06,
      "loss": 0.1991,
      "step": 36420
    },
    {
      "epoch": 1.16576,
      "grad_norm": 0.018688390031456947,
      "learning_rate": 4.17136e-06,
      "loss": 0.2167,
      "step": 36430
    },
    {
      "epoch": 1.16608,
      "grad_norm": 0.01912480965256691,
      "learning_rate": 4.169760000000001e-06,
      "loss": 0.2167,
      "step": 36440
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.03493792936205864,
      "learning_rate": 4.168160000000001e-06,
      "loss": 0.1994,
      "step": 36450
    },
    {
      "epoch": 1.16672,
      "grad_norm": 0.012703189626336098,
      "learning_rate": 4.166560000000001e-06,
      "loss": 0.1997,
      "step": 36460
    },
    {
      "epoch": 1.16704,
      "grad_norm": 0.6701457500457764,
      "learning_rate": 4.1649600000000005e-06,
      "loss": 0.2127,
      "step": 36470
    },
    {
      "epoch": 1.16736,
      "grad_norm": 0.04007210209965706,
      "learning_rate": 4.16336e-06,
      "loss": 0.1995,
      "step": 36480
    },
    {
      "epoch": 1.16768,
      "grad_norm": 0.008445419371128082,
      "learning_rate": 4.16176e-06,
      "loss": 0.2406,
      "step": 36490
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.0774487778544426,
      "learning_rate": 4.16016e-06,
      "loss": 0.2143,
      "step": 36500
    },
    {
      "epoch": 1.16832,
      "grad_norm": 0.027194183319807053,
      "learning_rate": 4.15856e-06,
      "loss": 0.2012,
      "step": 36510
    },
    {
      "epoch": 1.16864,
      "grad_norm": 0.03169453144073486,
      "learning_rate": 4.156960000000001e-06,
      "loss": 0.1993,
      "step": 36520
    },
    {
      "epoch": 1.16896,
      "grad_norm": 0.0364704392850399,
      "learning_rate": 4.155360000000001e-06,
      "loss": 0.1995,
      "step": 36530
    },
    {
      "epoch": 1.16928,
      "grad_norm": 0.04437996819615364,
      "learning_rate": 4.1537600000000005e-06,
      "loss": 0.2021,
      "step": 36540
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.045571569353342056,
      "learning_rate": 4.15216e-06,
      "loss": 0.1992,
      "step": 36550
    },
    {
      "epoch": 1.16992,
      "grad_norm": 0.4123982787132263,
      "learning_rate": 4.15056e-06,
      "loss": 0.2,
      "step": 36560
    },
    {
      "epoch": 1.17024,
      "grad_norm": 0.03964313492178917,
      "learning_rate": 4.14896e-06,
      "loss": 0.1995,
      "step": 36570
    },
    {
      "epoch": 1.17056,
      "grad_norm": 0.02253333292901516,
      "learning_rate": 4.14736e-06,
      "loss": 0.1992,
      "step": 36580
    },
    {
      "epoch": 1.17088,
      "grad_norm": 0.024664362892508507,
      "learning_rate": 4.14576e-06,
      "loss": 0.1991,
      "step": 36590
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.05048954486846924,
      "learning_rate": 4.144160000000001e-06,
      "loss": 0.2236,
      "step": 36600
    },
    {
      "epoch": 1.1715200000000001,
      "grad_norm": 0.017160726711153984,
      "learning_rate": 4.1425600000000005e-06,
      "loss": 0.1991,
      "step": 36610
    },
    {
      "epoch": 1.17184,
      "grad_norm": 0.02610369585454464,
      "learning_rate": 4.14096e-06,
      "loss": 0.1992,
      "step": 36620
    },
    {
      "epoch": 1.17216,
      "grad_norm": 0.039395563304424286,
      "learning_rate": 4.13936e-06,
      "loss": 0.1993,
      "step": 36630
    },
    {
      "epoch": 1.17248,
      "grad_norm": 0.04818014055490494,
      "learning_rate": 4.13776e-06,
      "loss": 0.1993,
      "step": 36640
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.03506334125995636,
      "learning_rate": 4.13616e-06,
      "loss": 0.1992,
      "step": 36650
    },
    {
      "epoch": 1.17312,
      "grad_norm": 0.056503839790821075,
      "learning_rate": 4.13456e-06,
      "loss": 0.1995,
      "step": 36660
    },
    {
      "epoch": 1.17344,
      "grad_norm": 0.015973880887031555,
      "learning_rate": 4.132960000000001e-06,
      "loss": 0.1992,
      "step": 36670
    },
    {
      "epoch": 1.17376,
      "grad_norm": 0.052757106721401215,
      "learning_rate": 4.1313600000000005e-06,
      "loss": 0.1994,
      "step": 36680
    },
    {
      "epoch": 1.17408,
      "grad_norm": 0.07045046985149384,
      "learning_rate": 4.12976e-06,
      "loss": 0.1995,
      "step": 36690
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.32689833641052246,
      "learning_rate": 4.12816e-06,
      "loss": 0.2161,
      "step": 36700
    },
    {
      "epoch": 1.17472,
      "grad_norm": 0.01290454063564539,
      "learning_rate": 4.12656e-06,
      "loss": 0.1991,
      "step": 36710
    },
    {
      "epoch": 1.17504,
      "grad_norm": 0.022204793989658356,
      "learning_rate": 4.12496e-06,
      "loss": 0.2,
      "step": 36720
    },
    {
      "epoch": 1.17536,
      "grad_norm": 0.36577484011650085,
      "learning_rate": 4.12336e-06,
      "loss": 0.1999,
      "step": 36730
    },
    {
      "epoch": 1.17568,
      "grad_norm": 0.028363162651658058,
      "learning_rate": 4.12176e-06,
      "loss": 0.2169,
      "step": 36740
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.035617318004369736,
      "learning_rate": 4.1201600000000005e-06,
      "loss": 0.1994,
      "step": 36750
    },
    {
      "epoch": 1.17632,
      "grad_norm": 0.03253770247101784,
      "learning_rate": 4.1185600000000004e-06,
      "loss": 0.1991,
      "step": 36760
    },
    {
      "epoch": 1.17664,
      "grad_norm": 0.02731603942811489,
      "learning_rate": 4.11696e-06,
      "loss": 0.2014,
      "step": 36770
    },
    {
      "epoch": 1.17696,
      "grad_norm": 0.019780198112130165,
      "learning_rate": 4.11536e-06,
      "loss": 0.1992,
      "step": 36780
    },
    {
      "epoch": 1.17728,
      "grad_norm": 0.019664961844682693,
      "learning_rate": 4.113760000000001e-06,
      "loss": 0.2128,
      "step": 36790
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.014255916699767113,
      "learning_rate": 4.112160000000001e-06,
      "loss": 0.1994,
      "step": 36800
    },
    {
      "epoch": 1.17792,
      "grad_norm": 0.03498077765107155,
      "learning_rate": 4.11056e-06,
      "loss": 0.1991,
      "step": 36810
    },
    {
      "epoch": 1.17824,
      "grad_norm": 0.09966641664505005,
      "learning_rate": 4.1089600000000006e-06,
      "loss": 0.1996,
      "step": 36820
    },
    {
      "epoch": 1.17856,
      "grad_norm": 0.012587049975991249,
      "learning_rate": 4.1073600000000004e-06,
      "loss": 0.1991,
      "step": 36830
    },
    {
      "epoch": 1.17888,
      "grad_norm": 0.2205105721950531,
      "learning_rate": 4.10576e-06,
      "loss": 0.2034,
      "step": 36840
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.032061539590358734,
      "learning_rate": 4.10416e-06,
      "loss": 0.1994,
      "step": 36850
    },
    {
      "epoch": 1.1795200000000001,
      "grad_norm": 0.017833510413765907,
      "learning_rate": 4.10256e-06,
      "loss": 0.1993,
      "step": 36860
    },
    {
      "epoch": 1.17984,
      "grad_norm": 0.01461492758244276,
      "learning_rate": 4.100960000000001e-06,
      "loss": 0.2163,
      "step": 36870
    },
    {
      "epoch": 1.1801599999999999,
      "grad_norm": 0.019223129376769066,
      "learning_rate": 4.099360000000001e-06,
      "loss": 0.204,
      "step": 36880
    },
    {
      "epoch": 1.18048,
      "grad_norm": 0.03953203931450844,
      "learning_rate": 4.09776e-06,
      "loss": 0.2147,
      "step": 36890
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.015970993787050247,
      "learning_rate": 4.0961600000000004e-06,
      "loss": 0.1993,
      "step": 36900
    },
    {
      "epoch": 1.18112,
      "grad_norm": 0.015010692179203033,
      "learning_rate": 4.09456e-06,
      "loss": 0.1991,
      "step": 36910
    },
    {
      "epoch": 1.18144,
      "grad_norm": 0.05086769163608551,
      "learning_rate": 4.09296e-06,
      "loss": 0.2152,
      "step": 36920
    },
    {
      "epoch": 1.18176,
      "grad_norm": 0.01729356311261654,
      "learning_rate": 4.09136e-06,
      "loss": 0.2142,
      "step": 36930
    },
    {
      "epoch": 1.18208,
      "grad_norm": 0.018248945474624634,
      "learning_rate": 4.089760000000001e-06,
      "loss": 0.1992,
      "step": 36940
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.01779274083673954,
      "learning_rate": 4.088160000000001e-06,
      "loss": 0.2006,
      "step": 36950
    },
    {
      "epoch": 1.18272,
      "grad_norm": 0.011554829776287079,
      "learning_rate": 4.0865600000000006e-06,
      "loss": 0.1992,
      "step": 36960
    },
    {
      "epoch": 1.18304,
      "grad_norm": 0.020114634186029434,
      "learning_rate": 4.0849600000000004e-06,
      "loss": 0.1992,
      "step": 36970
    },
    {
      "epoch": 1.18336,
      "grad_norm": 0.051090165972709656,
      "learning_rate": 4.08336e-06,
      "loss": 0.1995,
      "step": 36980
    },
    {
      "epoch": 1.18368,
      "grad_norm": 0.6995905637741089,
      "learning_rate": 4.08176e-06,
      "loss": 0.2146,
      "step": 36990
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.05826187506318092,
      "learning_rate": 4.08016e-06,
      "loss": 0.1994,
      "step": 37000
    },
    {
      "epoch": 1.184,
      "eval_runtime": 52.5443,
      "eval_samples_per_second": 190.316,
      "eval_steps_per_second": 11.895,
      "step": 37000
    },
    {
      "epoch": 1.18432,
      "grad_norm": 0.022080430760979652,
      "learning_rate": 4.07856e-06,
      "loss": 0.2003,
      "step": 37010
    },
    {
      "epoch": 1.18464,
      "grad_norm": 0.01739201694726944,
      "learning_rate": 4.076960000000001e-06,
      "loss": 0.1993,
      "step": 37020
    },
    {
      "epoch": 1.18496,
      "grad_norm": 0.045553144067525864,
      "learning_rate": 4.0753600000000006e-06,
      "loss": 0.2037,
      "step": 37030
    },
    {
      "epoch": 1.1852800000000001,
      "grad_norm": 0.03212553262710571,
      "learning_rate": 4.0737600000000004e-06,
      "loss": 0.1993,
      "step": 37040
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.023780157789587975,
      "learning_rate": 4.07216e-06,
      "loss": 0.1994,
      "step": 37050
    },
    {
      "epoch": 1.18592,
      "grad_norm": 0.021420113742351532,
      "learning_rate": 4.07056e-06,
      "loss": 0.1991,
      "step": 37060
    },
    {
      "epoch": 1.18624,
      "grad_norm": 0.010920657776296139,
      "learning_rate": 4.06896e-06,
      "loss": 0.1997,
      "step": 37070
    },
    {
      "epoch": 1.18656,
      "grad_norm": 0.024990173056721687,
      "learning_rate": 4.06736e-06,
      "loss": 0.2163,
      "step": 37080
    },
    {
      "epoch": 1.18688,
      "grad_norm": 0.026272481307387352,
      "learning_rate": 4.06576e-06,
      "loss": 0.2291,
      "step": 37090
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.022274943068623543,
      "learning_rate": 4.0641600000000006e-06,
      "loss": 0.212,
      "step": 37100
    },
    {
      "epoch": 1.18752,
      "grad_norm": 0.03266524896025658,
      "learning_rate": 4.0625600000000005e-06,
      "loss": 0.1996,
      "step": 37110
    },
    {
      "epoch": 1.18784,
      "grad_norm": 0.018475668504834175,
      "learning_rate": 4.06096e-06,
      "loss": 0.2132,
      "step": 37120
    },
    {
      "epoch": 1.1881599999999999,
      "grad_norm": 0.023112956434488297,
      "learning_rate": 4.05936e-06,
      "loss": 0.1992,
      "step": 37130
    },
    {
      "epoch": 1.18848,
      "grad_norm": 0.6280829906463623,
      "learning_rate": 4.057760000000001e-06,
      "loss": 0.2191,
      "step": 37140
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.022760067135095596,
      "learning_rate": 4.05616e-06,
      "loss": 0.1994,
      "step": 37150
    },
    {
      "epoch": 1.18912,
      "grad_norm": 0.028123928233981133,
      "learning_rate": 4.05456e-06,
      "loss": 0.2137,
      "step": 37160
    },
    {
      "epoch": 1.18944,
      "grad_norm": 0.040460750460624695,
      "learning_rate": 4.052960000000001e-06,
      "loss": 0.1991,
      "step": 37170
    },
    {
      "epoch": 1.18976,
      "grad_norm": 0.027449849992990494,
      "learning_rate": 4.0513600000000005e-06,
      "loss": 0.1991,
      "step": 37180
    },
    {
      "epoch": 1.19008,
      "grad_norm": 0.014888286590576172,
      "learning_rate": 4.04976e-06,
      "loss": 0.2017,
      "step": 37190
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.07890207320451736,
      "learning_rate": 4.04816e-06,
      "loss": 0.2142,
      "step": 37200
    },
    {
      "epoch": 1.19072,
      "grad_norm": 0.01817968115210533,
      "learning_rate": 4.04656e-06,
      "loss": 0.1993,
      "step": 37210
    },
    {
      "epoch": 1.19104,
      "grad_norm": 0.018139084801077843,
      "learning_rate": 4.044960000000001e-06,
      "loss": 0.1992,
      "step": 37220
    },
    {
      "epoch": 1.19136,
      "grad_norm": 0.0188002847135067,
      "learning_rate": 4.04336e-06,
      "loss": 0.1992,
      "step": 37230
    },
    {
      "epoch": 1.19168,
      "grad_norm": 0.10564170032739639,
      "learning_rate": 4.04176e-06,
      "loss": 0.2136,
      "step": 37240
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.018680037930607796,
      "learning_rate": 4.0401600000000005e-06,
      "loss": 0.1992,
      "step": 37250
    },
    {
      "epoch": 1.19232,
      "grad_norm": 0.013556734658777714,
      "learning_rate": 4.03856e-06,
      "loss": 0.1991,
      "step": 37260
    },
    {
      "epoch": 1.19264,
      "grad_norm": 0.06853030622005463,
      "learning_rate": 4.03696e-06,
      "loss": 0.1992,
      "step": 37270
    },
    {
      "epoch": 1.19296,
      "grad_norm": 0.026069143787026405,
      "learning_rate": 4.03536e-06,
      "loss": 0.2018,
      "step": 37280
    },
    {
      "epoch": 1.1932800000000001,
      "grad_norm": 2.834604263305664,
      "learning_rate": 4.033760000000001e-06,
      "loss": 0.2038,
      "step": 37290
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.02375909872353077,
      "learning_rate": 4.032160000000001e-06,
      "loss": 0.1992,
      "step": 37300
    },
    {
      "epoch": 1.19392,
      "grad_norm": 0.03517187759280205,
      "learning_rate": 4.030560000000001e-06,
      "loss": 0.1996,
      "step": 37310
    },
    {
      "epoch": 1.19424,
      "grad_norm": 0.049202751368284225,
      "learning_rate": 4.0289600000000005e-06,
      "loss": 0.2047,
      "step": 37320
    },
    {
      "epoch": 1.19456,
      "grad_norm": 1.6497236490249634,
      "learning_rate": 4.02736e-06,
      "loss": 0.2025,
      "step": 37330
    },
    {
      "epoch": 1.19488,
      "grad_norm": 0.06301405280828476,
      "learning_rate": 4.02576e-06,
      "loss": 0.1993,
      "step": 37340
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.05635417625308037,
      "learning_rate": 4.02416e-06,
      "loss": 0.2094,
      "step": 37350
    },
    {
      "epoch": 1.19552,
      "grad_norm": 0.01197097823023796,
      "learning_rate": 4.02256e-06,
      "loss": 0.1993,
      "step": 37360
    },
    {
      "epoch": 1.19584,
      "grad_norm": 0.014752269722521305,
      "learning_rate": 4.020960000000001e-06,
      "loss": 0.1992,
      "step": 37370
    },
    {
      "epoch": 1.19616,
      "grad_norm": 0.03687532618641853,
      "learning_rate": 4.019360000000001e-06,
      "loss": 0.2019,
      "step": 37380
    },
    {
      "epoch": 1.19648,
      "grad_norm": 0.03771147504448891,
      "learning_rate": 4.0177600000000005e-06,
      "loss": 0.1992,
      "step": 37390
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.013406474143266678,
      "learning_rate": 4.01616e-06,
      "loss": 0.1991,
      "step": 37400
    },
    {
      "epoch": 1.19712,
      "grad_norm": 0.02258705347776413,
      "learning_rate": 4.01456e-06,
      "loss": 0.1992,
      "step": 37410
    },
    {
      "epoch": 1.19744,
      "grad_norm": 0.03614117205142975,
      "learning_rate": 4.01296e-06,
      "loss": 0.199,
      "step": 37420
    },
    {
      "epoch": 1.19776,
      "grad_norm": 0.06060672923922539,
      "learning_rate": 4.01136e-06,
      "loss": 0.1996,
      "step": 37430
    },
    {
      "epoch": 1.19808,
      "grad_norm": 0.02372995764017105,
      "learning_rate": 4.009760000000001e-06,
      "loss": 0.2029,
      "step": 37440
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.015071758069097996,
      "learning_rate": 4.008160000000001e-06,
      "loss": 0.2002,
      "step": 37450
    },
    {
      "epoch": 1.19872,
      "grad_norm": 0.018523264676332474,
      "learning_rate": 4.0065600000000005e-06,
      "loss": 0.1992,
      "step": 37460
    },
    {
      "epoch": 1.19904,
      "grad_norm": 1.4032670259475708,
      "learning_rate": 4.00496e-06,
      "loss": 0.2134,
      "step": 37470
    },
    {
      "epoch": 1.19936,
      "grad_norm": 0.03177198767662048,
      "learning_rate": 4.00336e-06,
      "loss": 0.1991,
      "step": 37480
    },
    {
      "epoch": 1.19968,
      "grad_norm": 0.02285836823284626,
      "learning_rate": 4.00176e-06,
      "loss": 0.1994,
      "step": 37490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.02794855460524559,
      "learning_rate": 4.00016e-06,
      "loss": 0.2139,
      "step": 37500
    },
    {
      "epoch": 1.20032,
      "grad_norm": 0.978548526763916,
      "learning_rate": 3.99856e-06,
      "loss": 0.22,
      "step": 37510
    },
    {
      "epoch": 1.20064,
      "grad_norm": 0.031179029494524002,
      "learning_rate": 3.996960000000001e-06,
      "loss": 0.1992,
      "step": 37520
    },
    {
      "epoch": 1.20096,
      "grad_norm": 0.01859167404472828,
      "learning_rate": 3.9953600000000005e-06,
      "loss": 0.202,
      "step": 37530
    },
    {
      "epoch": 1.20128,
      "grad_norm": 0.016370078548789024,
      "learning_rate": 3.99376e-06,
      "loss": 0.199,
      "step": 37540
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.01586916856467724,
      "learning_rate": 3.99216e-06,
      "loss": 0.1992,
      "step": 37550
    },
    {
      "epoch": 1.2019199999999999,
      "grad_norm": 0.014232947491109371,
      "learning_rate": 3.99056e-06,
      "loss": 0.1994,
      "step": 37560
    },
    {
      "epoch": 1.20224,
      "grad_norm": 0.018410073593258858,
      "learning_rate": 3.98896e-06,
      "loss": 0.1991,
      "step": 37570
    },
    {
      "epoch": 1.20256,
      "grad_norm": 0.022627221420407295,
      "learning_rate": 3.98736e-06,
      "loss": 0.1991,
      "step": 37580
    },
    {
      "epoch": 1.20288,
      "grad_norm": 0.022476650774478912,
      "learning_rate": 3.98576e-06,
      "loss": 0.1993,
      "step": 37590
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.07046566158533096,
      "learning_rate": 3.9841600000000005e-06,
      "loss": 0.2154,
      "step": 37600
    },
    {
      "epoch": 1.20352,
      "grad_norm": 0.020511917769908905,
      "learning_rate": 3.98256e-06,
      "loss": 0.216,
      "step": 37610
    },
    {
      "epoch": 1.20384,
      "grad_norm": 0.032726895064115524,
      "learning_rate": 3.98096e-06,
      "loss": 0.1996,
      "step": 37620
    },
    {
      "epoch": 1.20416,
      "grad_norm": 0.015501921996474266,
      "learning_rate": 3.97936e-06,
      "loss": 0.1992,
      "step": 37630
    },
    {
      "epoch": 1.20448,
      "grad_norm": 0.026892973110079765,
      "learning_rate": 3.977760000000001e-06,
      "loss": 0.2099,
      "step": 37640
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.028816496953368187,
      "learning_rate": 3.976160000000001e-06,
      "loss": 0.1991,
      "step": 37650
    },
    {
      "epoch": 1.20512,
      "grad_norm": 0.12304249405860901,
      "learning_rate": 3.97456e-06,
      "loss": 0.1995,
      "step": 37660
    },
    {
      "epoch": 1.20544,
      "grad_norm": 0.01856827735900879,
      "learning_rate": 3.9729600000000005e-06,
      "loss": 0.1993,
      "step": 37670
    },
    {
      "epoch": 1.20576,
      "grad_norm": 0.013803883455693722,
      "learning_rate": 3.97136e-06,
      "loss": 0.1993,
      "step": 37680
    },
    {
      "epoch": 1.20608,
      "grad_norm": 0.04753443971276283,
      "learning_rate": 3.96976e-06,
      "loss": 0.199,
      "step": 37690
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.04880547523498535,
      "learning_rate": 3.96816e-06,
      "loss": 0.1992,
      "step": 37700
    },
    {
      "epoch": 1.20672,
      "grad_norm": 0.022226793691515923,
      "learning_rate": 3.96656e-06,
      "loss": 0.2148,
      "step": 37710
    },
    {
      "epoch": 1.2070400000000001,
      "grad_norm": 0.021493544802069664,
      "learning_rate": 3.964960000000001e-06,
      "loss": 0.1994,
      "step": 37720
    },
    {
      "epoch": 1.20736,
      "grad_norm": 0.026104582473635674,
      "learning_rate": 3.963360000000001e-06,
      "loss": 0.1992,
      "step": 37730
    },
    {
      "epoch": 1.20768,
      "grad_norm": 0.040111612528562546,
      "learning_rate": 3.96176e-06,
      "loss": 0.1994,
      "step": 37740
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.025077732279896736,
      "learning_rate": 3.96016e-06,
      "loss": 0.1992,
      "step": 37750
    },
    {
      "epoch": 1.20832,
      "grad_norm": 0.013217155821621418,
      "learning_rate": 3.95856e-06,
      "loss": 0.1992,
      "step": 37760
    },
    {
      "epoch": 1.20864,
      "grad_norm": 0.03131495416164398,
      "learning_rate": 3.95696e-06,
      "loss": 0.2041,
      "step": 37770
    },
    {
      "epoch": 1.20896,
      "grad_norm": 0.023720012977719307,
      "learning_rate": 3.95536e-06,
      "loss": 0.1998,
      "step": 37780
    },
    {
      "epoch": 1.20928,
      "grad_norm": 0.030456626787781715,
      "learning_rate": 3.953760000000001e-06,
      "loss": 0.1992,
      "step": 37790
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.05147155001759529,
      "learning_rate": 3.952160000000001e-06,
      "loss": 0.213,
      "step": 37800
    },
    {
      "epoch": 1.2099199999999999,
      "grad_norm": 0.7092418670654297,
      "learning_rate": 3.9505600000000005e-06,
      "loss": 0.1995,
      "step": 37810
    },
    {
      "epoch": 1.21024,
      "grad_norm": 0.016395632177591324,
      "learning_rate": 3.94896e-06,
      "loss": 0.1992,
      "step": 37820
    },
    {
      "epoch": 1.21056,
      "grad_norm": 0.22955943644046783,
      "learning_rate": 3.94736e-06,
      "loss": 0.2007,
      "step": 37830
    },
    {
      "epoch": 1.21088,
      "grad_norm": 0.0192048829048872,
      "learning_rate": 3.94576e-06,
      "loss": 0.2129,
      "step": 37840
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.023286912590265274,
      "learning_rate": 3.94416e-06,
      "loss": 0.2075,
      "step": 37850
    },
    {
      "epoch": 1.21152,
      "grad_norm": 0.024756215512752533,
      "learning_rate": 3.94256e-06,
      "loss": 0.1991,
      "step": 37860
    },
    {
      "epoch": 1.21184,
      "grad_norm": 0.04365069791674614,
      "learning_rate": 3.940960000000001e-06,
      "loss": 0.2175,
      "step": 37870
    },
    {
      "epoch": 1.21216,
      "grad_norm": 0.08509372919797897,
      "learning_rate": 3.9393600000000005e-06,
      "loss": 0.2091,
      "step": 37880
    },
    {
      "epoch": 1.21248,
      "grad_norm": 0.016909126192331314,
      "learning_rate": 3.93776e-06,
      "loss": 0.1992,
      "step": 37890
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.030708828940987587,
      "learning_rate": 3.93616e-06,
      "loss": 0.2174,
      "step": 37900
    },
    {
      "epoch": 1.21312,
      "grad_norm": 0.019031936302781105,
      "learning_rate": 3.93456e-06,
      "loss": 0.1991,
      "step": 37910
    },
    {
      "epoch": 1.21344,
      "grad_norm": 0.04381567984819412,
      "learning_rate": 3.93296e-06,
      "loss": 0.1991,
      "step": 37920
    },
    {
      "epoch": 1.21376,
      "grad_norm": 0.026775982230901718,
      "learning_rate": 3.93136e-06,
      "loss": 0.1992,
      "step": 37930
    },
    {
      "epoch": 1.21408,
      "grad_norm": 0.024887513369321823,
      "learning_rate": 3.929760000000001e-06,
      "loss": 0.1992,
      "step": 37940
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.0214498583227396,
      "learning_rate": 3.9281600000000005e-06,
      "loss": 0.2086,
      "step": 37950
    },
    {
      "epoch": 1.21472,
      "grad_norm": 0.671380341053009,
      "learning_rate": 3.92656e-06,
      "loss": 0.2125,
      "step": 37960
    },
    {
      "epoch": 1.2150400000000001,
      "grad_norm": 0.051345642656087875,
      "learning_rate": 3.92496e-06,
      "loss": 0.1992,
      "step": 37970
    },
    {
      "epoch": 1.21536,
      "grad_norm": 0.02197382226586342,
      "learning_rate": 3.92336e-06,
      "loss": 0.1994,
      "step": 37980
    },
    {
      "epoch": 1.21568,
      "grad_norm": 0.01604374311864376,
      "learning_rate": 3.921760000000001e-06,
      "loss": 0.1992,
      "step": 37990
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.03499188646674156,
      "learning_rate": 3.92016e-06,
      "loss": 0.2019,
      "step": 38000
    },
    {
      "epoch": 1.216,
      "eval_runtime": 52.8484,
      "eval_samples_per_second": 189.22,
      "eval_steps_per_second": 11.826,
      "step": 38000
    },
    {
      "epoch": 1.21632,
      "grad_norm": 0.02399097941815853,
      "learning_rate": 3.91856e-06,
      "loss": 0.1992,
      "step": 38010
    },
    {
      "epoch": 1.21664,
      "grad_norm": 0.06705285608768463,
      "learning_rate": 3.9169600000000005e-06,
      "loss": 0.1993,
      "step": 38020
    },
    {
      "epoch": 1.21696,
      "grad_norm": 0.036534860730171204,
      "learning_rate": 3.91536e-06,
      "loss": 0.2014,
      "step": 38030
    },
    {
      "epoch": 1.21728,
      "grad_norm": 0.011669080704450607,
      "learning_rate": 3.91376e-06,
      "loss": 0.2101,
      "step": 38040
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.018225789070129395,
      "learning_rate": 3.91216e-06,
      "loss": 0.1995,
      "step": 38050
    },
    {
      "epoch": 1.21792,
      "grad_norm": 0.010235634632408619,
      "learning_rate": 3.91056e-06,
      "loss": 0.1998,
      "step": 38060
    },
    {
      "epoch": 1.21824,
      "grad_norm": 0.035524237900972366,
      "learning_rate": 3.908960000000001e-06,
      "loss": 0.1992,
      "step": 38070
    },
    {
      "epoch": 1.21856,
      "grad_norm": 0.02498576231300831,
      "learning_rate": 3.907360000000001e-06,
      "loss": 0.215,
      "step": 38080
    },
    {
      "epoch": 1.21888,
      "grad_norm": 0.021659033372998238,
      "learning_rate": 3.90576e-06,
      "loss": 0.1993,
      "step": 38090
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.020227141678333282,
      "learning_rate": 3.90416e-06,
      "loss": 0.1996,
      "step": 38100
    },
    {
      "epoch": 1.21952,
      "grad_norm": 0.017161210998892784,
      "learning_rate": 3.90256e-06,
      "loss": 0.1993,
      "step": 38110
    },
    {
      "epoch": 1.21984,
      "grad_norm": 0.02322598174214363,
      "learning_rate": 3.90096e-06,
      "loss": 0.2283,
      "step": 38120
    },
    {
      "epoch": 1.22016,
      "grad_norm": 0.020619144663214684,
      "learning_rate": 3.89936e-06,
      "loss": 0.2159,
      "step": 38130
    },
    {
      "epoch": 1.22048,
      "grad_norm": 0.02100873365998268,
      "learning_rate": 3.897760000000001e-06,
      "loss": 0.2146,
      "step": 38140
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.7904833555221558,
      "learning_rate": 3.896160000000001e-06,
      "loss": 0.2146,
      "step": 38150
    },
    {
      "epoch": 1.22112,
      "grad_norm": 0.01149788312613964,
      "learning_rate": 3.8945600000000005e-06,
      "loss": 0.1993,
      "step": 38160
    },
    {
      "epoch": 1.22144,
      "grad_norm": 0.010157538577914238,
      "learning_rate": 3.89296e-06,
      "loss": 0.2148,
      "step": 38170
    },
    {
      "epoch": 1.22176,
      "grad_norm": 0.12434257566928864,
      "learning_rate": 3.89136e-06,
      "loss": 0.1993,
      "step": 38180
    },
    {
      "epoch": 1.22208,
      "grad_norm": 0.03000161424279213,
      "learning_rate": 3.88976e-06,
      "loss": 0.1993,
      "step": 38190
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.07118400931358337,
      "learning_rate": 3.88816e-06,
      "loss": 0.1993,
      "step": 38200
    },
    {
      "epoch": 1.22272,
      "grad_norm": 0.03784636780619621,
      "learning_rate": 3.88656e-06,
      "loss": 0.1992,
      "step": 38210
    },
    {
      "epoch": 1.22304,
      "grad_norm": 0.014972945675253868,
      "learning_rate": 3.884960000000001e-06,
      "loss": 0.1992,
      "step": 38220
    },
    {
      "epoch": 1.22336,
      "grad_norm": 0.010608203709125519,
      "learning_rate": 3.8833600000000005e-06,
      "loss": 0.2074,
      "step": 38230
    },
    {
      "epoch": 1.2236799999999999,
      "grad_norm": 0.05264883115887642,
      "learning_rate": 3.88176e-06,
      "loss": 0.1995,
      "step": 38240
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.0337260477244854,
      "learning_rate": 3.88016e-06,
      "loss": 0.1993,
      "step": 38250
    },
    {
      "epoch": 1.22432,
      "grad_norm": 0.6145846843719482,
      "learning_rate": 3.87856e-06,
      "loss": 0.2006,
      "step": 38260
    },
    {
      "epoch": 1.22464,
      "grad_norm": 0.009781764820218086,
      "learning_rate": 3.87696e-06,
      "loss": 0.1991,
      "step": 38270
    },
    {
      "epoch": 1.22496,
      "grad_norm": 0.02487121894955635,
      "learning_rate": 3.87536e-06,
      "loss": 0.1995,
      "step": 38280
    },
    {
      "epoch": 1.22528,
      "grad_norm": 0.027627483010292053,
      "learning_rate": 3.873760000000001e-06,
      "loss": 0.1992,
      "step": 38290
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.045629244297742844,
      "learning_rate": 3.8721600000000005e-06,
      "loss": 0.1993,
      "step": 38300
    },
    {
      "epoch": 1.22592,
      "grad_norm": 0.05920550972223282,
      "learning_rate": 3.8705600000000004e-06,
      "loss": 0.1997,
      "step": 38310
    },
    {
      "epoch": 1.22624,
      "grad_norm": 0.01315920241177082,
      "learning_rate": 3.86896e-06,
      "loss": 0.1999,
      "step": 38320
    },
    {
      "epoch": 1.22656,
      "grad_norm": 0.02433636039495468,
      "learning_rate": 3.86736e-06,
      "loss": 0.1992,
      "step": 38330
    },
    {
      "epoch": 1.22688,
      "grad_norm": 0.0399613156914711,
      "learning_rate": 3.86576e-06,
      "loss": 0.1992,
      "step": 38340
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.01869254745543003,
      "learning_rate": 3.86416e-06,
      "loss": 0.2175,
      "step": 38350
    },
    {
      "epoch": 1.22752,
      "grad_norm": 0.013868100009858608,
      "learning_rate": 3.86256e-06,
      "loss": 0.1995,
      "step": 38360
    },
    {
      "epoch": 1.22784,
      "grad_norm": 0.019572988152503967,
      "learning_rate": 3.8609600000000006e-06,
      "loss": 0.1992,
      "step": 38370
    },
    {
      "epoch": 1.22816,
      "grad_norm": 0.03126801922917366,
      "learning_rate": 3.8593600000000004e-06,
      "loss": 0.1992,
      "step": 38380
    },
    {
      "epoch": 1.22848,
      "grad_norm": 0.013779496774077415,
      "learning_rate": 3.85776e-06,
      "loss": 0.1995,
      "step": 38390
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.03440694510936737,
      "learning_rate": 3.85616e-06,
      "loss": 0.1993,
      "step": 38400
    },
    {
      "epoch": 1.22912,
      "grad_norm": 0.03458625078201294,
      "learning_rate": 3.854560000000001e-06,
      "loss": 0.2245,
      "step": 38410
    },
    {
      "epoch": 1.22944,
      "grad_norm": 0.025472335517406464,
      "learning_rate": 3.852960000000001e-06,
      "loss": 0.213,
      "step": 38420
    },
    {
      "epoch": 1.22976,
      "grad_norm": 2.8563036918640137,
      "learning_rate": 3.85136e-06,
      "loss": 0.2063,
      "step": 38430
    },
    {
      "epoch": 1.23008,
      "grad_norm": 0.02203608863055706,
      "learning_rate": 3.8497600000000006e-06,
      "loss": 0.2012,
      "step": 38440
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.013670075684785843,
      "learning_rate": 3.8481600000000004e-06,
      "loss": 0.2047,
      "step": 38450
    },
    {
      "epoch": 1.23072,
      "grad_norm": 0.023252248764038086,
      "learning_rate": 3.84656e-06,
      "loss": 0.1994,
      "step": 38460
    },
    {
      "epoch": 1.23104,
      "grad_norm": 0.015259253792464733,
      "learning_rate": 3.84496e-06,
      "loss": 0.1991,
      "step": 38470
    },
    {
      "epoch": 1.23136,
      "grad_norm": 0.015965372323989868,
      "learning_rate": 3.84336e-06,
      "loss": 0.1992,
      "step": 38480
    },
    {
      "epoch": 1.2316799999999999,
      "grad_norm": 0.028423473238945007,
      "learning_rate": 3.841760000000001e-06,
      "loss": 0.1991,
      "step": 38490
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.033172961324453354,
      "learning_rate": 3.840160000000001e-06,
      "loss": 0.1992,
      "step": 38500
    },
    {
      "epoch": 1.23232,
      "grad_norm": 0.020221559330821037,
      "learning_rate": 3.83856e-06,
      "loss": 0.1992,
      "step": 38510
    },
    {
      "epoch": 1.23264,
      "grad_norm": 0.022300366312265396,
      "learning_rate": 3.8369600000000004e-06,
      "loss": 0.1991,
      "step": 38520
    },
    {
      "epoch": 1.23296,
      "grad_norm": 0.17956025898456573,
      "learning_rate": 3.83536e-06,
      "loss": 0.2147,
      "step": 38530
    },
    {
      "epoch": 1.23328,
      "grad_norm": 0.01504809781908989,
      "learning_rate": 3.83376e-06,
      "loss": 0.2151,
      "step": 38540
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.017937134951353073,
      "learning_rate": 3.83216e-06,
      "loss": 0.2166,
      "step": 38550
    },
    {
      "epoch": 1.23392,
      "grad_norm": 0.023493653163313866,
      "learning_rate": 3.830560000000001e-06,
      "loss": 0.199,
      "step": 38560
    },
    {
      "epoch": 1.23424,
      "grad_norm": 0.029950356110930443,
      "learning_rate": 3.828960000000001e-06,
      "loss": 0.1992,
      "step": 38570
    },
    {
      "epoch": 1.23456,
      "grad_norm": 0.033551640808582306,
      "learning_rate": 3.8273600000000006e-06,
      "loss": 0.212,
      "step": 38580
    },
    {
      "epoch": 1.23488,
      "grad_norm": 0.062124960124492645,
      "learning_rate": 3.8257600000000005e-06,
      "loss": 0.1994,
      "step": 38590
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.057512953877449036,
      "learning_rate": 3.82416e-06,
      "loss": 0.2088,
      "step": 38600
    },
    {
      "epoch": 1.23552,
      "grad_norm": 0.03607403486967087,
      "learning_rate": 3.82256e-06,
      "loss": 0.1991,
      "step": 38610
    },
    {
      "epoch": 1.23584,
      "grad_norm": 0.010284007526934147,
      "learning_rate": 3.82096e-06,
      "loss": 0.1996,
      "step": 38620
    },
    {
      "epoch": 1.23616,
      "grad_norm": 0.019495880231261253,
      "learning_rate": 3.81936e-06,
      "loss": 0.1991,
      "step": 38630
    },
    {
      "epoch": 1.23648,
      "grad_norm": 0.10875075310468674,
      "learning_rate": 3.817760000000001e-06,
      "loss": 0.2145,
      "step": 38640
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.03395569697022438,
      "learning_rate": 3.8161600000000006e-06,
      "loss": 0.1994,
      "step": 38650
    },
    {
      "epoch": 1.23712,
      "grad_norm": 0.013145503588020802,
      "learning_rate": 3.8145600000000005e-06,
      "loss": 0.1995,
      "step": 38660
    },
    {
      "epoch": 1.23744,
      "grad_norm": 0.04358724132180214,
      "learning_rate": 3.8129600000000008e-06,
      "loss": 0.2141,
      "step": 38670
    },
    {
      "epoch": 1.23776,
      "grad_norm": 0.08626115322113037,
      "learning_rate": 3.8113600000000002e-06,
      "loss": 0.202,
      "step": 38680
    },
    {
      "epoch": 1.23808,
      "grad_norm": 0.031413231045007706,
      "learning_rate": 3.80976e-06,
      "loss": 0.1993,
      "step": 38690
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.024211712181568146,
      "learning_rate": 3.8081600000000004e-06,
      "loss": 0.2151,
      "step": 38700
    },
    {
      "epoch": 1.23872,
      "grad_norm": 0.010999691672623158,
      "learning_rate": 3.8065600000000003e-06,
      "loss": 0.1996,
      "step": 38710
    },
    {
      "epoch": 1.23904,
      "grad_norm": 0.5730366706848145,
      "learning_rate": 3.80496e-06,
      "loss": 0.2078,
      "step": 38720
    },
    {
      "epoch": 1.23936,
      "grad_norm": 0.330789178609848,
      "learning_rate": 3.8033600000000005e-06,
      "loss": 0.2119,
      "step": 38730
    },
    {
      "epoch": 1.23968,
      "grad_norm": 0.048424676060676575,
      "learning_rate": 3.8017600000000003e-06,
      "loss": 0.1991,
      "step": 38740
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.070241093635559,
      "learning_rate": 3.8001600000000006e-06,
      "loss": 0.2098,
      "step": 38750
    },
    {
      "epoch": 1.24032,
      "grad_norm": 0.03313690796494484,
      "learning_rate": 3.7985600000000005e-06,
      "loss": 0.199,
      "step": 38760
    },
    {
      "epoch": 1.24064,
      "grad_norm": 0.02887953817844391,
      "learning_rate": 3.79696e-06,
      "loss": 0.215,
      "step": 38770
    },
    {
      "epoch": 1.24096,
      "grad_norm": 0.07628272473812103,
      "learning_rate": 3.7953600000000003e-06,
      "loss": 0.1993,
      "step": 38780
    },
    {
      "epoch": 1.24128,
      "grad_norm": 0.016181645914912224,
      "learning_rate": 3.79376e-06,
      "loss": 0.2099,
      "step": 38790
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.01248638890683651,
      "learning_rate": 3.7921600000000005e-06,
      "loss": 0.1994,
      "step": 38800
    },
    {
      "epoch": 1.24192,
      "grad_norm": 0.01815752685070038,
      "learning_rate": 3.7905600000000003e-06,
      "loss": 0.1995,
      "step": 38810
    },
    {
      "epoch": 1.24224,
      "grad_norm": 0.02377863973379135,
      "learning_rate": 3.7889600000000002e-06,
      "loss": 0.1991,
      "step": 38820
    },
    {
      "epoch": 1.24256,
      "grad_norm": 0.04405198618769646,
      "learning_rate": 3.7873600000000005e-06,
      "loss": 0.199,
      "step": 38830
    },
    {
      "epoch": 1.24288,
      "grad_norm": 0.04779400676488876,
      "learning_rate": 3.7857600000000004e-06,
      "loss": 0.1994,
      "step": 38840
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.02275773510336876,
      "learning_rate": 3.78416e-06,
      "loss": 0.2379,
      "step": 38850
    },
    {
      "epoch": 1.24352,
      "grad_norm": 1.1263126134872437,
      "learning_rate": 3.78256e-06,
      "loss": 0.2344,
      "step": 38860
    },
    {
      "epoch": 1.24384,
      "grad_norm": 0.03342210873961449,
      "learning_rate": 3.78096e-06,
      "loss": 0.1991,
      "step": 38870
    },
    {
      "epoch": 1.24416,
      "grad_norm": 0.014254041947424412,
      "learning_rate": 3.7793600000000004e-06,
      "loss": 0.199,
      "step": 38880
    },
    {
      "epoch": 1.24448,
      "grad_norm": 0.02276403084397316,
      "learning_rate": 3.7777600000000002e-06,
      "loss": 0.1991,
      "step": 38890
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.2732442319393158,
      "learning_rate": 3.7761600000000005e-06,
      "loss": 0.1997,
      "step": 38900
    },
    {
      "epoch": 1.24512,
      "grad_norm": 1.1142939329147339,
      "learning_rate": 3.7745600000000004e-06,
      "loss": 0.202,
      "step": 38910
    },
    {
      "epoch": 1.2454399999999999,
      "grad_norm": 0.018857041373848915,
      "learning_rate": 3.7729600000000007e-06,
      "loss": 0.2164,
      "step": 38920
    },
    {
      "epoch": 1.24576,
      "grad_norm": 0.04385235533118248,
      "learning_rate": 3.7713600000000006e-06,
      "loss": 0.2225,
      "step": 38930
    },
    {
      "epoch": 1.24608,
      "grad_norm": 0.04482660070061684,
      "learning_rate": 3.76976e-06,
      "loss": 0.1997,
      "step": 38940
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.017580753192305565,
      "learning_rate": 3.7681600000000004e-06,
      "loss": 0.1997,
      "step": 38950
    },
    {
      "epoch": 1.24672,
      "grad_norm": 0.03064112365245819,
      "learning_rate": 3.7665600000000002e-06,
      "loss": 0.1991,
      "step": 38960
    },
    {
      "epoch": 1.24704,
      "grad_norm": 0.029285896569490433,
      "learning_rate": 3.76496e-06,
      "loss": 0.2016,
      "step": 38970
    },
    {
      "epoch": 1.24736,
      "grad_norm": 0.02335202321410179,
      "learning_rate": 3.7633600000000004e-06,
      "loss": 0.1993,
      "step": 38980
    },
    {
      "epoch": 1.24768,
      "grad_norm": 0.03720642998814583,
      "learning_rate": 3.7617600000000003e-06,
      "loss": 0.216,
      "step": 38990
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.06060315668582916,
      "learning_rate": 3.7601600000000006e-06,
      "loss": 0.1993,
      "step": 39000
    },
    {
      "epoch": 1.248,
      "eval_runtime": 49.4776,
      "eval_samples_per_second": 202.112,
      "eval_steps_per_second": 12.632,
      "step": 39000
    },
    {
      "epoch": 1.24832,
      "grad_norm": 0.012886731885373592,
      "learning_rate": 3.7585600000000005e-06,
      "loss": 0.1998,
      "step": 39010
    },
    {
      "epoch": 1.24864,
      "grad_norm": 0.022015387192368507,
      "learning_rate": 3.75696e-06,
      "loss": 0.2008,
      "step": 39020
    },
    {
      "epoch": 1.24896,
      "grad_norm": 0.03911609575152397,
      "learning_rate": 3.7553600000000002e-06,
      "loss": 0.1992,
      "step": 39030
    },
    {
      "epoch": 1.24928,
      "grad_norm": 0.33377379179000854,
      "learning_rate": 3.75376e-06,
      "loss": 0.1997,
      "step": 39040
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.021538468077778816,
      "learning_rate": 3.7521600000000004e-06,
      "loss": 0.1993,
      "step": 39050
    },
    {
      "epoch": 1.24992,
      "grad_norm": 0.11576216667890549,
      "learning_rate": 3.7505600000000003e-06,
      "loss": 0.2188,
      "step": 39060
    },
    {
      "epoch": 1.25024,
      "grad_norm": 0.03780863806605339,
      "learning_rate": 3.74896e-06,
      "loss": 0.1992,
      "step": 39070
    },
    {
      "epoch": 1.2505600000000001,
      "grad_norm": 0.026809249073266983,
      "learning_rate": 3.7473600000000005e-06,
      "loss": 0.2155,
      "step": 39080
    },
    {
      "epoch": 1.25088,
      "grad_norm": 0.028330199420452118,
      "learning_rate": 3.7457600000000004e-06,
      "loss": 0.2146,
      "step": 39090
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.09703127294778824,
      "learning_rate": 3.7441600000000007e-06,
      "loss": 0.2096,
      "step": 39100
    },
    {
      "epoch": 1.25152,
      "grad_norm": 0.03115590661764145,
      "learning_rate": 3.74256e-06,
      "loss": 0.206,
      "step": 39110
    },
    {
      "epoch": 1.25184,
      "grad_norm": 0.03367605060338974,
      "learning_rate": 3.74096e-06,
      "loss": 0.1991,
      "step": 39120
    },
    {
      "epoch": 1.25216,
      "grad_norm": 0.02878672257065773,
      "learning_rate": 3.7393600000000003e-06,
      "loss": 0.1993,
      "step": 39130
    },
    {
      "epoch": 1.25248,
      "grad_norm": 0.02582699991762638,
      "learning_rate": 3.73776e-06,
      "loss": 0.1997,
      "step": 39140
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.05075838789343834,
      "learning_rate": 3.7361600000000005e-06,
      "loss": 0.2046,
      "step": 39150
    },
    {
      "epoch": 1.25312,
      "grad_norm": 0.06517111510038376,
      "learning_rate": 3.7345600000000004e-06,
      "loss": 0.2016,
      "step": 39160
    },
    {
      "epoch": 1.2534399999999999,
      "grad_norm": 0.03828445076942444,
      "learning_rate": 3.7329600000000007e-06,
      "loss": 0.1991,
      "step": 39170
    },
    {
      "epoch": 1.25376,
      "grad_norm": 0.03183230757713318,
      "learning_rate": 3.7313600000000006e-06,
      "loss": 0.2003,
      "step": 39180
    },
    {
      "epoch": 1.25408,
      "grad_norm": 0.025584300979971886,
      "learning_rate": 3.72976e-06,
      "loss": 0.2268,
      "step": 39190
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.016608022153377533,
      "learning_rate": 3.7281600000000003e-06,
      "loss": 0.1992,
      "step": 39200
    },
    {
      "epoch": 1.25472,
      "grad_norm": 0.019663147628307343,
      "learning_rate": 3.72656e-06,
      "loss": 0.1993,
      "step": 39210
    },
    {
      "epoch": 1.25504,
      "grad_norm": 0.014812824316322803,
      "learning_rate": 3.72496e-06,
      "loss": 0.2142,
      "step": 39220
    },
    {
      "epoch": 1.25536,
      "grad_norm": 0.03799397125840187,
      "learning_rate": 3.7233600000000004e-06,
      "loss": 0.1998,
      "step": 39230
    },
    {
      "epoch": 1.25568,
      "grad_norm": 0.02351965755224228,
      "learning_rate": 3.7217600000000003e-06,
      "loss": 0.2,
      "step": 39240
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.042366378009319305,
      "learning_rate": 3.7201600000000006e-06,
      "loss": 0.1993,
      "step": 39250
    },
    {
      "epoch": 1.25632,
      "grad_norm": 0.0614129863679409,
      "learning_rate": 3.7185600000000004e-06,
      "loss": 0.1992,
      "step": 39260
    },
    {
      "epoch": 1.25664,
      "grad_norm": 0.024984639137983322,
      "learning_rate": 3.7169600000000007e-06,
      "loss": 0.203,
      "step": 39270
    },
    {
      "epoch": 1.25696,
      "grad_norm": 0.028636062517762184,
      "learning_rate": 3.71536e-06,
      "loss": 0.1993,
      "step": 39280
    },
    {
      "epoch": 1.25728,
      "grad_norm": 0.02124587818980217,
      "learning_rate": 3.71376e-06,
      "loss": 0.1993,
      "step": 39290
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.039447490125894547,
      "learning_rate": 3.7121600000000004e-06,
      "loss": 0.1991,
      "step": 39300
    },
    {
      "epoch": 1.25792,
      "grad_norm": 0.06349662691354752,
      "learning_rate": 3.7105600000000003e-06,
      "loss": 0.1992,
      "step": 39310
    },
    {
      "epoch": 1.25824,
      "grad_norm": 0.06679857522249222,
      "learning_rate": 3.70896e-06,
      "loss": 0.2062,
      "step": 39320
    },
    {
      "epoch": 1.2585600000000001,
      "grad_norm": 0.0241280198097229,
      "learning_rate": 3.7073600000000004e-06,
      "loss": 0.2046,
      "step": 39330
    },
    {
      "epoch": 1.25888,
      "grad_norm": 0.02040502242743969,
      "learning_rate": 3.7057600000000003e-06,
      "loss": 0.1997,
      "step": 39340
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.08522626757621765,
      "learning_rate": 3.7041600000000006e-06,
      "loss": 0.2166,
      "step": 39350
    },
    {
      "epoch": 1.25952,
      "grad_norm": 0.01583937369287014,
      "learning_rate": 3.70256e-06,
      "loss": 0.1994,
      "step": 39360
    },
    {
      "epoch": 1.25984,
      "grad_norm": 0.04237521067261696,
      "learning_rate": 3.70096e-06,
      "loss": 0.1994,
      "step": 39370
    },
    {
      "epoch": 1.26016,
      "grad_norm": 0.03344140574336052,
      "learning_rate": 3.6993600000000003e-06,
      "loss": 0.1993,
      "step": 39380
    },
    {
      "epoch": 1.26048,
      "grad_norm": 0.12178858369588852,
      "learning_rate": 3.69776e-06,
      "loss": 0.1994,
      "step": 39390
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.034327417612075806,
      "learning_rate": 3.6961600000000005e-06,
      "loss": 0.2069,
      "step": 39400
    },
    {
      "epoch": 1.26112,
      "grad_norm": 0.013238205574452877,
      "learning_rate": 3.6945600000000003e-06,
      "loss": 0.1996,
      "step": 39410
    },
    {
      "epoch": 1.26144,
      "grad_norm": 0.04968158155679703,
      "learning_rate": 3.6929600000000006e-06,
      "loss": 0.1994,
      "step": 39420
    },
    {
      "epoch": 1.26176,
      "grad_norm": 0.026468327268958092,
      "learning_rate": 3.6913600000000005e-06,
      "loss": 0.1992,
      "step": 39430
    },
    {
      "epoch": 1.26208,
      "grad_norm": 0.027036746963858604,
      "learning_rate": 3.6897600000000004e-06,
      "loss": 0.1992,
      "step": 39440
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.010895177721977234,
      "learning_rate": 3.6881600000000003e-06,
      "loss": 0.1992,
      "step": 39450
    },
    {
      "epoch": 1.26272,
      "grad_norm": 0.18429459631443024,
      "learning_rate": 3.68656e-06,
      "loss": 0.1995,
      "step": 39460
    },
    {
      "epoch": 1.26304,
      "grad_norm": 0.024513494223356247,
      "learning_rate": 3.68496e-06,
      "loss": 0.2007,
      "step": 39470
    },
    {
      "epoch": 1.26336,
      "grad_norm": 0.022712908685207367,
      "learning_rate": 3.6833600000000003e-06,
      "loss": 0.1992,
      "step": 39480
    },
    {
      "epoch": 1.26368,
      "grad_norm": 0.03469870984554291,
      "learning_rate": 3.6817600000000002e-06,
      "loss": 0.199,
      "step": 39490
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.011043381877243519,
      "learning_rate": 3.6801600000000005e-06,
      "loss": 0.2075,
      "step": 39500
    },
    {
      "epoch": 1.26432,
      "grad_norm": 0.033886417746543884,
      "learning_rate": 3.6785600000000004e-06,
      "loss": 0.2051,
      "step": 39510
    },
    {
      "epoch": 1.26464,
      "grad_norm": 0.04226125404238701,
      "learning_rate": 3.6769600000000007e-06,
      "loss": 0.2195,
      "step": 39520
    },
    {
      "epoch": 1.2649599999999999,
      "grad_norm": 1.0159153938293457,
      "learning_rate": 3.67536e-06,
      "loss": 0.2103,
      "step": 39530
    },
    {
      "epoch": 1.26528,
      "grad_norm": 0.0196521133184433,
      "learning_rate": 3.67376e-06,
      "loss": 0.2162,
      "step": 39540
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.02186921238899231,
      "learning_rate": 3.6721600000000003e-06,
      "loss": 0.2088,
      "step": 39550
    },
    {
      "epoch": 1.26592,
      "grad_norm": 0.02086353674530983,
      "learning_rate": 3.6705600000000002e-06,
      "loss": 0.1991,
      "step": 39560
    },
    {
      "epoch": 1.26624,
      "grad_norm": 0.014658091589808464,
      "learning_rate": 3.66896e-06,
      "loss": 0.1992,
      "step": 39570
    },
    {
      "epoch": 1.2665600000000001,
      "grad_norm": 0.03793574497103691,
      "learning_rate": 3.6673600000000004e-06,
      "loss": 0.1992,
      "step": 39580
    },
    {
      "epoch": 1.26688,
      "grad_norm": 0.020541280508041382,
      "learning_rate": 3.6657600000000003e-06,
      "loss": 0.1998,
      "step": 39590
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.014150843024253845,
      "learning_rate": 3.6641600000000006e-06,
      "loss": 0.1993,
      "step": 39600
    },
    {
      "epoch": 1.26752,
      "grad_norm": 0.026411933824419975,
      "learning_rate": 3.6625600000000005e-06,
      "loss": 0.1992,
      "step": 39610
    },
    {
      "epoch": 1.26784,
      "grad_norm": 0.025263207033276558,
      "learning_rate": 3.66096e-06,
      "loss": 0.1994,
      "step": 39620
    },
    {
      "epoch": 1.26816,
      "grad_norm": 0.022659435868263245,
      "learning_rate": 3.6593600000000002e-06,
      "loss": 0.2189,
      "step": 39630
    },
    {
      "epoch": 1.26848,
      "grad_norm": 0.035824261605739594,
      "learning_rate": 3.65776e-06,
      "loss": 0.1992,
      "step": 39640
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.033528152853250504,
      "learning_rate": 3.6561600000000004e-06,
      "loss": 0.2143,
      "step": 39650
    },
    {
      "epoch": 1.26912,
      "grad_norm": 0.036822423338890076,
      "learning_rate": 3.6545600000000003e-06,
      "loss": 0.2141,
      "step": 39660
    },
    {
      "epoch": 1.26944,
      "grad_norm": 0.030023308470845222,
      "learning_rate": 3.6529600000000006e-06,
      "loss": 0.2105,
      "step": 39670
    },
    {
      "epoch": 1.26976,
      "grad_norm": 0.026549996808171272,
      "learning_rate": 3.6513600000000005e-06,
      "loss": 0.1996,
      "step": 39680
    },
    {
      "epoch": 1.27008,
      "grad_norm": 0.05301930755376816,
      "learning_rate": 3.6497600000000004e-06,
      "loss": 0.1998,
      "step": 39690
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.012992526404559612,
      "learning_rate": 3.6481600000000002e-06,
      "loss": 0.2042,
      "step": 39700
    },
    {
      "epoch": 1.27072,
      "grad_norm": 0.01722628064453602,
      "learning_rate": 3.64656e-06,
      "loss": 0.1991,
      "step": 39710
    },
    {
      "epoch": 1.27104,
      "grad_norm": 0.014302399940788746,
      "learning_rate": 3.64496e-06,
      "loss": 0.199,
      "step": 39720
    },
    {
      "epoch": 1.27136,
      "grad_norm": 0.014396611601114273,
      "learning_rate": 3.6433600000000003e-06,
      "loss": 0.1996,
      "step": 39730
    },
    {
      "epoch": 1.27168,
      "grad_norm": 0.01433334220200777,
      "learning_rate": 3.64176e-06,
      "loss": 0.1996,
      "step": 39740
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.020333589985966682,
      "learning_rate": 3.6401600000000005e-06,
      "loss": 0.1993,
      "step": 39750
    },
    {
      "epoch": 1.2723200000000001,
      "grad_norm": 0.07023236900568008,
      "learning_rate": 3.6385600000000004e-06,
      "loss": 0.1993,
      "step": 39760
    },
    {
      "epoch": 1.27264,
      "grad_norm": 0.018550079315900803,
      "learning_rate": 3.6369600000000007e-06,
      "loss": 0.1991,
      "step": 39770
    },
    {
      "epoch": 1.2729599999999999,
      "grad_norm": 0.019407009705901146,
      "learning_rate": 3.6353600000000005e-06,
      "loss": 0.2074,
      "step": 39780
    },
    {
      "epoch": 1.27328,
      "grad_norm": 0.013359551317989826,
      "learning_rate": 3.63376e-06,
      "loss": 0.1991,
      "step": 39790
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.015084570273756981,
      "learning_rate": 3.6321600000000003e-06,
      "loss": 0.2113,
      "step": 39800
    },
    {
      "epoch": 1.27392,
      "grad_norm": 0.049992065876722336,
      "learning_rate": 3.63056e-06,
      "loss": 0.1992,
      "step": 39810
    },
    {
      "epoch": 1.27424,
      "grad_norm": 0.039814457297325134,
      "learning_rate": 3.62896e-06,
      "loss": 0.1991,
      "step": 39820
    },
    {
      "epoch": 1.2745600000000001,
      "grad_norm": 0.020737070590257645,
      "learning_rate": 3.6273600000000004e-06,
      "loss": 0.212,
      "step": 39830
    },
    {
      "epoch": 1.27488,
      "grad_norm": 0.039800502359867096,
      "learning_rate": 3.6257600000000002e-06,
      "loss": 0.1992,
      "step": 39840
    },
    {
      "epoch": 1.2752,
      "grad_norm": 1.7810461521148682,
      "learning_rate": 3.6241600000000006e-06,
      "loss": 0.2157,
      "step": 39850
    },
    {
      "epoch": 1.27552,
      "grad_norm": 0.016100993379950523,
      "learning_rate": 3.6225600000000004e-06,
      "loss": 0.1994,
      "step": 39860
    },
    {
      "epoch": 1.27584,
      "grad_norm": 0.03553535416722298,
      "learning_rate": 3.62096e-06,
      "loss": 0.1992,
      "step": 39870
    },
    {
      "epoch": 1.27616,
      "grad_norm": 0.5992897748947144,
      "learning_rate": 3.61936e-06,
      "loss": 0.2008,
      "step": 39880
    },
    {
      "epoch": 1.27648,
      "grad_norm": 0.026044722646474838,
      "learning_rate": 3.61776e-06,
      "loss": 0.1992,
      "step": 39890
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.015917284414172173,
      "learning_rate": 3.6161600000000004e-06,
      "loss": 0.2149,
      "step": 39900
    },
    {
      "epoch": 1.27712,
      "grad_norm": 0.04998933896422386,
      "learning_rate": 3.6145600000000003e-06,
      "loss": 0.1996,
      "step": 39910
    },
    {
      "epoch": 1.27744,
      "grad_norm": 0.010677964426577091,
      "learning_rate": 3.6129600000000006e-06,
      "loss": 0.1997,
      "step": 39920
    },
    {
      "epoch": 1.27776,
      "grad_norm": 0.020261120051145554,
      "learning_rate": 3.6113600000000004e-06,
      "loss": 0.1991,
      "step": 39930
    },
    {
      "epoch": 1.27808,
      "grad_norm": 0.025753479450941086,
      "learning_rate": 3.6097600000000003e-06,
      "loss": 0.1993,
      "step": 39940
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.01740110293030739,
      "learning_rate": 3.6081600000000006e-06,
      "loss": 0.1993,
      "step": 39950
    },
    {
      "epoch": 1.27872,
      "grad_norm": 0.031495023518800735,
      "learning_rate": 3.60656e-06,
      "loss": 0.1992,
      "step": 39960
    },
    {
      "epoch": 1.27904,
      "grad_norm": 0.01447139773517847,
      "learning_rate": 3.60496e-06,
      "loss": 0.1994,
      "step": 39970
    },
    {
      "epoch": 1.27936,
      "grad_norm": 0.023165704682469368,
      "learning_rate": 3.6033600000000003e-06,
      "loss": 0.1994,
      "step": 39980
    },
    {
      "epoch": 1.27968,
      "grad_norm": 0.022506214678287506,
      "learning_rate": 3.60176e-06,
      "loss": 0.2198,
      "step": 39990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.017133403569459915,
      "learning_rate": 3.6001600000000004e-06,
      "loss": 0.2009,
      "step": 40000
    },
    {
      "epoch": 1.28,
      "eval_runtime": 72.0367,
      "eval_samples_per_second": 138.818,
      "eval_steps_per_second": 8.676,
      "step": 40000
    },
    {
      "epoch": 1.2803200000000001,
      "grad_norm": 0.031334247440099716,
      "learning_rate": 3.5985600000000003e-06,
      "loss": 0.1992,
      "step": 40010
    },
    {
      "epoch": 1.28064,
      "grad_norm": 0.045520439743995667,
      "learning_rate": 3.5969600000000006e-06,
      "loss": 0.1993,
      "step": 40020
    },
    {
      "epoch": 1.2809599999999999,
      "grad_norm": 0.036313656717538834,
      "learning_rate": 3.5953600000000005e-06,
      "loss": 0.2117,
      "step": 40030
    },
    {
      "epoch": 1.28128,
      "grad_norm": 0.04332664981484413,
      "learning_rate": 3.59376e-06,
      "loss": 0.1992,
      "step": 40040
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.033454395830631256,
      "learning_rate": 3.5921600000000003e-06,
      "loss": 0.1992,
      "step": 40050
    },
    {
      "epoch": 1.28192,
      "grad_norm": 0.019831858575344086,
      "learning_rate": 3.59056e-06,
      "loss": 0.1995,
      "step": 40060
    },
    {
      "epoch": 1.28224,
      "grad_norm": 0.022897610440850258,
      "learning_rate": 3.58896e-06,
      "loss": 0.2068,
      "step": 40070
    },
    {
      "epoch": 1.28256,
      "grad_norm": 0.03342614695429802,
      "learning_rate": 3.5873600000000003e-06,
      "loss": 0.1993,
      "step": 40080
    },
    {
      "epoch": 1.28288,
      "grad_norm": 0.023819060996174812,
      "learning_rate": 3.58576e-06,
      "loss": 0.2306,
      "step": 40090
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.0394539013504982,
      "learning_rate": 3.5841600000000005e-06,
      "loss": 0.201,
      "step": 40100
    },
    {
      "epoch": 1.28352,
      "grad_norm": 0.012621819972991943,
      "learning_rate": 3.5825600000000004e-06,
      "loss": 0.1991,
      "step": 40110
    },
    {
      "epoch": 1.28384,
      "grad_norm": 0.02097325213253498,
      "learning_rate": 3.5809600000000007e-06,
      "loss": 0.2162,
      "step": 40120
    },
    {
      "epoch": 1.28416,
      "grad_norm": 0.02831624634563923,
      "learning_rate": 3.57936e-06,
      "loss": 0.1993,
      "step": 40130
    },
    {
      "epoch": 1.28448,
      "grad_norm": 0.014863965101540089,
      "learning_rate": 3.57776e-06,
      "loss": 0.1995,
      "step": 40140
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.03481455519795418,
      "learning_rate": 3.5761600000000003e-06,
      "loss": 0.1991,
      "step": 40150
    },
    {
      "epoch": 1.28512,
      "grad_norm": 0.02649247646331787,
      "learning_rate": 3.5745600000000002e-06,
      "loss": 0.2001,
      "step": 40160
    },
    {
      "epoch": 1.28544,
      "grad_norm": 0.016285762190818787,
      "learning_rate": 3.5729600000000005e-06,
      "loss": 0.2281,
      "step": 40170
    },
    {
      "epoch": 1.28576,
      "grad_norm": 0.03405217081308365,
      "learning_rate": 3.5713600000000004e-06,
      "loss": 0.2132,
      "step": 40180
    },
    {
      "epoch": 1.2860800000000001,
      "grad_norm": 0.02203637920320034,
      "learning_rate": 3.5697600000000003e-06,
      "loss": 0.1991,
      "step": 40190
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.04442669078707695,
      "learning_rate": 3.5681600000000006e-06,
      "loss": 0.1996,
      "step": 40200
    },
    {
      "epoch": 1.2867199999999999,
      "grad_norm": 0.027844199910759926,
      "learning_rate": 3.56656e-06,
      "loss": 0.1991,
      "step": 40210
    },
    {
      "epoch": 1.28704,
      "grad_norm": 0.07230915874242783,
      "learning_rate": 3.56496e-06,
      "loss": 0.1995,
      "step": 40220
    },
    {
      "epoch": 1.28736,
      "grad_norm": 0.04096928983926773,
      "learning_rate": 3.5633600000000002e-06,
      "loss": 0.1992,
      "step": 40230
    },
    {
      "epoch": 1.28768,
      "grad_norm": 0.047772038727998734,
      "learning_rate": 3.56176e-06,
      "loss": 0.1995,
      "step": 40240
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.018777592107653618,
      "learning_rate": 3.5601600000000004e-06,
      "loss": 0.1993,
      "step": 40250
    },
    {
      "epoch": 1.2883200000000001,
      "grad_norm": 0.03295062482357025,
      "learning_rate": 3.5585600000000003e-06,
      "loss": 0.2057,
      "step": 40260
    },
    {
      "epoch": 1.28864,
      "grad_norm": 0.025191664695739746,
      "learning_rate": 3.5569600000000006e-06,
      "loss": 0.1993,
      "step": 40270
    },
    {
      "epoch": 1.2889599999999999,
      "grad_norm": 0.028131628409028053,
      "learning_rate": 3.5553600000000005e-06,
      "loss": 0.2003,
      "step": 40280
    },
    {
      "epoch": 1.28928,
      "grad_norm": 0.04967223480343819,
      "learning_rate": 3.5537600000000003e-06,
      "loss": 0.1992,
      "step": 40290
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.020756620913743973,
      "learning_rate": 3.5521600000000002e-06,
      "loss": 0.2005,
      "step": 40300
    },
    {
      "epoch": 1.28992,
      "grad_norm": 0.019720109179615974,
      "learning_rate": 3.55056e-06,
      "loss": 0.1991,
      "step": 40310
    },
    {
      "epoch": 1.29024,
      "grad_norm": 0.04607414826750755,
      "learning_rate": 3.54896e-06,
      "loss": 0.1993,
      "step": 40320
    },
    {
      "epoch": 1.29056,
      "grad_norm": 0.02074175700545311,
      "learning_rate": 3.5473600000000003e-06,
      "loss": 0.2002,
      "step": 40330
    },
    {
      "epoch": 1.29088,
      "grad_norm": 0.012586119584739208,
      "learning_rate": 3.54576e-06,
      "loss": 0.1992,
      "step": 40340
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.021945690736174583,
      "learning_rate": 3.5441600000000005e-06,
      "loss": 0.1994,
      "step": 40350
    },
    {
      "epoch": 1.29152,
      "grad_norm": 0.030384821817278862,
      "learning_rate": 3.5425600000000003e-06,
      "loss": 0.1993,
      "step": 40360
    },
    {
      "epoch": 1.29184,
      "grad_norm": 0.03928027302026749,
      "learning_rate": 3.5409600000000007e-06,
      "loss": 0.1991,
      "step": 40370
    },
    {
      "epoch": 1.29216,
      "grad_norm": 0.027128852903842926,
      "learning_rate": 3.53936e-06,
      "loss": 0.1997,
      "step": 40380
    },
    {
      "epoch": 1.29248,
      "grad_norm": 1.148598551750183,
      "learning_rate": 3.53776e-06,
      "loss": 0.2151,
      "step": 40390
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.019710958003997803,
      "learning_rate": 3.5361600000000003e-06,
      "loss": 0.2152,
      "step": 40400
    },
    {
      "epoch": 1.29312,
      "grad_norm": 0.012008098885416985,
      "learning_rate": 3.53456e-06,
      "loss": 0.1991,
      "step": 40410
    },
    {
      "epoch": 1.29344,
      "grad_norm": 0.0462777316570282,
      "learning_rate": 3.5329600000000005e-06,
      "loss": 0.2144,
      "step": 40420
    },
    {
      "epoch": 1.29376,
      "grad_norm": 0.034815628081560135,
      "learning_rate": 3.5313600000000004e-06,
      "loss": 0.1996,
      "step": 40430
    },
    {
      "epoch": 1.2940800000000001,
      "grad_norm": 0.022866984829306602,
      "learning_rate": 3.5297600000000002e-06,
      "loss": 0.1992,
      "step": 40440
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.037496279925107956,
      "learning_rate": 3.5281600000000005e-06,
      "loss": 0.1995,
      "step": 40450
    },
    {
      "epoch": 1.2947199999999999,
      "grad_norm": 0.047876518219709396,
      "learning_rate": 3.5265600000000004e-06,
      "loss": 0.2149,
      "step": 40460
    },
    {
      "epoch": 1.29504,
      "grad_norm": 0.014037895016372204,
      "learning_rate": 3.52496e-06,
      "loss": 0.1994,
      "step": 40470
    },
    {
      "epoch": 1.29536,
      "grad_norm": 0.010676187463104725,
      "learning_rate": 3.52336e-06,
      "loss": 0.1991,
      "step": 40480
    },
    {
      "epoch": 1.29568,
      "grad_norm": 0.04706812649965286,
      "learning_rate": 3.52176e-06,
      "loss": 0.1995,
      "step": 40490
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.054359953850507736,
      "learning_rate": 3.5201600000000004e-06,
      "loss": 0.2151,
      "step": 40500
    },
    {
      "epoch": 1.29632,
      "grad_norm": 0.010488176718354225,
      "learning_rate": 3.5185600000000002e-06,
      "loss": 0.1992,
      "step": 40510
    },
    {
      "epoch": 1.29664,
      "grad_norm": 0.03598403558135033,
      "learning_rate": 3.5169600000000005e-06,
      "loss": 0.2151,
      "step": 40520
    },
    {
      "epoch": 1.29696,
      "grad_norm": 0.024522719904780388,
      "learning_rate": 3.5153600000000004e-06,
      "loss": 0.199,
      "step": 40530
    },
    {
      "epoch": 1.29728,
      "grad_norm": 0.0175283532589674,
      "learning_rate": 3.5137600000000003e-06,
      "loss": 0.2127,
      "step": 40540
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.05140144005417824,
      "learning_rate": 3.51216e-06,
      "loss": 0.2157,
      "step": 40550
    },
    {
      "epoch": 1.29792,
      "grad_norm": 0.014153522439301014,
      "learning_rate": 3.51056e-06,
      "loss": 0.1995,
      "step": 40560
    },
    {
      "epoch": 1.29824,
      "grad_norm": 0.03602194786071777,
      "learning_rate": 3.50896e-06,
      "loss": 0.1993,
      "step": 40570
    },
    {
      "epoch": 1.29856,
      "grad_norm": 0.010518635623157024,
      "learning_rate": 3.5073600000000002e-06,
      "loss": 0.202,
      "step": 40580
    },
    {
      "epoch": 1.29888,
      "grad_norm": 0.013204229064285755,
      "learning_rate": 3.50576e-06,
      "loss": 0.2236,
      "step": 40590
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.017001064494252205,
      "learning_rate": 3.5041600000000004e-06,
      "loss": 0.1992,
      "step": 40600
    },
    {
      "epoch": 1.29952,
      "grad_norm": 0.6766783595085144,
      "learning_rate": 3.5025600000000003e-06,
      "loss": 0.2009,
      "step": 40610
    },
    {
      "epoch": 1.29984,
      "grad_norm": 0.020517835393548012,
      "learning_rate": 3.5009600000000006e-06,
      "loss": 0.2169,
      "step": 40620
    },
    {
      "epoch": 1.30016,
      "grad_norm": 0.018169615417718887,
      "learning_rate": 3.4993600000000005e-06,
      "loss": 0.2296,
      "step": 40630
    },
    {
      "epoch": 1.30048,
      "grad_norm": 0.05334779620170593,
      "learning_rate": 3.49776e-06,
      "loss": 0.2009,
      "step": 40640
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.0391976535320282,
      "learning_rate": 3.4961600000000003e-06,
      "loss": 0.2174,
      "step": 40650
    },
    {
      "epoch": 1.30112,
      "grad_norm": 0.051826126873493195,
      "learning_rate": 3.49456e-06,
      "loss": 0.1994,
      "step": 40660
    },
    {
      "epoch": 1.30144,
      "grad_norm": 0.02619807794690132,
      "learning_rate": 3.4929600000000004e-06,
      "loss": 0.199,
      "step": 40670
    },
    {
      "epoch": 1.30176,
      "grad_norm": 0.03533141314983368,
      "learning_rate": 3.4913600000000003e-06,
      "loss": 0.1991,
      "step": 40680
    },
    {
      "epoch": 1.3020800000000001,
      "grad_norm": 0.03724066913127899,
      "learning_rate": 3.48976e-06,
      "loss": 0.1995,
      "step": 40690
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.02509075216948986,
      "learning_rate": 3.4881600000000005e-06,
      "loss": 0.1991,
      "step": 40700
    },
    {
      "epoch": 1.3027199999999999,
      "grad_norm": 0.021818872541189194,
      "learning_rate": 3.4865600000000004e-06,
      "loss": 0.199,
      "step": 40710
    },
    {
      "epoch": 1.30304,
      "grad_norm": 0.03165653347969055,
      "learning_rate": 3.48496e-06,
      "loss": 0.1994,
      "step": 40720
    },
    {
      "epoch": 1.30336,
      "grad_norm": 0.026389895007014275,
      "learning_rate": 3.48336e-06,
      "loss": 0.1997,
      "step": 40730
    },
    {
      "epoch": 1.30368,
      "grad_norm": 0.011236095800995827,
      "learning_rate": 3.48176e-06,
      "loss": 0.1991,
      "step": 40740
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.028163820505142212,
      "learning_rate": 3.4801600000000003e-06,
      "loss": 0.2084,
      "step": 40750
    },
    {
      "epoch": 1.30432,
      "grad_norm": 0.008122419938445091,
      "learning_rate": 3.47856e-06,
      "loss": 0.1994,
      "step": 40760
    },
    {
      "epoch": 1.30464,
      "grad_norm": 0.02629614621400833,
      "learning_rate": 3.4769600000000005e-06,
      "loss": 0.209,
      "step": 40770
    },
    {
      "epoch": 1.30496,
      "grad_norm": 0.031580593436956406,
      "learning_rate": 3.4753600000000004e-06,
      "loss": 0.1996,
      "step": 40780
    },
    {
      "epoch": 1.30528,
      "grad_norm": 0.03770708292722702,
      "learning_rate": 3.4737600000000003e-06,
      "loss": 0.1992,
      "step": 40790
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.03209781274199486,
      "learning_rate": 3.4721600000000006e-06,
      "loss": 0.1991,
      "step": 40800
    },
    {
      "epoch": 1.30592,
      "grad_norm": 0.027503332123160362,
      "learning_rate": 3.47056e-06,
      "loss": 0.2085,
      "step": 40810
    },
    {
      "epoch": 1.30624,
      "grad_norm": 0.027892503887414932,
      "learning_rate": 3.46896e-06,
      "loss": 0.2197,
      "step": 40820
    },
    {
      "epoch": 1.30656,
      "grad_norm": 0.029760630801320076,
      "learning_rate": 3.46736e-06,
      "loss": 0.1995,
      "step": 40830
    },
    {
      "epoch": 1.30688,
      "grad_norm": 0.03027970716357231,
      "learning_rate": 3.46576e-06,
      "loss": 0.1991,
      "step": 40840
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.04994232580065727,
      "learning_rate": 3.4641600000000004e-06,
      "loss": 0.1992,
      "step": 40850
    },
    {
      "epoch": 1.30752,
      "grad_norm": 0.04058913141489029,
      "learning_rate": 3.4625600000000003e-06,
      "loss": 0.1991,
      "step": 40860
    },
    {
      "epoch": 1.3078400000000001,
      "grad_norm": 0.03820034861564636,
      "learning_rate": 3.4609600000000006e-06,
      "loss": 0.2093,
      "step": 40870
    },
    {
      "epoch": 1.30816,
      "grad_norm": 0.058254774659872055,
      "learning_rate": 3.4593600000000005e-06,
      "loss": 0.2166,
      "step": 40880
    },
    {
      "epoch": 1.3084799999999999,
      "grad_norm": 0.03912654519081116,
      "learning_rate": 3.45776e-06,
      "loss": 0.2009,
      "step": 40890
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.017596907913684845,
      "learning_rate": 3.45616e-06,
      "loss": 0.2022,
      "step": 40900
    },
    {
      "epoch": 1.30912,
      "grad_norm": 0.01719563454389572,
      "learning_rate": 3.45456e-06,
      "loss": 0.1991,
      "step": 40910
    },
    {
      "epoch": 1.30944,
      "grad_norm": 0.028858663514256477,
      "learning_rate": 3.4529600000000004e-06,
      "loss": 0.2164,
      "step": 40920
    },
    {
      "epoch": 1.30976,
      "grad_norm": 0.8715400099754333,
      "learning_rate": 3.4513600000000003e-06,
      "loss": 0.2136,
      "step": 40930
    },
    {
      "epoch": 1.3100800000000001,
      "grad_norm": 0.02378634363412857,
      "learning_rate": 3.44976e-06,
      "loss": 0.1991,
      "step": 40940
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.016415704041719437,
      "learning_rate": 3.4481600000000005e-06,
      "loss": 0.1992,
      "step": 40950
    },
    {
      "epoch": 1.3107199999999999,
      "grad_norm": 0.013133657164871693,
      "learning_rate": 3.4465600000000003e-06,
      "loss": 0.1993,
      "step": 40960
    },
    {
      "epoch": 1.31104,
      "grad_norm": 0.044246166944503784,
      "learning_rate": 3.4449600000000006e-06,
      "loss": 0.1992,
      "step": 40970
    },
    {
      "epoch": 1.31136,
      "grad_norm": 0.03443817049264908,
      "learning_rate": 3.44336e-06,
      "loss": 0.2003,
      "step": 40980
    },
    {
      "epoch": 1.31168,
      "grad_norm": 0.6372781991958618,
      "learning_rate": 3.44176e-06,
      "loss": 0.2144,
      "step": 40990
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.04391208291053772,
      "learning_rate": 3.4401600000000003e-06,
      "loss": 0.1992,
      "step": 41000
    },
    {
      "epoch": 1.312,
      "eval_runtime": 67.3967,
      "eval_samples_per_second": 148.375,
      "eval_steps_per_second": 9.273,
      "step": 41000
    },
    {
      "epoch": 1.31232,
      "grad_norm": 0.018041498959064484,
      "learning_rate": 3.43856e-06,
      "loss": 0.199,
      "step": 41010
    },
    {
      "epoch": 1.31264,
      "grad_norm": 0.025140266865491867,
      "learning_rate": 3.4369600000000005e-06,
      "loss": 0.2095,
      "step": 41020
    },
    {
      "epoch": 1.31296,
      "grad_norm": 0.05762956291437149,
      "learning_rate": 3.4353600000000003e-06,
      "loss": 0.2019,
      "step": 41030
    },
    {
      "epoch": 1.31328,
      "grad_norm": 0.04937883839011192,
      "learning_rate": 3.4337600000000002e-06,
      "loss": 0.1994,
      "step": 41040
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.04133331775665283,
      "learning_rate": 3.4321600000000005e-06,
      "loss": 0.1995,
      "step": 41050
    },
    {
      "epoch": 1.31392,
      "grad_norm": 0.030646178871393204,
      "learning_rate": 3.4305600000000004e-06,
      "loss": 0.1992,
      "step": 41060
    },
    {
      "epoch": 1.31424,
      "grad_norm": 0.02001919597387314,
      "learning_rate": 3.42896e-06,
      "loss": 0.1994,
      "step": 41070
    },
    {
      "epoch": 1.31456,
      "grad_norm": 0.017061512917280197,
      "learning_rate": 3.42736e-06,
      "loss": 0.1991,
      "step": 41080
    },
    {
      "epoch": 1.31488,
      "grad_norm": 1.5128735303878784,
      "learning_rate": 3.42576e-06,
      "loss": 0.209,
      "step": 41090
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.03507157042622566,
      "learning_rate": 3.4241600000000003e-06,
      "loss": 0.1992,
      "step": 41100
    },
    {
      "epoch": 1.31552,
      "grad_norm": 0.009997460059821606,
      "learning_rate": 3.4225600000000002e-06,
      "loss": 0.2024,
      "step": 41110
    },
    {
      "epoch": 1.3158400000000001,
      "grad_norm": 0.021972863003611565,
      "learning_rate": 3.4209600000000005e-06,
      "loss": 0.2135,
      "step": 41120
    },
    {
      "epoch": 1.31616,
      "grad_norm": 0.01301419548690319,
      "learning_rate": 3.4193600000000004e-06,
      "loss": 0.1991,
      "step": 41130
    },
    {
      "epoch": 1.3164799999999999,
      "grad_norm": 0.026627643033862114,
      "learning_rate": 3.4177600000000007e-06,
      "loss": 0.199,
      "step": 41140
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.04314344376325607,
      "learning_rate": 3.41616e-06,
      "loss": 0.1993,
      "step": 41150
    },
    {
      "epoch": 1.31712,
      "grad_norm": 0.020398978143930435,
      "learning_rate": 3.41456e-06,
      "loss": 0.2115,
      "step": 41160
    },
    {
      "epoch": 1.31744,
      "grad_norm": 0.022936593741178513,
      "learning_rate": 3.4129600000000004e-06,
      "loss": 0.1991,
      "step": 41170
    },
    {
      "epoch": 1.31776,
      "grad_norm": 0.6496685147285461,
      "learning_rate": 3.4113600000000002e-06,
      "loss": 0.2109,
      "step": 41180
    },
    {
      "epoch": 1.31808,
      "grad_norm": 0.05200344696640968,
      "learning_rate": 3.40976e-06,
      "loss": 0.2121,
      "step": 41190
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.03372970223426819,
      "learning_rate": 3.4081600000000004e-06,
      "loss": 0.2136,
      "step": 41200
    },
    {
      "epoch": 1.31872,
      "grad_norm": 0.029297271743416786,
      "learning_rate": 3.4065600000000003e-06,
      "loss": 0.1992,
      "step": 41210
    },
    {
      "epoch": 1.31904,
      "grad_norm": 0.023199385032057762,
      "learning_rate": 3.4049600000000006e-06,
      "loss": 0.211,
      "step": 41220
    },
    {
      "epoch": 1.31936,
      "grad_norm": 0.4626944065093994,
      "learning_rate": 3.4033600000000005e-06,
      "loss": 0.2159,
      "step": 41230
    },
    {
      "epoch": 1.31968,
      "grad_norm": 1.0135225057601929,
      "learning_rate": 3.40176e-06,
      "loss": 0.2005,
      "step": 41240
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.07370170950889587,
      "learning_rate": 3.4001600000000002e-06,
      "loss": 0.2015,
      "step": 41250
    },
    {
      "epoch": 1.32032,
      "grad_norm": 0.5415651202201843,
      "learning_rate": 3.39856e-06,
      "loss": 0.2186,
      "step": 41260
    },
    {
      "epoch": 1.32064,
      "grad_norm": 0.02923400327563286,
      "learning_rate": 3.3969600000000004e-06,
      "loss": 0.199,
      "step": 41270
    },
    {
      "epoch": 1.32096,
      "grad_norm": 0.019796960055828094,
      "learning_rate": 3.3953600000000003e-06,
      "loss": 0.1992,
      "step": 41280
    },
    {
      "epoch": 1.32128,
      "grad_norm": 0.8375629186630249,
      "learning_rate": 3.39376e-06,
      "loss": 0.2016,
      "step": 41290
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.01283381786197424,
      "learning_rate": 3.3921600000000005e-06,
      "loss": 0.2263,
      "step": 41300
    },
    {
      "epoch": 1.32192,
      "grad_norm": 0.026931310072541237,
      "learning_rate": 3.3905600000000004e-06,
      "loss": 0.1995,
      "step": 41310
    },
    {
      "epoch": 1.32224,
      "grad_norm": 0.030094079673290253,
      "learning_rate": 3.38896e-06,
      "loss": 0.1996,
      "step": 41320
    },
    {
      "epoch": 1.32256,
      "grad_norm": 0.027962109073996544,
      "learning_rate": 3.38736e-06,
      "loss": 0.2017,
      "step": 41330
    },
    {
      "epoch": 1.32288,
      "grad_norm": 0.01761174574494362,
      "learning_rate": 3.38576e-06,
      "loss": 0.1998,
      "step": 41340
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.027328893542289734,
      "learning_rate": 3.3841600000000003e-06,
      "loss": 0.1993,
      "step": 41350
    },
    {
      "epoch": 1.32352,
      "grad_norm": 0.034907396882772446,
      "learning_rate": 3.38256e-06,
      "loss": 0.2006,
      "step": 41360
    },
    {
      "epoch": 1.3238400000000001,
      "grad_norm": 0.014521402306854725,
      "learning_rate": 3.3809600000000005e-06,
      "loss": 0.2104,
      "step": 41370
    },
    {
      "epoch": 1.32416,
      "grad_norm": 0.0069222827441990376,
      "learning_rate": 3.3793600000000004e-06,
      "loss": 0.1991,
      "step": 41380
    },
    {
      "epoch": 1.3244799999999999,
      "grad_norm": 0.04430237412452698,
      "learning_rate": 3.3777600000000007e-06,
      "loss": 0.1991,
      "step": 41390
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.023144055157899857,
      "learning_rate": 3.3761600000000006e-06,
      "loss": 0.2126,
      "step": 41400
    },
    {
      "epoch": 1.32512,
      "grad_norm": 0.011747232638299465,
      "learning_rate": 3.37456e-06,
      "loss": 0.1992,
      "step": 41410
    },
    {
      "epoch": 1.32544,
      "grad_norm": 0.051162756979465485,
      "learning_rate": 3.3729600000000003e-06,
      "loss": 0.1996,
      "step": 41420
    },
    {
      "epoch": 1.32576,
      "grad_norm": 0.016882015392184258,
      "learning_rate": 3.37136e-06,
      "loss": 0.2075,
      "step": 41430
    },
    {
      "epoch": 1.32608,
      "grad_norm": 0.22769038379192352,
      "learning_rate": 3.36976e-06,
      "loss": 0.1996,
      "step": 41440
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.015030884183943272,
      "learning_rate": 3.3681600000000004e-06,
      "loss": 0.1991,
      "step": 41450
    },
    {
      "epoch": 1.32672,
      "grad_norm": 0.01618976704776287,
      "learning_rate": 3.3665600000000003e-06,
      "loss": 0.1991,
      "step": 41460
    },
    {
      "epoch": 1.32704,
      "grad_norm": 0.04036405310034752,
      "learning_rate": 3.3649600000000006e-06,
      "loss": 0.1992,
      "step": 41470
    },
    {
      "epoch": 1.32736,
      "grad_norm": 0.014218125492334366,
      "learning_rate": 3.3633600000000004e-06,
      "loss": 0.1992,
      "step": 41480
    },
    {
      "epoch": 1.32768,
      "grad_norm": 0.020699867978692055,
      "learning_rate": 3.36176e-06,
      "loss": 0.2023,
      "step": 41490
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.04447426274418831,
      "learning_rate": 3.36016e-06,
      "loss": 0.2008,
      "step": 41500
    },
    {
      "epoch": 1.32832,
      "grad_norm": 0.02546222135424614,
      "learning_rate": 3.35856e-06,
      "loss": 0.218,
      "step": 41510
    },
    {
      "epoch": 1.32864,
      "grad_norm": 0.020794346928596497,
      "learning_rate": 3.3569600000000004e-06,
      "loss": 0.1995,
      "step": 41520
    },
    {
      "epoch": 1.32896,
      "grad_norm": 0.014201975427567959,
      "learning_rate": 3.3553600000000003e-06,
      "loss": 0.1992,
      "step": 41530
    },
    {
      "epoch": 1.32928,
      "grad_norm": 0.04982501268386841,
      "learning_rate": 3.35376e-06,
      "loss": 0.21,
      "step": 41540
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.01576174423098564,
      "learning_rate": 3.3521600000000004e-06,
      "loss": 0.1996,
      "step": 41550
    },
    {
      "epoch": 1.32992,
      "grad_norm": 0.025335635989904404,
      "learning_rate": 3.3505600000000003e-06,
      "loss": 0.2238,
      "step": 41560
    },
    {
      "epoch": 1.3302399999999999,
      "grad_norm": 0.041161857545375824,
      "learning_rate": 3.3489600000000006e-06,
      "loss": 0.1993,
      "step": 41570
    },
    {
      "epoch": 1.33056,
      "grad_norm": 0.03843528404831886,
      "learning_rate": 3.34736e-06,
      "loss": 0.1995,
      "step": 41580
    },
    {
      "epoch": 1.33088,
      "grad_norm": 0.01566888391971588,
      "learning_rate": 3.34576e-06,
      "loss": 0.1995,
      "step": 41590
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.02313879504799843,
      "learning_rate": 3.3441600000000003e-06,
      "loss": 0.1991,
      "step": 41600
    },
    {
      "epoch": 1.33152,
      "grad_norm": 0.008434171788394451,
      "learning_rate": 3.34256e-06,
      "loss": 0.21,
      "step": 41610
    },
    {
      "epoch": 1.3318400000000001,
      "grad_norm": 0.024149490520358086,
      "learning_rate": 3.3409600000000005e-06,
      "loss": 0.1994,
      "step": 41620
    },
    {
      "epoch": 1.33216,
      "grad_norm": 0.04963449388742447,
      "learning_rate": 3.3393600000000003e-06,
      "loss": 0.1996,
      "step": 41630
    },
    {
      "epoch": 1.3324799999999999,
      "grad_norm": 0.017929542809724808,
      "learning_rate": 3.3377600000000006e-06,
      "loss": 0.1993,
      "step": 41640
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.035003554075956345,
      "learning_rate": 3.3361600000000005e-06,
      "loss": 0.1993,
      "step": 41650
    },
    {
      "epoch": 1.33312,
      "grad_norm": 0.14613576233386993,
      "learning_rate": 3.33456e-06,
      "loss": 0.1995,
      "step": 41660
    },
    {
      "epoch": 1.33344,
      "grad_norm": 0.01820369064807892,
      "learning_rate": 3.3329600000000003e-06,
      "loss": 0.1991,
      "step": 41670
    },
    {
      "epoch": 1.33376,
      "grad_norm": 0.01779407635331154,
      "learning_rate": 3.33136e-06,
      "loss": 0.2162,
      "step": 41680
    },
    {
      "epoch": 1.33408,
      "grad_norm": 0.013421949930489063,
      "learning_rate": 3.32976e-06,
      "loss": 0.2026,
      "step": 41690
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.04563988372683525,
      "learning_rate": 3.3281600000000003e-06,
      "loss": 0.1995,
      "step": 41700
    },
    {
      "epoch": 1.33472,
      "grad_norm": 0.01937693916261196,
      "learning_rate": 3.3265600000000002e-06,
      "loss": 0.2119,
      "step": 41710
    },
    {
      "epoch": 1.33504,
      "grad_norm": 0.037299834191799164,
      "learning_rate": 3.3249600000000005e-06,
      "loss": 0.1994,
      "step": 41720
    },
    {
      "epoch": 1.33536,
      "grad_norm": 0.04900003597140312,
      "learning_rate": 3.3233600000000004e-06,
      "loss": 0.1993,
      "step": 41730
    },
    {
      "epoch": 1.33568,
      "grad_norm": 0.024746034294366837,
      "learning_rate": 3.3217600000000007e-06,
      "loss": 0.2089,
      "step": 41740
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.1119987890124321,
      "learning_rate": 3.32016e-06,
      "loss": 0.2155,
      "step": 41750
    },
    {
      "epoch": 1.33632,
      "grad_norm": 0.01599733717739582,
      "learning_rate": 3.31856e-06,
      "loss": 0.1993,
      "step": 41760
    },
    {
      "epoch": 1.33664,
      "grad_norm": 0.03250247985124588,
      "learning_rate": 3.3169600000000003e-06,
      "loss": 0.1994,
      "step": 41770
    },
    {
      "epoch": 1.33696,
      "grad_norm": 0.009976888075470924,
      "learning_rate": 3.3153600000000002e-06,
      "loss": 0.1997,
      "step": 41780
    },
    {
      "epoch": 1.33728,
      "grad_norm": 0.023370103910565376,
      "learning_rate": 3.31376e-06,
      "loss": 0.1991,
      "step": 41790
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.02252412959933281,
      "learning_rate": 3.3121600000000004e-06,
      "loss": 0.1991,
      "step": 41800
    },
    {
      "epoch": 1.33792,
      "grad_norm": 0.046452321112155914,
      "learning_rate": 3.3105600000000003e-06,
      "loss": 0.1995,
      "step": 41810
    },
    {
      "epoch": 1.3382399999999999,
      "grad_norm": 0.017704594880342484,
      "learning_rate": 3.3089600000000006e-06,
      "loss": 0.1994,
      "step": 41820
    },
    {
      "epoch": 1.33856,
      "grad_norm": 0.0173957496881485,
      "learning_rate": 3.30736e-06,
      "loss": 0.1994,
      "step": 41830
    },
    {
      "epoch": 1.33888,
      "grad_norm": 0.027528196573257446,
      "learning_rate": 3.30576e-06,
      "loss": 0.2114,
      "step": 41840
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.3872475028038025,
      "learning_rate": 3.3041600000000002e-06,
      "loss": 0.2146,
      "step": 41850
    },
    {
      "epoch": 1.33952,
      "grad_norm": 0.011454121209681034,
      "learning_rate": 3.30256e-06,
      "loss": 0.1994,
      "step": 41860
    },
    {
      "epoch": 1.33984,
      "grad_norm": 0.026461735367774963,
      "learning_rate": 3.3009600000000004e-06,
      "loss": 0.211,
      "step": 41870
    },
    {
      "epoch": 1.34016,
      "grad_norm": 0.08178801089525223,
      "learning_rate": 3.2993600000000003e-06,
      "loss": 0.1993,
      "step": 41880
    },
    {
      "epoch": 1.34048,
      "grad_norm": 0.01317442487925291,
      "learning_rate": 3.2977600000000006e-06,
      "loss": 0.1993,
      "step": 41890
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.031375590711832047,
      "learning_rate": 3.2961600000000005e-06,
      "loss": 0.1993,
      "step": 41900
    },
    {
      "epoch": 1.34112,
      "grad_norm": 0.032946739345788956,
      "learning_rate": 3.2945600000000004e-06,
      "loss": 0.1998,
      "step": 41910
    },
    {
      "epoch": 1.34144,
      "grad_norm": 0.029926400631666183,
      "learning_rate": 3.2929600000000002e-06,
      "loss": 0.1994,
      "step": 41920
    },
    {
      "epoch": 1.34176,
      "grad_norm": 0.019320813938975334,
      "learning_rate": 3.29136e-06,
      "loss": 0.1993,
      "step": 41930
    },
    {
      "epoch": 1.34208,
      "grad_norm": 0.02695479802787304,
      "learning_rate": 3.28976e-06,
      "loss": 0.2116,
      "step": 41940
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.021260198205709457,
      "learning_rate": 3.2881600000000003e-06,
      "loss": 0.2137,
      "step": 41950
    },
    {
      "epoch": 1.34272,
      "grad_norm": 0.023668475449085236,
      "learning_rate": 3.28656e-06,
      "loss": 0.2217,
      "step": 41960
    },
    {
      "epoch": 1.34304,
      "grad_norm": 0.04525582864880562,
      "learning_rate": 3.2849600000000005e-06,
      "loss": 0.1991,
      "step": 41970
    },
    {
      "epoch": 1.34336,
      "grad_norm": 0.03267914056777954,
      "learning_rate": 3.2833600000000004e-06,
      "loss": 0.2006,
      "step": 41980
    },
    {
      "epoch": 1.34368,
      "grad_norm": 0.07161945849657059,
      "learning_rate": 3.2817600000000007e-06,
      "loss": 0.1992,
      "step": 41990
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.017475826665759087,
      "learning_rate": 3.28016e-06,
      "loss": 0.2157,
      "step": 42000
    },
    {
      "epoch": 1.3439999999999999,
      "eval_runtime": 48.16,
      "eval_samples_per_second": 207.641,
      "eval_steps_per_second": 12.978,
      "step": 42000
    },
    {
      "epoch": 1.34432,
      "grad_norm": 0.03224915266036987,
      "learning_rate": 3.27856e-06,
      "loss": 0.1996,
      "step": 42010
    },
    {
      "epoch": 1.34464,
      "grad_norm": 0.024981867522001266,
      "learning_rate": 3.2769600000000003e-06,
      "loss": 0.2095,
      "step": 42020
    },
    {
      "epoch": 1.34496,
      "grad_norm": 0.01703864336013794,
      "learning_rate": 3.27536e-06,
      "loss": 0.2149,
      "step": 42030
    },
    {
      "epoch": 1.34528,
      "grad_norm": 0.012071109376847744,
      "learning_rate": 3.27376e-06,
      "loss": 0.1991,
      "step": 42040
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.03885680064558983,
      "learning_rate": 3.2721600000000004e-06,
      "loss": 0.1994,
      "step": 42050
    },
    {
      "epoch": 1.34592,
      "grad_norm": 0.2698976397514343,
      "learning_rate": 3.2705600000000002e-06,
      "loss": 0.1993,
      "step": 42060
    },
    {
      "epoch": 1.3462399999999999,
      "grad_norm": 0.025222348049283028,
      "learning_rate": 3.2689600000000005e-06,
      "loss": 0.1992,
      "step": 42070
    },
    {
      "epoch": 1.34656,
      "grad_norm": 0.7132100462913513,
      "learning_rate": 3.2673600000000004e-06,
      "loss": 0.2215,
      "step": 42080
    },
    {
      "epoch": 1.34688,
      "grad_norm": 0.020441042259335518,
      "learning_rate": 3.26576e-06,
      "loss": 0.1992,
      "step": 42090
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.026747286319732666,
      "learning_rate": 3.26416e-06,
      "loss": 0.2132,
      "step": 42100
    },
    {
      "epoch": 1.34752,
      "grad_norm": 0.03847949951887131,
      "learning_rate": 3.26256e-06,
      "loss": 0.203,
      "step": 42110
    },
    {
      "epoch": 1.34784,
      "grad_norm": 0.03093574196100235,
      "learning_rate": 3.2609600000000004e-06,
      "loss": 0.1992,
      "step": 42120
    },
    {
      "epoch": 1.34816,
      "grad_norm": 0.04600577801465988,
      "learning_rate": 3.2593600000000002e-06,
      "loss": 0.2158,
      "step": 42130
    },
    {
      "epoch": 1.34848,
      "grad_norm": 0.013926415704190731,
      "learning_rate": 3.2577600000000006e-06,
      "loss": 0.2036,
      "step": 42140
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.08208386600017548,
      "learning_rate": 3.2561600000000004e-06,
      "loss": 0.1995,
      "step": 42150
    },
    {
      "epoch": 1.34912,
      "grad_norm": 0.025029996410012245,
      "learning_rate": 3.2545600000000003e-06,
      "loss": 0.1993,
      "step": 42160
    },
    {
      "epoch": 1.34944,
      "grad_norm": 0.03915790840983391,
      "learning_rate": 3.25296e-06,
      "loss": 0.1991,
      "step": 42170
    },
    {
      "epoch": 1.34976,
      "grad_norm": 0.03856460005044937,
      "learning_rate": 3.25136e-06,
      "loss": 0.1995,
      "step": 42180
    },
    {
      "epoch": 1.35008,
      "grad_norm": 0.10561507195234299,
      "learning_rate": 3.24976e-06,
      "loss": 0.1994,
      "step": 42190
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.04640807583928108,
      "learning_rate": 3.2481600000000003e-06,
      "loss": 0.1995,
      "step": 42200
    },
    {
      "epoch": 1.35072,
      "grad_norm": 0.01513610314577818,
      "learning_rate": 3.24656e-06,
      "loss": 0.1992,
      "step": 42210
    },
    {
      "epoch": 1.35104,
      "grad_norm": 0.04419655352830887,
      "learning_rate": 3.2449600000000004e-06,
      "loss": 0.21,
      "step": 42220
    },
    {
      "epoch": 1.3513600000000001,
      "grad_norm": 0.04252732917666435,
      "learning_rate": 3.2433600000000003e-06,
      "loss": 0.199,
      "step": 42230
    },
    {
      "epoch": 1.35168,
      "grad_norm": 0.042028360068798065,
      "learning_rate": 3.2417600000000006e-06,
      "loss": 0.1992,
      "step": 42240
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.032196737825870514,
      "learning_rate": 3.2401600000000005e-06,
      "loss": 0.1991,
      "step": 42250
    },
    {
      "epoch": 1.35232,
      "grad_norm": 0.024599667638540268,
      "learning_rate": 3.23856e-06,
      "loss": 0.214,
      "step": 42260
    },
    {
      "epoch": 1.35264,
      "grad_norm": 0.010506277903914452,
      "learning_rate": 3.2369600000000003e-06,
      "loss": 0.227,
      "step": 42270
    },
    {
      "epoch": 1.35296,
      "grad_norm": 0.06549369543790817,
      "learning_rate": 3.23536e-06,
      "loss": 0.2015,
      "step": 42280
    },
    {
      "epoch": 1.35328,
      "grad_norm": 0.035782020539045334,
      "learning_rate": 3.2337600000000004e-06,
      "loss": 0.1991,
      "step": 42290
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.023396186530590057,
      "learning_rate": 3.2321600000000003e-06,
      "loss": 0.1992,
      "step": 42300
    },
    {
      "epoch": 1.35392,
      "grad_norm": 0.10193859785795212,
      "learning_rate": 3.23056e-06,
      "loss": 0.2094,
      "step": 42310
    },
    {
      "epoch": 1.3542399999999999,
      "grad_norm": 0.03092189133167267,
      "learning_rate": 3.2289600000000005e-06,
      "loss": 0.2167,
      "step": 42320
    },
    {
      "epoch": 1.35456,
      "grad_norm": 0.02548869140446186,
      "learning_rate": 3.2273600000000004e-06,
      "loss": 0.1991,
      "step": 42330
    },
    {
      "epoch": 1.35488,
      "grad_norm": 0.04806836321949959,
      "learning_rate": 3.22576e-06,
      "loss": 0.1992,
      "step": 42340
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.05260566622018814,
      "learning_rate": 3.22416e-06,
      "loss": 0.2275,
      "step": 42350
    },
    {
      "epoch": 1.35552,
      "grad_norm": 0.028607932850718498,
      "learning_rate": 3.22256e-06,
      "loss": 0.1991,
      "step": 42360
    },
    {
      "epoch": 1.35584,
      "grad_norm": 0.013429563492536545,
      "learning_rate": 3.2209600000000003e-06,
      "loss": 0.1993,
      "step": 42370
    },
    {
      "epoch": 1.35616,
      "grad_norm": 0.00867458339780569,
      "learning_rate": 3.21936e-06,
      "loss": 0.1995,
      "step": 42380
    },
    {
      "epoch": 1.35648,
      "grad_norm": 0.01807861216366291,
      "learning_rate": 3.2177600000000005e-06,
      "loss": 0.1997,
      "step": 42390
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.0249814223498106,
      "learning_rate": 3.2161600000000004e-06,
      "loss": 0.2102,
      "step": 42400
    },
    {
      "epoch": 1.35712,
      "grad_norm": 0.03977048769593239,
      "learning_rate": 3.2145600000000003e-06,
      "loss": 0.1992,
      "step": 42410
    },
    {
      "epoch": 1.35744,
      "grad_norm": 0.02523023635149002,
      "learning_rate": 3.2129600000000006e-06,
      "loss": 0.1993,
      "step": 42420
    },
    {
      "epoch": 1.35776,
      "grad_norm": 0.057468123733997345,
      "learning_rate": 3.21136e-06,
      "loss": 0.1991,
      "step": 42430
    },
    {
      "epoch": 1.35808,
      "grad_norm": 0.035605788230895996,
      "learning_rate": 3.20976e-06,
      "loss": 0.1994,
      "step": 42440
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.03873879835009575,
      "learning_rate": 3.2081600000000002e-06,
      "loss": 0.1991,
      "step": 42450
    },
    {
      "epoch": 1.35872,
      "grad_norm": 0.04919063672423363,
      "learning_rate": 3.20656e-06,
      "loss": 0.1996,
      "step": 42460
    },
    {
      "epoch": 1.35904,
      "grad_norm": 0.034599870443344116,
      "learning_rate": 3.2049600000000004e-06,
      "loss": 0.1992,
      "step": 42470
    },
    {
      "epoch": 1.3593600000000001,
      "grad_norm": 0.009470910765230656,
      "learning_rate": 3.2033600000000003e-06,
      "loss": 0.2124,
      "step": 42480
    },
    {
      "epoch": 1.35968,
      "grad_norm": 0.010969283059239388,
      "learning_rate": 3.2017600000000006e-06,
      "loss": 0.216,
      "step": 42490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.029552947729825974,
      "learning_rate": 3.2001600000000005e-06,
      "loss": 0.1991,
      "step": 42500
    },
    {
      "epoch": 1.36032,
      "grad_norm": 0.019627589732408524,
      "learning_rate": 3.19856e-06,
      "loss": 0.1995,
      "step": 42510
    },
    {
      "epoch": 1.36064,
      "grad_norm": 0.60371333360672,
      "learning_rate": 3.1969600000000002e-06,
      "loss": 0.2002,
      "step": 42520
    },
    {
      "epoch": 1.36096,
      "grad_norm": 0.03676362335681915,
      "learning_rate": 3.19536e-06,
      "loss": 0.1998,
      "step": 42530
    },
    {
      "epoch": 1.36128,
      "grad_norm": 0.03163380175828934,
      "learning_rate": 3.1937600000000004e-06,
      "loss": 0.2093,
      "step": 42540
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.012490760535001755,
      "learning_rate": 3.1921600000000003e-06,
      "loss": 0.2155,
      "step": 42550
    },
    {
      "epoch": 1.36192,
      "grad_norm": 0.020798586308956146,
      "learning_rate": 3.19056e-06,
      "loss": 0.2003,
      "step": 42560
    },
    {
      "epoch": 1.36224,
      "grad_norm": 0.027583081275224686,
      "learning_rate": 3.1889600000000005e-06,
      "loss": 0.2001,
      "step": 42570
    },
    {
      "epoch": 1.36256,
      "grad_norm": 0.017505502328276634,
      "learning_rate": 3.1873600000000003e-06,
      "loss": 0.2131,
      "step": 42580
    },
    {
      "epoch": 1.36288,
      "grad_norm": 0.03188431262969971,
      "learning_rate": 3.1857600000000006e-06,
      "loss": 0.1991,
      "step": 42590
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.012836943380534649,
      "learning_rate": 3.18416e-06,
      "loss": 0.1991,
      "step": 42600
    },
    {
      "epoch": 1.36352,
      "grad_norm": 0.04802456125617027,
      "learning_rate": 3.18256e-06,
      "loss": 0.1993,
      "step": 42610
    },
    {
      "epoch": 1.36384,
      "grad_norm": 0.015335815958678722,
      "learning_rate": 3.1809600000000003e-06,
      "loss": 0.1991,
      "step": 42620
    },
    {
      "epoch": 1.36416,
      "grad_norm": 0.04775634780526161,
      "learning_rate": 3.17936e-06,
      "loss": 0.1992,
      "step": 42630
    },
    {
      "epoch": 1.36448,
      "grad_norm": 0.045702461153268814,
      "learning_rate": 3.1777600000000005e-06,
      "loss": 0.1992,
      "step": 42640
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.04021397605538368,
      "learning_rate": 3.1761600000000004e-06,
      "loss": 0.2043,
      "step": 42650
    },
    {
      "epoch": 1.3651200000000001,
      "grad_norm": 0.10714777559041977,
      "learning_rate": 3.1745600000000002e-06,
      "loss": 0.215,
      "step": 42660
    },
    {
      "epoch": 1.36544,
      "grad_norm": 0.05000298097729683,
      "learning_rate": 3.1729600000000005e-06,
      "loss": 0.1994,
      "step": 42670
    },
    {
      "epoch": 1.3657599999999999,
      "grad_norm": 0.022851336747407913,
      "learning_rate": 3.17136e-06,
      "loss": 0.1995,
      "step": 42680
    },
    {
      "epoch": 1.36608,
      "grad_norm": 0.025720974430441856,
      "learning_rate": 3.16976e-06,
      "loss": 0.1991,
      "step": 42690
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.02416916750371456,
      "learning_rate": 3.16816e-06,
      "loss": 0.2012,
      "step": 42700
    },
    {
      "epoch": 1.36672,
      "grad_norm": 0.03989851102232933,
      "learning_rate": 3.16656e-06,
      "loss": 0.1992,
      "step": 42710
    },
    {
      "epoch": 1.36704,
      "grad_norm": 0.014634902589023113,
      "learning_rate": 3.1649600000000004e-06,
      "loss": 0.2255,
      "step": 42720
    },
    {
      "epoch": 1.3673600000000001,
      "grad_norm": 0.04192326217889786,
      "learning_rate": 3.1633600000000002e-06,
      "loss": 0.1994,
      "step": 42730
    },
    {
      "epoch": 1.36768,
      "grad_norm": 0.019715923815965652,
      "learning_rate": 3.1617600000000005e-06,
      "loss": 0.1994,
      "step": 42740
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.033241353929042816,
      "learning_rate": 3.1601600000000004e-06,
      "loss": 0.2083,
      "step": 42750
    },
    {
      "epoch": 1.36832,
      "grad_norm": 0.014104936271905899,
      "learning_rate": 3.1585600000000007e-06,
      "loss": 0.1991,
      "step": 42760
    },
    {
      "epoch": 1.36864,
      "grad_norm": 1.4425238370895386,
      "learning_rate": 3.15696e-06,
      "loss": 0.2069,
      "step": 42770
    },
    {
      "epoch": 1.36896,
      "grad_norm": 0.03345603123307228,
      "learning_rate": 3.15536e-06,
      "loss": 0.1992,
      "step": 42780
    },
    {
      "epoch": 1.36928,
      "grad_norm": 0.028107184916734695,
      "learning_rate": 3.1537600000000004e-06,
      "loss": 0.1992,
      "step": 42790
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.04035434126853943,
      "learning_rate": 3.1521600000000002e-06,
      "loss": 0.2127,
      "step": 42800
    },
    {
      "epoch": 1.36992,
      "grad_norm": 0.014179750345647335,
      "learning_rate": 3.15056e-06,
      "loss": 0.1993,
      "step": 42810
    },
    {
      "epoch": 1.37024,
      "grad_norm": 0.02907874807715416,
      "learning_rate": 3.1489600000000004e-06,
      "loss": 0.1992,
      "step": 42820
    },
    {
      "epoch": 1.37056,
      "grad_norm": 0.027394773438572884,
      "learning_rate": 3.1473600000000003e-06,
      "loss": 0.2123,
      "step": 42830
    },
    {
      "epoch": 1.37088,
      "grad_norm": 0.013858322985470295,
      "learning_rate": 3.1457600000000006e-06,
      "loss": 0.1992,
      "step": 42840
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.034802064299583435,
      "learning_rate": 3.14416e-06,
      "loss": 0.1993,
      "step": 42850
    },
    {
      "epoch": 1.37152,
      "grad_norm": 0.03398716077208519,
      "learning_rate": 3.14256e-06,
      "loss": 0.1991,
      "step": 42860
    },
    {
      "epoch": 1.37184,
      "grad_norm": 0.025439487770199776,
      "learning_rate": 3.1409600000000002e-06,
      "loss": 0.1992,
      "step": 42870
    },
    {
      "epoch": 1.37216,
      "grad_norm": 0.029657578095793724,
      "learning_rate": 3.13936e-06,
      "loss": 0.2056,
      "step": 42880
    },
    {
      "epoch": 1.37248,
      "grad_norm": 0.02891743928194046,
      "learning_rate": 3.1377600000000004e-06,
      "loss": 0.2021,
      "step": 42890
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.011754904873669147,
      "learning_rate": 3.1361600000000003e-06,
      "loss": 0.1991,
      "step": 42900
    },
    {
      "epoch": 1.3731200000000001,
      "grad_norm": 0.02525850012898445,
      "learning_rate": 3.13456e-06,
      "loss": 0.1993,
      "step": 42910
    },
    {
      "epoch": 1.37344,
      "grad_norm": 0.02102305367588997,
      "learning_rate": 3.1329600000000005e-06,
      "loss": 0.1991,
      "step": 42920
    },
    {
      "epoch": 1.3737599999999999,
      "grad_norm": 0.019643623381853104,
      "learning_rate": 3.1313600000000004e-06,
      "loss": 0.1992,
      "step": 42930
    },
    {
      "epoch": 1.37408,
      "grad_norm": 0.11901954561471939,
      "learning_rate": 3.12976e-06,
      "loss": 0.1994,
      "step": 42940
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.20764519274234772,
      "learning_rate": 3.12816e-06,
      "loss": 0.1995,
      "step": 42950
    },
    {
      "epoch": 1.37472,
      "grad_norm": 0.03538333997130394,
      "learning_rate": 3.12656e-06,
      "loss": 0.1992,
      "step": 42960
    },
    {
      "epoch": 1.37504,
      "grad_norm": 0.08123143762350082,
      "learning_rate": 3.1249600000000003e-06,
      "loss": 0.1994,
      "step": 42970
    },
    {
      "epoch": 1.3753600000000001,
      "grad_norm": 0.02905299700796604,
      "learning_rate": 3.12336e-06,
      "loss": 0.1991,
      "step": 42980
    },
    {
      "epoch": 1.37568,
      "grad_norm": 0.03753558546304703,
      "learning_rate": 3.1217600000000005e-06,
      "loss": 0.212,
      "step": 42990
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.012779648415744305,
      "learning_rate": 3.1201600000000004e-06,
      "loss": 0.199,
      "step": 43000
    },
    {
      "epoch": 1.376,
      "eval_runtime": 50.3271,
      "eval_samples_per_second": 198.7,
      "eval_steps_per_second": 12.419,
      "step": 43000
    },
    {
      "epoch": 1.37632,
      "grad_norm": 0.041593629866838455,
      "learning_rate": 3.1185600000000007e-06,
      "loss": 0.1994,
      "step": 43010
    },
    {
      "epoch": 1.37664,
      "grad_norm": 0.010582997463643551,
      "learning_rate": 3.11696e-06,
      "loss": 0.199,
      "step": 43020
    },
    {
      "epoch": 1.37696,
      "grad_norm": 0.02761625126004219,
      "learning_rate": 3.11536e-06,
      "loss": 0.2157,
      "step": 43030
    },
    {
      "epoch": 1.37728,
      "grad_norm": 0.020344389602541924,
      "learning_rate": 3.1137600000000003e-06,
      "loss": 0.1998,
      "step": 43040
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.040423862636089325,
      "learning_rate": 3.11216e-06,
      "loss": 0.199,
      "step": 43050
    },
    {
      "epoch": 1.37792,
      "grad_norm": 0.04669209569692612,
      "learning_rate": 3.11056e-06,
      "loss": 0.1992,
      "step": 43060
    },
    {
      "epoch": 1.37824,
      "grad_norm": 0.05952848494052887,
      "learning_rate": 3.1089600000000004e-06,
      "loss": 0.1991,
      "step": 43070
    },
    {
      "epoch": 1.37856,
      "grad_norm": 0.06518808752298355,
      "learning_rate": 3.1073600000000003e-06,
      "loss": 0.1994,
      "step": 43080
    },
    {
      "epoch": 1.37888,
      "grad_norm": 0.013553890399634838,
      "learning_rate": 3.1057600000000006e-06,
      "loss": 0.2086,
      "step": 43090
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.11299452930688858,
      "learning_rate": 3.1041600000000004e-06,
      "loss": 0.2058,
      "step": 43100
    },
    {
      "epoch": 1.37952,
      "grad_norm": 0.02205680124461651,
      "learning_rate": 3.10256e-06,
      "loss": 0.1991,
      "step": 43110
    },
    {
      "epoch": 1.37984,
      "grad_norm": 0.06268662214279175,
      "learning_rate": 3.10096e-06,
      "loss": 0.1991,
      "step": 43120
    },
    {
      "epoch": 1.38016,
      "grad_norm": 0.050566546618938446,
      "learning_rate": 3.09936e-06,
      "loss": 0.1991,
      "step": 43130
    },
    {
      "epoch": 1.38048,
      "grad_norm": 0.009883038699626923,
      "learning_rate": 3.0977600000000004e-06,
      "loss": 0.2043,
      "step": 43140
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.07088261842727661,
      "learning_rate": 3.0961600000000003e-06,
      "loss": 0.2046,
      "step": 43150
    },
    {
      "epoch": 1.3811200000000001,
      "grad_norm": 0.13828034698963165,
      "learning_rate": 3.09456e-06,
      "loss": 0.1996,
      "step": 43160
    },
    {
      "epoch": 1.38144,
      "grad_norm": 0.04022905230522156,
      "learning_rate": 3.0929600000000005e-06,
      "loss": 0.2149,
      "step": 43170
    },
    {
      "epoch": 1.3817599999999999,
      "grad_norm": 0.028050141409039497,
      "learning_rate": 3.0913600000000003e-06,
      "loss": 0.1992,
      "step": 43180
    },
    {
      "epoch": 1.38208,
      "grad_norm": 0.023867672309279442,
      "learning_rate": 3.08976e-06,
      "loss": 0.1991,
      "step": 43190
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.02993091382086277,
      "learning_rate": 3.08816e-06,
      "loss": 0.1992,
      "step": 43200
    },
    {
      "epoch": 1.38272,
      "grad_norm": 0.014487062580883503,
      "learning_rate": 3.08656e-06,
      "loss": 0.1994,
      "step": 43210
    },
    {
      "epoch": 1.38304,
      "grad_norm": 0.7889834642410278,
      "learning_rate": 3.0849600000000003e-06,
      "loss": 0.2,
      "step": 43220
    },
    {
      "epoch": 1.38336,
      "grad_norm": 0.02599303238093853,
      "learning_rate": 3.08336e-06,
      "loss": 0.2113,
      "step": 43230
    },
    {
      "epoch": 1.38368,
      "grad_norm": 0.1222240999341011,
      "learning_rate": 3.0817600000000005e-06,
      "loss": 0.1993,
      "step": 43240
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.019554657861590385,
      "learning_rate": 3.0801600000000003e-06,
      "loss": 0.2018,
      "step": 43250
    },
    {
      "epoch": 1.38432,
      "grad_norm": 1.2431416511535645,
      "learning_rate": 3.0785600000000006e-06,
      "loss": 0.2058,
      "step": 43260
    },
    {
      "epoch": 1.38464,
      "grad_norm": 0.031711969524621964,
      "learning_rate": 3.0769600000000005e-06,
      "loss": 0.2088,
      "step": 43270
    },
    {
      "epoch": 1.38496,
      "grad_norm": 0.01659555919468403,
      "learning_rate": 3.07536e-06,
      "loss": 0.2003,
      "step": 43280
    },
    {
      "epoch": 1.38528,
      "grad_norm": 0.013954554684460163,
      "learning_rate": 3.0737600000000003e-06,
      "loss": 0.1994,
      "step": 43290
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.022554779425263405,
      "learning_rate": 3.07216e-06,
      "loss": 0.1992,
      "step": 43300
    },
    {
      "epoch": 1.38592,
      "grad_norm": 0.03806772828102112,
      "learning_rate": 3.07056e-06,
      "loss": 0.1999,
      "step": 43310
    },
    {
      "epoch": 1.38624,
      "grad_norm": 0.028624003753066063,
      "learning_rate": 3.0689600000000003e-06,
      "loss": 0.1998,
      "step": 43320
    },
    {
      "epoch": 1.38656,
      "grad_norm": 0.011305437423288822,
      "learning_rate": 3.0673600000000002e-06,
      "loss": 0.2013,
      "step": 43330
    },
    {
      "epoch": 1.3868800000000001,
      "grad_norm": 0.028514007106423378,
      "learning_rate": 3.0657600000000005e-06,
      "loss": 0.1998,
      "step": 43340
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.07951520383358002,
      "learning_rate": 3.0641600000000004e-06,
      "loss": 0.1993,
      "step": 43350
    },
    {
      "epoch": 1.3875199999999999,
      "grad_norm": 0.02672259695827961,
      "learning_rate": 3.06256e-06,
      "loss": 0.1993,
      "step": 43360
    },
    {
      "epoch": 1.38784,
      "grad_norm": 0.017147928476333618,
      "learning_rate": 3.06096e-06,
      "loss": 0.2002,
      "step": 43370
    },
    {
      "epoch": 1.38816,
      "grad_norm": 0.03445703908801079,
      "learning_rate": 3.05936e-06,
      "loss": 0.2168,
      "step": 43380
    },
    {
      "epoch": 1.38848,
      "grad_norm": 0.015123390592634678,
      "learning_rate": 3.0577600000000003e-06,
      "loss": 0.1992,
      "step": 43390
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.017206311225891113,
      "learning_rate": 3.0561600000000002e-06,
      "loss": 0.2035,
      "step": 43400
    },
    {
      "epoch": 1.3891200000000001,
      "grad_norm": 0.018518321216106415,
      "learning_rate": 3.05456e-06,
      "loss": 0.2138,
      "step": 43410
    },
    {
      "epoch": 1.38944,
      "grad_norm": 0.015199942514300346,
      "learning_rate": 3.0529600000000004e-06,
      "loss": 0.1992,
      "step": 43420
    },
    {
      "epoch": 1.3897599999999999,
      "grad_norm": 0.036333974450826645,
      "learning_rate": 3.0513600000000003e-06,
      "loss": 0.1994,
      "step": 43430
    },
    {
      "epoch": 1.39008,
      "grad_norm": 0.02527276799082756,
      "learning_rate": 3.0497600000000006e-06,
      "loss": 0.1992,
      "step": 43440
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.0456363819539547,
      "learning_rate": 3.04816e-06,
      "loss": 0.2027,
      "step": 43450
    },
    {
      "epoch": 1.39072,
      "grad_norm": 0.02378329075872898,
      "learning_rate": 3.04656e-06,
      "loss": 0.1991,
      "step": 43460
    },
    {
      "epoch": 1.39104,
      "grad_norm": 0.01637646183371544,
      "learning_rate": 3.0449600000000002e-06,
      "loss": 0.1992,
      "step": 43470
    },
    {
      "epoch": 1.39136,
      "grad_norm": 0.025962622836232185,
      "learning_rate": 3.04336e-06,
      "loss": 0.1992,
      "step": 43480
    },
    {
      "epoch": 1.39168,
      "grad_norm": 0.013884184882044792,
      "learning_rate": 3.0417600000000004e-06,
      "loss": 0.2063,
      "step": 43490
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.01829027570784092,
      "learning_rate": 3.0401600000000003e-06,
      "loss": 0.2264,
      "step": 43500
    },
    {
      "epoch": 1.39232,
      "grad_norm": 0.02459684945642948,
      "learning_rate": 3.0385600000000006e-06,
      "loss": 0.1992,
      "step": 43510
    },
    {
      "epoch": 1.39264,
      "grad_norm": 0.01593427173793316,
      "learning_rate": 3.0369600000000005e-06,
      "loss": 0.1992,
      "step": 43520
    },
    {
      "epoch": 1.39296,
      "grad_norm": 0.08272743970155716,
      "learning_rate": 3.03536e-06,
      "loss": 0.2088,
      "step": 43530
    },
    {
      "epoch": 1.39328,
      "grad_norm": 0.01869128830730915,
      "learning_rate": 3.0337600000000002e-06,
      "loss": 0.1994,
      "step": 43540
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.09257436543703079,
      "learning_rate": 3.03216e-06,
      "loss": 0.1992,
      "step": 43550
    },
    {
      "epoch": 1.39392,
      "grad_norm": 0.020663578063249588,
      "learning_rate": 3.03056e-06,
      "loss": 0.2008,
      "step": 43560
    },
    {
      "epoch": 1.39424,
      "grad_norm": 0.029017437249422073,
      "learning_rate": 3.0289600000000003e-06,
      "loss": 0.1992,
      "step": 43570
    },
    {
      "epoch": 1.39456,
      "grad_norm": 0.12301886081695557,
      "learning_rate": 3.02736e-06,
      "loss": 0.1995,
      "step": 43580
    },
    {
      "epoch": 1.3948800000000001,
      "grad_norm": 0.140904039144516,
      "learning_rate": 3.0257600000000005e-06,
      "loss": 0.1996,
      "step": 43590
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.03323971480131149,
      "learning_rate": 3.0241600000000004e-06,
      "loss": 0.1991,
      "step": 43600
    },
    {
      "epoch": 1.3955199999999999,
      "grad_norm": 0.03936538100242615,
      "learning_rate": 3.0225600000000007e-06,
      "loss": 0.1994,
      "step": 43610
    },
    {
      "epoch": 1.39584,
      "grad_norm": 0.01136595569550991,
      "learning_rate": 3.02096e-06,
      "loss": 0.199,
      "step": 43620
    },
    {
      "epoch": 1.39616,
      "grad_norm": 1.3688592910766602,
      "learning_rate": 3.01936e-06,
      "loss": 0.2282,
      "step": 43630
    },
    {
      "epoch": 1.39648,
      "grad_norm": 0.025785421952605247,
      "learning_rate": 3.0177600000000003e-06,
      "loss": 0.1996,
      "step": 43640
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.027525242418050766,
      "learning_rate": 3.01616e-06,
      "loss": 0.2013,
      "step": 43650
    },
    {
      "epoch": 1.39712,
      "grad_norm": 0.013221683911979198,
      "learning_rate": 3.01456e-06,
      "loss": 0.2091,
      "step": 43660
    },
    {
      "epoch": 1.39744,
      "grad_norm": 0.015744537115097046,
      "learning_rate": 3.0129600000000004e-06,
      "loss": 0.1993,
      "step": 43670
    },
    {
      "epoch": 1.39776,
      "grad_norm": 1.186975359916687,
      "learning_rate": 3.0113600000000003e-06,
      "loss": 0.2016,
      "step": 43680
    },
    {
      "epoch": 1.39808,
      "grad_norm": 0.02591637521982193,
      "learning_rate": 3.0097600000000006e-06,
      "loss": 0.2152,
      "step": 43690
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.04423996061086655,
      "learning_rate": 3.00816e-06,
      "loss": 0.2159,
      "step": 43700
    },
    {
      "epoch": 1.39872,
      "grad_norm": 0.034800924360752106,
      "learning_rate": 3.00656e-06,
      "loss": 0.1992,
      "step": 43710
    },
    {
      "epoch": 1.39904,
      "grad_norm": 0.048424772918224335,
      "learning_rate": 3.00496e-06,
      "loss": 0.2051,
      "step": 43720
    },
    {
      "epoch": 1.39936,
      "grad_norm": 0.047360409051179886,
      "learning_rate": 3.00336e-06,
      "loss": 0.2,
      "step": 43730
    },
    {
      "epoch": 1.39968,
      "grad_norm": 0.7764111161231995,
      "learning_rate": 3.0017600000000004e-06,
      "loss": 0.2301,
      "step": 43740
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.06396834552288055,
      "learning_rate": 3.0001600000000003e-06,
      "loss": 0.2001,
      "step": 43750
    },
    {
      "epoch": 1.40032,
      "grad_norm": 0.029263148084282875,
      "learning_rate": 2.9985600000000006e-06,
      "loss": 0.1993,
      "step": 43760
    },
    {
      "epoch": 1.40064,
      "grad_norm": 0.061469048261642456,
      "learning_rate": 2.9969600000000004e-06,
      "loss": 0.1993,
      "step": 43770
    },
    {
      "epoch": 1.40096,
      "grad_norm": 0.021037058904767036,
      "learning_rate": 2.9953600000000003e-06,
      "loss": 0.1991,
      "step": 43780
    },
    {
      "epoch": 1.40128,
      "grad_norm": 0.04001927748322487,
      "learning_rate": 2.99376e-06,
      "loss": 0.1993,
      "step": 43790
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.021974926814436913,
      "learning_rate": 2.99216e-06,
      "loss": 0.2146,
      "step": 43800
    },
    {
      "epoch": 1.40192,
      "grad_norm": 0.024625886231660843,
      "learning_rate": 2.99056e-06,
      "loss": 0.1992,
      "step": 43810
    },
    {
      "epoch": 1.40224,
      "grad_norm": 0.04768872633576393,
      "learning_rate": 2.9889600000000003e-06,
      "loss": 0.2088,
      "step": 43820
    },
    {
      "epoch": 1.40256,
      "grad_norm": 0.4161050617694855,
      "learning_rate": 2.98736e-06,
      "loss": 0.2002,
      "step": 43830
    },
    {
      "epoch": 1.4028800000000001,
      "grad_norm": 0.055044036358594894,
      "learning_rate": 2.9857600000000004e-06,
      "loss": 0.1995,
      "step": 43840
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.018674654886126518,
      "learning_rate": 2.9841600000000003e-06,
      "loss": 0.2061,
      "step": 43850
    },
    {
      "epoch": 1.4035199999999999,
      "grad_norm": 0.031485673040151596,
      "learning_rate": 2.9825600000000006e-06,
      "loss": 0.1991,
      "step": 43860
    },
    {
      "epoch": 1.40384,
      "grad_norm": 0.04357869550585747,
      "learning_rate": 2.9809600000000005e-06,
      "loss": 0.1993,
      "step": 43870
    },
    {
      "epoch": 1.40416,
      "grad_norm": 0.07168068736791611,
      "learning_rate": 2.97936e-06,
      "loss": 0.2162,
      "step": 43880
    },
    {
      "epoch": 1.40448,
      "grad_norm": 0.026982663199305534,
      "learning_rate": 2.9777600000000003e-06,
      "loss": 0.1991,
      "step": 43890
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.02129369229078293,
      "learning_rate": 2.97616e-06,
      "loss": 0.1992,
      "step": 43900
    },
    {
      "epoch": 1.40512,
      "grad_norm": 0.017766449600458145,
      "learning_rate": 2.97456e-06,
      "loss": 0.1999,
      "step": 43910
    },
    {
      "epoch": 1.40544,
      "grad_norm": 0.04469606280326843,
      "learning_rate": 2.9729600000000003e-06,
      "loss": 0.1999,
      "step": 43920
    },
    {
      "epoch": 1.40576,
      "grad_norm": 0.0257634986191988,
      "learning_rate": 2.97136e-06,
      "loss": 0.1994,
      "step": 43930
    },
    {
      "epoch": 1.40608,
      "grad_norm": 0.022562596946954727,
      "learning_rate": 2.9697600000000005e-06,
      "loss": 0.1992,
      "step": 43940
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.01792585290968418,
      "learning_rate": 2.9681600000000004e-06,
      "loss": 0.2136,
      "step": 43950
    },
    {
      "epoch": 1.40672,
      "grad_norm": 0.03173207864165306,
      "learning_rate": 2.96656e-06,
      "loss": 0.1999,
      "step": 43960
    },
    {
      "epoch": 1.40704,
      "grad_norm": 0.030024241656064987,
      "learning_rate": 2.96496e-06,
      "loss": 0.2043,
      "step": 43970
    },
    {
      "epoch": 1.40736,
      "grad_norm": 0.01110901404172182,
      "learning_rate": 2.96336e-06,
      "loss": 0.2097,
      "step": 43980
    },
    {
      "epoch": 1.40768,
      "grad_norm": 0.012621983885765076,
      "learning_rate": 2.9617600000000003e-06,
      "loss": 0.1991,
      "step": 43990
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.02254640869796276,
      "learning_rate": 2.9601600000000002e-06,
      "loss": 0.2113,
      "step": 44000
    },
    {
      "epoch": 1.408,
      "eval_runtime": 55.2419,
      "eval_samples_per_second": 181.022,
      "eval_steps_per_second": 11.314,
      "step": 44000
    },
    {
      "epoch": 1.40832,
      "grad_norm": 0.03074715845286846,
      "learning_rate": 2.9585600000000005e-06,
      "loss": 0.1992,
      "step": 44010
    },
    {
      "epoch": 1.4086400000000001,
      "grad_norm": 0.02203456126153469,
      "learning_rate": 2.9569600000000004e-06,
      "loss": 0.1991,
      "step": 44020
    },
    {
      "epoch": 1.40896,
      "grad_norm": 0.041659437119960785,
      "learning_rate": 2.9553600000000003e-06,
      "loss": 0.2151,
      "step": 44030
    },
    {
      "epoch": 1.4092799999999999,
      "grad_norm": 0.01545895915478468,
      "learning_rate": 2.9537600000000006e-06,
      "loss": 0.1993,
      "step": 44040
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.07052623480558395,
      "learning_rate": 2.95216e-06,
      "loss": 0.204,
      "step": 44050
    },
    {
      "epoch": 1.40992,
      "grad_norm": 0.0481465645134449,
      "learning_rate": 2.95056e-06,
      "loss": 0.1994,
      "step": 44060
    },
    {
      "epoch": 1.41024,
      "grad_norm": 0.038285017013549805,
      "learning_rate": 2.9489600000000002e-06,
      "loss": 0.1991,
      "step": 44070
    },
    {
      "epoch": 1.41056,
      "grad_norm": 0.01758657954633236,
      "learning_rate": 2.94736e-06,
      "loss": 0.2016,
      "step": 44080
    },
    {
      "epoch": 1.4108800000000001,
      "grad_norm": 0.06680595874786377,
      "learning_rate": 2.9457600000000004e-06,
      "loss": 0.1995,
      "step": 44090
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.04059958457946777,
      "learning_rate": 2.9441600000000003e-06,
      "loss": 0.1993,
      "step": 44100
    },
    {
      "epoch": 1.4115199999999999,
      "grad_norm": 0.03270217403769493,
      "learning_rate": 2.9425600000000006e-06,
      "loss": 0.1991,
      "step": 44110
    },
    {
      "epoch": 1.41184,
      "grad_norm": 0.02720279060304165,
      "learning_rate": 2.9409600000000005e-06,
      "loss": 0.1993,
      "step": 44120
    },
    {
      "epoch": 1.41216,
      "grad_norm": 0.039629966020584106,
      "learning_rate": 2.93936e-06,
      "loss": 0.199,
      "step": 44130
    },
    {
      "epoch": 1.41248,
      "grad_norm": 0.019219938665628433,
      "learning_rate": 2.9377600000000002e-06,
      "loss": 0.1991,
      "step": 44140
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.02090049721300602,
      "learning_rate": 2.93616e-06,
      "loss": 0.2,
      "step": 44150
    },
    {
      "epoch": 1.41312,
      "grad_norm": 0.058167751878499985,
      "learning_rate": 2.93456e-06,
      "loss": 0.1992,
      "step": 44160
    },
    {
      "epoch": 1.41344,
      "grad_norm": 0.044937897473573685,
      "learning_rate": 2.9329600000000003e-06,
      "loss": 0.1994,
      "step": 44170
    },
    {
      "epoch": 1.41376,
      "grad_norm": 0.02620537579059601,
      "learning_rate": 2.93136e-06,
      "loss": 0.199,
      "step": 44180
    },
    {
      "epoch": 1.41408,
      "grad_norm": 0.033625926822423935,
      "learning_rate": 2.9297600000000005e-06,
      "loss": 0.1996,
      "step": 44190
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.024623969569802284,
      "learning_rate": 2.9281600000000004e-06,
      "loss": 0.1992,
      "step": 44200
    },
    {
      "epoch": 1.41472,
      "grad_norm": 0.016800161451101303,
      "learning_rate": 2.9265600000000007e-06,
      "loss": 0.2023,
      "step": 44210
    },
    {
      "epoch": 1.41504,
      "grad_norm": 0.019857261329889297,
      "learning_rate": 2.92496e-06,
      "loss": 0.1992,
      "step": 44220
    },
    {
      "epoch": 1.41536,
      "grad_norm": 0.030229534953832626,
      "learning_rate": 2.92336e-06,
      "loss": 0.2168,
      "step": 44230
    },
    {
      "epoch": 1.41568,
      "grad_norm": 0.04028109088540077,
      "learning_rate": 2.9217600000000003e-06,
      "loss": 0.21,
      "step": 44240
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.01118655689060688,
      "learning_rate": 2.92016e-06,
      "loss": 0.1994,
      "step": 44250
    },
    {
      "epoch": 1.41632,
      "grad_norm": 0.02965596131980419,
      "learning_rate": 2.9185600000000005e-06,
      "loss": 0.1994,
      "step": 44260
    },
    {
      "epoch": 1.4166400000000001,
      "grad_norm": 0.008595619350671768,
      "learning_rate": 2.9169600000000004e-06,
      "loss": 0.199,
      "step": 44270
    },
    {
      "epoch": 1.41696,
      "grad_norm": 0.01651247963309288,
      "learning_rate": 2.9153600000000002e-06,
      "loss": 0.1993,
      "step": 44280
    },
    {
      "epoch": 1.4172799999999999,
      "grad_norm": 0.010431128554046154,
      "learning_rate": 2.9137600000000005e-06,
      "loss": 0.2164,
      "step": 44290
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.010098012164235115,
      "learning_rate": 2.91216e-06,
      "loss": 0.1992,
      "step": 44300
    },
    {
      "epoch": 1.41792,
      "grad_norm": 0.01425615418702364,
      "learning_rate": 2.91056e-06,
      "loss": 0.1995,
      "step": 44310
    },
    {
      "epoch": 1.41824,
      "grad_norm": 0.021709464490413666,
      "learning_rate": 2.90896e-06,
      "loss": 0.1991,
      "step": 44320
    },
    {
      "epoch": 1.41856,
      "grad_norm": 0.019371937960386276,
      "learning_rate": 2.90736e-06,
      "loss": 0.209,
      "step": 44330
    },
    {
      "epoch": 1.41888,
      "grad_norm": 0.03785539045929909,
      "learning_rate": 2.9057600000000004e-06,
      "loss": 0.1992,
      "step": 44340
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.05305885896086693,
      "learning_rate": 2.9041600000000002e-06,
      "loss": 0.1992,
      "step": 44350
    },
    {
      "epoch": 1.41952,
      "grad_norm": 0.015965523198246956,
      "learning_rate": 2.9025600000000005e-06,
      "loss": 0.1992,
      "step": 44360
    },
    {
      "epoch": 1.41984,
      "grad_norm": 0.02616022527217865,
      "learning_rate": 2.9009600000000004e-06,
      "loss": 0.2147,
      "step": 44370
    },
    {
      "epoch": 1.42016,
      "grad_norm": 0.02724587731063366,
      "learning_rate": 2.8993600000000003e-06,
      "loss": 0.1992,
      "step": 44380
    },
    {
      "epoch": 1.42048,
      "grad_norm": 0.011484085582196712,
      "learning_rate": 2.89776e-06,
      "loss": 0.1997,
      "step": 44390
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.03998544439673424,
      "learning_rate": 2.89616e-06,
      "loss": 0.1992,
      "step": 44400
    },
    {
      "epoch": 1.42112,
      "grad_norm": 0.043534424155950546,
      "learning_rate": 2.89456e-06,
      "loss": 0.1992,
      "step": 44410
    },
    {
      "epoch": 1.42144,
      "grad_norm": 0.014244156889617443,
      "learning_rate": 2.8929600000000003e-06,
      "loss": 0.2083,
      "step": 44420
    },
    {
      "epoch": 1.42176,
      "grad_norm": 0.028734255582094193,
      "learning_rate": 2.89136e-06,
      "loss": 0.1991,
      "step": 44430
    },
    {
      "epoch": 1.42208,
      "grad_norm": 0.031336475163698196,
      "learning_rate": 2.8897600000000004e-06,
      "loss": 0.1991,
      "step": 44440
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.03532581031322479,
      "learning_rate": 2.8881600000000003e-06,
      "loss": 0.1992,
      "step": 44450
    },
    {
      "epoch": 1.42272,
      "grad_norm": 0.07756586372852325,
      "learning_rate": 2.8865600000000006e-06,
      "loss": 0.2023,
      "step": 44460
    },
    {
      "epoch": 1.42304,
      "grad_norm": 0.01428159698843956,
      "learning_rate": 2.88496e-06,
      "loss": 0.1991,
      "step": 44470
    },
    {
      "epoch": 1.42336,
      "grad_norm": 0.031145183369517326,
      "learning_rate": 2.88336e-06,
      "loss": 0.2021,
      "step": 44480
    },
    {
      "epoch": 1.42368,
      "grad_norm": 0.01737801358103752,
      "learning_rate": 2.8817600000000003e-06,
      "loss": 0.1996,
      "step": 44490
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.012694498524069786,
      "learning_rate": 2.88016e-06,
      "loss": 0.1993,
      "step": 44500
    },
    {
      "epoch": 1.42432,
      "grad_norm": 0.029452694579958916,
      "learning_rate": 2.8785600000000004e-06,
      "loss": 0.2004,
      "step": 44510
    },
    {
      "epoch": 1.4246400000000001,
      "grad_norm": 0.03844596445560455,
      "learning_rate": 2.8769600000000003e-06,
      "loss": 0.1992,
      "step": 44520
    },
    {
      "epoch": 1.42496,
      "grad_norm": 0.02047993801534176,
      "learning_rate": 2.87536e-06,
      "loss": 0.199,
      "step": 44530
    },
    {
      "epoch": 1.4252799999999999,
      "grad_norm": 0.02323821559548378,
      "learning_rate": 2.8737600000000005e-06,
      "loss": 0.2002,
      "step": 44540
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.02697961963713169,
      "learning_rate": 2.8721600000000004e-06,
      "loss": 0.2094,
      "step": 44550
    },
    {
      "epoch": 1.42592,
      "grad_norm": 0.011122982949018478,
      "learning_rate": 2.87056e-06,
      "loss": 0.1991,
      "step": 44560
    },
    {
      "epoch": 1.42624,
      "grad_norm": 0.04031965509057045,
      "learning_rate": 2.86896e-06,
      "loss": 0.2231,
      "step": 44570
    },
    {
      "epoch": 1.42656,
      "grad_norm": 0.020131641998887062,
      "learning_rate": 2.86736e-06,
      "loss": 0.1992,
      "step": 44580
    },
    {
      "epoch": 1.42688,
      "grad_norm": 0.06981594115495682,
      "learning_rate": 2.8657600000000003e-06,
      "loss": 0.1994,
      "step": 44590
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.0212328489869833,
      "learning_rate": 2.86416e-06,
      "loss": 0.1992,
      "step": 44600
    },
    {
      "epoch": 1.42752,
      "grad_norm": 0.038304198533296585,
      "learning_rate": 2.8625600000000005e-06,
      "loss": 0.1994,
      "step": 44610
    },
    {
      "epoch": 1.42784,
      "grad_norm": 0.04079766571521759,
      "learning_rate": 2.8609600000000004e-06,
      "loss": 0.1993,
      "step": 44620
    },
    {
      "epoch": 1.42816,
      "grad_norm": 0.023776346817612648,
      "learning_rate": 2.8593600000000003e-06,
      "loss": 0.1993,
      "step": 44630
    },
    {
      "epoch": 1.42848,
      "grad_norm": 0.027783211320638657,
      "learning_rate": 2.85776e-06,
      "loss": 0.1994,
      "step": 44640
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.011114241555333138,
      "learning_rate": 2.85616e-06,
      "loss": 0.1991,
      "step": 44650
    },
    {
      "epoch": 1.42912,
      "grad_norm": 0.014712320640683174,
      "learning_rate": 2.85456e-06,
      "loss": 0.1996,
      "step": 44660
    },
    {
      "epoch": 1.42944,
      "grad_norm": 0.00870803464204073,
      "learning_rate": 2.85296e-06,
      "loss": 0.199,
      "step": 44670
    },
    {
      "epoch": 1.42976,
      "grad_norm": 0.02060428448021412,
      "learning_rate": 2.85136e-06,
      "loss": 0.1996,
      "step": 44680
    },
    {
      "epoch": 1.43008,
      "grad_norm": 0.012377721257507801,
      "learning_rate": 2.8497600000000004e-06,
      "loss": 0.1992,
      "step": 44690
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.021501418203115463,
      "learning_rate": 2.8481600000000003e-06,
      "loss": 0.1992,
      "step": 44700
    },
    {
      "epoch": 1.43072,
      "grad_norm": 0.02236180566251278,
      "learning_rate": 2.8465600000000006e-06,
      "loss": 0.1993,
      "step": 44710
    },
    {
      "epoch": 1.4310399999999999,
      "grad_norm": 0.111732617020607,
      "learning_rate": 2.8449600000000005e-06,
      "loss": 0.2141,
      "step": 44720
    },
    {
      "epoch": 1.43136,
      "grad_norm": 0.023051630705595016,
      "learning_rate": 2.84336e-06,
      "loss": 0.2151,
      "step": 44730
    },
    {
      "epoch": 1.43168,
      "grad_norm": 0.0256318598985672,
      "learning_rate": 2.8417600000000002e-06,
      "loss": 0.2098,
      "step": 44740
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.019558269530534744,
      "learning_rate": 2.84016e-06,
      "loss": 0.2108,
      "step": 44750
    },
    {
      "epoch": 1.43232,
      "grad_norm": 0.017393384128808975,
      "learning_rate": 2.8385600000000004e-06,
      "loss": 0.1996,
      "step": 44760
    },
    {
      "epoch": 1.4326400000000001,
      "grad_norm": 0.044178638607263565,
      "learning_rate": 2.8369600000000003e-06,
      "loss": 0.2141,
      "step": 44770
    },
    {
      "epoch": 1.43296,
      "grad_norm": 0.03210832178592682,
      "learning_rate": 2.83536e-06,
      "loss": 0.1991,
      "step": 44780
    },
    {
      "epoch": 1.4332799999999999,
      "grad_norm": 0.02320070192217827,
      "learning_rate": 2.8337600000000005e-06,
      "loss": 0.224,
      "step": 44790
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.06784648448228836,
      "learning_rate": 2.8321600000000003e-06,
      "loss": 0.1993,
      "step": 44800
    },
    {
      "epoch": 1.43392,
      "grad_norm": 0.00973881408572197,
      "learning_rate": 2.83056e-06,
      "loss": 0.2039,
      "step": 44810
    },
    {
      "epoch": 1.43424,
      "grad_norm": 0.0377347432076931,
      "learning_rate": 2.82896e-06,
      "loss": 0.2113,
      "step": 44820
    },
    {
      "epoch": 1.43456,
      "grad_norm": 0.027804430574178696,
      "learning_rate": 2.82736e-06,
      "loss": 0.1991,
      "step": 44830
    },
    {
      "epoch": 1.43488,
      "grad_norm": 0.019622744992375374,
      "learning_rate": 2.8257600000000003e-06,
      "loss": 0.2,
      "step": 44840
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.06197182089090347,
      "learning_rate": 2.82416e-06,
      "loss": 0.1993,
      "step": 44850
    },
    {
      "epoch": 1.43552,
      "grad_norm": 0.015810202807188034,
      "learning_rate": 2.8225600000000005e-06,
      "loss": 0.1993,
      "step": 44860
    },
    {
      "epoch": 1.43584,
      "grad_norm": 0.02850729040801525,
      "learning_rate": 2.8209600000000003e-06,
      "loss": 0.1994,
      "step": 44870
    },
    {
      "epoch": 1.43616,
      "grad_norm": 0.0224011093378067,
      "learning_rate": 2.8193600000000002e-06,
      "loss": 0.1992,
      "step": 44880
    },
    {
      "epoch": 1.43648,
      "grad_norm": 0.03359592333436012,
      "learning_rate": 2.8177600000000005e-06,
      "loss": 0.1994,
      "step": 44890
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.009224198758602142,
      "learning_rate": 2.81616e-06,
      "loss": 0.2192,
      "step": 44900
    },
    {
      "epoch": 1.43712,
      "grad_norm": 0.016820251941680908,
      "learning_rate": 2.81456e-06,
      "loss": 0.1994,
      "step": 44910
    },
    {
      "epoch": 1.43744,
      "grad_norm": 0.01269858330488205,
      "learning_rate": 2.81296e-06,
      "loss": 0.1991,
      "step": 44920
    },
    {
      "epoch": 1.43776,
      "grad_norm": 0.022213196381926537,
      "learning_rate": 2.81136e-06,
      "loss": 0.2121,
      "step": 44930
    },
    {
      "epoch": 1.43808,
      "grad_norm": 0.08970630168914795,
      "learning_rate": 2.8097600000000004e-06,
      "loss": 0.2089,
      "step": 44940
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.02596268802881241,
      "learning_rate": 2.8081600000000002e-06,
      "loss": 0.1993,
      "step": 44950
    },
    {
      "epoch": 1.43872,
      "grad_norm": 0.02194506861269474,
      "learning_rate": 2.8065600000000005e-06,
      "loss": 0.2129,
      "step": 44960
    },
    {
      "epoch": 1.4390399999999999,
      "grad_norm": 0.014117666520178318,
      "learning_rate": 2.8049600000000004e-06,
      "loss": 0.1991,
      "step": 44970
    },
    {
      "epoch": 1.43936,
      "grad_norm": 0.01183438953012228,
      "learning_rate": 2.80336e-06,
      "loss": 0.2022,
      "step": 44980
    },
    {
      "epoch": 1.43968,
      "grad_norm": 0.03223806619644165,
      "learning_rate": 2.80176e-06,
      "loss": 0.1994,
      "step": 44990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.058478228747844696,
      "learning_rate": 2.80016e-06,
      "loss": 0.1995,
      "step": 45000
    },
    {
      "epoch": 1.44,
      "eval_runtime": 70.5999,
      "eval_samples_per_second": 141.643,
      "eval_steps_per_second": 8.853,
      "step": 45000
    },
    {
      "epoch": 1.44032,
      "grad_norm": 0.017234625294804573,
      "learning_rate": 2.7985600000000004e-06,
      "loss": 0.2153,
      "step": 45010
    },
    {
      "epoch": 1.44064,
      "grad_norm": 0.014157064259052277,
      "learning_rate": 2.7969600000000002e-06,
      "loss": 0.1992,
      "step": 45020
    },
    {
      "epoch": 1.44096,
      "grad_norm": 0.012240036390721798,
      "learning_rate": 2.79536e-06,
      "loss": 0.1997,
      "step": 45030
    },
    {
      "epoch": 1.44128,
      "grad_norm": 0.025226717814803123,
      "learning_rate": 2.7937600000000004e-06,
      "loss": 0.1991,
      "step": 45040
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.022459745407104492,
      "learning_rate": 2.7921600000000003e-06,
      "loss": 0.1991,
      "step": 45050
    },
    {
      "epoch": 1.44192,
      "grad_norm": 0.017729084938764572,
      "learning_rate": 2.7905600000000006e-06,
      "loss": 0.2115,
      "step": 45060
    },
    {
      "epoch": 1.44224,
      "grad_norm": 0.022366097196936607,
      "learning_rate": 2.78896e-06,
      "loss": 0.1992,
      "step": 45070
    },
    {
      "epoch": 1.44256,
      "grad_norm": 0.15188834071159363,
      "learning_rate": 2.78736e-06,
      "loss": 0.1993,
      "step": 45080
    },
    {
      "epoch": 1.44288,
      "grad_norm": 0.019815703853964806,
      "learning_rate": 2.7857600000000002e-06,
      "loss": 0.1992,
      "step": 45090
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.0743572935461998,
      "learning_rate": 2.78416e-06,
      "loss": 0.2007,
      "step": 45100
    },
    {
      "epoch": 1.44352,
      "grad_norm": 0.01534317433834076,
      "learning_rate": 2.7825600000000004e-06,
      "loss": 0.1993,
      "step": 45110
    },
    {
      "epoch": 1.44384,
      "grad_norm": 0.02930237539112568,
      "learning_rate": 2.7809600000000003e-06,
      "loss": 0.199,
      "step": 45120
    },
    {
      "epoch": 1.44416,
      "grad_norm": 0.042105335742235184,
      "learning_rate": 2.77936e-06,
      "loss": 0.2311,
      "step": 45130
    },
    {
      "epoch": 1.44448,
      "grad_norm": 0.028436075896024704,
      "learning_rate": 2.7777600000000005e-06,
      "loss": 0.1992,
      "step": 45140
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.05798999220132828,
      "learning_rate": 2.77616e-06,
      "loss": 0.2065,
      "step": 45150
    },
    {
      "epoch": 1.44512,
      "grad_norm": 0.02788218855857849,
      "learning_rate": 2.77456e-06,
      "loss": 0.2022,
      "step": 45160
    },
    {
      "epoch": 1.44544,
      "grad_norm": 0.040158409625291824,
      "learning_rate": 2.77296e-06,
      "loss": 0.2203,
      "step": 45170
    },
    {
      "epoch": 1.44576,
      "grad_norm": 0.027088601142168045,
      "learning_rate": 2.77136e-06,
      "loss": 0.199,
      "step": 45180
    },
    {
      "epoch": 1.44608,
      "grad_norm": 0.011631878092885017,
      "learning_rate": 2.7697600000000003e-06,
      "loss": 0.2203,
      "step": 45190
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.01891699992120266,
      "learning_rate": 2.76816e-06,
      "loss": 0.1999,
      "step": 45200
    },
    {
      "epoch": 1.44672,
      "grad_norm": 1.2666361331939697,
      "learning_rate": 2.7665600000000005e-06,
      "loss": 0.223,
      "step": 45210
    },
    {
      "epoch": 1.4470399999999999,
      "grad_norm": 0.0766458660364151,
      "learning_rate": 2.7649600000000004e-06,
      "loss": 0.1991,
      "step": 45220
    },
    {
      "epoch": 1.44736,
      "grad_norm": 0.09716781228780746,
      "learning_rate": 2.7633600000000007e-06,
      "loss": 0.1991,
      "step": 45230
    },
    {
      "epoch": 1.44768,
      "grad_norm": 1.1620149612426758,
      "learning_rate": 2.76176e-06,
      "loss": 0.2068,
      "step": 45240
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.022611312568187714,
      "learning_rate": 2.76016e-06,
      "loss": 0.1992,
      "step": 45250
    },
    {
      "epoch": 1.44832,
      "grad_norm": 0.05619458109140396,
      "learning_rate": 2.7585600000000003e-06,
      "loss": 0.1993,
      "step": 45260
    },
    {
      "epoch": 1.44864,
      "grad_norm": 0.021546173840761185,
      "learning_rate": 2.75696e-06,
      "loss": 0.2126,
      "step": 45270
    },
    {
      "epoch": 1.44896,
      "grad_norm": 0.02673368528485298,
      "learning_rate": 2.75536e-06,
      "loss": 0.1991,
      "step": 45280
    },
    {
      "epoch": 1.44928,
      "grad_norm": 0.030952876433730125,
      "learning_rate": 2.7537600000000004e-06,
      "loss": 0.2006,
      "step": 45290
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.040375955402851105,
      "learning_rate": 2.7521600000000003e-06,
      "loss": 0.2049,
      "step": 45300
    },
    {
      "epoch": 1.44992,
      "grad_norm": 0.03563230484724045,
      "learning_rate": 2.7505600000000006e-06,
      "loss": 0.1991,
      "step": 45310
    },
    {
      "epoch": 1.45024,
      "grad_norm": 0.010969447903335094,
      "learning_rate": 2.74896e-06,
      "loss": 0.2211,
      "step": 45320
    },
    {
      "epoch": 1.45056,
      "grad_norm": 0.022707868367433548,
      "learning_rate": 2.74736e-06,
      "loss": 0.199,
      "step": 45330
    },
    {
      "epoch": 1.45088,
      "grad_norm": 0.7154028415679932,
      "learning_rate": 2.74576e-06,
      "loss": 0.2146,
      "step": 45340
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.02881709858775139,
      "learning_rate": 2.74416e-06,
      "loss": 0.2011,
      "step": 45350
    },
    {
      "epoch": 1.45152,
      "grad_norm": 0.046682458370923996,
      "learning_rate": 2.7425600000000004e-06,
      "loss": 0.2153,
      "step": 45360
    },
    {
      "epoch": 1.45184,
      "grad_norm": 0.07290969043970108,
      "learning_rate": 2.7409600000000003e-06,
      "loss": 0.1994,
      "step": 45370
    },
    {
      "epoch": 1.4521600000000001,
      "grad_norm": 0.03393373638391495,
      "learning_rate": 2.73936e-06,
      "loss": 0.1993,
      "step": 45380
    },
    {
      "epoch": 1.45248,
      "grad_norm": 0.03504480794072151,
      "learning_rate": 2.7377600000000004e-06,
      "loss": 0.1997,
      "step": 45390
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.03027063049376011,
      "learning_rate": 2.7361600000000003e-06,
      "loss": 0.1992,
      "step": 45400
    },
    {
      "epoch": 1.45312,
      "grad_norm": 0.02202790416777134,
      "learning_rate": 2.7345599999999998e-06,
      "loss": 0.1998,
      "step": 45410
    },
    {
      "epoch": 1.45344,
      "grad_norm": 0.023347683250904083,
      "learning_rate": 2.73296e-06,
      "loss": 0.1992,
      "step": 45420
    },
    {
      "epoch": 1.45376,
      "grad_norm": 0.09110968559980392,
      "learning_rate": 2.73136e-06,
      "loss": 0.1992,
      "step": 45430
    },
    {
      "epoch": 1.45408,
      "grad_norm": 0.024401405826210976,
      "learning_rate": 2.7297600000000003e-06,
      "loss": 0.1993,
      "step": 45440
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.03814336284995079,
      "learning_rate": 2.72816e-06,
      "loss": 0.231,
      "step": 45450
    },
    {
      "epoch": 1.45472,
      "grad_norm": 0.010562930256128311,
      "learning_rate": 2.7265600000000005e-06,
      "loss": 0.1995,
      "step": 45460
    },
    {
      "epoch": 1.45504,
      "grad_norm": 0.03593549132347107,
      "learning_rate": 2.7249600000000003e-06,
      "loss": 0.1993,
      "step": 45470
    },
    {
      "epoch": 1.45536,
      "grad_norm": 0.042522381991147995,
      "learning_rate": 2.7233600000000006e-06,
      "loss": 0.2003,
      "step": 45480
    },
    {
      "epoch": 1.45568,
      "grad_norm": 0.011162593960762024,
      "learning_rate": 2.72176e-06,
      "loss": 0.2161,
      "step": 45490
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.0677170380949974,
      "learning_rate": 2.72016e-06,
      "loss": 0.1992,
      "step": 45500
    },
    {
      "epoch": 1.45632,
      "grad_norm": 0.026095369830727577,
      "learning_rate": 2.7185600000000003e-06,
      "loss": 0.2164,
      "step": 45510
    },
    {
      "epoch": 1.45664,
      "grad_norm": 0.033434923738241196,
      "learning_rate": 2.71696e-06,
      "loss": 0.1991,
      "step": 45520
    },
    {
      "epoch": 1.45696,
      "grad_norm": 0.031011011451482773,
      "learning_rate": 2.71536e-06,
      "loss": 0.2053,
      "step": 45530
    },
    {
      "epoch": 1.45728,
      "grad_norm": 0.0415269173681736,
      "learning_rate": 2.7137600000000003e-06,
      "loss": 0.2141,
      "step": 45540
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.02729620970785618,
      "learning_rate": 2.7121600000000002e-06,
      "loss": 0.1992,
      "step": 45550
    },
    {
      "epoch": 1.45792,
      "grad_norm": 0.011566017754375935,
      "learning_rate": 2.7105600000000005e-06,
      "loss": 0.2147,
      "step": 45560
    },
    {
      "epoch": 1.45824,
      "grad_norm": 0.039597038179636,
      "learning_rate": 2.7089600000000004e-06,
      "loss": 0.1992,
      "step": 45570
    },
    {
      "epoch": 1.45856,
      "grad_norm": 0.023285971954464912,
      "learning_rate": 2.70736e-06,
      "loss": 0.1994,
      "step": 45580
    },
    {
      "epoch": 1.45888,
      "grad_norm": 0.03369731828570366,
      "learning_rate": 2.70576e-06,
      "loss": 0.1992,
      "step": 45590
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.021777255460619926,
      "learning_rate": 2.70416e-06,
      "loss": 0.209,
      "step": 45600
    },
    {
      "epoch": 1.45952,
      "grad_norm": 0.016045326367020607,
      "learning_rate": 2.7025600000000003e-06,
      "loss": 0.2144,
      "step": 45610
    },
    {
      "epoch": 1.45984,
      "grad_norm": 0.01946890912950039,
      "learning_rate": 2.7009600000000002e-06,
      "loss": 0.2002,
      "step": 45620
    },
    {
      "epoch": 1.4601600000000001,
      "grad_norm": 0.022123947739601135,
      "learning_rate": 2.69936e-06,
      "loss": 0.1995,
      "step": 45630
    },
    {
      "epoch": 1.46048,
      "grad_norm": 0.022946374490857124,
      "learning_rate": 2.6977600000000004e-06,
      "loss": 0.1993,
      "step": 45640
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.024807773530483246,
      "learning_rate": 2.6961600000000003e-06,
      "loss": 0.2104,
      "step": 45650
    },
    {
      "epoch": 1.46112,
      "grad_norm": 0.026164010167121887,
      "learning_rate": 2.6945599999999997e-06,
      "loss": 0.1995,
      "step": 45660
    },
    {
      "epoch": 1.46144,
      "grad_norm": 0.020875247195363045,
      "learning_rate": 2.69296e-06,
      "loss": 0.1993,
      "step": 45670
    },
    {
      "epoch": 1.46176,
      "grad_norm": 0.008420469239354134,
      "learning_rate": 2.69136e-06,
      "loss": 0.1992,
      "step": 45680
    },
    {
      "epoch": 1.46208,
      "grad_norm": 0.053755953907966614,
      "learning_rate": 2.6897600000000002e-06,
      "loss": 0.2176,
      "step": 45690
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.019104864448308945,
      "learning_rate": 2.68816e-06,
      "loss": 0.1993,
      "step": 45700
    },
    {
      "epoch": 1.46272,
      "grad_norm": 0.03774099051952362,
      "learning_rate": 2.6865600000000004e-06,
      "loss": 0.1993,
      "step": 45710
    },
    {
      "epoch": 1.46304,
      "grad_norm": 0.01894330233335495,
      "learning_rate": 2.6849600000000003e-06,
      "loss": 0.1991,
      "step": 45720
    },
    {
      "epoch": 1.46336,
      "grad_norm": 0.016247086226940155,
      "learning_rate": 2.6833600000000006e-06,
      "loss": 0.2006,
      "step": 45730
    },
    {
      "epoch": 1.46368,
      "grad_norm": 0.036508772522211075,
      "learning_rate": 2.6817600000000005e-06,
      "loss": 0.1991,
      "step": 45740
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.04490144923329353,
      "learning_rate": 2.68016e-06,
      "loss": 0.2109,
      "step": 45750
    },
    {
      "epoch": 1.46432,
      "grad_norm": 0.02335582673549652,
      "learning_rate": 2.6785600000000002e-06,
      "loss": 0.1992,
      "step": 45760
    },
    {
      "epoch": 1.46464,
      "grad_norm": 0.024916870519518852,
      "learning_rate": 2.67696e-06,
      "loss": 0.2083,
      "step": 45770
    },
    {
      "epoch": 1.46496,
      "grad_norm": 0.03797099366784096,
      "learning_rate": 2.67536e-06,
      "loss": 0.2028,
      "step": 45780
    },
    {
      "epoch": 1.46528,
      "grad_norm": 0.012372363358736038,
      "learning_rate": 2.6737600000000003e-06,
      "loss": 0.1998,
      "step": 45790
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.01523652020841837,
      "learning_rate": 2.67216e-06,
      "loss": 0.2135,
      "step": 45800
    },
    {
      "epoch": 1.4659200000000001,
      "grad_norm": 0.03060891292989254,
      "learning_rate": 2.6705600000000005e-06,
      "loss": 0.2067,
      "step": 45810
    },
    {
      "epoch": 1.46624,
      "grad_norm": 0.1319914162158966,
      "learning_rate": 2.6689600000000004e-06,
      "loss": 0.2002,
      "step": 45820
    },
    {
      "epoch": 1.4665599999999999,
      "grad_norm": 0.02972470410168171,
      "learning_rate": 2.66736e-06,
      "loss": 0.1991,
      "step": 45830
    },
    {
      "epoch": 1.46688,
      "grad_norm": 0.03972228989005089,
      "learning_rate": 2.66576e-06,
      "loss": 0.1992,
      "step": 45840
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.024060025811195374,
      "learning_rate": 2.66416e-06,
      "loss": 0.1991,
      "step": 45850
    },
    {
      "epoch": 1.46752,
      "grad_norm": 0.012299288995563984,
      "learning_rate": 2.6625600000000003e-06,
      "loss": 0.1992,
      "step": 45860
    },
    {
      "epoch": 1.46784,
      "grad_norm": 0.049886781722307205,
      "learning_rate": 2.66096e-06,
      "loss": 0.1994,
      "step": 45870
    },
    {
      "epoch": 1.4681600000000001,
      "grad_norm": 0.021698467433452606,
      "learning_rate": 2.65936e-06,
      "loss": 0.2148,
      "step": 45880
    },
    {
      "epoch": 1.46848,
      "grad_norm": 0.01431337557733059,
      "learning_rate": 2.6577600000000004e-06,
      "loss": 0.1996,
      "step": 45890
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.031101275235414505,
      "learning_rate": 2.6561600000000002e-06,
      "loss": 0.2029,
      "step": 45900
    },
    {
      "epoch": 1.46912,
      "grad_norm": 0.45102840662002563,
      "learning_rate": 2.6545600000000006e-06,
      "loss": 0.2052,
      "step": 45910
    },
    {
      "epoch": 1.46944,
      "grad_norm": 0.02257036231458187,
      "learning_rate": 2.65296e-06,
      "loss": 0.215,
      "step": 45920
    },
    {
      "epoch": 1.46976,
      "grad_norm": 0.026380572468042374,
      "learning_rate": 2.65136e-06,
      "loss": 0.1997,
      "step": 45930
    },
    {
      "epoch": 1.47008,
      "grad_norm": 0.029356133192777634,
      "learning_rate": 2.64976e-06,
      "loss": 0.1993,
      "step": 45940
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.0401373989880085,
      "learning_rate": 2.64816e-06,
      "loss": 0.1995,
      "step": 45950
    },
    {
      "epoch": 1.47072,
      "grad_norm": 0.02750173583626747,
      "learning_rate": 2.6465600000000004e-06,
      "loss": 0.1992,
      "step": 45960
    },
    {
      "epoch": 1.47104,
      "grad_norm": 0.04063527286052704,
      "learning_rate": 2.6449600000000003e-06,
      "loss": 0.2194,
      "step": 45970
    },
    {
      "epoch": 1.47136,
      "grad_norm": 0.7679737210273743,
      "learning_rate": 2.6433600000000006e-06,
      "loss": 0.216,
      "step": 45980
    },
    {
      "epoch": 1.47168,
      "grad_norm": 0.020593976601958275,
      "learning_rate": 2.6417600000000004e-06,
      "loss": 0.199,
      "step": 45990
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.026442432776093483,
      "learning_rate": 2.64016e-06,
      "loss": 0.2014,
      "step": 46000
    },
    {
      "epoch": 1.472,
      "eval_runtime": 63.9475,
      "eval_samples_per_second": 156.378,
      "eval_steps_per_second": 9.774,
      "step": 46000
    },
    {
      "epoch": 1.47232,
      "grad_norm": 0.021032873541116714,
      "learning_rate": 2.63856e-06,
      "loss": 0.1993,
      "step": 46010
    },
    {
      "epoch": 1.47264,
      "grad_norm": 0.03598921373486519,
      "learning_rate": 2.63696e-06,
      "loss": 0.1994,
      "step": 46020
    },
    {
      "epoch": 1.47296,
      "grad_norm": 0.013834602199494839,
      "learning_rate": 2.63536e-06,
      "loss": 0.1993,
      "step": 46030
    },
    {
      "epoch": 1.47328,
      "grad_norm": 0.022542785853147507,
      "learning_rate": 2.6337600000000003e-06,
      "loss": 0.199,
      "step": 46040
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.023820579051971436,
      "learning_rate": 2.63216e-06,
      "loss": 0.2173,
      "step": 46050
    },
    {
      "epoch": 1.4739200000000001,
      "grad_norm": 0.9358264803886414,
      "learning_rate": 2.6305600000000004e-06,
      "loss": 0.2134,
      "step": 46060
    },
    {
      "epoch": 1.47424,
      "grad_norm": 0.02695237286388874,
      "learning_rate": 2.6289600000000003e-06,
      "loss": 0.1995,
      "step": 46070
    },
    {
      "epoch": 1.4745599999999999,
      "grad_norm": 0.023611724376678467,
      "learning_rate": 2.6273600000000006e-06,
      "loss": 0.2155,
      "step": 46080
    },
    {
      "epoch": 1.47488,
      "grad_norm": 0.03397040069103241,
      "learning_rate": 2.62576e-06,
      "loss": 0.1989,
      "step": 46090
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.03765835240483284,
      "learning_rate": 2.62416e-06,
      "loss": 0.1996,
      "step": 46100
    },
    {
      "epoch": 1.47552,
      "grad_norm": 0.024558817967772484,
      "learning_rate": 2.6225600000000003e-06,
      "loss": 0.1992,
      "step": 46110
    },
    {
      "epoch": 1.47584,
      "grad_norm": 0.022917501628398895,
      "learning_rate": 2.62096e-06,
      "loss": 0.2132,
      "step": 46120
    },
    {
      "epoch": 1.4761600000000001,
      "grad_norm": 0.027854112908244133,
      "learning_rate": 2.6193600000000004e-06,
      "loss": 0.1991,
      "step": 46130
    },
    {
      "epoch": 1.47648,
      "grad_norm": 0.010011961683630943,
      "learning_rate": 2.6177600000000003e-06,
      "loss": 0.2009,
      "step": 46140
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.02983526885509491,
      "learning_rate": 2.61616e-06,
      "loss": 0.1993,
      "step": 46150
    },
    {
      "epoch": 1.47712,
      "grad_norm": 0.03870199993252754,
      "learning_rate": 2.6145600000000005e-06,
      "loss": 0.2126,
      "step": 46160
    },
    {
      "epoch": 1.47744,
      "grad_norm": 0.06743603199720383,
      "learning_rate": 2.61296e-06,
      "loss": 0.1994,
      "step": 46170
    },
    {
      "epoch": 1.47776,
      "grad_norm": 0.05870363861322403,
      "learning_rate": 2.61136e-06,
      "loss": 0.216,
      "step": 46180
    },
    {
      "epoch": 1.47808,
      "grad_norm": 0.01861553080379963,
      "learning_rate": 2.60976e-06,
      "loss": 0.1992,
      "step": 46190
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.029595719650387764,
      "learning_rate": 2.60816e-06,
      "loss": 0.1994,
      "step": 46200
    },
    {
      "epoch": 1.47872,
      "grad_norm": 0.028318146243691444,
      "learning_rate": 2.6065600000000003e-06,
      "loss": 0.1992,
      "step": 46210
    },
    {
      "epoch": 1.47904,
      "grad_norm": 0.06353047490119934,
      "learning_rate": 2.6049600000000002e-06,
      "loss": 0.1995,
      "step": 46220
    },
    {
      "epoch": 1.47936,
      "grad_norm": 0.07390287518501282,
      "learning_rate": 2.6033600000000005e-06,
      "loss": 0.1993,
      "step": 46230
    },
    {
      "epoch": 1.47968,
      "grad_norm": 0.013978269882500172,
      "learning_rate": 2.6017600000000004e-06,
      "loss": 0.1999,
      "step": 46240
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.029836785048246384,
      "learning_rate": 2.6001600000000003e-06,
      "loss": 0.2068,
      "step": 46250
    },
    {
      "epoch": 1.48032,
      "grad_norm": 0.010480879805982113,
      "learning_rate": 2.59856e-06,
      "loss": 0.2001,
      "step": 46260
    },
    {
      "epoch": 1.48064,
      "grad_norm": 0.3819829225540161,
      "learning_rate": 2.59696e-06,
      "loss": 0.2147,
      "step": 46270
    },
    {
      "epoch": 1.48096,
      "grad_norm": 0.019099151715636253,
      "learning_rate": 2.59536e-06,
      "loss": 0.1991,
      "step": 46280
    },
    {
      "epoch": 1.48128,
      "grad_norm": 0.014207344502210617,
      "learning_rate": 2.5937600000000002e-06,
      "loss": 0.2139,
      "step": 46290
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.008929421193897724,
      "learning_rate": 2.59216e-06,
      "loss": 0.1995,
      "step": 46300
    },
    {
      "epoch": 1.4819200000000001,
      "grad_norm": 0.7571218013763428,
      "learning_rate": 2.5905600000000004e-06,
      "loss": 0.2139,
      "step": 46310
    },
    {
      "epoch": 1.48224,
      "grad_norm": 0.0383421815931797,
      "learning_rate": 2.5889600000000003e-06,
      "loss": 0.1996,
      "step": 46320
    },
    {
      "epoch": 1.4825599999999999,
      "grad_norm": 0.044857628643512726,
      "learning_rate": 2.5873600000000006e-06,
      "loss": 0.1992,
      "step": 46330
    },
    {
      "epoch": 1.48288,
      "grad_norm": 0.07309276610612869,
      "learning_rate": 2.58576e-06,
      "loss": 0.199,
      "step": 46340
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.0581255704164505,
      "learning_rate": 2.58416e-06,
      "loss": 0.2144,
      "step": 46350
    },
    {
      "epoch": 1.48352,
      "grad_norm": 0.02508814074099064,
      "learning_rate": 2.5825600000000002e-06,
      "loss": 0.1998,
      "step": 46360
    },
    {
      "epoch": 1.48384,
      "grad_norm": 0.021299419924616814,
      "learning_rate": 2.58096e-06,
      "loss": 0.1999,
      "step": 46370
    },
    {
      "epoch": 1.48416,
      "grad_norm": 0.013526713475584984,
      "learning_rate": 2.5793600000000004e-06,
      "loss": 0.2011,
      "step": 46380
    },
    {
      "epoch": 1.48448,
      "grad_norm": 0.010790395550429821,
      "learning_rate": 2.5777600000000003e-06,
      "loss": 0.1993,
      "step": 46390
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.024330979213118553,
      "learning_rate": 2.57616e-06,
      "loss": 0.1995,
      "step": 46400
    },
    {
      "epoch": 1.48512,
      "grad_norm": 0.015007389709353447,
      "learning_rate": 2.5745600000000005e-06,
      "loss": 0.2028,
      "step": 46410
    },
    {
      "epoch": 1.48544,
      "grad_norm": 0.8126110434532166,
      "learning_rate": 2.5729600000000003e-06,
      "loss": 0.2042,
      "step": 46420
    },
    {
      "epoch": 1.48576,
      "grad_norm": 0.03323600813746452,
      "learning_rate": 2.57136e-06,
      "loss": 0.1992,
      "step": 46430
    },
    {
      "epoch": 1.48608,
      "grad_norm": 0.06225885450839996,
      "learning_rate": 2.56976e-06,
      "loss": 0.2314,
      "step": 46440
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.025893336161971092,
      "learning_rate": 2.56816e-06,
      "loss": 0.1992,
      "step": 46450
    },
    {
      "epoch": 1.48672,
      "grad_norm": 0.04533236101269722,
      "learning_rate": 2.5665600000000003e-06,
      "loss": 0.1992,
      "step": 46460
    },
    {
      "epoch": 1.48704,
      "grad_norm": 0.035935577005147934,
      "learning_rate": 2.56496e-06,
      "loss": 0.2117,
      "step": 46470
    },
    {
      "epoch": 1.48736,
      "grad_norm": 0.02886774204671383,
      "learning_rate": 2.5633600000000005e-06,
      "loss": 0.2108,
      "step": 46480
    },
    {
      "epoch": 1.4876800000000001,
      "grad_norm": 0.021896904334425926,
      "learning_rate": 2.5617600000000004e-06,
      "loss": 0.1991,
      "step": 46490
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.01296523492783308,
      "learning_rate": 2.5601600000000002e-06,
      "loss": 0.211,
      "step": 46500
    },
    {
      "epoch": 1.4883199999999999,
      "grad_norm": 0.01951911672949791,
      "learning_rate": 2.55856e-06,
      "loss": 0.1993,
      "step": 46510
    },
    {
      "epoch": 1.48864,
      "grad_norm": 0.012882104143500328,
      "learning_rate": 2.55696e-06,
      "loss": 0.1994,
      "step": 46520
    },
    {
      "epoch": 1.48896,
      "grad_norm": 0.8035735487937927,
      "learning_rate": 2.55536e-06,
      "loss": 0.2266,
      "step": 46530
    },
    {
      "epoch": 1.48928,
      "grad_norm": 0.016136737540364265,
      "learning_rate": 2.55376e-06,
      "loss": 0.2038,
      "step": 46540
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.019784249365329742,
      "learning_rate": 2.55216e-06,
      "loss": 0.1995,
      "step": 46550
    },
    {
      "epoch": 1.4899200000000001,
      "grad_norm": 0.3177337348461151,
      "learning_rate": 2.5505600000000004e-06,
      "loss": 0.2,
      "step": 46560
    },
    {
      "epoch": 1.49024,
      "grad_norm": 0.07043664157390594,
      "learning_rate": 2.5489600000000002e-06,
      "loss": 0.1996,
      "step": 46570
    },
    {
      "epoch": 1.4905599999999999,
      "grad_norm": 0.047764770686626434,
      "learning_rate": 2.5473600000000005e-06,
      "loss": 0.1994,
      "step": 46580
    },
    {
      "epoch": 1.49088,
      "grad_norm": 0.014444392174482346,
      "learning_rate": 2.5457600000000004e-06,
      "loss": 0.1994,
      "step": 46590
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.034905217587947845,
      "learning_rate": 2.54416e-06,
      "loss": 0.199,
      "step": 46600
    },
    {
      "epoch": 1.49152,
      "grad_norm": 0.01664602942764759,
      "learning_rate": 2.54256e-06,
      "loss": 0.2057,
      "step": 46610
    },
    {
      "epoch": 1.49184,
      "grad_norm": 0.019726252183318138,
      "learning_rate": 2.54096e-06,
      "loss": 0.1993,
      "step": 46620
    },
    {
      "epoch": 1.49216,
      "grad_norm": 0.021818561479449272,
      "learning_rate": 2.5393600000000004e-06,
      "loss": 0.2003,
      "step": 46630
    },
    {
      "epoch": 1.49248,
      "grad_norm": 1.2192648649215698,
      "learning_rate": 2.5377600000000002e-06,
      "loss": 0.2098,
      "step": 46640
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.036319393664598465,
      "learning_rate": 2.53616e-06,
      "loss": 0.1995,
      "step": 46650
    },
    {
      "epoch": 1.49312,
      "grad_norm": 0.03164812549948692,
      "learning_rate": 2.5345600000000004e-06,
      "loss": 0.2118,
      "step": 46660
    },
    {
      "epoch": 1.49344,
      "grad_norm": 0.019363010302186012,
      "learning_rate": 2.5329600000000003e-06,
      "loss": 0.199,
      "step": 46670
    },
    {
      "epoch": 1.49376,
      "grad_norm": 0.015125627629458904,
      "learning_rate": 2.5313600000000006e-06,
      "loss": 0.2027,
      "step": 46680
    },
    {
      "epoch": 1.49408,
      "grad_norm": 0.010470665991306305,
      "learning_rate": 2.52976e-06,
      "loss": 0.1999,
      "step": 46690
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.03353583440184593,
      "learning_rate": 2.52816e-06,
      "loss": 0.1993,
      "step": 46700
    },
    {
      "epoch": 1.49472,
      "grad_norm": 0.03931982070207596,
      "learning_rate": 2.5265600000000003e-06,
      "loss": 0.201,
      "step": 46710
    },
    {
      "epoch": 1.49504,
      "grad_norm": 0.012188547290861607,
      "learning_rate": 2.52496e-06,
      "loss": 0.1998,
      "step": 46720
    },
    {
      "epoch": 1.49536,
      "grad_norm": 0.13069768249988556,
      "learning_rate": 2.5233600000000004e-06,
      "loss": 0.2086,
      "step": 46730
    },
    {
      "epoch": 1.4956800000000001,
      "grad_norm": 0.011901034973561764,
      "learning_rate": 2.5217600000000003e-06,
      "loss": 0.2147,
      "step": 46740
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.0361570343375206,
      "learning_rate": 2.52016e-06,
      "loss": 0.2004,
      "step": 46750
    },
    {
      "epoch": 1.4963199999999999,
      "grad_norm": 0.037224650382995605,
      "learning_rate": 2.5185600000000005e-06,
      "loss": 0.1997,
      "step": 46760
    },
    {
      "epoch": 1.49664,
      "grad_norm": 0.03129028528928757,
      "learning_rate": 2.51696e-06,
      "loss": 0.2024,
      "step": 46770
    },
    {
      "epoch": 1.49696,
      "grad_norm": 0.013605411164462566,
      "learning_rate": 2.51536e-06,
      "loss": 0.1993,
      "step": 46780
    },
    {
      "epoch": 1.49728,
      "grad_norm": 0.04308401793241501,
      "learning_rate": 2.51376e-06,
      "loss": 0.2052,
      "step": 46790
    },
    {
      "epoch": 1.4976,
      "grad_norm": 1.9178436994552612,
      "learning_rate": 2.51216e-06,
      "loss": 0.2121,
      "step": 46800
    },
    {
      "epoch": 1.49792,
      "grad_norm": 0.027904849499464035,
      "learning_rate": 2.5105600000000003e-06,
      "loss": 0.2007,
      "step": 46810
    },
    {
      "epoch": 1.49824,
      "grad_norm": 0.035111576318740845,
      "learning_rate": 2.50896e-06,
      "loss": 0.1992,
      "step": 46820
    },
    {
      "epoch": 1.49856,
      "grad_norm": 0.01989024318754673,
      "learning_rate": 2.5073600000000005e-06,
      "loss": 0.1993,
      "step": 46830
    },
    {
      "epoch": 1.49888,
      "grad_norm": 0.016105586662888527,
      "learning_rate": 2.5057600000000004e-06,
      "loss": 0.1992,
      "step": 46840
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.021119287237524986,
      "learning_rate": 2.5041600000000007e-06,
      "loss": 0.1992,
      "step": 46850
    },
    {
      "epoch": 1.49952,
      "grad_norm": 0.014657043851912022,
      "learning_rate": 2.50256e-06,
      "loss": 0.1993,
      "step": 46860
    },
    {
      "epoch": 1.49984,
      "grad_norm": 0.013128098100423813,
      "learning_rate": 2.50096e-06,
      "loss": 0.1994,
      "step": 46870
    },
    {
      "epoch": 1.5001600000000002,
      "grad_norm": 0.024176545441150665,
      "learning_rate": 2.4993600000000003e-06,
      "loss": 0.2234,
      "step": 46880
    },
    {
      "epoch": 1.50048,
      "grad_norm": 0.010251087136566639,
      "learning_rate": 2.49776e-06,
      "loss": 0.1992,
      "step": 46890
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.016418440267443657,
      "learning_rate": 2.49616e-06,
      "loss": 0.2054,
      "step": 46900
    },
    {
      "epoch": 1.50112,
      "grad_norm": 0.12252648919820786,
      "learning_rate": 2.4945600000000004e-06,
      "loss": 0.1996,
      "step": 46910
    },
    {
      "epoch": 1.50144,
      "grad_norm": 0.036458998918533325,
      "learning_rate": 2.4929600000000003e-06,
      "loss": 0.1992,
      "step": 46920
    },
    {
      "epoch": 1.50176,
      "grad_norm": 0.3828674554824829,
      "learning_rate": 2.49136e-06,
      "loss": 0.1998,
      "step": 46930
    },
    {
      "epoch": 1.5020799999999999,
      "grad_norm": 0.039182454347610474,
      "learning_rate": 2.48976e-06,
      "loss": 0.1994,
      "step": 46940
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.04141876474022865,
      "learning_rate": 2.4881600000000003e-06,
      "loss": 0.1991,
      "step": 46950
    },
    {
      "epoch": 1.50272,
      "grad_norm": 0.014059691689908504,
      "learning_rate": 2.48656e-06,
      "loss": 0.2081,
      "step": 46960
    },
    {
      "epoch": 1.50304,
      "grad_norm": 0.03552451729774475,
      "learning_rate": 2.48496e-06,
      "loss": 0.1994,
      "step": 46970
    },
    {
      "epoch": 1.50336,
      "grad_norm": 0.026703741401433945,
      "learning_rate": 2.4833600000000004e-06,
      "loss": 0.1996,
      "step": 46980
    },
    {
      "epoch": 1.5036800000000001,
      "grad_norm": 0.02883557416498661,
      "learning_rate": 2.4817600000000003e-06,
      "loss": 0.1991,
      "step": 46990
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.02392585203051567,
      "learning_rate": 2.48016e-06,
      "loss": 0.2164,
      "step": 47000
    },
    {
      "epoch": 1.504,
      "eval_runtime": 61.2232,
      "eval_samples_per_second": 163.337,
      "eval_steps_per_second": 10.209,
      "step": 47000
    },
    {
      "epoch": 1.5043199999999999,
      "grad_norm": 0.02532310038805008,
      "learning_rate": 2.47856e-06,
      "loss": 0.1993,
      "step": 47010
    },
    {
      "epoch": 1.50464,
      "grad_norm": 0.016545481979846954,
      "learning_rate": 2.4769600000000003e-06,
      "loss": 0.1992,
      "step": 47020
    },
    {
      "epoch": 1.50496,
      "grad_norm": 0.01726902835071087,
      "learning_rate": 2.4753600000000002e-06,
      "loss": 0.1993,
      "step": 47030
    },
    {
      "epoch": 1.50528,
      "grad_norm": 0.009282486513257027,
      "learning_rate": 2.4737600000000005e-06,
      "loss": 0.199,
      "step": 47040
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.021728593856096268,
      "learning_rate": 2.47216e-06,
      "loss": 0.1994,
      "step": 47050
    },
    {
      "epoch": 1.5059200000000001,
      "grad_norm": 0.02522636391222477,
      "learning_rate": 2.4705600000000003e-06,
      "loss": 0.2154,
      "step": 47060
    },
    {
      "epoch": 1.50624,
      "grad_norm": 6.418166160583496,
      "learning_rate": 2.46896e-06,
      "loss": 0.2049,
      "step": 47070
    },
    {
      "epoch": 1.50656,
      "grad_norm": 0.024611977860331535,
      "learning_rate": 2.4673600000000005e-06,
      "loss": 0.1993,
      "step": 47080
    },
    {
      "epoch": 1.50688,
      "grad_norm": 0.010765847750008106,
      "learning_rate": 2.46576e-06,
      "loss": 0.199,
      "step": 47090
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.026049746200442314,
      "learning_rate": 2.4641600000000002e-06,
      "loss": 0.2124,
      "step": 47100
    },
    {
      "epoch": 1.50752,
      "grad_norm": 0.01737743429839611,
      "learning_rate": 2.46256e-06,
      "loss": 0.1992,
      "step": 47110
    },
    {
      "epoch": 1.5078399999999998,
      "grad_norm": 0.019817465916275978,
      "learning_rate": 2.4609600000000004e-06,
      "loss": 0.1991,
      "step": 47120
    },
    {
      "epoch": 1.50816,
      "grad_norm": 0.015926169231534004,
      "learning_rate": 2.4593600000000003e-06,
      "loss": 0.1992,
      "step": 47130
    },
    {
      "epoch": 1.50848,
      "grad_norm": 0.014608306810259819,
      "learning_rate": 2.45776e-06,
      "loss": 0.1992,
      "step": 47140
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.03674960136413574,
      "learning_rate": 2.45616e-06,
      "loss": 0.2106,
      "step": 47150
    },
    {
      "epoch": 1.50912,
      "grad_norm": 0.024225031957030296,
      "learning_rate": 2.4545600000000003e-06,
      "loss": 0.2105,
      "step": 47160
    },
    {
      "epoch": 1.5094400000000001,
      "grad_norm": 0.03456902131438255,
      "learning_rate": 2.4529600000000002e-06,
      "loss": 0.1994,
      "step": 47170
    },
    {
      "epoch": 1.50976,
      "grad_norm": 0.07409589737653732,
      "learning_rate": 2.45136e-06,
      "loss": 0.2,
      "step": 47180
    },
    {
      "epoch": 1.5100799999999999,
      "grad_norm": 0.11008940637111664,
      "learning_rate": 2.44976e-06,
      "loss": 0.1992,
      "step": 47190
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.010602438822388649,
      "learning_rate": 2.4481600000000003e-06,
      "loss": 0.1992,
      "step": 47200
    },
    {
      "epoch": 1.51072,
      "grad_norm": 0.01463447604328394,
      "learning_rate": 2.44656e-06,
      "loss": 0.2004,
      "step": 47210
    },
    {
      "epoch": 1.51104,
      "grad_norm": 0.032732732594013214,
      "learning_rate": 2.44496e-06,
      "loss": 0.1994,
      "step": 47220
    },
    {
      "epoch": 1.51136,
      "grad_norm": 0.021949876099824905,
      "learning_rate": 2.4433600000000004e-06,
      "loss": 0.21,
      "step": 47230
    },
    {
      "epoch": 1.5116800000000001,
      "grad_norm": 0.036898113787174225,
      "learning_rate": 2.4417600000000002e-06,
      "loss": 0.1991,
      "step": 47240
    },
    {
      "epoch": 1.512,
      "grad_norm": 2.692861557006836,
      "learning_rate": 2.44016e-06,
      "loss": 0.2158,
      "step": 47250
    },
    {
      "epoch": 1.5123199999999999,
      "grad_norm": 0.018690332770347595,
      "learning_rate": 2.43856e-06,
      "loss": 0.1992,
      "step": 47260
    },
    {
      "epoch": 1.51264,
      "grad_norm": 0.018524516373872757,
      "learning_rate": 2.4369600000000003e-06,
      "loss": 0.1992,
      "step": 47270
    },
    {
      "epoch": 1.51296,
      "grad_norm": 0.02357546053826809,
      "learning_rate": 2.43536e-06,
      "loss": 0.1991,
      "step": 47280
    },
    {
      "epoch": 1.51328,
      "grad_norm": 0.01678014174103737,
      "learning_rate": 2.4337600000000005e-06,
      "loss": 0.1991,
      "step": 47290
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.012567496858537197,
      "learning_rate": 2.43216e-06,
      "loss": 0.1995,
      "step": 47300
    },
    {
      "epoch": 1.5139200000000002,
      "grad_norm": 0.013553314842283726,
      "learning_rate": 2.4305600000000002e-06,
      "loss": 0.1992,
      "step": 47310
    },
    {
      "epoch": 1.51424,
      "grad_norm": 0.010766343213617802,
      "learning_rate": 2.42896e-06,
      "loss": 0.1995,
      "step": 47320
    },
    {
      "epoch": 1.51456,
      "grad_norm": 0.015897884964942932,
      "learning_rate": 2.4273600000000004e-06,
      "loss": 0.2166,
      "step": 47330
    },
    {
      "epoch": 1.51488,
      "grad_norm": 0.03792079538106918,
      "learning_rate": 2.4257600000000003e-06,
      "loss": 0.1993,
      "step": 47340
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.03239161893725395,
      "learning_rate": 2.42416e-06,
      "loss": 0.2044,
      "step": 47350
    },
    {
      "epoch": 1.51552,
      "grad_norm": 0.03445141017436981,
      "learning_rate": 2.42256e-06,
      "loss": 0.2049,
      "step": 47360
    },
    {
      "epoch": 1.5158399999999999,
      "grad_norm": 0.09611602127552032,
      "learning_rate": 2.4209600000000004e-06,
      "loss": 0.1993,
      "step": 47370
    },
    {
      "epoch": 1.51616,
      "grad_norm": 0.012731718830764294,
      "learning_rate": 2.4193600000000002e-06,
      "loss": 0.1993,
      "step": 47380
    },
    {
      "epoch": 1.51648,
      "grad_norm": 0.013869657181203365,
      "learning_rate": 2.41776e-06,
      "loss": 0.1994,
      "step": 47390
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.04274234548211098,
      "learning_rate": 2.41616e-06,
      "loss": 0.1991,
      "step": 47400
    },
    {
      "epoch": 1.51712,
      "grad_norm": 0.024346057325601578,
      "learning_rate": 2.4145600000000003e-06,
      "loss": 0.2121,
      "step": 47410
    },
    {
      "epoch": 1.5174400000000001,
      "grad_norm": 0.056599609553813934,
      "learning_rate": 2.41296e-06,
      "loss": 0.1993,
      "step": 47420
    },
    {
      "epoch": 1.51776,
      "grad_norm": 0.03952747583389282,
      "learning_rate": 2.41136e-06,
      "loss": 0.1994,
      "step": 47430
    },
    {
      "epoch": 1.5180799999999999,
      "grad_norm": 0.026178250089287758,
      "learning_rate": 2.40976e-06,
      "loss": 0.2153,
      "step": 47440
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.12926743924617767,
      "learning_rate": 2.4081600000000003e-06,
      "loss": 0.2061,
      "step": 47450
    },
    {
      "epoch": 1.51872,
      "grad_norm": 0.024310968816280365,
      "learning_rate": 2.40656e-06,
      "loss": 0.1993,
      "step": 47460
    },
    {
      "epoch": 1.51904,
      "grad_norm": 0.013678031973540783,
      "learning_rate": 2.40496e-06,
      "loss": 0.1991,
      "step": 47470
    },
    {
      "epoch": 1.51936,
      "grad_norm": 0.02359956130385399,
      "learning_rate": 2.4033600000000003e-06,
      "loss": 0.1994,
      "step": 47480
    },
    {
      "epoch": 1.5196800000000001,
      "grad_norm": 0.042711030691862106,
      "learning_rate": 2.40176e-06,
      "loss": 0.1993,
      "step": 47490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.017576655372977257,
      "learning_rate": 2.40016e-06,
      "loss": 0.199,
      "step": 47500
    },
    {
      "epoch": 1.52032,
      "grad_norm": 0.025228707119822502,
      "learning_rate": 2.3985600000000004e-06,
      "loss": 0.2049,
      "step": 47510
    },
    {
      "epoch": 1.52064,
      "grad_norm": 0.03940099850296974,
      "learning_rate": 2.3969600000000003e-06,
      "loss": 0.1993,
      "step": 47520
    },
    {
      "epoch": 1.52096,
      "grad_norm": 0.021741224452853203,
      "learning_rate": 2.39536e-06,
      "loss": 0.1994,
      "step": 47530
    },
    {
      "epoch": 1.52128,
      "grad_norm": 0.015346298925578594,
      "learning_rate": 2.3937600000000004e-06,
      "loss": 0.1992,
      "step": 47540
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.021186525002121925,
      "learning_rate": 2.3921600000000003e-06,
      "loss": 0.2035,
      "step": 47550
    },
    {
      "epoch": 1.5219200000000002,
      "grad_norm": 0.19985833764076233,
      "learning_rate": 2.39056e-06,
      "loss": 0.1996,
      "step": 47560
    },
    {
      "epoch": 1.52224,
      "grad_norm": 0.1088268905878067,
      "learning_rate": 2.38896e-06,
      "loss": 0.2097,
      "step": 47570
    },
    {
      "epoch": 1.52256,
      "grad_norm": 0.02115982212126255,
      "learning_rate": 2.3873600000000004e-06,
      "loss": 0.199,
      "step": 47580
    },
    {
      "epoch": 1.52288,
      "grad_norm": 0.0387800894677639,
      "learning_rate": 2.3857600000000003e-06,
      "loss": 0.1995,
      "step": 47590
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.01447889395058155,
      "learning_rate": 2.38416e-06,
      "loss": 0.1994,
      "step": 47600
    },
    {
      "epoch": 1.52352,
      "grad_norm": 0.02058124914765358,
      "learning_rate": 2.38256e-06,
      "loss": 0.1992,
      "step": 47610
    },
    {
      "epoch": 1.5238399999999999,
      "grad_norm": 0.044649168848991394,
      "learning_rate": 2.3809600000000003e-06,
      "loss": 0.1992,
      "step": 47620
    },
    {
      "epoch": 1.52416,
      "grad_norm": 0.015711316838860512,
      "learning_rate": 2.37936e-06,
      "loss": 0.1992,
      "step": 47630
    },
    {
      "epoch": 1.52448,
      "grad_norm": 0.011880217120051384,
      "learning_rate": 2.37776e-06,
      "loss": 0.2002,
      "step": 47640
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.020714709535241127,
      "learning_rate": 2.37616e-06,
      "loss": 0.208,
      "step": 47650
    },
    {
      "epoch": 1.52512,
      "grad_norm": 0.0110417315736413,
      "learning_rate": 2.3745600000000003e-06,
      "loss": 0.1991,
      "step": 47660
    },
    {
      "epoch": 1.5254400000000001,
      "grad_norm": 0.017958933487534523,
      "learning_rate": 2.37296e-06,
      "loss": 0.2209,
      "step": 47670
    },
    {
      "epoch": 1.52576,
      "grad_norm": 0.026163624599575996,
      "learning_rate": 2.3713600000000005e-06,
      "loss": 0.1991,
      "step": 47680
    },
    {
      "epoch": 1.5260799999999999,
      "grad_norm": 0.012131253257393837,
      "learning_rate": 2.36976e-06,
      "loss": 0.2016,
      "step": 47690
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.028735758736729622,
      "learning_rate": 2.36816e-06,
      "loss": 0.1991,
      "step": 47700
    },
    {
      "epoch": 1.52672,
      "grad_norm": 0.6288895606994629,
      "learning_rate": 2.36656e-06,
      "loss": 0.2387,
      "step": 47710
    },
    {
      "epoch": 1.52704,
      "grad_norm": 0.013960609212517738,
      "learning_rate": 2.3649600000000004e-06,
      "loss": 0.1992,
      "step": 47720
    },
    {
      "epoch": 1.52736,
      "grad_norm": 0.018357204273343086,
      "learning_rate": 2.3633600000000003e-06,
      "loss": 0.2023,
      "step": 47730
    },
    {
      "epoch": 1.5276800000000001,
      "grad_norm": 0.03925349935889244,
      "learning_rate": 2.36176e-06,
      "loss": 0.1997,
      "step": 47740
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.06235848739743233,
      "learning_rate": 2.36016e-06,
      "loss": 0.1996,
      "step": 47750
    },
    {
      "epoch": 1.52832,
      "grad_norm": 0.023519907146692276,
      "learning_rate": 2.3585600000000003e-06,
      "loss": 0.1993,
      "step": 47760
    },
    {
      "epoch": 1.52864,
      "grad_norm": 0.01724904403090477,
      "learning_rate": 2.3569600000000002e-06,
      "loss": 0.2155,
      "step": 47770
    },
    {
      "epoch": 1.52896,
      "grad_norm": 0.09076105058193207,
      "learning_rate": 2.35536e-06,
      "loss": 0.1992,
      "step": 47780
    },
    {
      "epoch": 1.52928,
      "grad_norm": 0.035377003252506256,
      "learning_rate": 2.3537600000000004e-06,
      "loss": 0.2046,
      "step": 47790
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.016874907538294792,
      "learning_rate": 2.3521600000000003e-06,
      "loss": 0.1992,
      "step": 47800
    },
    {
      "epoch": 1.52992,
      "grad_norm": 0.03281092271208763,
      "learning_rate": 2.35056e-06,
      "loss": 0.1997,
      "step": 47810
    },
    {
      "epoch": 1.53024,
      "grad_norm": 0.020561406388878822,
      "learning_rate": 2.34896e-06,
      "loss": 0.2005,
      "step": 47820
    },
    {
      "epoch": 1.53056,
      "grad_norm": 0.10783372819423676,
      "learning_rate": 2.3473600000000003e-06,
      "loss": 0.1995,
      "step": 47830
    },
    {
      "epoch": 1.53088,
      "grad_norm": 0.03558504581451416,
      "learning_rate": 2.3457600000000002e-06,
      "loss": 0.209,
      "step": 47840
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.029022565111517906,
      "learning_rate": 2.3441600000000005e-06,
      "loss": 0.2031,
      "step": 47850
    },
    {
      "epoch": 1.53152,
      "grad_norm": 0.018192503601312637,
      "learning_rate": 2.34256e-06,
      "loss": 0.2178,
      "step": 47860
    },
    {
      "epoch": 1.5318399999999999,
      "grad_norm": 0.02871612459421158,
      "learning_rate": 2.3409600000000003e-06,
      "loss": 0.1991,
      "step": 47870
    },
    {
      "epoch": 1.53216,
      "grad_norm": 0.023657286539673805,
      "learning_rate": 2.33936e-06,
      "loss": 0.1993,
      "step": 47880
    },
    {
      "epoch": 1.53248,
      "grad_norm": 0.017468497157096863,
      "learning_rate": 2.3377600000000005e-06,
      "loss": 0.2142,
      "step": 47890
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.023892175406217575,
      "learning_rate": 2.33616e-06,
      "loss": 0.1992,
      "step": 47900
    },
    {
      "epoch": 1.53312,
      "grad_norm": 0.023004233837127686,
      "learning_rate": 2.3345600000000002e-06,
      "loss": 0.217,
      "step": 47910
    },
    {
      "epoch": 1.5334400000000001,
      "grad_norm": 0.03139297664165497,
      "learning_rate": 2.33296e-06,
      "loss": 0.2252,
      "step": 47920
    },
    {
      "epoch": 1.53376,
      "grad_norm": 0.03457172214984894,
      "learning_rate": 2.3313600000000004e-06,
      "loss": 0.2118,
      "step": 47930
    },
    {
      "epoch": 1.5340799999999999,
      "grad_norm": 0.011119638569653034,
      "learning_rate": 2.32976e-06,
      "loss": 0.1999,
      "step": 47940
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.3085556626319885,
      "learning_rate": 2.32816e-06,
      "loss": 0.1997,
      "step": 47950
    },
    {
      "epoch": 1.53472,
      "grad_norm": 0.45214417576789856,
      "learning_rate": 2.32656e-06,
      "loss": 0.1997,
      "step": 47960
    },
    {
      "epoch": 1.53504,
      "grad_norm": 0.029244640842080116,
      "learning_rate": 2.3249600000000004e-06,
      "loss": 0.1993,
      "step": 47970
    },
    {
      "epoch": 1.5353599999999998,
      "grad_norm": 0.030734317377209663,
      "learning_rate": 2.3233600000000002e-06,
      "loss": 0.2042,
      "step": 47980
    },
    {
      "epoch": 1.5356800000000002,
      "grad_norm": 0.010621634311974049,
      "learning_rate": 2.32176e-06,
      "loss": 0.1992,
      "step": 47990
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.025051414966583252,
      "learning_rate": 2.32016e-06,
      "loss": 0.1992,
      "step": 48000
    },
    {
      "epoch": 1.536,
      "eval_runtime": 57.5554,
      "eval_samples_per_second": 173.746,
      "eval_steps_per_second": 10.859,
      "step": 48000
    },
    {
      "epoch": 1.53632,
      "grad_norm": 0.01912461221218109,
      "learning_rate": 2.3185600000000003e-06,
      "loss": 0.2007,
      "step": 48010
    },
    {
      "epoch": 1.53664,
      "grad_norm": 0.037141103297472,
      "learning_rate": 2.31696e-06,
      "loss": 0.2149,
      "step": 48020
    },
    {
      "epoch": 1.53696,
      "grad_norm": 0.0321638397872448,
      "learning_rate": 2.31536e-06,
      "loss": 0.199,
      "step": 48030
    },
    {
      "epoch": 1.53728,
      "grad_norm": 0.047052402049303055,
      "learning_rate": 2.3137600000000004e-06,
      "loss": 0.1993,
      "step": 48040
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.01724572293460369,
      "learning_rate": 2.3121600000000002e-06,
      "loss": 0.2005,
      "step": 48050
    },
    {
      "epoch": 1.53792,
      "grad_norm": 0.019893741235136986,
      "learning_rate": 2.31056e-06,
      "loss": 0.2087,
      "step": 48060
    },
    {
      "epoch": 1.53824,
      "grad_norm": 0.020683497190475464,
      "learning_rate": 2.30896e-06,
      "loss": 0.199,
      "step": 48070
    },
    {
      "epoch": 1.53856,
      "grad_norm": 0.021854154765605927,
      "learning_rate": 2.3073600000000003e-06,
      "loss": 0.2011,
      "step": 48080
    },
    {
      "epoch": 1.53888,
      "grad_norm": 0.02621784806251526,
      "learning_rate": 2.30576e-06,
      "loss": 0.1992,
      "step": 48090
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.02662891149520874,
      "learning_rate": 2.3041600000000005e-06,
      "loss": 0.2001,
      "step": 48100
    },
    {
      "epoch": 1.53952,
      "grad_norm": 0.034501560032367706,
      "learning_rate": 2.30256e-06,
      "loss": 0.199,
      "step": 48110
    },
    {
      "epoch": 1.5398399999999999,
      "grad_norm": 0.02457411214709282,
      "learning_rate": 2.3009600000000002e-06,
      "loss": 0.2097,
      "step": 48120
    },
    {
      "epoch": 1.54016,
      "grad_norm": 0.015842918306589127,
      "learning_rate": 2.29936e-06,
      "loss": 0.2137,
      "step": 48130
    },
    {
      "epoch": 1.54048,
      "grad_norm": 0.014030441641807556,
      "learning_rate": 2.2977600000000004e-06,
      "loss": 0.1992,
      "step": 48140
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.01176112238317728,
      "learning_rate": 2.29616e-06,
      "loss": 0.1994,
      "step": 48150
    },
    {
      "epoch": 1.54112,
      "grad_norm": 0.019861027598381042,
      "learning_rate": 2.29456e-06,
      "loss": 0.1992,
      "step": 48160
    },
    {
      "epoch": 1.5414400000000001,
      "grad_norm": 0.03217684105038643,
      "learning_rate": 2.29296e-06,
      "loss": 0.1997,
      "step": 48170
    },
    {
      "epoch": 1.54176,
      "grad_norm": 0.04263558238744736,
      "learning_rate": 2.2913600000000004e-06,
      "loss": 0.2078,
      "step": 48180
    },
    {
      "epoch": 1.54208,
      "grad_norm": 0.0138765349984169,
      "learning_rate": 2.2897600000000002e-06,
      "loss": 0.1993,
      "step": 48190
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.03232759237289429,
      "learning_rate": 2.28816e-06,
      "loss": 0.2008,
      "step": 48200
    },
    {
      "epoch": 1.54272,
      "grad_norm": 0.1669561266899109,
      "learning_rate": 2.28656e-06,
      "loss": 0.2038,
      "step": 48210
    },
    {
      "epoch": 1.54304,
      "grad_norm": 0.01918111927807331,
      "learning_rate": 2.2849600000000003e-06,
      "loss": 0.2101,
      "step": 48220
    },
    {
      "epoch": 1.5433599999999998,
      "grad_norm": 0.049683090299367905,
      "learning_rate": 2.28336e-06,
      "loss": 0.2,
      "step": 48230
    },
    {
      "epoch": 1.5436800000000002,
      "grad_norm": 0.0332871675491333,
      "learning_rate": 2.28176e-06,
      "loss": 0.2008,
      "step": 48240
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.014820955693721771,
      "learning_rate": 2.28016e-06,
      "loss": 0.2142,
      "step": 48250
    },
    {
      "epoch": 1.54432,
      "grad_norm": 0.04318157583475113,
      "learning_rate": 2.2785600000000003e-06,
      "loss": 0.2134,
      "step": 48260
    },
    {
      "epoch": 1.54464,
      "grad_norm": 0.017985140904784203,
      "learning_rate": 2.27696e-06,
      "loss": 0.1992,
      "step": 48270
    },
    {
      "epoch": 1.54496,
      "grad_norm": 0.026891620829701424,
      "learning_rate": 2.27536e-06,
      "loss": 0.2128,
      "step": 48280
    },
    {
      "epoch": 1.54528,
      "grad_norm": 0.03792659193277359,
      "learning_rate": 2.2737600000000003e-06,
      "loss": 0.1993,
      "step": 48290
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.011755062267184258,
      "learning_rate": 2.27216e-06,
      "loss": 0.1994,
      "step": 48300
    },
    {
      "epoch": 1.54592,
      "grad_norm": 0.018912414088845253,
      "learning_rate": 2.27056e-06,
      "loss": 0.199,
      "step": 48310
    },
    {
      "epoch": 1.54624,
      "grad_norm": 0.02945701591670513,
      "learning_rate": 2.26896e-06,
      "loss": 0.1993,
      "step": 48320
    },
    {
      "epoch": 1.54656,
      "grad_norm": 0.024091418832540512,
      "learning_rate": 2.2673600000000003e-06,
      "loss": 0.1992,
      "step": 48330
    },
    {
      "epoch": 1.54688,
      "grad_norm": 0.03402772918343544,
      "learning_rate": 2.26576e-06,
      "loss": 0.2014,
      "step": 48340
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.1160459965467453,
      "learning_rate": 2.2641600000000004e-06,
      "loss": 0.2014,
      "step": 48350
    },
    {
      "epoch": 1.54752,
      "grad_norm": 0.026432139798998833,
      "learning_rate": 2.2625600000000003e-06,
      "loss": 0.2262,
      "step": 48360
    },
    {
      "epoch": 1.5478399999999999,
      "grad_norm": 0.03046313114464283,
      "learning_rate": 2.26096e-06,
      "loss": 0.2095,
      "step": 48370
    },
    {
      "epoch": 1.54816,
      "grad_norm": 0.012379243038594723,
      "learning_rate": 2.25936e-06,
      "loss": 0.1998,
      "step": 48380
    },
    {
      "epoch": 1.54848,
      "grad_norm": 0.031207766383886337,
      "learning_rate": 2.2577600000000004e-06,
      "loss": 0.1994,
      "step": 48390
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.6562554240226746,
      "learning_rate": 2.2561600000000003e-06,
      "loss": 0.2002,
      "step": 48400
    },
    {
      "epoch": 1.54912,
      "grad_norm": 0.03208453580737114,
      "learning_rate": 2.25456e-06,
      "loss": 0.199,
      "step": 48410
    },
    {
      "epoch": 1.5494400000000002,
      "grad_norm": 0.02937942184507847,
      "learning_rate": 2.25296e-06,
      "loss": 0.2031,
      "step": 48420
    },
    {
      "epoch": 1.54976,
      "grad_norm": 0.01477142609655857,
      "learning_rate": 2.2513600000000003e-06,
      "loss": 0.199,
      "step": 48430
    },
    {
      "epoch": 1.55008,
      "grad_norm": 0.01138279214501381,
      "learning_rate": 2.24976e-06,
      "loss": 0.199,
      "step": 48440
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.007944080978631973,
      "learning_rate": 2.24816e-06,
      "loss": 0.1991,
      "step": 48450
    },
    {
      "epoch": 1.55072,
      "grad_norm": 0.030501358211040497,
      "learning_rate": 2.24656e-06,
      "loss": 0.1995,
      "step": 48460
    },
    {
      "epoch": 1.55104,
      "grad_norm": 0.03259836137294769,
      "learning_rate": 2.2449600000000003e-06,
      "loss": 0.1993,
      "step": 48470
    },
    {
      "epoch": 1.5513599999999999,
      "grad_norm": 0.02468651719391346,
      "learning_rate": 2.24336e-06,
      "loss": 0.1995,
      "step": 48480
    },
    {
      "epoch": 1.55168,
      "grad_norm": 0.015526067465543747,
      "learning_rate": 2.24176e-06,
      "loss": 0.1996,
      "step": 48490
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.02386849746108055,
      "learning_rate": 2.24016e-06,
      "loss": 0.199,
      "step": 48500
    },
    {
      "epoch": 1.55232,
      "grad_norm": 0.21540674567222595,
      "learning_rate": 2.2385600000000002e-06,
      "loss": 0.1997,
      "step": 48510
    },
    {
      "epoch": 1.55264,
      "grad_norm": 0.036736611276865005,
      "learning_rate": 2.23696e-06,
      "loss": 0.1992,
      "step": 48520
    },
    {
      "epoch": 1.5529600000000001,
      "grad_norm": 0.01669348031282425,
      "learning_rate": 2.2353600000000004e-06,
      "loss": 0.1993,
      "step": 48530
    },
    {
      "epoch": 1.55328,
      "grad_norm": 0.02761128358542919,
      "learning_rate": 2.2337600000000003e-06,
      "loss": 0.1994,
      "step": 48540
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.01614665612578392,
      "learning_rate": 2.23216e-06,
      "loss": 0.2077,
      "step": 48550
    },
    {
      "epoch": 1.55392,
      "grad_norm": 0.04267250746488571,
      "learning_rate": 2.23056e-06,
      "loss": 0.2166,
      "step": 48560
    },
    {
      "epoch": 1.55424,
      "grad_norm": 0.04938560351729393,
      "learning_rate": 2.2289600000000003e-06,
      "loss": 0.1993,
      "step": 48570
    },
    {
      "epoch": 1.55456,
      "grad_norm": 0.03172851353883743,
      "learning_rate": 2.2273600000000002e-06,
      "loss": 0.1991,
      "step": 48580
    },
    {
      "epoch": 1.55488,
      "grad_norm": 0.03809165209531784,
      "learning_rate": 2.22576e-06,
      "loss": 0.1992,
      "step": 48590
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.015536804683506489,
      "learning_rate": 2.2241600000000004e-06,
      "loss": 0.1992,
      "step": 48600
    },
    {
      "epoch": 1.55552,
      "grad_norm": 0.019515786319971085,
      "learning_rate": 2.2225600000000003e-06,
      "loss": 0.1991,
      "step": 48610
    },
    {
      "epoch": 1.55584,
      "grad_norm": 0.23855893313884735,
      "learning_rate": 2.22096e-06,
      "loss": 0.2004,
      "step": 48620
    },
    {
      "epoch": 1.55616,
      "grad_norm": 0.32750651240348816,
      "learning_rate": 2.21936e-06,
      "loss": 0.2002,
      "step": 48630
    },
    {
      "epoch": 1.55648,
      "grad_norm": 0.09312070161104202,
      "learning_rate": 2.2177600000000003e-06,
      "loss": 0.2058,
      "step": 48640
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.027508368715643883,
      "learning_rate": 2.2161600000000002e-06,
      "loss": 0.1991,
      "step": 48650
    },
    {
      "epoch": 1.5571199999999998,
      "grad_norm": 0.014271031133830547,
      "learning_rate": 2.21456e-06,
      "loss": 0.2058,
      "step": 48660
    },
    {
      "epoch": 1.5574400000000002,
      "grad_norm": 0.03565528616309166,
      "learning_rate": 2.21296e-06,
      "loss": 0.1991,
      "step": 48670
    },
    {
      "epoch": 1.55776,
      "grad_norm": 0.04576425999403,
      "learning_rate": 2.2113600000000003e-06,
      "loss": 0.2287,
      "step": 48680
    },
    {
      "epoch": 1.55808,
      "grad_norm": 0.02389666624367237,
      "learning_rate": 2.20976e-06,
      "loss": 0.1991,
      "step": 48690
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.02330402098596096,
      "learning_rate": 2.2081600000000005e-06,
      "loss": 0.1998,
      "step": 48700
    },
    {
      "epoch": 1.55872,
      "grad_norm": 0.02367914840579033,
      "learning_rate": 2.20656e-06,
      "loss": 0.1992,
      "step": 48710
    },
    {
      "epoch": 1.55904,
      "grad_norm": 0.024598829448223114,
      "learning_rate": 2.2049600000000002e-06,
      "loss": 0.1993,
      "step": 48720
    },
    {
      "epoch": 1.5593599999999999,
      "grad_norm": 0.0185383353382349,
      "learning_rate": 2.20336e-06,
      "loss": 0.1991,
      "step": 48730
    },
    {
      "epoch": 1.55968,
      "grad_norm": 0.027393393218517303,
      "learning_rate": 2.2017600000000004e-06,
      "loss": 0.2154,
      "step": 48740
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.04139738902449608,
      "learning_rate": 2.20016e-06,
      "loss": 0.2227,
      "step": 48750
    },
    {
      "epoch": 1.56032,
      "grad_norm": 0.015591815114021301,
      "learning_rate": 2.19856e-06,
      "loss": 0.1995,
      "step": 48760
    },
    {
      "epoch": 1.56064,
      "grad_norm": 0.012923672795295715,
      "learning_rate": 2.19696e-06,
      "loss": 0.2061,
      "step": 48770
    },
    {
      "epoch": 1.5609600000000001,
      "grad_norm": 0.020145462825894356,
      "learning_rate": 2.1953600000000004e-06,
      "loss": 0.1991,
      "step": 48780
    },
    {
      "epoch": 1.56128,
      "grad_norm": 0.009645896032452583,
      "learning_rate": 2.1937600000000002e-06,
      "loss": 0.2001,
      "step": 48790
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.020651839673519135,
      "learning_rate": 2.19216e-06,
      "loss": 0.1991,
      "step": 48800
    },
    {
      "epoch": 1.56192,
      "grad_norm": 0.025236640125513077,
      "learning_rate": 2.19056e-06,
      "loss": 0.2139,
      "step": 48810
    },
    {
      "epoch": 1.56224,
      "grad_norm": 0.040210213512182236,
      "learning_rate": 2.1889600000000003e-06,
      "loss": 0.1994,
      "step": 48820
    },
    {
      "epoch": 1.56256,
      "grad_norm": 0.022074682638049126,
      "learning_rate": 2.18736e-06,
      "loss": 0.1995,
      "step": 48830
    },
    {
      "epoch": 1.56288,
      "grad_norm": 0.017149342224001884,
      "learning_rate": 2.18576e-06,
      "loss": 0.2125,
      "step": 48840
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.041450876742601395,
      "learning_rate": 2.1841600000000004e-06,
      "loss": 0.2,
      "step": 48850
    },
    {
      "epoch": 1.56352,
      "grad_norm": 0.06955649703741074,
      "learning_rate": 2.1825600000000002e-06,
      "loss": 0.1993,
      "step": 48860
    },
    {
      "epoch": 1.56384,
      "grad_norm": 0.026339732110500336,
      "learning_rate": 2.18096e-06,
      "loss": 0.2078,
      "step": 48870
    },
    {
      "epoch": 1.56416,
      "grad_norm": 0.018015963956713676,
      "learning_rate": 2.17936e-06,
      "loss": 0.1991,
      "step": 48880
    },
    {
      "epoch": 1.56448,
      "grad_norm": 0.013344237580895424,
      "learning_rate": 2.1777600000000003e-06,
      "loss": 0.199,
      "step": 48890
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.03598778694868088,
      "learning_rate": 2.17616e-06,
      "loss": 0.1992,
      "step": 48900
    },
    {
      "epoch": 1.5651199999999998,
      "grad_norm": 0.03570278361439705,
      "learning_rate": 2.1745600000000005e-06,
      "loss": 0.1994,
      "step": 48910
    },
    {
      "epoch": 1.5654400000000002,
      "grad_norm": 0.022096604108810425,
      "learning_rate": 2.17296e-06,
      "loss": 0.1991,
      "step": 48920
    },
    {
      "epoch": 1.56576,
      "grad_norm": 0.5255405306816101,
      "learning_rate": 2.1713600000000002e-06,
      "loss": 0.2008,
      "step": 48930
    },
    {
      "epoch": 1.56608,
      "grad_norm": 0.014813453890383244,
      "learning_rate": 2.16976e-06,
      "loss": 0.2057,
      "step": 48940
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.02034750021994114,
      "learning_rate": 2.1681600000000004e-06,
      "loss": 0.2165,
      "step": 48950
    },
    {
      "epoch": 1.5667200000000001,
      "grad_norm": 0.02534744329750538,
      "learning_rate": 2.16656e-06,
      "loss": 0.1991,
      "step": 48960
    },
    {
      "epoch": 1.56704,
      "grad_norm": 0.03273790702223778,
      "learning_rate": 2.16496e-06,
      "loss": 0.2083,
      "step": 48970
    },
    {
      "epoch": 1.5673599999999999,
      "grad_norm": 0.018938612192869186,
      "learning_rate": 2.16336e-06,
      "loss": 0.2331,
      "step": 48980
    },
    {
      "epoch": 1.56768,
      "grad_norm": 0.0347214974462986,
      "learning_rate": 2.1617600000000004e-06,
      "loss": 0.1996,
      "step": 48990
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.02268865890800953,
      "learning_rate": 2.1601600000000003e-06,
      "loss": 0.2073,
      "step": 49000
    },
    {
      "epoch": 1.568,
      "eval_runtime": 57.3773,
      "eval_samples_per_second": 174.285,
      "eval_steps_per_second": 10.893,
      "step": 49000
    },
    {
      "epoch": 1.56832,
      "grad_norm": 0.035024531185626984,
      "learning_rate": 2.15856e-06,
      "loss": 0.2102,
      "step": 49010
    },
    {
      "epoch": 1.56864,
      "grad_norm": 0.024415383115410805,
      "learning_rate": 2.15696e-06,
      "loss": 0.1992,
      "step": 49020
    },
    {
      "epoch": 1.5689600000000001,
      "grad_norm": 0.02540871873497963,
      "learning_rate": 2.1553600000000003e-06,
      "loss": 0.2161,
      "step": 49030
    },
    {
      "epoch": 1.56928,
      "grad_norm": 0.10955742001533508,
      "learning_rate": 2.15376e-06,
      "loss": 0.2118,
      "step": 49040
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.7670359015464783,
      "learning_rate": 2.15216e-06,
      "loss": 0.2152,
      "step": 49050
    },
    {
      "epoch": 1.56992,
      "grad_norm": 0.030000876635313034,
      "learning_rate": 2.15056e-06,
      "loss": 0.1992,
      "step": 49060
    },
    {
      "epoch": 1.57024,
      "grad_norm": 0.041272252798080444,
      "learning_rate": 2.1489600000000003e-06,
      "loss": 0.2154,
      "step": 49070
    },
    {
      "epoch": 1.57056,
      "grad_norm": 0.028340866789221764,
      "learning_rate": 2.14736e-06,
      "loss": 0.2163,
      "step": 49080
    },
    {
      "epoch": 1.57088,
      "grad_norm": 0.822319507598877,
      "learning_rate": 2.14576e-06,
      "loss": 0.2141,
      "step": 49090
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.023950476199388504,
      "learning_rate": 2.1441600000000003e-06,
      "loss": 0.1999,
      "step": 49100
    },
    {
      "epoch": 1.57152,
      "grad_norm": 0.04092840477824211,
      "learning_rate": 2.14256e-06,
      "loss": 0.1996,
      "step": 49110
    },
    {
      "epoch": 1.57184,
      "grad_norm": 1.4266165494918823,
      "learning_rate": 2.14096e-06,
      "loss": 0.2052,
      "step": 49120
    },
    {
      "epoch": 1.57216,
      "grad_norm": 0.7875979542732239,
      "learning_rate": 2.13936e-06,
      "loss": 0.2052,
      "step": 49130
    },
    {
      "epoch": 1.57248,
      "grad_norm": 0.04304397851228714,
      "learning_rate": 2.1377600000000003e-06,
      "loss": 0.1992,
      "step": 49140
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.03061419539153576,
      "learning_rate": 2.13616e-06,
      "loss": 0.1995,
      "step": 49150
    },
    {
      "epoch": 1.5731199999999999,
      "grad_norm": 0.017622411251068115,
      "learning_rate": 2.1345600000000004e-06,
      "loss": 0.1992,
      "step": 49160
    },
    {
      "epoch": 1.57344,
      "grad_norm": 0.030750425532460213,
      "learning_rate": 2.1329600000000003e-06,
      "loss": 0.2167,
      "step": 49170
    },
    {
      "epoch": 1.57376,
      "grad_norm": 0.02981121651828289,
      "learning_rate": 2.13136e-06,
      "loss": 0.1992,
      "step": 49180
    },
    {
      "epoch": 1.57408,
      "grad_norm": 0.024448193609714508,
      "learning_rate": 2.12976e-06,
      "loss": 0.1993,
      "step": 49190
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.037416376173496246,
      "learning_rate": 2.1281600000000004e-06,
      "loss": 0.1992,
      "step": 49200
    },
    {
      "epoch": 1.5747200000000001,
      "grad_norm": 0.008013381622731686,
      "learning_rate": 2.1265600000000003e-06,
      "loss": 0.1992,
      "step": 49210
    },
    {
      "epoch": 1.57504,
      "grad_norm": 0.0184159092605114,
      "learning_rate": 2.12496e-06,
      "loss": 0.1992,
      "step": 49220
    },
    {
      "epoch": 1.5753599999999999,
      "grad_norm": 0.01585485227406025,
      "learning_rate": 2.12336e-06,
      "loss": 0.1994,
      "step": 49230
    },
    {
      "epoch": 1.57568,
      "grad_norm": 0.02084554173052311,
      "learning_rate": 2.1217600000000003e-06,
      "loss": 0.2017,
      "step": 49240
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.03104119375348091,
      "learning_rate": 2.1201600000000002e-06,
      "loss": 0.1992,
      "step": 49250
    },
    {
      "epoch": 1.57632,
      "grad_norm": 0.012531212531030178,
      "learning_rate": 2.11856e-06,
      "loss": 0.2203,
      "step": 49260
    },
    {
      "epoch": 1.57664,
      "grad_norm": 0.032260291278362274,
      "learning_rate": 2.11696e-06,
      "loss": 0.2145,
      "step": 49270
    },
    {
      "epoch": 1.5769600000000001,
      "grad_norm": 0.012061282992362976,
      "learning_rate": 2.1153600000000003e-06,
      "loss": 0.1992,
      "step": 49280
    },
    {
      "epoch": 1.57728,
      "grad_norm": 0.019217923283576965,
      "learning_rate": 2.11376e-06,
      "loss": 0.1991,
      "step": 49290
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.014102645218372345,
      "learning_rate": 2.11216e-06,
      "loss": 0.1991,
      "step": 49300
    },
    {
      "epoch": 1.57792,
      "grad_norm": 0.014700105413794518,
      "learning_rate": 2.11056e-06,
      "loss": 0.1993,
      "step": 49310
    },
    {
      "epoch": 1.57824,
      "grad_norm": 0.042838238179683685,
      "learning_rate": 2.1089600000000002e-06,
      "loss": 0.1993,
      "step": 49320
    },
    {
      "epoch": 1.57856,
      "grad_norm": 0.02749641053378582,
      "learning_rate": 2.10736e-06,
      "loss": 0.1993,
      "step": 49330
    },
    {
      "epoch": 1.5788799999999998,
      "grad_norm": 0.020171307027339935,
      "learning_rate": 2.1057600000000004e-06,
      "loss": 0.2127,
      "step": 49340
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.011745794676244259,
      "learning_rate": 2.1041600000000003e-06,
      "loss": 0.1991,
      "step": 49350
    },
    {
      "epoch": 1.57952,
      "grad_norm": 0.01825086399912834,
      "learning_rate": 2.10256e-06,
      "loss": 0.1998,
      "step": 49360
    },
    {
      "epoch": 1.57984,
      "grad_norm": 0.03825104236602783,
      "learning_rate": 2.10096e-06,
      "loss": 0.2284,
      "step": 49370
    },
    {
      "epoch": 1.58016,
      "grad_norm": 0.020496897399425507,
      "learning_rate": 2.0993600000000003e-06,
      "loss": 0.1991,
      "step": 49380
    },
    {
      "epoch": 1.58048,
      "grad_norm": 0.03122059442102909,
      "learning_rate": 2.0977600000000002e-06,
      "loss": 0.1994,
      "step": 49390
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.011217387393116951,
      "learning_rate": 2.09616e-06,
      "loss": 0.1991,
      "step": 49400
    },
    {
      "epoch": 1.5811199999999999,
      "grad_norm": 0.04779556766152382,
      "learning_rate": 2.0945600000000004e-06,
      "loss": 0.1991,
      "step": 49410
    },
    {
      "epoch": 1.58144,
      "grad_norm": 0.0688462033867836,
      "learning_rate": 2.0929600000000003e-06,
      "loss": 0.1992,
      "step": 49420
    },
    {
      "epoch": 1.58176,
      "grad_norm": 0.0459408164024353,
      "learning_rate": 2.09136e-06,
      "loss": 0.1997,
      "step": 49430
    },
    {
      "epoch": 1.58208,
      "grad_norm": 0.025019170716404915,
      "learning_rate": 2.08976e-06,
      "loss": 0.1995,
      "step": 49440
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.05051609128713608,
      "learning_rate": 2.0881600000000003e-06,
      "loss": 0.1991,
      "step": 49450
    },
    {
      "epoch": 1.5827200000000001,
      "grad_norm": 0.03688596561551094,
      "learning_rate": 2.0865600000000002e-06,
      "loss": 0.1991,
      "step": 49460
    },
    {
      "epoch": 1.58304,
      "grad_norm": 0.024651536718010902,
      "learning_rate": 2.08496e-06,
      "loss": 0.2156,
      "step": 49470
    },
    {
      "epoch": 1.5833599999999999,
      "grad_norm": 0.04185308516025543,
      "learning_rate": 2.08336e-06,
      "loss": 0.1991,
      "step": 49480
    },
    {
      "epoch": 1.58368,
      "grad_norm": 0.14405405521392822,
      "learning_rate": 2.0817600000000003e-06,
      "loss": 0.1998,
      "step": 49490
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.035594452172517776,
      "learning_rate": 2.08016e-06,
      "loss": 0.1992,
      "step": 49500
    },
    {
      "epoch": 1.58432,
      "grad_norm": 0.16824206709861755,
      "learning_rate": 2.0785600000000005e-06,
      "loss": 0.1995,
      "step": 49510
    },
    {
      "epoch": 1.58464,
      "grad_norm": 0.029088526964187622,
      "learning_rate": 2.07696e-06,
      "loss": 0.1992,
      "step": 49520
    },
    {
      "epoch": 1.5849600000000001,
      "grad_norm": 0.8082526326179504,
      "learning_rate": 2.0753600000000002e-06,
      "loss": 0.2159,
      "step": 49530
    },
    {
      "epoch": 1.58528,
      "grad_norm": 0.02662764862179756,
      "learning_rate": 2.07376e-06,
      "loss": 0.2157,
      "step": 49540
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.03211762756109238,
      "learning_rate": 2.0721600000000004e-06,
      "loss": 0.1992,
      "step": 49550
    },
    {
      "epoch": 1.58592,
      "grad_norm": 0.03822147846221924,
      "learning_rate": 2.07056e-06,
      "loss": 0.1991,
      "step": 49560
    },
    {
      "epoch": 1.58624,
      "grad_norm": 0.16898499429225922,
      "learning_rate": 2.06896e-06,
      "loss": 0.2,
      "step": 49570
    },
    {
      "epoch": 1.58656,
      "grad_norm": 0.014365382492542267,
      "learning_rate": 2.06736e-06,
      "loss": 0.2102,
      "step": 49580
    },
    {
      "epoch": 1.5868799999999998,
      "grad_norm": 0.020453820005059242,
      "learning_rate": 2.0657600000000004e-06,
      "loss": 0.1997,
      "step": 49590
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.01848362199962139,
      "learning_rate": 2.0641600000000002e-06,
      "loss": 0.1992,
      "step": 49600
    },
    {
      "epoch": 1.58752,
      "grad_norm": 0.04954664409160614,
      "learning_rate": 2.06256e-06,
      "loss": 0.2175,
      "step": 49610
    },
    {
      "epoch": 1.58784,
      "grad_norm": 0.019280429929494858,
      "learning_rate": 2.06096e-06,
      "loss": 0.2005,
      "step": 49620
    },
    {
      "epoch": 1.58816,
      "grad_norm": 0.011358022689819336,
      "learning_rate": 2.0593600000000003e-06,
      "loss": 0.1992,
      "step": 49630
    },
    {
      "epoch": 1.5884800000000001,
      "grad_norm": 0.030623147264122963,
      "learning_rate": 2.05776e-06,
      "loss": 0.1996,
      "step": 49640
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.0333857499063015,
      "learning_rate": 2.05616e-06,
      "loss": 0.2139,
      "step": 49650
    },
    {
      "epoch": 1.5891199999999999,
      "grad_norm": 0.024207983165979385,
      "learning_rate": 2.0545600000000004e-06,
      "loss": 0.1993,
      "step": 49660
    },
    {
      "epoch": 1.58944,
      "grad_norm": 0.025281168520450592,
      "learning_rate": 2.0529600000000002e-06,
      "loss": 0.2123,
      "step": 49670
    },
    {
      "epoch": 1.58976,
      "grad_norm": 0.014759720303118229,
      "learning_rate": 2.05136e-06,
      "loss": 0.2261,
      "step": 49680
    },
    {
      "epoch": 1.59008,
      "grad_norm": 0.054102733731269836,
      "learning_rate": 2.04976e-06,
      "loss": 0.2022,
      "step": 49690
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.016857806593179703,
      "learning_rate": 2.0481600000000003e-06,
      "loss": 0.1993,
      "step": 49700
    },
    {
      "epoch": 1.5907200000000001,
      "grad_norm": 0.04223714396357536,
      "learning_rate": 2.04656e-06,
      "loss": 0.1993,
      "step": 49710
    },
    {
      "epoch": 1.59104,
      "grad_norm": 0.03811228647828102,
      "learning_rate": 2.04496e-06,
      "loss": 0.1993,
      "step": 49720
    },
    {
      "epoch": 1.5913599999999999,
      "grad_norm": 0.019821632653474808,
      "learning_rate": 2.04336e-06,
      "loss": 0.1993,
      "step": 49730
    },
    {
      "epoch": 1.59168,
      "grad_norm": 0.012821083888411522,
      "learning_rate": 2.0417600000000003e-06,
      "loss": 0.1993,
      "step": 49740
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.03535088151693344,
      "learning_rate": 2.04016e-06,
      "loss": 0.1992,
      "step": 49750
    },
    {
      "epoch": 1.59232,
      "grad_norm": 0.018432440236210823,
      "learning_rate": 2.0385600000000004e-06,
      "loss": 0.1992,
      "step": 49760
    },
    {
      "epoch": 1.5926399999999998,
      "grad_norm": 0.02050972171127796,
      "learning_rate": 2.03696e-06,
      "loss": 0.1996,
      "step": 49770
    },
    {
      "epoch": 1.5929600000000002,
      "grad_norm": 0.03435636684298515,
      "learning_rate": 2.03536e-06,
      "loss": 0.1992,
      "step": 49780
    },
    {
      "epoch": 1.59328,
      "grad_norm": 0.010485637001693249,
      "learning_rate": 2.03376e-06,
      "loss": 0.1991,
      "step": 49790
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.026741979643702507,
      "learning_rate": 2.0321600000000004e-06,
      "loss": 0.1992,
      "step": 49800
    },
    {
      "epoch": 1.59392,
      "grad_norm": 0.03029802069067955,
      "learning_rate": 2.03056e-06,
      "loss": 0.1994,
      "step": 49810
    },
    {
      "epoch": 1.59424,
      "grad_norm": 0.021108053624629974,
      "learning_rate": 2.02896e-06,
      "loss": 0.1994,
      "step": 49820
    },
    {
      "epoch": 1.59456,
      "grad_norm": 0.09740882366895676,
      "learning_rate": 2.02736e-06,
      "loss": 0.1993,
      "step": 49830
    },
    {
      "epoch": 1.5948799999999999,
      "grad_norm": 0.03185497596859932,
      "learning_rate": 2.0257600000000003e-06,
      "loss": 0.1992,
      "step": 49840
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.052013445645570755,
      "learning_rate": 2.02416e-06,
      "loss": 0.1996,
      "step": 49850
    },
    {
      "epoch": 1.59552,
      "grad_norm": 0.025446759536862373,
      "learning_rate": 2.02256e-06,
      "loss": 0.2137,
      "step": 49860
    },
    {
      "epoch": 1.59584,
      "grad_norm": 0.012090628035366535,
      "learning_rate": 2.02096e-06,
      "loss": 0.1993,
      "step": 49870
    },
    {
      "epoch": 1.59616,
      "grad_norm": 0.01170848123729229,
      "learning_rate": 2.0193600000000003e-06,
      "loss": 0.1992,
      "step": 49880
    },
    {
      "epoch": 1.5964800000000001,
      "grad_norm": 0.12759269773960114,
      "learning_rate": 2.01776e-06,
      "loss": 0.2043,
      "step": 49890
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.02013418637216091,
      "learning_rate": 2.01616e-06,
      "loss": 0.1995,
      "step": 49900
    },
    {
      "epoch": 1.5971199999999999,
      "grad_norm": 0.022427961230278015,
      "learning_rate": 2.0145600000000003e-06,
      "loss": 0.1992,
      "step": 49910
    },
    {
      "epoch": 1.59744,
      "grad_norm": 0.025103721767663956,
      "learning_rate": 2.01296e-06,
      "loss": 0.2101,
      "step": 49920
    },
    {
      "epoch": 1.59776,
      "grad_norm": 0.027223380282521248,
      "learning_rate": 2.01136e-06,
      "loss": 0.2115,
      "step": 49930
    },
    {
      "epoch": 1.59808,
      "grad_norm": 0.02068505994975567,
      "learning_rate": 2.00976e-06,
      "loss": 0.1992,
      "step": 49940
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.03816219046711922,
      "learning_rate": 2.0081600000000003e-06,
      "loss": 0.1993,
      "step": 49950
    },
    {
      "epoch": 1.5987200000000001,
      "grad_norm": 0.0793033242225647,
      "learning_rate": 2.00656e-06,
      "loss": 0.2041,
      "step": 49960
    },
    {
      "epoch": 1.59904,
      "grad_norm": 0.030025528743863106,
      "learning_rate": 2.00496e-06,
      "loss": 0.1992,
      "step": 49970
    },
    {
      "epoch": 1.59936,
      "grad_norm": 0.043891943991184235,
      "learning_rate": 2.00336e-06,
      "loss": 0.2003,
      "step": 49980
    },
    {
      "epoch": 1.59968,
      "grad_norm": 0.015962781384587288,
      "learning_rate": 2.00176e-06,
      "loss": 0.1995,
      "step": 49990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.024597380310297012,
      "learning_rate": 2.00016e-06,
      "loss": 0.2073,
      "step": 50000
    },
    {
      "epoch": 1.6,
      "eval_runtime": 57.5309,
      "eval_samples_per_second": 173.82,
      "eval_steps_per_second": 10.864,
      "step": 50000
    },
    {
      "epoch": 1.60032,
      "grad_norm": 0.03482325002551079,
      "learning_rate": 1.9985600000000004e-06,
      "loss": 0.2046,
      "step": 50010
    },
    {
      "epoch": 1.6006399999999998,
      "grad_norm": 0.03769385442137718,
      "learning_rate": 1.9969600000000003e-06,
      "loss": 0.217,
      "step": 50020
    },
    {
      "epoch": 1.6009600000000002,
      "grad_norm": 0.02760433591902256,
      "learning_rate": 1.99536e-06,
      "loss": 0.1995,
      "step": 50030
    },
    {
      "epoch": 1.60128,
      "grad_norm": 0.04036952182650566,
      "learning_rate": 1.99376e-06,
      "loss": 0.2005,
      "step": 50040
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.0255107618868351,
      "learning_rate": 1.9921600000000003e-06,
      "loss": 0.2144,
      "step": 50050
    },
    {
      "epoch": 1.60192,
      "grad_norm": 0.03886169567704201,
      "learning_rate": 1.9905600000000002e-06,
      "loss": 0.1993,
      "step": 50060
    },
    {
      "epoch": 1.60224,
      "grad_norm": 0.04590285196900368,
      "learning_rate": 1.98896e-06,
      "loss": 0.2016,
      "step": 50070
    },
    {
      "epoch": 1.60256,
      "grad_norm": 0.03430658578872681,
      "learning_rate": 1.98736e-06,
      "loss": 0.1992,
      "step": 50080
    },
    {
      "epoch": 1.6028799999999999,
      "grad_norm": 0.017716022208333015,
      "learning_rate": 1.9857600000000003e-06,
      "loss": 0.1991,
      "step": 50090
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.02771827206015587,
      "learning_rate": 1.98416e-06,
      "loss": 0.2169,
      "step": 50100
    },
    {
      "epoch": 1.60352,
      "grad_norm": 0.026878735050559044,
      "learning_rate": 1.98256e-06,
      "loss": 0.2159,
      "step": 50110
    },
    {
      "epoch": 1.60384,
      "grad_norm": 1.4421732425689697,
      "learning_rate": 1.98096e-06,
      "loss": 0.2061,
      "step": 50120
    },
    {
      "epoch": 1.60416,
      "grad_norm": 0.07790921628475189,
      "learning_rate": 1.9793600000000002e-06,
      "loss": 0.2312,
      "step": 50130
    },
    {
      "epoch": 1.6044800000000001,
      "grad_norm": 0.03787169232964516,
      "learning_rate": 1.97776e-06,
      "loss": 0.1993,
      "step": 50140
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.06077781319618225,
      "learning_rate": 1.9761600000000004e-06,
      "loss": 0.1992,
      "step": 50150
    },
    {
      "epoch": 1.6051199999999999,
      "grad_norm": 0.033719584345817566,
      "learning_rate": 1.9745600000000003e-06,
      "loss": 0.1995,
      "step": 50160
    },
    {
      "epoch": 1.60544,
      "grad_norm": 0.02319880947470665,
      "learning_rate": 1.97296e-06,
      "loss": 0.2148,
      "step": 50170
    },
    {
      "epoch": 1.60576,
      "grad_norm": 0.84626305103302,
      "learning_rate": 1.97136e-06,
      "loss": 0.2173,
      "step": 50180
    },
    {
      "epoch": 1.60608,
      "grad_norm": 0.02238408848643303,
      "learning_rate": 1.9697600000000003e-06,
      "loss": 0.1992,
      "step": 50190
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.023657403886318207,
      "learning_rate": 1.9681600000000002e-06,
      "loss": 0.1995,
      "step": 50200
    },
    {
      "epoch": 1.6067200000000001,
      "grad_norm": 0.0205511674284935,
      "learning_rate": 1.96656e-06,
      "loss": 0.2161,
      "step": 50210
    },
    {
      "epoch": 1.60704,
      "grad_norm": 0.021503742784261703,
      "learning_rate": 1.9649600000000004e-06,
      "loss": 0.1991,
      "step": 50220
    },
    {
      "epoch": 1.60736,
      "grad_norm": 0.030892010778188705,
      "learning_rate": 1.9633600000000003e-06,
      "loss": 0.1994,
      "step": 50230
    },
    {
      "epoch": 1.60768,
      "grad_norm": 0.014794028364121914,
      "learning_rate": 1.96176e-06,
      "loss": 0.199,
      "step": 50240
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.011674892157316208,
      "learning_rate": 1.96016e-06,
      "loss": 0.1992,
      "step": 50250
    },
    {
      "epoch": 1.60832,
      "grad_norm": 0.023576371371746063,
      "learning_rate": 1.9585600000000004e-06,
      "loss": 0.2014,
      "step": 50260
    },
    {
      "epoch": 1.6086399999999998,
      "grad_norm": 0.015433765016496181,
      "learning_rate": 1.9569600000000002e-06,
      "loss": 0.1991,
      "step": 50270
    },
    {
      "epoch": 1.60896,
      "grad_norm": 0.03207409009337425,
      "learning_rate": 1.95536e-06,
      "loss": 0.1992,
      "step": 50280
    },
    {
      "epoch": 1.60928,
      "grad_norm": 0.039214521646499634,
      "learning_rate": 1.95376e-06,
      "loss": 0.1991,
      "step": 50290
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.02069779485464096,
      "learning_rate": 1.9521600000000003e-06,
      "loss": 0.1992,
      "step": 50300
    },
    {
      "epoch": 1.60992,
      "grad_norm": 0.06307036429643631,
      "learning_rate": 1.95056e-06,
      "loss": 0.2002,
      "step": 50310
    },
    {
      "epoch": 1.6102400000000001,
      "grad_norm": 0.023853685706853867,
      "learning_rate": 1.9489600000000005e-06,
      "loss": 0.1992,
      "step": 50320
    },
    {
      "epoch": 1.61056,
      "grad_norm": 0.019836891442537308,
      "learning_rate": 1.94736e-06,
      "loss": 0.1993,
      "step": 50330
    },
    {
      "epoch": 1.6108799999999999,
      "grad_norm": 0.02355489879846573,
      "learning_rate": 1.9457600000000002e-06,
      "loss": 0.2042,
      "step": 50340
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.009358162991702557,
      "learning_rate": 1.94416e-06,
      "loss": 0.1992,
      "step": 50350
    },
    {
      "epoch": 1.61152,
      "grad_norm": 0.03557613492012024,
      "learning_rate": 1.9425600000000004e-06,
      "loss": 0.2095,
      "step": 50360
    },
    {
      "epoch": 1.61184,
      "grad_norm": 0.012690197676420212,
      "learning_rate": 1.94096e-06,
      "loss": 0.2018,
      "step": 50370
    },
    {
      "epoch": 1.61216,
      "grad_norm": 0.013339389115571976,
      "learning_rate": 1.93936e-06,
      "loss": 0.1994,
      "step": 50380
    },
    {
      "epoch": 1.6124800000000001,
      "grad_norm": 0.014780093915760517,
      "learning_rate": 1.93776e-06,
      "loss": 0.1989,
      "step": 50390
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.012073298916220665,
      "learning_rate": 1.9361600000000004e-06,
      "loss": 0.212,
      "step": 50400
    },
    {
      "epoch": 1.6131199999999999,
      "grad_norm": 0.06573262810707092,
      "learning_rate": 1.9345600000000002e-06,
      "loss": 0.2126,
      "step": 50410
    },
    {
      "epoch": 1.61344,
      "grad_norm": 0.048864588141441345,
      "learning_rate": 1.93296e-06,
      "loss": 0.2241,
      "step": 50420
    },
    {
      "epoch": 1.61376,
      "grad_norm": 0.018295101821422577,
      "learning_rate": 1.93136e-06,
      "loss": 0.2102,
      "step": 50430
    },
    {
      "epoch": 1.61408,
      "grad_norm": 1.2004332542419434,
      "learning_rate": 1.9297600000000003e-06,
      "loss": 0.2072,
      "step": 50440
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.031956933438777924,
      "learning_rate": 1.92816e-06,
      "loss": 0.1992,
      "step": 50450
    },
    {
      "epoch": 1.6147200000000002,
      "grad_norm": 0.020272349938750267,
      "learning_rate": 1.92656e-06,
      "loss": 0.2044,
      "step": 50460
    },
    {
      "epoch": 1.61504,
      "grad_norm": 0.025238480418920517,
      "learning_rate": 1.9249600000000004e-06,
      "loss": 0.2147,
      "step": 50470
    },
    {
      "epoch": 1.61536,
      "grad_norm": 0.01613139733672142,
      "learning_rate": 1.9233600000000003e-06,
      "loss": 0.1994,
      "step": 50480
    },
    {
      "epoch": 1.61568,
      "grad_norm": 0.023320619016885757,
      "learning_rate": 1.92176e-06,
      "loss": 0.2082,
      "step": 50490
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.02752334624528885,
      "learning_rate": 1.92016e-06,
      "loss": 0.2148,
      "step": 50500
    },
    {
      "epoch": 1.61632,
      "grad_norm": 0.02246631681919098,
      "learning_rate": 1.9185600000000003e-06,
      "loss": 0.1993,
      "step": 50510
    },
    {
      "epoch": 1.6166399999999999,
      "grad_norm": 0.026110896840691566,
      "learning_rate": 1.91696e-06,
      "loss": 0.2165,
      "step": 50520
    },
    {
      "epoch": 1.61696,
      "grad_norm": 0.0177303459495306,
      "learning_rate": 1.91536e-06,
      "loss": 0.201,
      "step": 50530
    },
    {
      "epoch": 1.61728,
      "grad_norm": 0.020100299268960953,
      "learning_rate": 1.91376e-06,
      "loss": 0.1991,
      "step": 50540
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.030453499406576157,
      "learning_rate": 1.9121600000000003e-06,
      "loss": 0.2016,
      "step": 50550
    },
    {
      "epoch": 1.61792,
      "grad_norm": 0.033460456877946854,
      "learning_rate": 1.91056e-06,
      "loss": 0.2064,
      "step": 50560
    },
    {
      "epoch": 1.6182400000000001,
      "grad_norm": 0.028770357370376587,
      "learning_rate": 1.9089600000000004e-06,
      "loss": 0.2049,
      "step": 50570
    },
    {
      "epoch": 1.61856,
      "grad_norm": 0.01210537739098072,
      "learning_rate": 1.90736e-06,
      "loss": 0.1992,
      "step": 50580
    },
    {
      "epoch": 1.6188799999999999,
      "grad_norm": 0.015630051493644714,
      "learning_rate": 1.9057600000000002e-06,
      "loss": 0.1996,
      "step": 50590
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.016793183982372284,
      "learning_rate": 1.90416e-06,
      "loss": 0.2098,
      "step": 50600
    },
    {
      "epoch": 1.61952,
      "grad_norm": 0.052589476108551025,
      "learning_rate": 1.9025600000000002e-06,
      "loss": 0.1994,
      "step": 50610
    },
    {
      "epoch": 1.61984,
      "grad_norm": 0.06430990248918533,
      "learning_rate": 1.90096e-06,
      "loss": 0.2025,
      "step": 50620
    },
    {
      "epoch": 1.62016,
      "grad_norm": 0.3972319960594177,
      "learning_rate": 1.8993600000000001e-06,
      "loss": 0.1996,
      "step": 50630
    },
    {
      "epoch": 1.6204800000000001,
      "grad_norm": 0.8260248303413391,
      "learning_rate": 1.8977600000000002e-06,
      "loss": 0.2014,
      "step": 50640
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.0351429358124733,
      "learning_rate": 1.8961600000000003e-06,
      "loss": 0.1991,
      "step": 50650
    },
    {
      "epoch": 1.62112,
      "grad_norm": 0.020308949053287506,
      "learning_rate": 1.8945600000000002e-06,
      "loss": 0.199,
      "step": 50660
    },
    {
      "epoch": 1.62144,
      "grad_norm": 0.02360602281987667,
      "learning_rate": 1.89296e-06,
      "loss": 0.2007,
      "step": 50670
    },
    {
      "epoch": 1.62176,
      "grad_norm": 0.8306738138198853,
      "learning_rate": 1.8913600000000002e-06,
      "loss": 0.2347,
      "step": 50680
    },
    {
      "epoch": 1.62208,
      "grad_norm": 0.026479220017790794,
      "learning_rate": 1.8897600000000003e-06,
      "loss": 0.2153,
      "step": 50690
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.06173134595155716,
      "learning_rate": 1.8881600000000004e-06,
      "loss": 0.1993,
      "step": 50700
    },
    {
      "epoch": 1.6227200000000002,
      "grad_norm": 0.05292503535747528,
      "learning_rate": 1.88656e-06,
      "loss": 0.1998,
      "step": 50710
    },
    {
      "epoch": 1.62304,
      "grad_norm": 0.03903943672776222,
      "learning_rate": 1.8849600000000001e-06,
      "loss": 0.1993,
      "step": 50720
    },
    {
      "epoch": 1.62336,
      "grad_norm": 0.04288027063012123,
      "learning_rate": 1.8833600000000002e-06,
      "loss": 0.2022,
      "step": 50730
    },
    {
      "epoch": 1.62368,
      "grad_norm": 0.016216596588492393,
      "learning_rate": 1.8817600000000003e-06,
      "loss": 0.1991,
      "step": 50740
    },
    {
      "epoch": 1.624,
      "grad_norm": 1.0833126306533813,
      "learning_rate": 1.88016e-06,
      "loss": 0.2102,
      "step": 50750
    },
    {
      "epoch": 1.62432,
      "grad_norm": 0.014281346462666988,
      "learning_rate": 1.87856e-06,
      "loss": 0.1996,
      "step": 50760
    },
    {
      "epoch": 1.6246399999999999,
      "grad_norm": 0.015491507016122341,
      "learning_rate": 1.8769600000000002e-06,
      "loss": 0.2045,
      "step": 50770
    },
    {
      "epoch": 1.62496,
      "grad_norm": 0.05220210179686546,
      "learning_rate": 1.8753600000000002e-06,
      "loss": 0.2001,
      "step": 50780
    },
    {
      "epoch": 1.62528,
      "grad_norm": 0.013935559429228306,
      "learning_rate": 1.8737600000000001e-06,
      "loss": 0.1992,
      "step": 50790
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.026629304513335228,
      "learning_rate": 1.87216e-06,
      "loss": 0.1992,
      "step": 50800
    },
    {
      "epoch": 1.62592,
      "grad_norm": 0.0722193717956543,
      "learning_rate": 1.87056e-06,
      "loss": 0.1992,
      "step": 50810
    },
    {
      "epoch": 1.6262400000000001,
      "grad_norm": 0.03174682706594467,
      "learning_rate": 1.8689600000000002e-06,
      "loss": 0.2003,
      "step": 50820
    },
    {
      "epoch": 1.62656,
      "grad_norm": 0.021536417305469513,
      "learning_rate": 1.8673600000000003e-06,
      "loss": 0.211,
      "step": 50830
    },
    {
      "epoch": 1.6268799999999999,
      "grad_norm": 0.013137721456587315,
      "learning_rate": 1.8657600000000002e-06,
      "loss": 0.1991,
      "step": 50840
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.04358008876442909,
      "learning_rate": 1.86416e-06,
      "loss": 0.1992,
      "step": 50850
    },
    {
      "epoch": 1.62752,
      "grad_norm": 0.02042936161160469,
      "learning_rate": 1.8625600000000001e-06,
      "loss": 0.1991,
      "step": 50860
    },
    {
      "epoch": 1.62784,
      "grad_norm": 0.02062118425965309,
      "learning_rate": 1.8609600000000002e-06,
      "loss": 0.1992,
      "step": 50870
    },
    {
      "epoch": 1.62816,
      "grad_norm": 0.027847986668348312,
      "learning_rate": 1.85936e-06,
      "loss": 0.1992,
      "step": 50880
    },
    {
      "epoch": 1.6284800000000001,
      "grad_norm": 0.01662510819733143,
      "learning_rate": 1.8577600000000002e-06,
      "loss": 0.1992,
      "step": 50890
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.15294568240642548,
      "learning_rate": 1.8561600000000003e-06,
      "loss": 0.2091,
      "step": 50900
    },
    {
      "epoch": 1.62912,
      "grad_norm": 0.038712501525878906,
      "learning_rate": 1.8545600000000002e-06,
      "loss": 0.2026,
      "step": 50910
    },
    {
      "epoch": 1.62944,
      "grad_norm": 0.031028276309370995,
      "learning_rate": 1.85296e-06,
      "loss": 0.1991,
      "step": 50920
    },
    {
      "epoch": 1.62976,
      "grad_norm": 0.027942754328250885,
      "learning_rate": 1.8513600000000001e-06,
      "loss": 0.1993,
      "step": 50930
    },
    {
      "epoch": 1.63008,
      "grad_norm": 0.02596944198012352,
      "learning_rate": 1.8497600000000002e-06,
      "loss": 0.2119,
      "step": 50940
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.018660303205251694,
      "learning_rate": 1.8481600000000003e-06,
      "loss": 0.1992,
      "step": 50950
    },
    {
      "epoch": 1.63072,
      "grad_norm": 0.01733177900314331,
      "learning_rate": 1.84656e-06,
      "loss": 0.1993,
      "step": 50960
    },
    {
      "epoch": 1.63104,
      "grad_norm": 0.023891093209385872,
      "learning_rate": 1.84496e-06,
      "loss": 0.2149,
      "step": 50970
    },
    {
      "epoch": 1.63136,
      "grad_norm": 0.028095880523324013,
      "learning_rate": 1.8433600000000002e-06,
      "loss": 0.2021,
      "step": 50980
    },
    {
      "epoch": 1.63168,
      "grad_norm": 0.039790812879800797,
      "learning_rate": 1.8417600000000003e-06,
      "loss": 0.2117,
      "step": 50990
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.011685886420309544,
      "learning_rate": 1.8401600000000004e-06,
      "loss": 0.1992,
      "step": 51000
    },
    {
      "epoch": 1.6320000000000001,
      "eval_runtime": 57.6381,
      "eval_samples_per_second": 173.496,
      "eval_steps_per_second": 10.844,
      "step": 51000
    },
    {
      "epoch": 1.63232,
      "grad_norm": 0.015500862151384354,
      "learning_rate": 1.83856e-06,
      "loss": 0.2005,
      "step": 51010
    },
    {
      "epoch": 1.6326399999999999,
      "grad_norm": 0.04527537152171135,
      "learning_rate": 1.8369600000000001e-06,
      "loss": 0.1994,
      "step": 51020
    },
    {
      "epoch": 1.63296,
      "grad_norm": 0.037785451859235764,
      "learning_rate": 1.8353600000000002e-06,
      "loss": 0.1991,
      "step": 51030
    },
    {
      "epoch": 1.63328,
      "grad_norm": 0.013521379791200161,
      "learning_rate": 1.8337600000000003e-06,
      "loss": 0.1991,
      "step": 51040
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.01952267624437809,
      "learning_rate": 1.83216e-06,
      "loss": 0.2009,
      "step": 51050
    },
    {
      "epoch": 1.63392,
      "grad_norm": 0.0213134977966547,
      "learning_rate": 1.83056e-06,
      "loss": 0.199,
      "step": 51060
    },
    {
      "epoch": 1.6342400000000001,
      "grad_norm": 0.013447711244225502,
      "learning_rate": 1.8289600000000001e-06,
      "loss": 0.2066,
      "step": 51070
    },
    {
      "epoch": 1.63456,
      "grad_norm": 0.026430584490299225,
      "learning_rate": 1.8273600000000002e-06,
      "loss": 0.1993,
      "step": 51080
    },
    {
      "epoch": 1.6348799999999999,
      "grad_norm": 0.01524574775248766,
      "learning_rate": 1.8257600000000001e-06,
      "loss": 0.1993,
      "step": 51090
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.04380900040268898,
      "learning_rate": 1.82416e-06,
      "loss": 0.199,
      "step": 51100
    },
    {
      "epoch": 1.63552,
      "grad_norm": 0.03704230114817619,
      "learning_rate": 1.82256e-06,
      "loss": 0.1992,
      "step": 51110
    },
    {
      "epoch": 1.63584,
      "grad_norm": 0.03307940065860748,
      "learning_rate": 1.8209600000000002e-06,
      "loss": 0.2074,
      "step": 51120
    },
    {
      "epoch": 1.6361599999999998,
      "grad_norm": 0.013311092741787434,
      "learning_rate": 1.81936e-06,
      "loss": 0.1992,
      "step": 51130
    },
    {
      "epoch": 1.6364800000000002,
      "grad_norm": 0.04556061327457428,
      "learning_rate": 1.8177600000000002e-06,
      "loss": 0.1993,
      "step": 51140
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.04084603115916252,
      "learning_rate": 1.8161600000000002e-06,
      "loss": 0.2318,
      "step": 51150
    },
    {
      "epoch": 1.63712,
      "grad_norm": 0.012821011245250702,
      "learning_rate": 1.8145600000000001e-06,
      "loss": 0.2111,
      "step": 51160
    },
    {
      "epoch": 1.63744,
      "grad_norm": 0.014403973706066608,
      "learning_rate": 1.8129600000000002e-06,
      "loss": 0.1993,
      "step": 51170
    },
    {
      "epoch": 1.63776,
      "grad_norm": 0.03542136028409004,
      "learning_rate": 1.81136e-06,
      "loss": 0.1993,
      "step": 51180
    },
    {
      "epoch": 1.63808,
      "grad_norm": 0.02305886335670948,
      "learning_rate": 1.8097600000000002e-06,
      "loss": 0.1996,
      "step": 51190
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.016478830948472023,
      "learning_rate": 1.8081600000000003e-06,
      "loss": 0.1993,
      "step": 51200
    },
    {
      "epoch": 1.63872,
      "grad_norm": 0.015989059582352638,
      "learning_rate": 1.8065600000000002e-06,
      "loss": 0.2108,
      "step": 51210
    },
    {
      "epoch": 1.63904,
      "grad_norm": 0.024705594405531883,
      "learning_rate": 1.80496e-06,
      "loss": 0.1993,
      "step": 51220
    },
    {
      "epoch": 1.63936,
      "grad_norm": 2.0008487701416016,
      "learning_rate": 1.8033600000000001e-06,
      "loss": 0.2055,
      "step": 51230
    },
    {
      "epoch": 1.63968,
      "grad_norm": 0.34683626890182495,
      "learning_rate": 1.8017600000000002e-06,
      "loss": 0.2162,
      "step": 51240
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.018449731171131134,
      "learning_rate": 1.8001600000000003e-06,
      "loss": 0.1991,
      "step": 51250
    },
    {
      "epoch": 1.64032,
      "grad_norm": 0.012674869038164616,
      "learning_rate": 1.79856e-06,
      "loss": 0.202,
      "step": 51260
    },
    {
      "epoch": 1.6406399999999999,
      "grad_norm": 0.030463360249996185,
      "learning_rate": 1.79696e-06,
      "loss": 0.2155,
      "step": 51270
    },
    {
      "epoch": 1.64096,
      "grad_norm": 0.03158249333500862,
      "learning_rate": 1.7953600000000002e-06,
      "loss": 0.2124,
      "step": 51280
    },
    {
      "epoch": 1.64128,
      "grad_norm": 0.029635019600391388,
      "learning_rate": 1.7937600000000003e-06,
      "loss": 0.1998,
      "step": 51290
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.016754793003201485,
      "learning_rate": 1.79216e-06,
      "loss": 0.2007,
      "step": 51300
    },
    {
      "epoch": 1.64192,
      "grad_norm": 0.016133323311805725,
      "learning_rate": 1.79056e-06,
      "loss": 0.1993,
      "step": 51310
    },
    {
      "epoch": 1.6422400000000001,
      "grad_norm": 0.01682920940220356,
      "learning_rate": 1.78896e-06,
      "loss": 0.1992,
      "step": 51320
    },
    {
      "epoch": 1.64256,
      "grad_norm": 0.018748899921774864,
      "learning_rate": 1.7873600000000002e-06,
      "loss": 0.1995,
      "step": 51330
    },
    {
      "epoch": 1.64288,
      "grad_norm": 0.017240596935153008,
      "learning_rate": 1.7857600000000003e-06,
      "loss": 0.2035,
      "step": 51340
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.13084375858306885,
      "learning_rate": 1.78416e-06,
      "loss": 0.2004,
      "step": 51350
    },
    {
      "epoch": 1.64352,
      "grad_norm": 0.03642251715064049,
      "learning_rate": 1.78256e-06,
      "loss": 0.1992,
      "step": 51360
    },
    {
      "epoch": 1.64384,
      "grad_norm": 0.048421911895275116,
      "learning_rate": 1.7809600000000001e-06,
      "loss": 0.1994,
      "step": 51370
    },
    {
      "epoch": 1.6441599999999998,
      "grad_norm": 0.34287315607070923,
      "learning_rate": 1.7793600000000002e-06,
      "loss": 0.1996,
      "step": 51380
    },
    {
      "epoch": 1.6444800000000002,
      "grad_norm": 0.014864877797663212,
      "learning_rate": 1.7777600000000001e-06,
      "loss": 0.1992,
      "step": 51390
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.009739069268107414,
      "learning_rate": 1.7761600000000002e-06,
      "loss": 0.2157,
      "step": 51400
    },
    {
      "epoch": 1.64512,
      "grad_norm": 0.027012638747692108,
      "learning_rate": 1.77456e-06,
      "loss": 0.1992,
      "step": 51410
    },
    {
      "epoch": 1.64544,
      "grad_norm": 0.0343327522277832,
      "learning_rate": 1.7729600000000002e-06,
      "loss": 0.1991,
      "step": 51420
    },
    {
      "epoch": 1.6457600000000001,
      "grad_norm": 0.02353091910481453,
      "learning_rate": 1.77136e-06,
      "loss": 0.1993,
      "step": 51430
    },
    {
      "epoch": 1.64608,
      "grad_norm": 0.032264336943626404,
      "learning_rate": 1.7697600000000001e-06,
      "loss": 0.2149,
      "step": 51440
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.03865766525268555,
      "learning_rate": 1.7681600000000002e-06,
      "loss": 0.1992,
      "step": 51450
    },
    {
      "epoch": 1.64672,
      "grad_norm": 0.028118066489696503,
      "learning_rate": 1.7665600000000001e-06,
      "loss": 0.1992,
      "step": 51460
    },
    {
      "epoch": 1.64704,
      "grad_norm": 0.06978343427181244,
      "learning_rate": 1.76496e-06,
      "loss": 0.1991,
      "step": 51470
    },
    {
      "epoch": 1.64736,
      "grad_norm": 0.03009628877043724,
      "learning_rate": 1.76336e-06,
      "loss": 0.1994,
      "step": 51480
    },
    {
      "epoch": 1.64768,
      "grad_norm": 0.023089198395609856,
      "learning_rate": 1.7617600000000002e-06,
      "loss": 0.2037,
      "step": 51490
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.012595362961292267,
      "learning_rate": 1.7601600000000003e-06,
      "loss": 0.1992,
      "step": 51500
    },
    {
      "epoch": 1.64832,
      "grad_norm": 0.040200669318437576,
      "learning_rate": 1.7585600000000004e-06,
      "loss": 0.199,
      "step": 51510
    },
    {
      "epoch": 1.6486399999999999,
      "grad_norm": 0.013270538300275803,
      "learning_rate": 1.75696e-06,
      "loss": 0.2155,
      "step": 51520
    },
    {
      "epoch": 1.64896,
      "grad_norm": 0.029215214774012566,
      "learning_rate": 1.7553600000000001e-06,
      "loss": 0.2004,
      "step": 51530
    },
    {
      "epoch": 1.64928,
      "grad_norm": 0.017672911286354065,
      "learning_rate": 1.7537600000000002e-06,
      "loss": 0.2119,
      "step": 51540
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.009541266597807407,
      "learning_rate": 1.7521600000000003e-06,
      "loss": 0.1994,
      "step": 51550
    },
    {
      "epoch": 1.64992,
      "grad_norm": 0.8106337785720825,
      "learning_rate": 1.75056e-06,
      "loss": 0.2156,
      "step": 51560
    },
    {
      "epoch": 1.6502400000000002,
      "grad_norm": 0.030133379623293877,
      "learning_rate": 1.74896e-06,
      "loss": 0.1991,
      "step": 51570
    },
    {
      "epoch": 1.65056,
      "grad_norm": 0.03192146494984627,
      "learning_rate": 1.7473600000000002e-06,
      "loss": 0.1991,
      "step": 51580
    },
    {
      "epoch": 1.65088,
      "grad_norm": 0.10320807993412018,
      "learning_rate": 1.7457600000000002e-06,
      "loss": 0.1994,
      "step": 51590
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.013179226778447628,
      "learning_rate": 1.74416e-06,
      "loss": 0.1993,
      "step": 51600
    },
    {
      "epoch": 1.65152,
      "grad_norm": 0.011640018783509731,
      "learning_rate": 1.74256e-06,
      "loss": 0.1991,
      "step": 51610
    },
    {
      "epoch": 1.65184,
      "grad_norm": 0.2147599309682846,
      "learning_rate": 1.74096e-06,
      "loss": 0.2074,
      "step": 51620
    },
    {
      "epoch": 1.6521599999999999,
      "grad_norm": 0.027660327032208443,
      "learning_rate": 1.7393600000000002e-06,
      "loss": 0.2044,
      "step": 51630
    },
    {
      "epoch": 1.65248,
      "grad_norm": 0.022533254697918892,
      "learning_rate": 1.7377600000000003e-06,
      "loss": 0.1993,
      "step": 51640
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.019438521936535835,
      "learning_rate": 1.7361600000000002e-06,
      "loss": 0.1992,
      "step": 51650
    },
    {
      "epoch": 1.65312,
      "grad_norm": 0.035208333283662796,
      "learning_rate": 1.73456e-06,
      "loss": 0.1993,
      "step": 51660
    },
    {
      "epoch": 1.65344,
      "grad_norm": 0.024583453312516212,
      "learning_rate": 1.7329600000000001e-06,
      "loss": 0.1992,
      "step": 51670
    },
    {
      "epoch": 1.6537600000000001,
      "grad_norm": 0.02878543548285961,
      "learning_rate": 1.7313600000000002e-06,
      "loss": 0.1991,
      "step": 51680
    },
    {
      "epoch": 1.65408,
      "grad_norm": 0.05190427228808403,
      "learning_rate": 1.72976e-06,
      "loss": 0.2148,
      "step": 51690
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.061950575560331345,
      "learning_rate": 1.7281600000000002e-06,
      "loss": 0.2033,
      "step": 51700
    },
    {
      "epoch": 1.65472,
      "grad_norm": 0.06508579105138779,
      "learning_rate": 1.72656e-06,
      "loss": 0.1995,
      "step": 51710
    },
    {
      "epoch": 1.65504,
      "grad_norm": 0.029970692470669746,
      "learning_rate": 1.7249600000000002e-06,
      "loss": 0.1992,
      "step": 51720
    },
    {
      "epoch": 1.65536,
      "grad_norm": 0.04539983719587326,
      "learning_rate": 1.72336e-06,
      "loss": 0.1994,
      "step": 51730
    },
    {
      "epoch": 1.65568,
      "grad_norm": 0.023196276277303696,
      "learning_rate": 1.7217600000000001e-06,
      "loss": 0.1993,
      "step": 51740
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.019583197310566902,
      "learning_rate": 1.7201600000000002e-06,
      "loss": 0.199,
      "step": 51750
    },
    {
      "epoch": 1.65632,
      "grad_norm": 0.020735476166009903,
      "learning_rate": 1.7185600000000003e-06,
      "loss": 0.2064,
      "step": 51760
    },
    {
      "epoch": 1.65664,
      "grad_norm": 0.02415846846997738,
      "learning_rate": 1.71696e-06,
      "loss": 0.1991,
      "step": 51770
    },
    {
      "epoch": 1.65696,
      "grad_norm": 0.03551020845770836,
      "learning_rate": 1.71536e-06,
      "loss": 0.199,
      "step": 51780
    },
    {
      "epoch": 1.65728,
      "grad_norm": 0.013198710978031158,
      "learning_rate": 1.7137600000000002e-06,
      "loss": 0.1992,
      "step": 51790
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.024434493854641914,
      "learning_rate": 1.7121600000000003e-06,
      "loss": 0.2068,
      "step": 51800
    },
    {
      "epoch": 1.6579199999999998,
      "grad_norm": 0.0210794098675251,
      "learning_rate": 1.7105600000000004e-06,
      "loss": 0.2132,
      "step": 51810
    },
    {
      "epoch": 1.6582400000000002,
      "grad_norm": 0.025072742253541946,
      "learning_rate": 1.70896e-06,
      "loss": 0.1991,
      "step": 51820
    },
    {
      "epoch": 1.65856,
      "grad_norm": 0.027380643412470818,
      "learning_rate": 1.7073600000000001e-06,
      "loss": 0.1996,
      "step": 51830
    },
    {
      "epoch": 1.65888,
      "grad_norm": 0.018555352464318275,
      "learning_rate": 1.7057600000000002e-06,
      "loss": 0.1991,
      "step": 51840
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.01510588452219963,
      "learning_rate": 1.7041600000000003e-06,
      "loss": 0.1992,
      "step": 51850
    },
    {
      "epoch": 1.65952,
      "grad_norm": 0.03334418311715126,
      "learning_rate": 1.70256e-06,
      "loss": 0.1994,
      "step": 51860
    },
    {
      "epoch": 1.65984,
      "grad_norm": 0.030703330412507057,
      "learning_rate": 1.70096e-06,
      "loss": 0.1992,
      "step": 51870
    },
    {
      "epoch": 1.6601599999999999,
      "grad_norm": 0.02887941710650921,
      "learning_rate": 1.6993600000000002e-06,
      "loss": 0.1995,
      "step": 51880
    },
    {
      "epoch": 1.66048,
      "grad_norm": 0.022544588893651962,
      "learning_rate": 1.6977600000000002e-06,
      "loss": 0.1993,
      "step": 51890
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.031493037939071655,
      "learning_rate": 1.6961600000000001e-06,
      "loss": 0.2154,
      "step": 51900
    },
    {
      "epoch": 1.66112,
      "grad_norm": 0.018971862271428108,
      "learning_rate": 1.69456e-06,
      "loss": 0.2004,
      "step": 51910
    },
    {
      "epoch": 1.66144,
      "grad_norm": 0.012808818370103836,
      "learning_rate": 1.69296e-06,
      "loss": 0.1992,
      "step": 51920
    },
    {
      "epoch": 1.6617600000000001,
      "grad_norm": 0.04904373362660408,
      "learning_rate": 1.6913600000000002e-06,
      "loss": 0.1992,
      "step": 51930
    },
    {
      "epoch": 1.66208,
      "grad_norm": 0.024429460987448692,
      "learning_rate": 1.68976e-06,
      "loss": 0.1998,
      "step": 51940
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.012470845133066177,
      "learning_rate": 1.6881600000000002e-06,
      "loss": 0.1992,
      "step": 51950
    },
    {
      "epoch": 1.66272,
      "grad_norm": 0.011452280916273594,
      "learning_rate": 1.6865600000000002e-06,
      "loss": 0.1991,
      "step": 51960
    },
    {
      "epoch": 1.66304,
      "grad_norm": 0.03997383266687393,
      "learning_rate": 1.6849600000000001e-06,
      "loss": 0.2124,
      "step": 51970
    },
    {
      "epoch": 1.66336,
      "grad_norm": 0.02138795703649521,
      "learning_rate": 1.6833600000000002e-06,
      "loss": 0.1992,
      "step": 51980
    },
    {
      "epoch": 1.66368,
      "grad_norm": 0.012327209115028381,
      "learning_rate": 1.68176e-06,
      "loss": 0.1992,
      "step": 51990
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.16726121306419373,
      "learning_rate": 1.6801600000000002e-06,
      "loss": 0.1996,
      "step": 52000
    },
    {
      "epoch": 1.6640000000000001,
      "eval_runtime": 57.2584,
      "eval_samples_per_second": 174.647,
      "eval_steps_per_second": 10.915,
      "step": 52000
    },
    {
      "epoch": 1.66432,
      "grad_norm": 0.033953554928302765,
      "learning_rate": 1.6785600000000003e-06,
      "loss": 0.1992,
      "step": 52010
    },
    {
      "epoch": 1.66464,
      "grad_norm": 0.03793976828455925,
      "learning_rate": 1.6769600000000002e-06,
      "loss": 0.1991,
      "step": 52020
    },
    {
      "epoch": 1.66496,
      "grad_norm": 0.017574360594153404,
      "learning_rate": 1.67536e-06,
      "loss": 0.1992,
      "step": 52030
    },
    {
      "epoch": 1.66528,
      "grad_norm": 0.021628234535455704,
      "learning_rate": 1.6737600000000001e-06,
      "loss": 0.2141,
      "step": 52040
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.051961127668619156,
      "learning_rate": 1.6721600000000002e-06,
      "loss": 0.2005,
      "step": 52050
    },
    {
      "epoch": 1.6659199999999998,
      "grad_norm": 0.028087247163057327,
      "learning_rate": 1.6705600000000003e-06,
      "loss": 0.199,
      "step": 52060
    },
    {
      "epoch": 1.6662400000000002,
      "grad_norm": 0.016258489340543747,
      "learning_rate": 1.66896e-06,
      "loss": 0.1991,
      "step": 52070
    },
    {
      "epoch": 1.66656,
      "grad_norm": 0.01261043269187212,
      "learning_rate": 1.66736e-06,
      "loss": 0.1991,
      "step": 52080
    },
    {
      "epoch": 1.66688,
      "grad_norm": 0.028064293786883354,
      "learning_rate": 1.6657600000000002e-06,
      "loss": 0.2078,
      "step": 52090
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.015988344326615334,
      "learning_rate": 1.6641600000000003e-06,
      "loss": 0.1993,
      "step": 52100
    },
    {
      "epoch": 1.6675200000000001,
      "grad_norm": 0.04546792060136795,
      "learning_rate": 1.66256e-06,
      "loss": 0.1992,
      "step": 52110
    },
    {
      "epoch": 1.66784,
      "grad_norm": 0.06197088584303856,
      "learning_rate": 1.66096e-06,
      "loss": 0.1993,
      "step": 52120
    },
    {
      "epoch": 1.6681599999999999,
      "grad_norm": 0.049808669835329056,
      "learning_rate": 1.6593600000000001e-06,
      "loss": 0.1997,
      "step": 52130
    },
    {
      "epoch": 1.66848,
      "grad_norm": 0.03972325474023819,
      "learning_rate": 1.6577600000000002e-06,
      "loss": 0.1993,
      "step": 52140
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.03082365356385708,
      "learning_rate": 1.6561600000000003e-06,
      "loss": 0.1992,
      "step": 52150
    },
    {
      "epoch": 1.66912,
      "grad_norm": 0.022861983627080917,
      "learning_rate": 1.65456e-06,
      "loss": 0.1998,
      "step": 52160
    },
    {
      "epoch": 1.66944,
      "grad_norm": 0.0168687105178833,
      "learning_rate": 1.65296e-06,
      "loss": 0.1992,
      "step": 52170
    },
    {
      "epoch": 1.6697600000000001,
      "grad_norm": 0.04099682345986366,
      "learning_rate": 1.6513600000000001e-06,
      "loss": 0.1992,
      "step": 52180
    },
    {
      "epoch": 1.67008,
      "grad_norm": 0.015964286401867867,
      "learning_rate": 1.6497600000000002e-06,
      "loss": 0.199,
      "step": 52190
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.020657513290643692,
      "learning_rate": 1.6481600000000001e-06,
      "loss": 0.2234,
      "step": 52200
    },
    {
      "epoch": 1.67072,
      "grad_norm": 0.030313406139612198,
      "learning_rate": 1.6465600000000002e-06,
      "loss": 0.1992,
      "step": 52210
    },
    {
      "epoch": 1.67104,
      "grad_norm": 0.03097407892346382,
      "learning_rate": 1.64496e-06,
      "loss": 0.1992,
      "step": 52220
    },
    {
      "epoch": 1.67136,
      "grad_norm": 0.01610204018652439,
      "learning_rate": 1.6433600000000002e-06,
      "loss": 0.1993,
      "step": 52230
    },
    {
      "epoch": 1.67168,
      "grad_norm": 0.014507687650620937,
      "learning_rate": 1.64176e-06,
      "loss": 0.1991,
      "step": 52240
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.0703466534614563,
      "learning_rate": 1.6401600000000001e-06,
      "loss": 0.1992,
      "step": 52250
    },
    {
      "epoch": 1.67232,
      "grad_norm": 0.0147660281509161,
      "learning_rate": 1.6385600000000002e-06,
      "loss": 0.2068,
      "step": 52260
    },
    {
      "epoch": 1.67264,
      "grad_norm": 1.2487456798553467,
      "learning_rate": 1.6369600000000001e-06,
      "loss": 0.2127,
      "step": 52270
    },
    {
      "epoch": 1.67296,
      "grad_norm": 0.03171896934509277,
      "learning_rate": 1.63536e-06,
      "loss": 0.1992,
      "step": 52280
    },
    {
      "epoch": 1.67328,
      "grad_norm": 0.02475842833518982,
      "learning_rate": 1.63376e-06,
      "loss": 0.1993,
      "step": 52290
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.01650964841246605,
      "learning_rate": 1.6321600000000002e-06,
      "loss": 0.199,
      "step": 52300
    },
    {
      "epoch": 1.6739199999999999,
      "grad_norm": 0.02008114941418171,
      "learning_rate": 1.6305600000000003e-06,
      "loss": 0.1997,
      "step": 52310
    },
    {
      "epoch": 1.67424,
      "grad_norm": 0.030463527888059616,
      "learning_rate": 1.6289600000000004e-06,
      "loss": 0.2029,
      "step": 52320
    },
    {
      "epoch": 1.67456,
      "grad_norm": 0.01472310721874237,
      "learning_rate": 1.62736e-06,
      "loss": 0.1992,
      "step": 52330
    },
    {
      "epoch": 1.67488,
      "grad_norm": 0.018539419397711754,
      "learning_rate": 1.6257600000000001e-06,
      "loss": 0.1993,
      "step": 52340
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.025413593277335167,
      "learning_rate": 1.6241600000000002e-06,
      "loss": 0.2058,
      "step": 52350
    },
    {
      "epoch": 1.6755200000000001,
      "grad_norm": 0.038196101784706116,
      "learning_rate": 1.6225600000000003e-06,
      "loss": 0.1991,
      "step": 52360
    },
    {
      "epoch": 1.67584,
      "grad_norm": 0.013184740208089352,
      "learning_rate": 1.62096e-06,
      "loss": 0.1991,
      "step": 52370
    },
    {
      "epoch": 1.6761599999999999,
      "grad_norm": 0.02619919367134571,
      "learning_rate": 1.61936e-06,
      "loss": 0.2122,
      "step": 52380
    },
    {
      "epoch": 1.67648,
      "grad_norm": 0.9062984585762024,
      "learning_rate": 1.6177600000000002e-06,
      "loss": 0.2133,
      "step": 52390
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.041617635637521744,
      "learning_rate": 1.6161600000000003e-06,
      "loss": 0.1994,
      "step": 52400
    },
    {
      "epoch": 1.67712,
      "grad_norm": 0.011457360349595547,
      "learning_rate": 1.61456e-06,
      "loss": 0.209,
      "step": 52410
    },
    {
      "epoch": 1.67744,
      "grad_norm": 0.0216317530721426,
      "learning_rate": 1.61296e-06,
      "loss": 0.1995,
      "step": 52420
    },
    {
      "epoch": 1.6777600000000001,
      "grad_norm": 0.02619718201458454,
      "learning_rate": 1.61136e-06,
      "loss": 0.1992,
      "step": 52430
    },
    {
      "epoch": 1.67808,
      "grad_norm": 0.040808502584695816,
      "learning_rate": 1.6097600000000002e-06,
      "loss": 0.199,
      "step": 52440
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.028908252716064453,
      "learning_rate": 1.60816e-06,
      "loss": 0.1992,
      "step": 52450
    },
    {
      "epoch": 1.67872,
      "grad_norm": 0.026568325236439705,
      "learning_rate": 1.6065600000000002e-06,
      "loss": 0.1992,
      "step": 52460
    },
    {
      "epoch": 1.67904,
      "grad_norm": 0.03285335376858711,
      "learning_rate": 1.60496e-06,
      "loss": 0.1993,
      "step": 52470
    },
    {
      "epoch": 1.67936,
      "grad_norm": 0.01658301241695881,
      "learning_rate": 1.6033600000000001e-06,
      "loss": 0.1992,
      "step": 52480
    },
    {
      "epoch": 1.6796799999999998,
      "grad_norm": 0.020398303866386414,
      "learning_rate": 1.6017600000000002e-06,
      "loss": 0.2068,
      "step": 52490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.025052379816770554,
      "learning_rate": 1.6001600000000001e-06,
      "loss": 0.2039,
      "step": 52500
    },
    {
      "epoch": 1.68032,
      "grad_norm": 0.021717559546232224,
      "learning_rate": 1.5985600000000002e-06,
      "loss": 0.1993,
      "step": 52510
    },
    {
      "epoch": 1.68064,
      "grad_norm": 0.013425217941403389,
      "learning_rate": 1.59696e-06,
      "loss": 0.1993,
      "step": 52520
    },
    {
      "epoch": 1.68096,
      "grad_norm": 0.012527873739600182,
      "learning_rate": 1.5953600000000002e-06,
      "loss": 0.1997,
      "step": 52530
    },
    {
      "epoch": 1.68128,
      "grad_norm": 0.03421427309513092,
      "learning_rate": 1.59376e-06,
      "loss": 0.1991,
      "step": 52540
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.19421899318695068,
      "learning_rate": 1.5921600000000001e-06,
      "loss": 0.1997,
      "step": 52550
    },
    {
      "epoch": 1.6819199999999999,
      "grad_norm": 0.015055363066494465,
      "learning_rate": 1.5905600000000002e-06,
      "loss": 0.1992,
      "step": 52560
    },
    {
      "epoch": 1.68224,
      "grad_norm": 0.014486252330243587,
      "learning_rate": 1.5889600000000003e-06,
      "loss": 0.216,
      "step": 52570
    },
    {
      "epoch": 1.68256,
      "grad_norm": 0.028524892404675484,
      "learning_rate": 1.58736e-06,
      "loss": 0.215,
      "step": 52580
    },
    {
      "epoch": 1.68288,
      "grad_norm": 0.016463764011859894,
      "learning_rate": 1.58576e-06,
      "loss": 0.1991,
      "step": 52590
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.03684869781136513,
      "learning_rate": 1.5841600000000002e-06,
      "loss": 0.2122,
      "step": 52600
    },
    {
      "epoch": 1.6835200000000001,
      "grad_norm": 0.010254424065351486,
      "learning_rate": 1.5825600000000003e-06,
      "loss": 0.2142,
      "step": 52610
    },
    {
      "epoch": 1.68384,
      "grad_norm": 0.03181932121515274,
      "learning_rate": 1.58096e-06,
      "loss": 0.2015,
      "step": 52620
    },
    {
      "epoch": 1.6841599999999999,
      "grad_norm": 0.03909687325358391,
      "learning_rate": 1.57936e-06,
      "loss": 0.1993,
      "step": 52630
    },
    {
      "epoch": 1.68448,
      "grad_norm": 0.16063661873340607,
      "learning_rate": 1.5777600000000001e-06,
      "loss": 0.1994,
      "step": 52640
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.12536446750164032,
      "learning_rate": 1.5761600000000002e-06,
      "loss": 0.2001,
      "step": 52650
    },
    {
      "epoch": 1.68512,
      "grad_norm": 0.023725582286715508,
      "learning_rate": 1.5745600000000003e-06,
      "loss": 0.2155,
      "step": 52660
    },
    {
      "epoch": 1.68544,
      "grad_norm": 0.018818886950612068,
      "learning_rate": 1.57296e-06,
      "loss": 0.1993,
      "step": 52670
    },
    {
      "epoch": 1.6857600000000001,
      "grad_norm": 0.016132637858390808,
      "learning_rate": 1.57136e-06,
      "loss": 0.1993,
      "step": 52680
    },
    {
      "epoch": 1.68608,
      "grad_norm": 0.04105941206216812,
      "learning_rate": 1.5697600000000002e-06,
      "loss": 0.1993,
      "step": 52690
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.030522536486387253,
      "learning_rate": 1.5681600000000002e-06,
      "loss": 0.1992,
      "step": 52700
    },
    {
      "epoch": 1.68672,
      "grad_norm": 0.023482615128159523,
      "learning_rate": 1.5665600000000001e-06,
      "loss": 0.1992,
      "step": 52710
    },
    {
      "epoch": 1.68704,
      "grad_norm": 0.021276311948895454,
      "learning_rate": 1.56496e-06,
      "loss": 0.1991,
      "step": 52720
    },
    {
      "epoch": 1.68736,
      "grad_norm": 0.03287361189723015,
      "learning_rate": 1.56336e-06,
      "loss": 0.2035,
      "step": 52730
    },
    {
      "epoch": 1.6876799999999998,
      "grad_norm": 0.019408483058214188,
      "learning_rate": 1.5617600000000002e-06,
      "loss": 0.2016,
      "step": 52740
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.011047011241316795,
      "learning_rate": 1.56016e-06,
      "loss": 0.2105,
      "step": 52750
    },
    {
      "epoch": 1.68832,
      "grad_norm": 0.019751962274312973,
      "learning_rate": 1.5585600000000002e-06,
      "loss": 0.1993,
      "step": 52760
    },
    {
      "epoch": 1.68864,
      "grad_norm": 0.013197429478168488,
      "learning_rate": 1.55696e-06,
      "loss": 0.2011,
      "step": 52770
    },
    {
      "epoch": 1.68896,
      "grad_norm": 0.05791088566184044,
      "learning_rate": 1.5553600000000001e-06,
      "loss": 0.1992,
      "step": 52780
    },
    {
      "epoch": 1.6892800000000001,
      "grad_norm": 0.043006058782339096,
      "learning_rate": 1.55376e-06,
      "loss": 0.1991,
      "step": 52790
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.03703932836651802,
      "learning_rate": 1.55216e-06,
      "loss": 0.1995,
      "step": 52800
    },
    {
      "epoch": 1.6899199999999999,
      "grad_norm": 0.022876763716340065,
      "learning_rate": 1.5505600000000002e-06,
      "loss": 0.1998,
      "step": 52810
    },
    {
      "epoch": 1.69024,
      "grad_norm": 0.018238235265016556,
      "learning_rate": 1.5489600000000003e-06,
      "loss": 0.203,
      "step": 52820
    },
    {
      "epoch": 1.69056,
      "grad_norm": 0.022189682349562645,
      "learning_rate": 1.5473600000000002e-06,
      "loss": 0.1992,
      "step": 52830
    },
    {
      "epoch": 1.69088,
      "grad_norm": 0.02490188740193844,
      "learning_rate": 1.54576e-06,
      "loss": 0.2202,
      "step": 52840
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.019160741940140724,
      "learning_rate": 1.5441600000000001e-06,
      "loss": 0.1991,
      "step": 52850
    },
    {
      "epoch": 1.6915200000000001,
      "grad_norm": 0.011221140623092651,
      "learning_rate": 1.5425600000000002e-06,
      "loss": 0.1991,
      "step": 52860
    },
    {
      "epoch": 1.69184,
      "grad_norm": 0.02321522869169712,
      "learning_rate": 1.5409600000000003e-06,
      "loss": 0.2069,
      "step": 52870
    },
    {
      "epoch": 1.6921599999999999,
      "grad_norm": 0.046153515577316284,
      "learning_rate": 1.53936e-06,
      "loss": 0.2017,
      "step": 52880
    },
    {
      "epoch": 1.69248,
      "grad_norm": 0.01601889543235302,
      "learning_rate": 1.53776e-06,
      "loss": 0.2148,
      "step": 52890
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.022489376366138458,
      "learning_rate": 1.5361600000000002e-06,
      "loss": 0.2005,
      "step": 52900
    },
    {
      "epoch": 1.69312,
      "grad_norm": 0.011111409403383732,
      "learning_rate": 1.5345600000000003e-06,
      "loss": 0.2006,
      "step": 52910
    },
    {
      "epoch": 1.6934399999999998,
      "grad_norm": 0.024318242445588112,
      "learning_rate": 1.53296e-06,
      "loss": 0.1993,
      "step": 52920
    },
    {
      "epoch": 1.6937600000000002,
      "grad_norm": 0.02088608406484127,
      "learning_rate": 1.53136e-06,
      "loss": 0.219,
      "step": 52930
    },
    {
      "epoch": 1.69408,
      "grad_norm": 0.04928137734532356,
      "learning_rate": 1.5297600000000001e-06,
      "loss": 0.1991,
      "step": 52940
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.02452120929956436,
      "learning_rate": 1.5281600000000002e-06,
      "loss": 0.1991,
      "step": 52950
    },
    {
      "epoch": 1.69472,
      "grad_norm": 0.055390629917383194,
      "learning_rate": 1.52656e-06,
      "loss": 0.1999,
      "step": 52960
    },
    {
      "epoch": 1.69504,
      "grad_norm": 0.016217002645134926,
      "learning_rate": 1.52496e-06,
      "loss": 0.2045,
      "step": 52970
    },
    {
      "epoch": 1.69536,
      "grad_norm": 0.03804857283830643,
      "learning_rate": 1.52336e-06,
      "loss": 0.1992,
      "step": 52980
    },
    {
      "epoch": 1.6956799999999999,
      "grad_norm": 0.896769106388092,
      "learning_rate": 1.5217600000000001e-06,
      "loss": 0.2301,
      "step": 52990
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.03865014761686325,
      "learning_rate": 1.5201600000000002e-06,
      "loss": 0.1991,
      "step": 53000
    },
    {
      "epoch": 1.696,
      "eval_runtime": 57.4421,
      "eval_samples_per_second": 174.088,
      "eval_steps_per_second": 10.881,
      "step": 53000
    },
    {
      "epoch": 1.69632,
      "grad_norm": 0.015237927436828613,
      "learning_rate": 1.5185600000000001e-06,
      "loss": 0.1996,
      "step": 53010
    },
    {
      "epoch": 1.69664,
      "grad_norm": 0.017150167375802994,
      "learning_rate": 1.51696e-06,
      "loss": 0.1993,
      "step": 53020
    },
    {
      "epoch": 1.69696,
      "grad_norm": 0.017826730385422707,
      "learning_rate": 1.51536e-06,
      "loss": 0.2095,
      "step": 53030
    },
    {
      "epoch": 1.6972800000000001,
      "grad_norm": 0.08573032915592194,
      "learning_rate": 1.5137600000000002e-06,
      "loss": 0.1994,
      "step": 53040
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.01223914884030819,
      "learning_rate": 1.51216e-06,
      "loss": 0.2025,
      "step": 53050
    },
    {
      "epoch": 1.6979199999999999,
      "grad_norm": 0.040218502283096313,
      "learning_rate": 1.5105600000000002e-06,
      "loss": 0.1994,
      "step": 53060
    },
    {
      "epoch": 1.69824,
      "grad_norm": 0.10721974074840546,
      "learning_rate": 1.5089600000000002e-06,
      "loss": 0.1995,
      "step": 53070
    },
    {
      "epoch": 1.69856,
      "grad_norm": 0.022492369636893272,
      "learning_rate": 1.5073600000000001e-06,
      "loss": 0.2025,
      "step": 53080
    },
    {
      "epoch": 1.69888,
      "grad_norm": 0.7345892786979675,
      "learning_rate": 1.50576e-06,
      "loss": 0.217,
      "step": 53090
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.049711957573890686,
      "learning_rate": 1.50416e-06,
      "loss": 0.1991,
      "step": 53100
    },
    {
      "epoch": 1.6995200000000001,
      "grad_norm": 0.033233847469091415,
      "learning_rate": 1.5025600000000002e-06,
      "loss": 0.1991,
      "step": 53110
    },
    {
      "epoch": 1.69984,
      "grad_norm": 0.024871645495295525,
      "learning_rate": 1.5009600000000003e-06,
      "loss": 0.2075,
      "step": 53120
    },
    {
      "epoch": 1.70016,
      "grad_norm": 0.030846435576677322,
      "learning_rate": 1.4993600000000002e-06,
      "loss": 0.1991,
      "step": 53130
    },
    {
      "epoch": 1.70048,
      "grad_norm": 0.027823293581604958,
      "learning_rate": 1.49776e-06,
      "loss": 0.2098,
      "step": 53140
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.03367672860622406,
      "learning_rate": 1.4961600000000001e-06,
      "loss": 0.2003,
      "step": 53150
    },
    {
      "epoch": 1.70112,
      "grad_norm": 0.017967628315091133,
      "learning_rate": 1.4945600000000002e-06,
      "loss": 0.1992,
      "step": 53160
    },
    {
      "epoch": 1.7014399999999998,
      "grad_norm": 0.030380887910723686,
      "learning_rate": 1.4929600000000003e-06,
      "loss": 0.2014,
      "step": 53170
    },
    {
      "epoch": 1.7017600000000002,
      "grad_norm": 0.013697216287255287,
      "learning_rate": 1.49136e-06,
      "loss": 0.206,
      "step": 53180
    },
    {
      "epoch": 1.70208,
      "grad_norm": 0.011624063365161419,
      "learning_rate": 1.48976e-06,
      "loss": 0.199,
      "step": 53190
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.09576182812452316,
      "learning_rate": 1.4881600000000002e-06,
      "loss": 0.1993,
      "step": 53200
    },
    {
      "epoch": 1.70272,
      "grad_norm": 0.042492084205150604,
      "learning_rate": 1.4865600000000003e-06,
      "loss": 0.1991,
      "step": 53210
    },
    {
      "epoch": 1.70304,
      "grad_norm": 0.03946441784501076,
      "learning_rate": 1.48496e-06,
      "loss": 0.1992,
      "step": 53220
    },
    {
      "epoch": 1.70336,
      "grad_norm": 0.012554943561553955,
      "learning_rate": 1.48336e-06,
      "loss": 0.1993,
      "step": 53230
    },
    {
      "epoch": 1.7036799999999999,
      "grad_norm": 0.01859266124665737,
      "learning_rate": 1.4817600000000001e-06,
      "loss": 0.1996,
      "step": 53240
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.013461973518133163,
      "learning_rate": 1.4801600000000002e-06,
      "loss": 0.1996,
      "step": 53250
    },
    {
      "epoch": 1.70432,
      "grad_norm": 0.0134951863437891,
      "learning_rate": 1.47856e-06,
      "loss": 0.2134,
      "step": 53260
    },
    {
      "epoch": 1.70464,
      "grad_norm": 0.01139000616967678,
      "learning_rate": 1.47696e-06,
      "loss": 0.1992,
      "step": 53270
    },
    {
      "epoch": 1.70496,
      "grad_norm": 0.018565896898508072,
      "learning_rate": 1.47536e-06,
      "loss": 0.2171,
      "step": 53280
    },
    {
      "epoch": 1.7052800000000001,
      "grad_norm": 0.0592036210000515,
      "learning_rate": 1.4737600000000001e-06,
      "loss": 0.1992,
      "step": 53290
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.024571895599365234,
      "learning_rate": 1.4721600000000002e-06,
      "loss": 0.199,
      "step": 53300
    },
    {
      "epoch": 1.7059199999999999,
      "grad_norm": 0.01252729818224907,
      "learning_rate": 1.4705600000000001e-06,
      "loss": 0.2167,
      "step": 53310
    },
    {
      "epoch": 1.70624,
      "grad_norm": 0.023424457758665085,
      "learning_rate": 1.4689600000000002e-06,
      "loss": 0.1999,
      "step": 53320
    },
    {
      "epoch": 1.70656,
      "grad_norm": 0.035444844514131546,
      "learning_rate": 1.46736e-06,
      "loss": 0.1991,
      "step": 53330
    },
    {
      "epoch": 1.70688,
      "grad_norm": 0.03628772124648094,
      "learning_rate": 1.4657600000000002e-06,
      "loss": 0.2153,
      "step": 53340
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.01864815689623356,
      "learning_rate": 1.46416e-06,
      "loss": 0.1993,
      "step": 53350
    },
    {
      "epoch": 1.7075200000000001,
      "grad_norm": 0.03131531551480293,
      "learning_rate": 1.4625600000000001e-06,
      "loss": 0.1992,
      "step": 53360
    },
    {
      "epoch": 1.70784,
      "grad_norm": 0.022148817777633667,
      "learning_rate": 1.4609600000000002e-06,
      "loss": 0.2079,
      "step": 53370
    },
    {
      "epoch": 1.70816,
      "grad_norm": 0.02561725489795208,
      "learning_rate": 1.4593600000000001e-06,
      "loss": 0.1992,
      "step": 53380
    },
    {
      "epoch": 1.70848,
      "grad_norm": 0.29662227630615234,
      "learning_rate": 1.45776e-06,
      "loss": 0.207,
      "step": 53390
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.013194099068641663,
      "learning_rate": 1.45616e-06,
      "loss": 0.1992,
      "step": 53400
    },
    {
      "epoch": 1.70912,
      "grad_norm": 0.013047355227172375,
      "learning_rate": 1.4545600000000002e-06,
      "loss": 0.1991,
      "step": 53410
    },
    {
      "epoch": 1.7094399999999998,
      "grad_norm": 0.00956933293491602,
      "learning_rate": 1.4529600000000003e-06,
      "loss": 0.202,
      "step": 53420
    },
    {
      "epoch": 1.70976,
      "grad_norm": 0.017122233286499977,
      "learning_rate": 1.45136e-06,
      "loss": 0.1993,
      "step": 53430
    },
    {
      "epoch": 1.71008,
      "grad_norm": 0.8563888072967529,
      "learning_rate": 1.44976e-06,
      "loss": 0.2171,
      "step": 53440
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.030207615345716476,
      "learning_rate": 1.4481600000000001e-06,
      "loss": 0.1992,
      "step": 53450
    },
    {
      "epoch": 1.71072,
      "grad_norm": 0.016764959320425987,
      "learning_rate": 1.4465600000000002e-06,
      "loss": 0.1992,
      "step": 53460
    },
    {
      "epoch": 1.7110400000000001,
      "grad_norm": 0.03264264389872551,
      "learning_rate": 1.4449600000000003e-06,
      "loss": 0.2216,
      "step": 53470
    },
    {
      "epoch": 1.71136,
      "grad_norm": 0.06534969061613083,
      "learning_rate": 1.44336e-06,
      "loss": 0.2102,
      "step": 53480
    },
    {
      "epoch": 1.7116799999999999,
      "grad_norm": 0.017067795619368553,
      "learning_rate": 1.44176e-06,
      "loss": 0.1992,
      "step": 53490
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.0718330517411232,
      "learning_rate": 1.4401600000000002e-06,
      "loss": 0.1994,
      "step": 53500
    },
    {
      "epoch": 1.71232,
      "grad_norm": 0.5797916054725647,
      "learning_rate": 1.4385600000000003e-06,
      "loss": 0.2013,
      "step": 53510
    },
    {
      "epoch": 1.71264,
      "grad_norm": 0.020552560687065125,
      "learning_rate": 1.43696e-06,
      "loss": 0.1991,
      "step": 53520
    },
    {
      "epoch": 1.71296,
      "grad_norm": 0.17527011036872864,
      "learning_rate": 1.43536e-06,
      "loss": 0.1997,
      "step": 53530
    },
    {
      "epoch": 1.7132800000000001,
      "grad_norm": 0.01805681362748146,
      "learning_rate": 1.43376e-06,
      "loss": 0.2145,
      "step": 53540
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.011228960007429123,
      "learning_rate": 1.4321600000000002e-06,
      "loss": 0.2016,
      "step": 53550
    },
    {
      "epoch": 1.7139199999999999,
      "grad_norm": 0.03295886516571045,
      "learning_rate": 1.43056e-06,
      "loss": 0.199,
      "step": 53560
    },
    {
      "epoch": 1.71424,
      "grad_norm": 0.029190396890044212,
      "learning_rate": 1.4289600000000002e-06,
      "loss": 0.1992,
      "step": 53570
    },
    {
      "epoch": 1.71456,
      "grad_norm": 0.036192942410707474,
      "learning_rate": 1.42736e-06,
      "loss": 0.1991,
      "step": 53580
    },
    {
      "epoch": 1.71488,
      "grad_norm": 0.020067112520337105,
      "learning_rate": 1.4257600000000001e-06,
      "loss": 0.1992,
      "step": 53590
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.03290942683815956,
      "learning_rate": 1.42416e-06,
      "loss": 0.199,
      "step": 53600
    },
    {
      "epoch": 1.7155200000000002,
      "grad_norm": 0.040819402784109116,
      "learning_rate": 1.4225600000000001e-06,
      "loss": 0.1993,
      "step": 53610
    },
    {
      "epoch": 1.71584,
      "grad_norm": 0.09977571666240692,
      "learning_rate": 1.4209600000000002e-06,
      "loss": 0.1992,
      "step": 53620
    },
    {
      "epoch": 1.71616,
      "grad_norm": 0.08300899714231491,
      "learning_rate": 1.41936e-06,
      "loss": 0.1993,
      "step": 53630
    },
    {
      "epoch": 1.71648,
      "grad_norm": 0.016099713742733,
      "learning_rate": 1.4177600000000002e-06,
      "loss": 0.2,
      "step": 53640
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.015541044063866138,
      "learning_rate": 1.41616e-06,
      "loss": 0.1991,
      "step": 53650
    },
    {
      "epoch": 1.71712,
      "grad_norm": 0.035822734236717224,
      "learning_rate": 1.4145600000000001e-06,
      "loss": 0.2156,
      "step": 53660
    },
    {
      "epoch": 1.7174399999999999,
      "grad_norm": 0.03729796037077904,
      "learning_rate": 1.4129600000000002e-06,
      "loss": 0.2,
      "step": 53670
    },
    {
      "epoch": 1.71776,
      "grad_norm": 0.012650399468839169,
      "learning_rate": 1.4113600000000003e-06,
      "loss": 0.1992,
      "step": 53680
    },
    {
      "epoch": 1.71808,
      "grad_norm": 0.0356765054166317,
      "learning_rate": 1.40976e-06,
      "loss": 0.1991,
      "step": 53690
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.028712941333651543,
      "learning_rate": 1.40816e-06,
      "loss": 0.1995,
      "step": 53700
    },
    {
      "epoch": 1.71872,
      "grad_norm": 1.8241809606552124,
      "learning_rate": 1.4065600000000002e-06,
      "loss": 0.2188,
      "step": 53710
    },
    {
      "epoch": 1.7190400000000001,
      "grad_norm": 0.031486041843891144,
      "learning_rate": 1.4049600000000003e-06,
      "loss": 0.199,
      "step": 53720
    },
    {
      "epoch": 1.71936,
      "grad_norm": 0.06518129259347916,
      "learning_rate": 1.40336e-06,
      "loss": 0.1991,
      "step": 53730
    },
    {
      "epoch": 1.7196799999999999,
      "grad_norm": 0.013487014919519424,
      "learning_rate": 1.40176e-06,
      "loss": 0.199,
      "step": 53740
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.010688572190701962,
      "learning_rate": 1.4001600000000001e-06,
      "loss": 0.1991,
      "step": 53750
    },
    {
      "epoch": 1.72032,
      "grad_norm": 0.037771351635456085,
      "learning_rate": 1.3985600000000002e-06,
      "loss": 0.2007,
      "step": 53760
    },
    {
      "epoch": 1.72064,
      "grad_norm": 0.04102255031466484,
      "learning_rate": 1.3969599999999999e-06,
      "loss": 0.1991,
      "step": 53770
    },
    {
      "epoch": 1.72096,
      "grad_norm": 0.025234762579202652,
      "learning_rate": 1.39536e-06,
      "loss": 0.1992,
      "step": 53780
    },
    {
      "epoch": 1.7212800000000001,
      "grad_norm": 0.02159011736512184,
      "learning_rate": 1.39376e-06,
      "loss": 0.1991,
      "step": 53790
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.03701936453580856,
      "learning_rate": 1.3921600000000002e-06,
      "loss": 0.2169,
      "step": 53800
    },
    {
      "epoch": 1.72192,
      "grad_norm": 0.016100265085697174,
      "learning_rate": 1.3905600000000002e-06,
      "loss": 0.1996,
      "step": 53810
    },
    {
      "epoch": 1.72224,
      "grad_norm": 0.04008927941322327,
      "learning_rate": 1.3889600000000001e-06,
      "loss": 0.2004,
      "step": 53820
    },
    {
      "epoch": 1.72256,
      "grad_norm": 0.03780050948262215,
      "learning_rate": 1.38736e-06,
      "loss": 0.2111,
      "step": 53830
    },
    {
      "epoch": 1.72288,
      "grad_norm": 0.04182139411568642,
      "learning_rate": 1.38576e-06,
      "loss": 0.2006,
      "step": 53840
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.019455241039395332,
      "learning_rate": 1.3841600000000002e-06,
      "loss": 0.2078,
      "step": 53850
    },
    {
      "epoch": 1.7235200000000002,
      "grad_norm": 0.026695488020777702,
      "learning_rate": 1.38256e-06,
      "loss": 0.1992,
      "step": 53860
    },
    {
      "epoch": 1.72384,
      "grad_norm": 0.025872332975268364,
      "learning_rate": 1.3809600000000002e-06,
      "loss": 0.1996,
      "step": 53870
    },
    {
      "epoch": 1.72416,
      "grad_norm": 0.012906842865049839,
      "learning_rate": 1.37936e-06,
      "loss": 0.2005,
      "step": 53880
    },
    {
      "epoch": 1.72448,
      "grad_norm": 0.014808892272412777,
      "learning_rate": 1.3777600000000001e-06,
      "loss": 0.1991,
      "step": 53890
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.017115291208028793,
      "learning_rate": 1.37616e-06,
      "loss": 0.2003,
      "step": 53900
    },
    {
      "epoch": 1.72512,
      "grad_norm": 0.02574499510228634,
      "learning_rate": 1.37456e-06,
      "loss": 0.2033,
      "step": 53910
    },
    {
      "epoch": 1.7254399999999999,
      "grad_norm": 0.030347222462296486,
      "learning_rate": 1.3729600000000002e-06,
      "loss": 0.2016,
      "step": 53920
    },
    {
      "epoch": 1.72576,
      "grad_norm": 0.04465007036924362,
      "learning_rate": 1.3713600000000003e-06,
      "loss": 0.2153,
      "step": 53930
    },
    {
      "epoch": 1.72608,
      "grad_norm": 0.018962152302265167,
      "learning_rate": 1.36976e-06,
      "loss": 0.1991,
      "step": 53940
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.012394224293529987,
      "learning_rate": 1.36816e-06,
      "loss": 0.208,
      "step": 53950
    },
    {
      "epoch": 1.72672,
      "grad_norm": 0.014374195598065853,
      "learning_rate": 1.3665600000000001e-06,
      "loss": 0.2006,
      "step": 53960
    },
    {
      "epoch": 1.7270400000000001,
      "grad_norm": 0.022242067381739616,
      "learning_rate": 1.3649600000000002e-06,
      "loss": 0.2149,
      "step": 53970
    },
    {
      "epoch": 1.72736,
      "grad_norm": 0.02025521546602249,
      "learning_rate": 1.3633600000000003e-06,
      "loss": 0.1991,
      "step": 53980
    },
    {
      "epoch": 1.7276799999999999,
      "grad_norm": 0.06013371795415878,
      "learning_rate": 1.36176e-06,
      "loss": 0.1994,
      "step": 53990
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.026073381304740906,
      "learning_rate": 1.36016e-06,
      "loss": 0.1992,
      "step": 54000
    },
    {
      "epoch": 1.728,
      "eval_runtime": 75.7566,
      "eval_samples_per_second": 132.002,
      "eval_steps_per_second": 8.25,
      "step": 54000
    },
    {
      "epoch": 1.72832,
      "grad_norm": 0.04676840081810951,
      "learning_rate": 1.3585600000000002e-06,
      "loss": 0.1992,
      "step": 54010
    },
    {
      "epoch": 1.72864,
      "grad_norm": 0.04139452055096626,
      "learning_rate": 1.3569600000000003e-06,
      "loss": 0.1994,
      "step": 54020
    },
    {
      "epoch": 1.72896,
      "grad_norm": 0.08430441468954086,
      "learning_rate": 1.35536e-06,
      "loss": 0.2043,
      "step": 54030
    },
    {
      "epoch": 1.7292800000000002,
      "grad_norm": 0.016682596877217293,
      "learning_rate": 1.35376e-06,
      "loss": 0.1994,
      "step": 54040
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.014589022845029831,
      "learning_rate": 1.3521600000000001e-06,
      "loss": 0.1991,
      "step": 54050
    },
    {
      "epoch": 1.72992,
      "grad_norm": 0.027022840455174446,
      "learning_rate": 1.3505600000000002e-06,
      "loss": 0.1992,
      "step": 54060
    },
    {
      "epoch": 1.73024,
      "grad_norm": 0.024528471753001213,
      "learning_rate": 1.34896e-06,
      "loss": 0.199,
      "step": 54070
    },
    {
      "epoch": 1.73056,
      "grad_norm": 0.025608045980334282,
      "learning_rate": 1.34736e-06,
      "loss": 0.1993,
      "step": 54080
    },
    {
      "epoch": 1.73088,
      "grad_norm": 0.022517800331115723,
      "learning_rate": 1.34576e-06,
      "loss": 0.199,
      "step": 54090
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.01679512858390808,
      "learning_rate": 1.3441600000000001e-06,
      "loss": 0.2111,
      "step": 54100
    },
    {
      "epoch": 1.73152,
      "grad_norm": 0.028094960376620293,
      "learning_rate": 1.34256e-06,
      "loss": 0.1992,
      "step": 54110
    },
    {
      "epoch": 1.73184,
      "grad_norm": 0.04222104698419571,
      "learning_rate": 1.3409600000000001e-06,
      "loss": 0.2005,
      "step": 54120
    },
    {
      "epoch": 1.73216,
      "grad_norm": 0.016827644780278206,
      "learning_rate": 1.3393600000000002e-06,
      "loss": 0.2162,
      "step": 54130
    },
    {
      "epoch": 1.73248,
      "grad_norm": 0.019466038793325424,
      "learning_rate": 1.33776e-06,
      "loss": 0.2151,
      "step": 54140
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.05367298796772957,
      "learning_rate": 1.3361600000000002e-06,
      "loss": 0.1992,
      "step": 54150
    },
    {
      "epoch": 1.73312,
      "grad_norm": 0.02796250209212303,
      "learning_rate": 1.33456e-06,
      "loss": 0.1993,
      "step": 54160
    },
    {
      "epoch": 1.7334399999999999,
      "grad_norm": 0.012433795258402824,
      "learning_rate": 1.3329600000000002e-06,
      "loss": 0.1992,
      "step": 54170
    },
    {
      "epoch": 1.73376,
      "grad_norm": 0.012393893674015999,
      "learning_rate": 1.3313600000000002e-06,
      "loss": 0.1992,
      "step": 54180
    },
    {
      "epoch": 1.73408,
      "grad_norm": 0.021178800612688065,
      "learning_rate": 1.3297600000000001e-06,
      "loss": 0.1995,
      "step": 54190
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.03283735737204552,
      "learning_rate": 1.32816e-06,
      "loss": 0.1993,
      "step": 54200
    },
    {
      "epoch": 1.73472,
      "grad_norm": 0.5129655003547668,
      "learning_rate": 1.32656e-06,
      "loss": 0.2141,
      "step": 54210
    },
    {
      "epoch": 1.7350400000000001,
      "grad_norm": 0.036222003400325775,
      "learning_rate": 1.3249600000000002e-06,
      "loss": 0.1991,
      "step": 54220
    },
    {
      "epoch": 1.73536,
      "grad_norm": 0.02077876403927803,
      "learning_rate": 1.3233600000000003e-06,
      "loss": 0.1991,
      "step": 54230
    },
    {
      "epoch": 1.73568,
      "grad_norm": 0.018021712079644203,
      "learning_rate": 1.32176e-06,
      "loss": 0.2001,
      "step": 54240
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.02803986147046089,
      "learning_rate": 1.32016e-06,
      "loss": 0.1992,
      "step": 54250
    },
    {
      "epoch": 1.73632,
      "grad_norm": 0.02863440103828907,
      "learning_rate": 1.3185600000000001e-06,
      "loss": 0.1998,
      "step": 54260
    },
    {
      "epoch": 1.73664,
      "grad_norm": 0.01523150410503149,
      "learning_rate": 1.3169600000000002e-06,
      "loss": 0.1996,
      "step": 54270
    },
    {
      "epoch": 1.7369599999999998,
      "grad_norm": 0.031058073043823242,
      "learning_rate": 1.3153599999999999e-06,
      "loss": 0.1996,
      "step": 54280
    },
    {
      "epoch": 1.7372800000000002,
      "grad_norm": 0.013770616613328457,
      "learning_rate": 1.31376e-06,
      "loss": 0.1991,
      "step": 54290
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.039960529655218124,
      "learning_rate": 1.31216e-06,
      "loss": 0.2003,
      "step": 54300
    },
    {
      "epoch": 1.73792,
      "grad_norm": 2.697303056716919,
      "learning_rate": 1.3105600000000002e-06,
      "loss": 0.2238,
      "step": 54310
    },
    {
      "epoch": 1.73824,
      "grad_norm": 0.03119325079023838,
      "learning_rate": 1.3089600000000003e-06,
      "loss": 0.2009,
      "step": 54320
    },
    {
      "epoch": 1.73856,
      "grad_norm": 0.07312201708555222,
      "learning_rate": 1.30736e-06,
      "loss": 0.1991,
      "step": 54330
    },
    {
      "epoch": 1.73888,
      "grad_norm": 0.03428298979997635,
      "learning_rate": 1.30576e-06,
      "loss": 0.1993,
      "step": 54340
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.015479671768844128,
      "learning_rate": 1.30416e-06,
      "loss": 0.2147,
      "step": 54350
    },
    {
      "epoch": 1.73952,
      "grad_norm": 0.012657382525503635,
      "learning_rate": 1.3025600000000002e-06,
      "loss": 0.1992,
      "step": 54360
    },
    {
      "epoch": 1.73984,
      "grad_norm": 1.0304449796676636,
      "learning_rate": 1.30096e-06,
      "loss": 0.2052,
      "step": 54370
    },
    {
      "epoch": 1.74016,
      "grad_norm": 0.026959147304296494,
      "learning_rate": 1.2993600000000002e-06,
      "loss": 0.1993,
      "step": 54380
    },
    {
      "epoch": 1.74048,
      "grad_norm": 0.029747623950242996,
      "learning_rate": 1.29776e-06,
      "loss": 0.1991,
      "step": 54390
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.022454367950558662,
      "learning_rate": 1.2961600000000001e-06,
      "loss": 0.2,
      "step": 54400
    },
    {
      "epoch": 1.74112,
      "grad_norm": 0.03838444501161575,
      "learning_rate": 1.29456e-06,
      "loss": 0.2091,
      "step": 54410
    },
    {
      "epoch": 1.7414399999999999,
      "grad_norm": 0.01119332853704691,
      "learning_rate": 1.2929600000000001e-06,
      "loss": 0.1991,
      "step": 54420
    },
    {
      "epoch": 1.74176,
      "grad_norm": 0.029362540692090988,
      "learning_rate": 1.2913600000000002e-06,
      "loss": 0.1992,
      "step": 54430
    },
    {
      "epoch": 1.74208,
      "grad_norm": 0.028848567977547646,
      "learning_rate": 1.28976e-06,
      "loss": 0.1995,
      "step": 54440
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.01756533980369568,
      "learning_rate": 1.2881600000000002e-06,
      "loss": 0.199,
      "step": 54450
    },
    {
      "epoch": 1.74272,
      "grad_norm": 0.01569574512541294,
      "learning_rate": 1.28656e-06,
      "loss": 0.1992,
      "step": 54460
    },
    {
      "epoch": 1.7430400000000001,
      "grad_norm": 0.01768878661096096,
      "learning_rate": 1.2849600000000001e-06,
      "loss": 0.2111,
      "step": 54470
    },
    {
      "epoch": 1.74336,
      "grad_norm": 1.2238686084747314,
      "learning_rate": 1.2833600000000002e-06,
      "loss": 0.2279,
      "step": 54480
    },
    {
      "epoch": 1.74368,
      "grad_norm": 0.02925519086420536,
      "learning_rate": 1.2817600000000003e-06,
      "loss": 0.1991,
      "step": 54490
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.026851631700992584,
      "learning_rate": 1.28016e-06,
      "loss": 0.2009,
      "step": 54500
    },
    {
      "epoch": 1.74432,
      "grad_norm": 0.04395052418112755,
      "learning_rate": 1.27856e-06,
      "loss": 0.1996,
      "step": 54510
    },
    {
      "epoch": 1.74464,
      "grad_norm": 0.01496376097202301,
      "learning_rate": 1.2769600000000002e-06,
      "loss": 0.1992,
      "step": 54520
    },
    {
      "epoch": 1.7449599999999998,
      "grad_norm": 0.0827721580862999,
      "learning_rate": 1.2753600000000003e-06,
      "loss": 0.1998,
      "step": 54530
    },
    {
      "epoch": 1.7452800000000002,
      "grad_norm": 0.034949567168951035,
      "learning_rate": 1.27376e-06,
      "loss": 0.2116,
      "step": 54540
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.019388681277632713,
      "learning_rate": 1.27216e-06,
      "loss": 0.2107,
      "step": 54550
    },
    {
      "epoch": 1.74592,
      "grad_norm": 0.038976993411779404,
      "learning_rate": 1.2705600000000001e-06,
      "loss": 0.2041,
      "step": 54560
    },
    {
      "epoch": 1.74624,
      "grad_norm": 0.1966811716556549,
      "learning_rate": 1.2689600000000002e-06,
      "loss": 0.1998,
      "step": 54570
    },
    {
      "epoch": 1.7465600000000001,
      "grad_norm": 0.016876444220542908,
      "learning_rate": 1.2673599999999999e-06,
      "loss": 0.1991,
      "step": 54580
    },
    {
      "epoch": 1.74688,
      "grad_norm": 0.036827124655246735,
      "learning_rate": 1.26576e-06,
      "loss": 0.1996,
      "step": 54590
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.03748965635895729,
      "learning_rate": 1.26416e-06,
      "loss": 0.2157,
      "step": 54600
    },
    {
      "epoch": 1.74752,
      "grad_norm": 0.014130214229226112,
      "learning_rate": 1.2625600000000002e-06,
      "loss": 0.1993,
      "step": 54610
    },
    {
      "epoch": 1.74784,
      "grad_norm": 0.01845891959965229,
      "learning_rate": 1.2609600000000002e-06,
      "loss": 0.1991,
      "step": 54620
    },
    {
      "epoch": 1.74816,
      "grad_norm": 0.7637059092521667,
      "learning_rate": 1.2593600000000001e-06,
      "loss": 0.2151,
      "step": 54630
    },
    {
      "epoch": 1.74848,
      "grad_norm": 0.01738329790532589,
      "learning_rate": 1.25776e-06,
      "loss": 0.1991,
      "step": 54640
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.8516407608985901,
      "learning_rate": 1.25616e-06,
      "loss": 0.2009,
      "step": 54650
    },
    {
      "epoch": 1.74912,
      "grad_norm": 0.018731947988271713,
      "learning_rate": 1.2545600000000002e-06,
      "loss": 0.2147,
      "step": 54660
    },
    {
      "epoch": 1.7494399999999999,
      "grad_norm": 0.056484054774045944,
      "learning_rate": 1.25296e-06,
      "loss": 0.1991,
      "step": 54670
    },
    {
      "epoch": 1.74976,
      "grad_norm": 0.025417078286409378,
      "learning_rate": 1.2513600000000002e-06,
      "loss": 0.2032,
      "step": 54680
    },
    {
      "epoch": 1.75008,
      "grad_norm": 2.723173141479492,
      "learning_rate": 1.24976e-06,
      "loss": 0.2216,
      "step": 54690
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.05706237629055977,
      "learning_rate": 1.2481600000000001e-06,
      "loss": 0.2003,
      "step": 54700
    },
    {
      "epoch": 1.75072,
      "grad_norm": 0.03592119738459587,
      "learning_rate": 1.2465600000000002e-06,
      "loss": 0.1991,
      "step": 54710
    },
    {
      "epoch": 1.7510400000000002,
      "grad_norm": 0.7645247578620911,
      "learning_rate": 1.24496e-06,
      "loss": 0.2014,
      "step": 54720
    },
    {
      "epoch": 1.75136,
      "grad_norm": 0.027731003239750862,
      "learning_rate": 1.2433600000000002e-06,
      "loss": 0.1995,
      "step": 54730
    },
    {
      "epoch": 1.75168,
      "grad_norm": 0.020262375473976135,
      "learning_rate": 1.24176e-06,
      "loss": 0.2136,
      "step": 54740
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.014440529979765415,
      "learning_rate": 1.2401600000000002e-06,
      "loss": 0.1994,
      "step": 54750
    },
    {
      "epoch": 1.75232,
      "grad_norm": 0.029860850423574448,
      "learning_rate": 1.23856e-06,
      "loss": 0.1991,
      "step": 54760
    },
    {
      "epoch": 1.75264,
      "grad_norm": 0.030547084286808968,
      "learning_rate": 1.2369600000000001e-06,
      "loss": 0.1994,
      "step": 54770
    },
    {
      "epoch": 1.7529599999999999,
      "grad_norm": 0.019865619018673897,
      "learning_rate": 1.2353600000000002e-06,
      "loss": 0.1992,
      "step": 54780
    },
    {
      "epoch": 1.75328,
      "grad_norm": 0.10542593151330948,
      "learning_rate": 1.2337600000000001e-06,
      "loss": 0.2,
      "step": 54790
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.021905340254306793,
      "learning_rate": 1.2321600000000002e-06,
      "loss": 0.1994,
      "step": 54800
    },
    {
      "epoch": 1.75392,
      "grad_norm": 0.01750340685248375,
      "learning_rate": 1.23056e-06,
      "loss": 0.1991,
      "step": 54810
    },
    {
      "epoch": 1.75424,
      "grad_norm": 0.014934137463569641,
      "learning_rate": 1.2289600000000002e-06,
      "loss": 0.2044,
      "step": 54820
    },
    {
      "epoch": 1.7545600000000001,
      "grad_norm": 0.02093086577951908,
      "learning_rate": 1.22736e-06,
      "loss": 0.1992,
      "step": 54830
    },
    {
      "epoch": 1.75488,
      "grad_norm": 0.027289656922221184,
      "learning_rate": 1.2257600000000001e-06,
      "loss": 0.1992,
      "step": 54840
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.028311824426054955,
      "learning_rate": 1.22416e-06,
      "loss": 0.1992,
      "step": 54850
    },
    {
      "epoch": 1.75552,
      "grad_norm": 0.028814643621444702,
      "learning_rate": 1.2225600000000001e-06,
      "loss": 0.2022,
      "step": 54860
    },
    {
      "epoch": 1.75584,
      "grad_norm": 0.02523696981370449,
      "learning_rate": 1.22096e-06,
      "loss": 0.1991,
      "step": 54870
    },
    {
      "epoch": 1.75616,
      "grad_norm": 0.026087144389748573,
      "learning_rate": 1.21936e-06,
      "loss": 0.1993,
      "step": 54880
    },
    {
      "epoch": 1.75648,
      "grad_norm": 0.02130831964313984,
      "learning_rate": 1.21776e-06,
      "loss": 0.2011,
      "step": 54890
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.0198175311088562,
      "learning_rate": 1.21616e-06,
      "loss": 0.1991,
      "step": 54900
    },
    {
      "epoch": 1.75712,
      "grad_norm": 0.0192109327763319,
      "learning_rate": 1.2145600000000002e-06,
      "loss": 0.1994,
      "step": 54910
    },
    {
      "epoch": 1.75744,
      "grad_norm": 0.018108965829014778,
      "learning_rate": 1.21296e-06,
      "loss": 0.2139,
      "step": 54920
    },
    {
      "epoch": 1.75776,
      "grad_norm": 0.012308236211538315,
      "learning_rate": 1.2113600000000001e-06,
      "loss": 0.2002,
      "step": 54930
    },
    {
      "epoch": 1.75808,
      "grad_norm": 0.03527003899216652,
      "learning_rate": 1.20976e-06,
      "loss": 0.1995,
      "step": 54940
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.030203139409422874,
      "learning_rate": 1.20816e-06,
      "loss": 0.1999,
      "step": 54950
    },
    {
      "epoch": 1.7587199999999998,
      "grad_norm": 0.021019287407398224,
      "learning_rate": 1.2065600000000002e-06,
      "loss": 0.1991,
      "step": 54960
    },
    {
      "epoch": 1.7590400000000002,
      "grad_norm": 0.03083733841776848,
      "learning_rate": 1.20496e-06,
      "loss": 0.1995,
      "step": 54970
    },
    {
      "epoch": 1.75936,
      "grad_norm": 0.02140689268708229,
      "learning_rate": 1.2033600000000002e-06,
      "loss": 0.2009,
      "step": 54980
    },
    {
      "epoch": 1.75968,
      "grad_norm": 0.04849034920334816,
      "learning_rate": 1.2017600000000002e-06,
      "loss": 0.1992,
      "step": 54990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.02851417288184166,
      "learning_rate": 1.2001600000000001e-06,
      "loss": 0.1992,
      "step": 55000
    },
    {
      "epoch": 1.76,
      "eval_runtime": 75.7508,
      "eval_samples_per_second": 132.012,
      "eval_steps_per_second": 8.251,
      "step": 55000
    },
    {
      "epoch": 1.76032,
      "grad_norm": 0.04374008998274803,
      "learning_rate": 1.1985600000000002e-06,
      "loss": 0.1991,
      "step": 55010
    },
    {
      "epoch": 1.76064,
      "grad_norm": 0.058452360332012177,
      "learning_rate": 1.19696e-06,
      "loss": 0.2119,
      "step": 55020
    },
    {
      "epoch": 1.7609599999999999,
      "grad_norm": 0.022284021601080894,
      "learning_rate": 1.1953600000000002e-06,
      "loss": 0.2036,
      "step": 55030
    },
    {
      "epoch": 1.76128,
      "grad_norm": 0.029278697445988655,
      "learning_rate": 1.19376e-06,
      "loss": 0.1993,
      "step": 55040
    },
    {
      "epoch": 1.7616,
      "grad_norm": 1.4709328413009644,
      "learning_rate": 1.1921600000000002e-06,
      "loss": 0.2209,
      "step": 55050
    },
    {
      "epoch": 1.76192,
      "grad_norm": 0.014618479646742344,
      "learning_rate": 1.19056e-06,
      "loss": 0.2159,
      "step": 55060
    },
    {
      "epoch": 1.76224,
      "grad_norm": 0.02153310552239418,
      "learning_rate": 1.1889600000000001e-06,
      "loss": 0.1992,
      "step": 55070
    },
    {
      "epoch": 1.7625600000000001,
      "grad_norm": 0.032427702099084854,
      "learning_rate": 1.18736e-06,
      "loss": 0.1992,
      "step": 55080
    },
    {
      "epoch": 1.76288,
      "grad_norm": 0.013095400296151638,
      "learning_rate": 1.18576e-06,
      "loss": 0.2005,
      "step": 55090
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 2.083699941635132,
      "learning_rate": 1.18416e-06,
      "loss": 0.2256,
      "step": 55100
    },
    {
      "epoch": 1.76352,
      "grad_norm": 0.05564960464835167,
      "learning_rate": 1.18256e-06,
      "loss": 0.2009,
      "step": 55110
    },
    {
      "epoch": 1.76384,
      "grad_norm": 0.015328268520534039,
      "learning_rate": 1.1809600000000002e-06,
      "loss": 0.199,
      "step": 55120
    },
    {
      "epoch": 1.76416,
      "grad_norm": 0.016961336135864258,
      "learning_rate": 1.17936e-06,
      "loss": 0.1993,
      "step": 55130
    },
    {
      "epoch": 1.76448,
      "grad_norm": 0.02002405747771263,
      "learning_rate": 1.1777600000000001e-06,
      "loss": 0.1992,
      "step": 55140
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.023559940978884697,
      "learning_rate": 1.17616e-06,
      "loss": 0.2109,
      "step": 55150
    },
    {
      "epoch": 1.76512,
      "grad_norm": 0.02045351080596447,
      "learning_rate": 1.1745600000000001e-06,
      "loss": 0.1993,
      "step": 55160
    },
    {
      "epoch": 1.76544,
      "grad_norm": 0.056960172951221466,
      "learning_rate": 1.17296e-06,
      "loss": 0.2156,
      "step": 55170
    },
    {
      "epoch": 1.76576,
      "grad_norm": 0.03921372815966606,
      "learning_rate": 1.17136e-06,
      "loss": 0.199,
      "step": 55180
    },
    {
      "epoch": 1.76608,
      "grad_norm": 0.010989727452397346,
      "learning_rate": 1.16976e-06,
      "loss": 0.1992,
      "step": 55190
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.016788214445114136,
      "learning_rate": 1.16816e-06,
      "loss": 0.1991,
      "step": 55200
    },
    {
      "epoch": 1.7667199999999998,
      "grad_norm": 0.027340997010469437,
      "learning_rate": 1.1665600000000001e-06,
      "loss": 0.2042,
      "step": 55210
    },
    {
      "epoch": 1.7670400000000002,
      "grad_norm": 0.010206897743046284,
      "learning_rate": 1.16496e-06,
      "loss": 0.1992,
      "step": 55220
    },
    {
      "epoch": 1.76736,
      "grad_norm": 0.03067612275481224,
      "learning_rate": 1.1633600000000001e-06,
      "loss": 0.1994,
      "step": 55230
    },
    {
      "epoch": 1.76768,
      "grad_norm": 0.0062744892202317715,
      "learning_rate": 1.1617600000000002e-06,
      "loss": 0.2011,
      "step": 55240
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.04440612345933914,
      "learning_rate": 1.16016e-06,
      "loss": 0.2138,
      "step": 55250
    },
    {
      "epoch": 1.7683200000000001,
      "grad_norm": 0.02261502481997013,
      "learning_rate": 1.1585600000000002e-06,
      "loss": 0.1996,
      "step": 55260
    },
    {
      "epoch": 1.76864,
      "grad_norm": 0.23852340877056122,
      "learning_rate": 1.15696e-06,
      "loss": 0.2065,
      "step": 55270
    },
    {
      "epoch": 1.7689599999999999,
      "grad_norm": 0.031161153689026833,
      "learning_rate": 1.1553600000000001e-06,
      "loss": 0.1997,
      "step": 55280
    },
    {
      "epoch": 1.76928,
      "grad_norm": 0.017662322148680687,
      "learning_rate": 1.1537600000000002e-06,
      "loss": 0.1991,
      "step": 55290
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.01652081497013569,
      "learning_rate": 1.1521600000000001e-06,
      "loss": 0.1991,
      "step": 55300
    },
    {
      "epoch": 1.76992,
      "grad_norm": 0.019609887152910233,
      "learning_rate": 1.1505600000000002e-06,
      "loss": 0.216,
      "step": 55310
    },
    {
      "epoch": 1.77024,
      "grad_norm": 0.06239965185523033,
      "learning_rate": 1.14896e-06,
      "loss": 0.2013,
      "step": 55320
    },
    {
      "epoch": 1.7705600000000001,
      "grad_norm": 0.03780512511730194,
      "learning_rate": 1.1473600000000002e-06,
      "loss": 0.199,
      "step": 55330
    },
    {
      "epoch": 1.77088,
      "grad_norm": 0.025136757642030716,
      "learning_rate": 1.14576e-06,
      "loss": 0.2168,
      "step": 55340
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.02626602165400982,
      "learning_rate": 1.1441600000000002e-06,
      "loss": 0.1992,
      "step": 55350
    },
    {
      "epoch": 1.77152,
      "grad_norm": 0.03577558323740959,
      "learning_rate": 1.14256e-06,
      "loss": 0.1996,
      "step": 55360
    },
    {
      "epoch": 1.77184,
      "grad_norm": 0.07492789626121521,
      "learning_rate": 1.1409600000000001e-06,
      "loss": 0.2141,
      "step": 55370
    },
    {
      "epoch": 1.77216,
      "grad_norm": 0.027069322764873505,
      "learning_rate": 1.13936e-06,
      "loss": 0.1991,
      "step": 55380
    },
    {
      "epoch": 1.77248,
      "grad_norm": 0.022527247667312622,
      "learning_rate": 1.13776e-06,
      "loss": 0.2003,
      "step": 55390
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 2.177518844604492,
      "learning_rate": 1.13616e-06,
      "loss": 0.2041,
      "step": 55400
    },
    {
      "epoch": 1.77312,
      "grad_norm": 0.035315729677677155,
      "learning_rate": 1.13456e-06,
      "loss": 0.1991,
      "step": 55410
    },
    {
      "epoch": 1.77344,
      "grad_norm": 0.04033996909856796,
      "learning_rate": 1.13296e-06,
      "loss": 0.2203,
      "step": 55420
    },
    {
      "epoch": 1.77376,
      "grad_norm": 0.023480715230107307,
      "learning_rate": 1.13136e-06,
      "loss": 0.2059,
      "step": 55430
    },
    {
      "epoch": 1.77408,
      "grad_norm": 0.015656111761927605,
      "learning_rate": 1.12976e-06,
      "loss": 0.2039,
      "step": 55440
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.014905259013175964,
      "learning_rate": 1.12816e-06,
      "loss": 0.199,
      "step": 55450
    },
    {
      "epoch": 1.7747199999999999,
      "grad_norm": 0.0762505978345871,
      "learning_rate": 1.12656e-06,
      "loss": 0.1997,
      "step": 55460
    },
    {
      "epoch": 1.77504,
      "grad_norm": 0.03435820713639259,
      "learning_rate": 1.12496e-06,
      "loss": 0.1993,
      "step": 55470
    },
    {
      "epoch": 1.77536,
      "grad_norm": 0.028516573831439018,
      "learning_rate": 1.12336e-06,
      "loss": 0.1995,
      "step": 55480
    },
    {
      "epoch": 1.77568,
      "grad_norm": 0.026317227631807327,
      "learning_rate": 1.1217600000000002e-06,
      "loss": 0.1994,
      "step": 55490
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.0257685799151659,
      "learning_rate": 1.12016e-06,
      "loss": 0.1993,
      "step": 55500
    },
    {
      "epoch": 1.7763200000000001,
      "grad_norm": 0.019252030178904533,
      "learning_rate": 1.1185600000000001e-06,
      "loss": 0.2128,
      "step": 55510
    },
    {
      "epoch": 1.77664,
      "grad_norm": 0.02091788314282894,
      "learning_rate": 1.1169600000000002e-06,
      "loss": 0.1993,
      "step": 55520
    },
    {
      "epoch": 1.7769599999999999,
      "grad_norm": 0.034061990678310394,
      "learning_rate": 1.1153600000000001e-06,
      "loss": 0.2166,
      "step": 55530
    },
    {
      "epoch": 1.77728,
      "grad_norm": 0.01931583508849144,
      "learning_rate": 1.1137600000000002e-06,
      "loss": 0.2007,
      "step": 55540
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.02512991614639759,
      "learning_rate": 1.11216e-06,
      "loss": 0.2004,
      "step": 55550
    },
    {
      "epoch": 1.77792,
      "grad_norm": 0.025614893063902855,
      "learning_rate": 1.1105600000000002e-06,
      "loss": 0.2146,
      "step": 55560
    },
    {
      "epoch": 1.77824,
      "grad_norm": 0.02099950984120369,
      "learning_rate": 1.10896e-06,
      "loss": 0.199,
      "step": 55570
    },
    {
      "epoch": 1.7785600000000001,
      "grad_norm": 0.029661932960152626,
      "learning_rate": 1.1073600000000001e-06,
      "loss": 0.2158,
      "step": 55580
    },
    {
      "epoch": 1.77888,
      "grad_norm": 0.022690191864967346,
      "learning_rate": 1.10576e-06,
      "loss": 0.1991,
      "step": 55590
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.013419870287179947,
      "learning_rate": 1.1041600000000001e-06,
      "loss": 0.199,
      "step": 55600
    },
    {
      "epoch": 1.77952,
      "grad_norm": 0.008613500744104385,
      "learning_rate": 1.1025600000000002e-06,
      "loss": 0.1993,
      "step": 55610
    },
    {
      "epoch": 1.77984,
      "grad_norm": 0.03278876468539238,
      "learning_rate": 1.10096e-06,
      "loss": 0.1996,
      "step": 55620
    },
    {
      "epoch": 1.78016,
      "grad_norm": 0.06622210890054703,
      "learning_rate": 1.0993600000000002e-06,
      "loss": 0.2037,
      "step": 55630
    },
    {
      "epoch": 1.7804799999999998,
      "grad_norm": 0.055315520614385605,
      "learning_rate": 1.09776e-06,
      "loss": 0.1992,
      "step": 55640
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.06489014625549316,
      "learning_rate": 1.0961600000000001e-06,
      "loss": 0.2096,
      "step": 55650
    },
    {
      "epoch": 1.78112,
      "grad_norm": 0.07526751607656479,
      "learning_rate": 1.09456e-06,
      "loss": 0.1993,
      "step": 55660
    },
    {
      "epoch": 1.78144,
      "grad_norm": 0.02139158546924591,
      "learning_rate": 1.0929600000000001e-06,
      "loss": 0.1991,
      "step": 55670
    },
    {
      "epoch": 1.78176,
      "grad_norm": 0.02616087906062603,
      "learning_rate": 1.09136e-06,
      "loss": 0.1992,
      "step": 55680
    },
    {
      "epoch": 1.78208,
      "grad_norm": 0.04143018648028374,
      "learning_rate": 1.08976e-06,
      "loss": 0.2018,
      "step": 55690
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.032541222870349884,
      "learning_rate": 1.08816e-06,
      "loss": 0.1995,
      "step": 55700
    },
    {
      "epoch": 1.7827199999999999,
      "grad_norm": 0.8077732920646667,
      "learning_rate": 1.08656e-06,
      "loss": 0.2113,
      "step": 55710
    },
    {
      "epoch": 1.78304,
      "grad_norm": 0.09892283380031586,
      "learning_rate": 1.08496e-06,
      "loss": 0.1993,
      "step": 55720
    },
    {
      "epoch": 1.78336,
      "grad_norm": 0.02449149638414383,
      "learning_rate": 1.08336e-06,
      "loss": 0.1991,
      "step": 55730
    },
    {
      "epoch": 1.78368,
      "grad_norm": 0.02648603729903698,
      "learning_rate": 1.0817600000000001e-06,
      "loss": 0.1991,
      "step": 55740
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.034717630594968796,
      "learning_rate": 1.08016e-06,
      "loss": 0.216,
      "step": 55750
    },
    {
      "epoch": 1.7843200000000001,
      "grad_norm": 0.028016893193125725,
      "learning_rate": 1.07856e-06,
      "loss": 0.2112,
      "step": 55760
    },
    {
      "epoch": 1.78464,
      "grad_norm": 0.02237631380558014,
      "learning_rate": 1.0769600000000002e-06,
      "loss": 0.1995,
      "step": 55770
    },
    {
      "epoch": 1.7849599999999999,
      "grad_norm": 0.046072110533714294,
      "learning_rate": 1.07536e-06,
      "loss": 0.2007,
      "step": 55780
    },
    {
      "epoch": 1.78528,
      "grad_norm": 0.03758572041988373,
      "learning_rate": 1.0737600000000002e-06,
      "loss": 0.1991,
      "step": 55790
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.04879637062549591,
      "learning_rate": 1.07216e-06,
      "loss": 0.1991,
      "step": 55800
    },
    {
      "epoch": 1.78592,
      "grad_norm": 0.04066116735339165,
      "learning_rate": 1.0705600000000001e-06,
      "loss": 0.2148,
      "step": 55810
    },
    {
      "epoch": 1.78624,
      "grad_norm": 0.01542743481695652,
      "learning_rate": 1.0689600000000002e-06,
      "loss": 0.1996,
      "step": 55820
    },
    {
      "epoch": 1.7865600000000001,
      "grad_norm": 0.03704770281910896,
      "learning_rate": 1.06736e-06,
      "loss": 0.2087,
      "step": 55830
    },
    {
      "epoch": 1.78688,
      "grad_norm": 0.016915375366806984,
      "learning_rate": 1.0657600000000002e-06,
      "loss": 0.2253,
      "step": 55840
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.024557137861847878,
      "learning_rate": 1.06416e-06,
      "loss": 0.2101,
      "step": 55850
    },
    {
      "epoch": 1.78752,
      "grad_norm": 0.01766522228717804,
      "learning_rate": 1.0625600000000002e-06,
      "loss": 0.1993,
      "step": 55860
    },
    {
      "epoch": 1.78784,
      "grad_norm": 0.016238218173384666,
      "learning_rate": 1.06096e-06,
      "loss": 0.1996,
      "step": 55870
    },
    {
      "epoch": 1.78816,
      "grad_norm": 0.025445904582738876,
      "learning_rate": 1.0593600000000001e-06,
      "loss": 0.2117,
      "step": 55880
    },
    {
      "epoch": 1.7884799999999998,
      "grad_norm": 0.015137226320803165,
      "learning_rate": 1.05776e-06,
      "loss": 0.2018,
      "step": 55890
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.03672104701399803,
      "learning_rate": 1.0561600000000001e-06,
      "loss": 0.1993,
      "step": 55900
    },
    {
      "epoch": 1.78912,
      "grad_norm": 0.02916536293923855,
      "learning_rate": 1.05456e-06,
      "loss": 0.1993,
      "step": 55910
    },
    {
      "epoch": 1.78944,
      "grad_norm": 0.04501086100935936,
      "learning_rate": 1.05296e-06,
      "loss": 0.1991,
      "step": 55920
    },
    {
      "epoch": 1.78976,
      "grad_norm": 0.03337545320391655,
      "learning_rate": 1.05136e-06,
      "loss": 0.1993,
      "step": 55930
    },
    {
      "epoch": 1.7900800000000001,
      "grad_norm": 0.015326722525060177,
      "learning_rate": 1.04976e-06,
      "loss": 0.2174,
      "step": 55940
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.01995484158396721,
      "learning_rate": 1.0481600000000001e-06,
      "loss": 0.2133,
      "step": 55950
    },
    {
      "epoch": 1.7907199999999999,
      "grad_norm": 0.044614698737859726,
      "learning_rate": 1.04656e-06,
      "loss": 0.1998,
      "step": 55960
    },
    {
      "epoch": 1.79104,
      "grad_norm": 0.02156974747776985,
      "learning_rate": 1.0449600000000001e-06,
      "loss": 0.1991,
      "step": 55970
    },
    {
      "epoch": 1.79136,
      "grad_norm": 0.014014206826686859,
      "learning_rate": 1.04336e-06,
      "loss": 0.1992,
      "step": 55980
    },
    {
      "epoch": 1.79168,
      "grad_norm": 0.018053928390145302,
      "learning_rate": 1.04176e-06,
      "loss": 0.2297,
      "step": 55990
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.03120383992791176,
      "learning_rate": 1.04016e-06,
      "loss": 0.1993,
      "step": 56000
    },
    {
      "epoch": 1.792,
      "eval_runtime": 77.4737,
      "eval_samples_per_second": 129.076,
      "eval_steps_per_second": 8.067,
      "step": 56000
    },
    {
      "epoch": 1.7923200000000001,
      "grad_norm": 0.013892422430217266,
      "learning_rate": 1.03856e-06,
      "loss": 0.1994,
      "step": 56010
    },
    {
      "epoch": 1.79264,
      "grad_norm": 0.018908832222223282,
      "learning_rate": 1.0369600000000001e-06,
      "loss": 0.2074,
      "step": 56020
    },
    {
      "epoch": 1.7929599999999999,
      "grad_norm": 0.01615086942911148,
      "learning_rate": 1.03536e-06,
      "loss": 0.1991,
      "step": 56030
    },
    {
      "epoch": 1.79328,
      "grad_norm": 0.10847202688455582,
      "learning_rate": 1.0337600000000001e-06,
      "loss": 0.1994,
      "step": 56040
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.05141732096672058,
      "learning_rate": 1.0321600000000002e-06,
      "loss": 0.1992,
      "step": 56050
    },
    {
      "epoch": 1.79392,
      "grad_norm": 0.026029236614704132,
      "learning_rate": 1.03056e-06,
      "loss": 0.1995,
      "step": 56060
    },
    {
      "epoch": 1.7942399999999998,
      "grad_norm": 0.018230820074677467,
      "learning_rate": 1.0289600000000002e-06,
      "loss": 0.2173,
      "step": 56070
    },
    {
      "epoch": 1.7945600000000002,
      "grad_norm": 0.016888197511434555,
      "learning_rate": 1.02736e-06,
      "loss": 0.2244,
      "step": 56080
    },
    {
      "epoch": 1.79488,
      "grad_norm": 0.02112988382577896,
      "learning_rate": 1.0257600000000002e-06,
      "loss": 0.1999,
      "step": 56090
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.05580226331949234,
      "learning_rate": 1.02416e-06,
      "loss": 0.2009,
      "step": 56100
    },
    {
      "epoch": 1.79552,
      "grad_norm": 0.015194159932434559,
      "learning_rate": 1.0225600000000001e-06,
      "loss": 0.1992,
      "step": 56110
    },
    {
      "epoch": 1.79584,
      "grad_norm": 0.01143970899283886,
      "learning_rate": 1.0209600000000002e-06,
      "loss": 0.1992,
      "step": 56120
    },
    {
      "epoch": 1.79616,
      "grad_norm": 0.03573686629533768,
      "learning_rate": 1.01936e-06,
      "loss": 0.2133,
      "step": 56130
    },
    {
      "epoch": 1.7964799999999999,
      "grad_norm": 0.024784401059150696,
      "learning_rate": 1.0177600000000002e-06,
      "loss": 0.2063,
      "step": 56140
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.016997111961245537,
      "learning_rate": 1.01616e-06,
      "loss": 0.1994,
      "step": 56150
    },
    {
      "epoch": 1.79712,
      "grad_norm": 0.018006129190325737,
      "learning_rate": 1.0145600000000002e-06,
      "loss": 0.2002,
      "step": 56160
    },
    {
      "epoch": 1.79744,
      "grad_norm": 0.020245663821697235,
      "learning_rate": 1.01296e-06,
      "loss": 0.1991,
      "step": 56170
    },
    {
      "epoch": 1.79776,
      "grad_norm": 0.22828848659992218,
      "learning_rate": 1.0113600000000001e-06,
      "loss": 0.1996,
      "step": 56180
    },
    {
      "epoch": 1.7980800000000001,
      "grad_norm": 0.021154193207621574,
      "learning_rate": 1.00976e-06,
      "loss": 0.1992,
      "step": 56190
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.01311265118420124,
      "learning_rate": 1.00816e-06,
      "loss": 0.2011,
      "step": 56200
    },
    {
      "epoch": 1.7987199999999999,
      "grad_norm": 0.03347541391849518,
      "learning_rate": 1.00656e-06,
      "loss": 0.2126,
      "step": 56210
    },
    {
      "epoch": 1.79904,
      "grad_norm": 0.03743883594870567,
      "learning_rate": 1.00496e-06,
      "loss": 0.1994,
      "step": 56220
    },
    {
      "epoch": 1.79936,
      "grad_norm": 0.011265400797128677,
      "learning_rate": 1.00336e-06,
      "loss": 0.2002,
      "step": 56230
    },
    {
      "epoch": 1.79968,
      "grad_norm": 0.02513529174029827,
      "learning_rate": 1.00176e-06,
      "loss": 0.1992,
      "step": 56240
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.010430202819406986,
      "learning_rate": 1.00016e-06,
      "loss": 0.1993,
      "step": 56250
    },
    {
      "epoch": 1.8003200000000001,
      "grad_norm": 0.03497757762670517,
      "learning_rate": 9.9856e-07,
      "loss": 0.2036,
      "step": 56260
    },
    {
      "epoch": 1.80064,
      "grad_norm": 0.03537556901574135,
      "learning_rate": 9.969600000000001e-07,
      "loss": 0.1992,
      "step": 56270
    },
    {
      "epoch": 1.80096,
      "grad_norm": 0.027942053973674774,
      "learning_rate": 9.9536e-07,
      "loss": 0.2008,
      "step": 56280
    },
    {
      "epoch": 1.80128,
      "grad_norm": 0.03140974044799805,
      "learning_rate": 9.9376e-07,
      "loss": 0.1992,
      "step": 56290
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.022555287927389145,
      "learning_rate": 9.921600000000002e-07,
      "loss": 0.1995,
      "step": 56300
    },
    {
      "epoch": 1.80192,
      "grad_norm": 0.14511363208293915,
      "learning_rate": 9.9056e-07,
      "loss": 0.1992,
      "step": 56310
    },
    {
      "epoch": 1.8022399999999998,
      "grad_norm": 0.02507607266306877,
      "learning_rate": 9.889600000000001e-07,
      "loss": 0.1991,
      "step": 56320
    },
    {
      "epoch": 1.8025600000000002,
      "grad_norm": 0.020808326080441475,
      "learning_rate": 9.8736e-07,
      "loss": 0.2022,
      "step": 56330
    },
    {
      "epoch": 1.80288,
      "grad_norm": 0.016378900036215782,
      "learning_rate": 9.857600000000001e-07,
      "loss": 0.2131,
      "step": 56340
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.022630952298641205,
      "learning_rate": 9.841600000000002e-07,
      "loss": 0.199,
      "step": 56350
    },
    {
      "epoch": 1.80352,
      "grad_norm": 0.02961321733891964,
      "learning_rate": 9.8256e-07,
      "loss": 0.1998,
      "step": 56360
    },
    {
      "epoch": 1.80384,
      "grad_norm": 0.019687900319695473,
      "learning_rate": 9.809600000000002e-07,
      "loss": 0.1994,
      "step": 56370
    },
    {
      "epoch": 1.80416,
      "grad_norm": 0.03219104930758476,
      "learning_rate": 9.7936e-07,
      "loss": 0.1991,
      "step": 56380
    },
    {
      "epoch": 1.8044799999999999,
      "grad_norm": 0.01674381084740162,
      "learning_rate": 9.777600000000001e-07,
      "loss": 0.2177,
      "step": 56390
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.015341667458415031,
      "learning_rate": 9.7616e-07,
      "loss": 0.2139,
      "step": 56400
    },
    {
      "epoch": 1.80512,
      "grad_norm": 0.03402850404381752,
      "learning_rate": 9.745600000000001e-07,
      "loss": 0.1992,
      "step": 56410
    },
    {
      "epoch": 1.80544,
      "grad_norm": 0.01834459975361824,
      "learning_rate": 9.7296e-07,
      "loss": 0.2014,
      "step": 56420
    },
    {
      "epoch": 1.80576,
      "grad_norm": 0.008176016621291637,
      "learning_rate": 9.7136e-07,
      "loss": 0.1992,
      "step": 56430
    },
    {
      "epoch": 1.8060800000000001,
      "grad_norm": 0.021239500492811203,
      "learning_rate": 9.697600000000002e-07,
      "loss": 0.1999,
      "step": 56440
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.03071647509932518,
      "learning_rate": 9.6816e-07,
      "loss": 0.2151,
      "step": 56450
    },
    {
      "epoch": 1.8067199999999999,
      "grad_norm": 0.021994104608893394,
      "learning_rate": 9.665600000000002e-07,
      "loss": 0.1993,
      "step": 56460
    },
    {
      "epoch": 1.80704,
      "grad_norm": 0.020124344155192375,
      "learning_rate": 9.6496e-07,
      "loss": 0.2127,
      "step": 56470
    },
    {
      "epoch": 1.80736,
      "grad_norm": 0.03398248553276062,
      "learning_rate": 9.633600000000001e-07,
      "loss": 0.2124,
      "step": 56480
    },
    {
      "epoch": 1.80768,
      "grad_norm": 0.01953561045229435,
      "learning_rate": 9.6176e-07,
      "loss": 0.1992,
      "step": 56490
    },
    {
      "epoch": 1.808,
      "grad_norm": 1.8935176134109497,
      "learning_rate": 9.6016e-07,
      "loss": 0.2041,
      "step": 56500
    },
    {
      "epoch": 1.8083200000000001,
      "grad_norm": 0.07001689821481705,
      "learning_rate": 9.5856e-07,
      "loss": 0.1995,
      "step": 56510
    },
    {
      "epoch": 1.80864,
      "grad_norm": 0.03592633828520775,
      "learning_rate": 9.5696e-07,
      "loss": 0.2092,
      "step": 56520
    },
    {
      "epoch": 1.80896,
      "grad_norm": 0.046735696494579315,
      "learning_rate": 9.5536e-07,
      "loss": 0.1991,
      "step": 56530
    },
    {
      "epoch": 1.80928,
      "grad_norm": 0.014556776732206345,
      "learning_rate": 9.5376e-07,
      "loss": 0.1991,
      "step": 56540
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.018375203013420105,
      "learning_rate": 9.5216e-07,
      "loss": 0.2042,
      "step": 56550
    },
    {
      "epoch": 1.80992,
      "grad_norm": 0.03437375649809837,
      "learning_rate": 9.505600000000001e-07,
      "loss": 0.1993,
      "step": 56560
    },
    {
      "epoch": 1.8102399999999998,
      "grad_norm": 0.03821329399943352,
      "learning_rate": 9.4896e-07,
      "loss": 0.2212,
      "step": 56570
    },
    {
      "epoch": 1.81056,
      "grad_norm": 0.0344090461730957,
      "learning_rate": 9.473600000000001e-07,
      "loss": 0.2031,
      "step": 56580
    },
    {
      "epoch": 1.81088,
      "grad_norm": 0.024464840069413185,
      "learning_rate": 9.4576e-07,
      "loss": 0.1991,
      "step": 56590
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.03927036002278328,
      "learning_rate": 9.441600000000001e-07,
      "loss": 0.1991,
      "step": 56600
    },
    {
      "epoch": 1.81152,
      "grad_norm": 0.030492903664708138,
      "learning_rate": 9.425600000000001e-07,
      "loss": 0.2017,
      "step": 56610
    },
    {
      "epoch": 1.8118400000000001,
      "grad_norm": 0.030595771968364716,
      "learning_rate": 9.4096e-07,
      "loss": 0.1995,
      "step": 56620
    },
    {
      "epoch": 1.81216,
      "grad_norm": 0.02637440338730812,
      "learning_rate": 9.393600000000001e-07,
      "loss": 0.199,
      "step": 56630
    },
    {
      "epoch": 1.8124799999999999,
      "grad_norm": 0.02337539941072464,
      "learning_rate": 9.377600000000001e-07,
      "loss": 0.1991,
      "step": 56640
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.012377358973026276,
      "learning_rate": 9.361600000000001e-07,
      "loss": 0.2117,
      "step": 56650
    },
    {
      "epoch": 1.81312,
      "grad_norm": 0.017453700304031372,
      "learning_rate": 9.345600000000001e-07,
      "loss": 0.1994,
      "step": 56660
    },
    {
      "epoch": 1.81344,
      "grad_norm": 0.02120889164507389,
      "learning_rate": 9.329600000000001e-07,
      "loss": 0.1992,
      "step": 56670
    },
    {
      "epoch": 1.81376,
      "grad_norm": 0.032618824392557144,
      "learning_rate": 9.3136e-07,
      "loss": 0.1994,
      "step": 56680
    },
    {
      "epoch": 1.8140800000000001,
      "grad_norm": 0.032633863389492035,
      "learning_rate": 9.297600000000001e-07,
      "loss": 0.2003,
      "step": 56690
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.029051044955849648,
      "learning_rate": 9.2816e-07,
      "loss": 0.1992,
      "step": 56700
    },
    {
      "epoch": 1.8147199999999999,
      "grad_norm": 0.027255864813923836,
      "learning_rate": 9.265600000000001e-07,
      "loss": 0.1991,
      "step": 56710
    },
    {
      "epoch": 1.81504,
      "grad_norm": 0.016779663041234016,
      "learning_rate": 9.2496e-07,
      "loss": 0.2225,
      "step": 56720
    },
    {
      "epoch": 1.81536,
      "grad_norm": 0.00955193117260933,
      "learning_rate": 9.233600000000001e-07,
      "loss": 0.1991,
      "step": 56730
    },
    {
      "epoch": 1.81568,
      "grad_norm": 0.026462778449058533,
      "learning_rate": 9.2176e-07,
      "loss": 0.1994,
      "step": 56740
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.025044430047273636,
      "learning_rate": 9.201600000000001e-07,
      "loss": 0.2037,
      "step": 56750
    },
    {
      "epoch": 1.8163200000000002,
      "grad_norm": 0.7911863327026367,
      "learning_rate": 9.1856e-07,
      "loss": 0.2022,
      "step": 56760
    },
    {
      "epoch": 1.81664,
      "grad_norm": 0.25192058086395264,
      "learning_rate": 9.1696e-07,
      "loss": 0.1994,
      "step": 56770
    },
    {
      "epoch": 1.81696,
      "grad_norm": 0.030386395752429962,
      "learning_rate": 9.153600000000001e-07,
      "loss": 0.1992,
      "step": 56780
    },
    {
      "epoch": 1.81728,
      "grad_norm": 0.01967356540262699,
      "learning_rate": 9.137600000000001e-07,
      "loss": 0.1991,
      "step": 56790
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.05184992030262947,
      "learning_rate": 9.121600000000001e-07,
      "loss": 0.1992,
      "step": 56800
    },
    {
      "epoch": 1.81792,
      "grad_norm": 0.02685459516942501,
      "learning_rate": 9.105600000000001e-07,
      "loss": 0.1992,
      "step": 56810
    },
    {
      "epoch": 1.8182399999999999,
      "grad_norm": 0.015772143378853798,
      "learning_rate": 9.089600000000002e-07,
      "loss": 0.1995,
      "step": 56820
    },
    {
      "epoch": 1.81856,
      "grad_norm": 0.022456075996160507,
      "learning_rate": 9.0736e-07,
      "loss": 0.1991,
      "step": 56830
    },
    {
      "epoch": 1.81888,
      "grad_norm": 0.5876016616821289,
      "learning_rate": 9.057600000000001e-07,
      "loss": 0.1998,
      "step": 56840
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.020994151011109352,
      "learning_rate": 9.0416e-07,
      "loss": 0.1991,
      "step": 56850
    },
    {
      "epoch": 1.81952,
      "grad_norm": 0.018096618354320526,
      "learning_rate": 9.025600000000001e-07,
      "loss": 0.1993,
      "step": 56860
    },
    {
      "epoch": 1.8198400000000001,
      "grad_norm": 0.02209244668483734,
      "learning_rate": 9.0096e-07,
      "loss": 0.1991,
      "step": 56870
    },
    {
      "epoch": 1.82016,
      "grad_norm": 0.021106652915477753,
      "learning_rate": 8.993600000000001e-07,
      "loss": 0.1991,
      "step": 56880
    },
    {
      "epoch": 1.8204799999999999,
      "grad_norm": 0.3194137513637543,
      "learning_rate": 8.977600000000001e-07,
      "loss": 0.1997,
      "step": 56890
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.03160683810710907,
      "learning_rate": 8.9616e-07,
      "loss": 0.2091,
      "step": 56900
    },
    {
      "epoch": 1.82112,
      "grad_norm": 0.02167188562452793,
      "learning_rate": 8.9456e-07,
      "loss": 0.2178,
      "step": 56910
    },
    {
      "epoch": 1.82144,
      "grad_norm": 0.09853704273700714,
      "learning_rate": 8.929600000000001e-07,
      "loss": 0.2014,
      "step": 56920
    },
    {
      "epoch": 1.82176,
      "grad_norm": 0.010156203992664814,
      "learning_rate": 8.913600000000001e-07,
      "loss": 0.199,
      "step": 56930
    },
    {
      "epoch": 1.8220800000000001,
      "grad_norm": 2.4805335998535156,
      "learning_rate": 8.897600000000001e-07,
      "loss": 0.2172,
      "step": 56940
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.02811136096715927,
      "learning_rate": 8.881600000000001e-07,
      "loss": 0.1991,
      "step": 56950
    },
    {
      "epoch": 1.82272,
      "grad_norm": 0.013759089633822441,
      "learning_rate": 8.865600000000001e-07,
      "loss": 0.1995,
      "step": 56960
    },
    {
      "epoch": 1.82304,
      "grad_norm": 0.019080987200140953,
      "learning_rate": 8.849600000000002e-07,
      "loss": 0.199,
      "step": 56970
    },
    {
      "epoch": 1.82336,
      "grad_norm": 0.021276729181408882,
      "learning_rate": 8.8336e-07,
      "loss": 0.1991,
      "step": 56980
    },
    {
      "epoch": 1.82368,
      "grad_norm": 0.019179677590727806,
      "learning_rate": 8.817600000000001e-07,
      "loss": 0.199,
      "step": 56990
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.01857568509876728,
      "learning_rate": 8.8016e-07,
      "loss": 0.2049,
      "step": 57000
    },
    {
      "epoch": 1.8239999999999998,
      "eval_runtime": 76.0937,
      "eval_samples_per_second": 131.417,
      "eval_steps_per_second": 8.214,
      "step": 57000
    },
    {
      "epoch": 1.8243200000000002,
      "grad_norm": 0.046459633857011795,
      "learning_rate": 8.785600000000001e-07,
      "loss": 0.202,
      "step": 57010
    },
    {
      "epoch": 1.82464,
      "grad_norm": 0.03736430034041405,
      "learning_rate": 8.7696e-07,
      "loss": 0.1995,
      "step": 57020
    },
    {
      "epoch": 1.82496,
      "grad_norm": 0.02167363464832306,
      "learning_rate": 8.753600000000001e-07,
      "loss": 0.2076,
      "step": 57030
    },
    {
      "epoch": 1.82528,
      "grad_norm": 0.01317780464887619,
      "learning_rate": 8.737600000000001e-07,
      "loss": 0.1998,
      "step": 57040
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.15201838314533234,
      "learning_rate": 8.7216e-07,
      "loss": 0.1993,
      "step": 57050
    },
    {
      "epoch": 1.82592,
      "grad_norm": 0.034823913127183914,
      "learning_rate": 8.7056e-07,
      "loss": 0.199,
      "step": 57060
    },
    {
      "epoch": 1.8262399999999999,
      "grad_norm": 0.029293620958924294,
      "learning_rate": 8.689600000000001e-07,
      "loss": 0.2002,
      "step": 57070
    },
    {
      "epoch": 1.82656,
      "grad_norm": 0.023914068937301636,
      "learning_rate": 8.6736e-07,
      "loss": 0.1992,
      "step": 57080
    },
    {
      "epoch": 1.82688,
      "grad_norm": 0.04956298694014549,
      "learning_rate": 8.657600000000001e-07,
      "loss": 0.201,
      "step": 57090
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.04108157008886337,
      "learning_rate": 8.641600000000002e-07,
      "loss": 0.1991,
      "step": 57100
    },
    {
      "epoch": 1.82752,
      "grad_norm": 0.052174728363752365,
      "learning_rate": 8.625600000000001e-07,
      "loss": 0.1992,
      "step": 57110
    },
    {
      "epoch": 1.8278400000000001,
      "grad_norm": 0.023467974737286568,
      "learning_rate": 8.609600000000002e-07,
      "loss": 0.1994,
      "step": 57120
    },
    {
      "epoch": 1.82816,
      "grad_norm": 0.01512187160551548,
      "learning_rate": 8.5936e-07,
      "loss": 0.232,
      "step": 57130
    },
    {
      "epoch": 1.8284799999999999,
      "grad_norm": 0.030742796137928963,
      "learning_rate": 8.577600000000001e-07,
      "loss": 0.1991,
      "step": 57140
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.012925039045512676,
      "learning_rate": 8.5616e-07,
      "loss": 0.2141,
      "step": 57150
    },
    {
      "epoch": 1.82912,
      "grad_norm": 1.1620038747787476,
      "learning_rate": 8.545600000000001e-07,
      "loss": 0.2112,
      "step": 57160
    },
    {
      "epoch": 1.82944,
      "grad_norm": 0.031980421394109726,
      "learning_rate": 8.529600000000001e-07,
      "loss": 0.2024,
      "step": 57170
    },
    {
      "epoch": 1.82976,
      "grad_norm": 0.022511018440127373,
      "learning_rate": 8.513600000000001e-07,
      "loss": 0.1992,
      "step": 57180
    },
    {
      "epoch": 1.8300800000000002,
      "grad_norm": 0.0835522711277008,
      "learning_rate": 8.497600000000001e-07,
      "loss": 0.2137,
      "step": 57190
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.03416553884744644,
      "learning_rate": 8.4816e-07,
      "loss": 0.1991,
      "step": 57200
    },
    {
      "epoch": 1.83072,
      "grad_norm": 0.014890890568494797,
      "learning_rate": 8.4656e-07,
      "loss": 0.2146,
      "step": 57210
    },
    {
      "epoch": 1.83104,
      "grad_norm": 0.033112138509750366,
      "learning_rate": 8.449600000000001e-07,
      "loss": 0.1992,
      "step": 57220
    },
    {
      "epoch": 1.83136,
      "grad_norm": 0.027389928698539734,
      "learning_rate": 8.4336e-07,
      "loss": 0.1991,
      "step": 57230
    },
    {
      "epoch": 1.83168,
      "grad_norm": 0.026021122932434082,
      "learning_rate": 8.417600000000001e-07,
      "loss": 0.1992,
      "step": 57240
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.016827039420604706,
      "learning_rate": 8.4016e-07,
      "loss": 0.199,
      "step": 57250
    },
    {
      "epoch": 1.83232,
      "grad_norm": 0.031685568392276764,
      "learning_rate": 8.385600000000001e-07,
      "loss": 0.2037,
      "step": 57260
    },
    {
      "epoch": 1.83264,
      "grad_norm": 0.026186103001236916,
      "learning_rate": 8.369600000000002e-07,
      "loss": 0.1992,
      "step": 57270
    },
    {
      "epoch": 1.83296,
      "grad_norm": 0.024614783003926277,
      "learning_rate": 8.3536e-07,
      "loss": 0.2184,
      "step": 57280
    },
    {
      "epoch": 1.83328,
      "grad_norm": 0.027764515951275826,
      "learning_rate": 8.337600000000001e-07,
      "loss": 0.1992,
      "step": 57290
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.0227506086230278,
      "learning_rate": 8.3216e-07,
      "loss": 0.2001,
      "step": 57300
    },
    {
      "epoch": 1.83392,
      "grad_norm": 0.03052232414484024,
      "learning_rate": 8.305600000000001e-07,
      "loss": 0.2056,
      "step": 57310
    },
    {
      "epoch": 1.8342399999999999,
      "grad_norm": 0.021518738940358162,
      "learning_rate": 8.289600000000001e-07,
      "loss": 0.1994,
      "step": 57320
    },
    {
      "epoch": 1.83456,
      "grad_norm": 0.015845267102122307,
      "learning_rate": 8.273600000000001e-07,
      "loss": 0.2093,
      "step": 57330
    },
    {
      "epoch": 1.83488,
      "grad_norm": 0.02916812337934971,
      "learning_rate": 8.257600000000001e-07,
      "loss": 0.1995,
      "step": 57340
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.012881631031632423,
      "learning_rate": 8.241600000000001e-07,
      "loss": 0.2284,
      "step": 57350
    },
    {
      "epoch": 1.83552,
      "grad_norm": 0.016711400821805,
      "learning_rate": 8.2256e-07,
      "loss": 0.1991,
      "step": 57360
    },
    {
      "epoch": 1.8358400000000001,
      "grad_norm": 0.025094320997595787,
      "learning_rate": 8.209600000000001e-07,
      "loss": 0.1992,
      "step": 57370
    },
    {
      "epoch": 1.83616,
      "grad_norm": 0.02948249876499176,
      "learning_rate": 8.1936e-07,
      "loss": 0.1991,
      "step": 57380
    },
    {
      "epoch": 1.83648,
      "grad_norm": 0.017567897215485573,
      "learning_rate": 8.177600000000001e-07,
      "loss": 0.1992,
      "step": 57390
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.018803751096129417,
      "learning_rate": 8.1616e-07,
      "loss": 0.1992,
      "step": 57400
    },
    {
      "epoch": 1.83712,
      "grad_norm": 0.04252663627266884,
      "learning_rate": 8.145600000000001e-07,
      "loss": 0.1999,
      "step": 57410
    },
    {
      "epoch": 1.83744,
      "grad_norm": 0.026866015046834946,
      "learning_rate": 8.1296e-07,
      "loss": 0.1995,
      "step": 57420
    },
    {
      "epoch": 1.8377599999999998,
      "grad_norm": 0.01514170877635479,
      "learning_rate": 8.1136e-07,
      "loss": 0.1992,
      "step": 57430
    },
    {
      "epoch": 1.8380800000000002,
      "grad_norm": 1.742551565170288,
      "learning_rate": 8.097600000000001e-07,
      "loss": 0.2015,
      "step": 57440
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.030582726001739502,
      "learning_rate": 8.0816e-07,
      "loss": 0.199,
      "step": 57450
    },
    {
      "epoch": 1.83872,
      "grad_norm": 0.04613075032830238,
      "learning_rate": 8.065600000000001e-07,
      "loss": 0.2005,
      "step": 57460
    },
    {
      "epoch": 1.83904,
      "grad_norm": 0.015268780291080475,
      "learning_rate": 8.049600000000001e-07,
      "loss": 0.1992,
      "step": 57470
    },
    {
      "epoch": 1.83936,
      "grad_norm": 0.01674065552651882,
      "learning_rate": 8.033600000000001e-07,
      "loss": 0.2001,
      "step": 57480
    },
    {
      "epoch": 1.83968,
      "grad_norm": 0.028249533846974373,
      "learning_rate": 8.017600000000001e-07,
      "loss": 0.1991,
      "step": 57490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.012491418980062008,
      "learning_rate": 8.001600000000001e-07,
      "loss": 0.2155,
      "step": 57500
    },
    {
      "epoch": 1.84032,
      "grad_norm": 0.031386665999889374,
      "learning_rate": 7.9856e-07,
      "loss": 0.1992,
      "step": 57510
    },
    {
      "epoch": 1.84064,
      "grad_norm": 0.019589073956012726,
      "learning_rate": 7.969600000000001e-07,
      "loss": 0.1993,
      "step": 57520
    },
    {
      "epoch": 1.84096,
      "grad_norm": 0.036442916840314865,
      "learning_rate": 7.9536e-07,
      "loss": 0.2063,
      "step": 57530
    },
    {
      "epoch": 1.84128,
      "grad_norm": 0.014146690256893635,
      "learning_rate": 7.937600000000001e-07,
      "loss": 0.1991,
      "step": 57540
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.04178137704730034,
      "learning_rate": 7.9216e-07,
      "loss": 0.1991,
      "step": 57550
    },
    {
      "epoch": 1.84192,
      "grad_norm": 0.02512231096625328,
      "learning_rate": 7.905600000000001e-07,
      "loss": 0.1998,
      "step": 57560
    },
    {
      "epoch": 1.8422399999999999,
      "grad_norm": 0.02428452856838703,
      "learning_rate": 7.8896e-07,
      "loss": 0.2158,
      "step": 57570
    },
    {
      "epoch": 1.84256,
      "grad_norm": 0.031233051791787148,
      "learning_rate": 7.8736e-07,
      "loss": 0.1991,
      "step": 57580
    },
    {
      "epoch": 1.84288,
      "grad_norm": 0.030673958361148834,
      "learning_rate": 7.8576e-07,
      "loss": 0.2156,
      "step": 57590
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.04095430672168732,
      "learning_rate": 7.841600000000001e-07,
      "loss": 0.1991,
      "step": 57600
    },
    {
      "epoch": 1.84352,
      "grad_norm": 0.03509923070669174,
      "learning_rate": 7.825600000000001e-07,
      "loss": 0.2037,
      "step": 57610
    },
    {
      "epoch": 1.8438400000000001,
      "grad_norm": 0.028984516859054565,
      "learning_rate": 7.809600000000001e-07,
      "loss": 0.1995,
      "step": 57620
    },
    {
      "epoch": 1.84416,
      "grad_norm": 0.030274411663413048,
      "learning_rate": 7.793600000000001e-07,
      "loss": 0.1995,
      "step": 57630
    },
    {
      "epoch": 1.84448,
      "grad_norm": 0.03159433975815773,
      "learning_rate": 7.7776e-07,
      "loss": 0.212,
      "step": 57640
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.022125007584691048,
      "learning_rate": 7.761600000000001e-07,
      "loss": 0.1991,
      "step": 57650
    },
    {
      "epoch": 1.84512,
      "grad_norm": 0.018626943230628967,
      "learning_rate": 7.7456e-07,
      "loss": 0.1993,
      "step": 57660
    },
    {
      "epoch": 1.84544,
      "grad_norm": 0.019666211679577827,
      "learning_rate": 7.729600000000001e-07,
      "loss": 0.1992,
      "step": 57670
    },
    {
      "epoch": 1.8457599999999998,
      "grad_norm": 0.017288032919168472,
      "learning_rate": 7.7136e-07,
      "loss": 0.1997,
      "step": 57680
    },
    {
      "epoch": 1.8460800000000002,
      "grad_norm": 0.6612461805343628,
      "learning_rate": 7.697600000000001e-07,
      "loss": 0.2018,
      "step": 57690
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.015114842914044857,
      "learning_rate": 7.6816e-07,
      "loss": 0.1993,
      "step": 57700
    },
    {
      "epoch": 1.84672,
      "grad_norm": 0.01273176446557045,
      "learning_rate": 7.665600000000001e-07,
      "loss": 0.2131,
      "step": 57710
    },
    {
      "epoch": 1.84704,
      "grad_norm": 0.012143858708441257,
      "learning_rate": 7.6496e-07,
      "loss": 0.2013,
      "step": 57720
    },
    {
      "epoch": 1.8473600000000001,
      "grad_norm": 0.02222088724374771,
      "learning_rate": 7.6336e-07,
      "loss": 0.1992,
      "step": 57730
    },
    {
      "epoch": 1.84768,
      "grad_norm": 0.023916542530059814,
      "learning_rate": 7.6176e-07,
      "loss": 0.1997,
      "step": 57740
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.0235038623213768,
      "learning_rate": 7.601600000000001e-07,
      "loss": 0.1994,
      "step": 57750
    },
    {
      "epoch": 1.84832,
      "grad_norm": 0.01992526277899742,
      "learning_rate": 7.585600000000001e-07,
      "loss": 0.1993,
      "step": 57760
    },
    {
      "epoch": 1.84864,
      "grad_norm": 0.09614359587430954,
      "learning_rate": 7.569600000000001e-07,
      "loss": 0.1994,
      "step": 57770
    },
    {
      "epoch": 1.84896,
      "grad_norm": 0.021983442828059196,
      "learning_rate": 7.553600000000002e-07,
      "loss": 0.1991,
      "step": 57780
    },
    {
      "epoch": 1.84928,
      "grad_norm": 0.022738825529813766,
      "learning_rate": 7.5376e-07,
      "loss": 0.2154,
      "step": 57790
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.023562615737318993,
      "learning_rate": 7.521600000000001e-07,
      "loss": 0.1992,
      "step": 57800
    },
    {
      "epoch": 1.84992,
      "grad_norm": 0.01606612093746662,
      "learning_rate": 7.5056e-07,
      "loss": 0.2144,
      "step": 57810
    },
    {
      "epoch": 1.8502399999999999,
      "grad_norm": 0.023772630840539932,
      "learning_rate": 7.489600000000001e-07,
      "loss": 0.1991,
      "step": 57820
    },
    {
      "epoch": 1.85056,
      "grad_norm": 0.020612983033061028,
      "learning_rate": 7.4736e-07,
      "loss": 0.2043,
      "step": 57830
    },
    {
      "epoch": 1.85088,
      "grad_norm": 0.04100523516535759,
      "learning_rate": 7.457600000000001e-07,
      "loss": 0.1995,
      "step": 57840
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.042542293667793274,
      "learning_rate": 7.441600000000001e-07,
      "loss": 0.2073,
      "step": 57850
    },
    {
      "epoch": 1.85152,
      "grad_norm": 0.08192300796508789,
      "learning_rate": 7.425600000000001e-07,
      "loss": 0.1992,
      "step": 57860
    },
    {
      "epoch": 1.8518400000000002,
      "grad_norm": 0.016774887219071388,
      "learning_rate": 7.4096e-07,
      "loss": 0.1996,
      "step": 57870
    },
    {
      "epoch": 1.85216,
      "grad_norm": 0.014027651399374008,
      "learning_rate": 7.393600000000001e-07,
      "loss": 0.1994,
      "step": 57880
    },
    {
      "epoch": 1.85248,
      "grad_norm": 0.013672079890966415,
      "learning_rate": 7.3776e-07,
      "loss": 0.1992,
      "step": 57890
    },
    {
      "epoch": 1.8528,
      "grad_norm": 1.1280102729797363,
      "learning_rate": 7.361600000000001e-07,
      "loss": 0.2163,
      "step": 57900
    },
    {
      "epoch": 1.85312,
      "grad_norm": 0.8069788217544556,
      "learning_rate": 7.3456e-07,
      "loss": 0.2284,
      "step": 57910
    },
    {
      "epoch": 1.85344,
      "grad_norm": 0.012778304517269135,
      "learning_rate": 7.329600000000001e-07,
      "loss": 0.2083,
      "step": 57920
    },
    {
      "epoch": 1.8537599999999999,
      "grad_norm": 0.19686570763587952,
      "learning_rate": 7.313600000000002e-07,
      "loss": 0.2087,
      "step": 57930
    },
    {
      "epoch": 1.85408,
      "grad_norm": 0.012232977896928787,
      "learning_rate": 7.2976e-07,
      "loss": 0.1993,
      "step": 57940
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.026403769850730896,
      "learning_rate": 7.281600000000001e-07,
      "loss": 0.1992,
      "step": 57950
    },
    {
      "epoch": 1.85472,
      "grad_norm": 0.014975191093981266,
      "learning_rate": 7.2656e-07,
      "loss": 0.1992,
      "step": 57960
    },
    {
      "epoch": 1.85504,
      "grad_norm": 0.03098370134830475,
      "learning_rate": 7.249600000000001e-07,
      "loss": 0.2052,
      "step": 57970
    },
    {
      "epoch": 1.8553600000000001,
      "grad_norm": 1.3223257064819336,
      "learning_rate": 7.2336e-07,
      "loss": 0.2036,
      "step": 57980
    },
    {
      "epoch": 1.85568,
      "grad_norm": 0.0241813063621521,
      "learning_rate": 7.217600000000001e-07,
      "loss": 0.199,
      "step": 57990
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.043731797486543655,
      "learning_rate": 7.201600000000001e-07,
      "loss": 0.1995,
      "step": 58000
    },
    {
      "epoch": 1.8559999999999999,
      "eval_runtime": 75.87,
      "eval_samples_per_second": 131.804,
      "eval_steps_per_second": 8.238,
      "step": 58000
    },
    {
      "epoch": 1.85632,
      "grad_norm": 0.028085609897971153,
      "learning_rate": 7.1856e-07,
      "loss": 0.1994,
      "step": 58010
    },
    {
      "epoch": 1.85664,
      "grad_norm": 0.03421647474169731,
      "learning_rate": 7.1696e-07,
      "loss": 0.1991,
      "step": 58020
    },
    {
      "epoch": 1.85696,
      "grad_norm": 0.022717386484146118,
      "learning_rate": 7.153600000000001e-07,
      "loss": 0.1991,
      "step": 58030
    },
    {
      "epoch": 1.85728,
      "grad_norm": 0.01960391364991665,
      "learning_rate": 7.1376e-07,
      "loss": 0.2171,
      "step": 58040
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.02212201990187168,
      "learning_rate": 7.121600000000001e-07,
      "loss": 0.2001,
      "step": 58050
    },
    {
      "epoch": 1.85792,
      "grad_norm": 0.013634959235787392,
      "learning_rate": 7.1056e-07,
      "loss": 0.1998,
      "step": 58060
    },
    {
      "epoch": 1.85824,
      "grad_norm": 0.011214450933039188,
      "learning_rate": 7.089600000000001e-07,
      "loss": 0.208,
      "step": 58070
    },
    {
      "epoch": 1.85856,
      "grad_norm": 0.0472046360373497,
      "learning_rate": 7.0736e-07,
      "loss": 0.1994,
      "step": 58080
    },
    {
      "epoch": 1.85888,
      "grad_norm": 0.06934450566768646,
      "learning_rate": 7.0576e-07,
      "loss": 0.1994,
      "step": 58090
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.044087156653404236,
      "learning_rate": 7.041600000000001e-07,
      "loss": 0.2089,
      "step": 58100
    },
    {
      "epoch": 1.8595199999999998,
      "grad_norm": 0.03531147912144661,
      "learning_rate": 7.0256e-07,
      "loss": 0.1994,
      "step": 58110
    },
    {
      "epoch": 1.8598400000000002,
      "grad_norm": 0.01401865016669035,
      "learning_rate": 7.009600000000001e-07,
      "loss": 0.1991,
      "step": 58120
    },
    {
      "epoch": 1.86016,
      "grad_norm": 0.02938799373805523,
      "learning_rate": 6.993600000000001e-07,
      "loss": 0.2136,
      "step": 58130
    },
    {
      "epoch": 1.86048,
      "grad_norm": 0.03840502351522446,
      "learning_rate": 6.977600000000001e-07,
      "loss": 0.2016,
      "step": 58140
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.011777020059525967,
      "learning_rate": 6.961600000000001e-07,
      "loss": 0.1993,
      "step": 58150
    },
    {
      "epoch": 1.86112,
      "grad_norm": 0.03764784708619118,
      "learning_rate": 6.9456e-07,
      "loss": 0.199,
      "step": 58160
    },
    {
      "epoch": 1.86144,
      "grad_norm": 0.026773886755108833,
      "learning_rate": 6.9296e-07,
      "loss": 0.1993,
      "step": 58170
    },
    {
      "epoch": 1.8617599999999999,
      "grad_norm": 0.022959547117352486,
      "learning_rate": 6.913600000000001e-07,
      "loss": 0.1994,
      "step": 58180
    },
    {
      "epoch": 1.86208,
      "grad_norm": 0.01873290352523327,
      "learning_rate": 6.8976e-07,
      "loss": 0.1994,
      "step": 58190
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.04413016512989998,
      "learning_rate": 6.881600000000001e-07,
      "loss": 0.1992,
      "step": 58200
    },
    {
      "epoch": 1.86272,
      "grad_norm": 1.56471586227417,
      "learning_rate": 6.8656e-07,
      "loss": 0.2102,
      "step": 58210
    },
    {
      "epoch": 1.86304,
      "grad_norm": 1.1795345544815063,
      "learning_rate": 6.849600000000001e-07,
      "loss": 0.2187,
      "step": 58220
    },
    {
      "epoch": 1.8633600000000001,
      "grad_norm": 0.04022069647908211,
      "learning_rate": 6.833599999999999e-07,
      "loss": 0.2016,
      "step": 58230
    },
    {
      "epoch": 1.86368,
      "grad_norm": 0.01178728323429823,
      "learning_rate": 6.8176e-07,
      "loss": 0.199,
      "step": 58240
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.014878948219120502,
      "learning_rate": 6.8016e-07,
      "loss": 0.1994,
      "step": 58250
    },
    {
      "epoch": 1.86432,
      "grad_norm": 0.6892878413200378,
      "learning_rate": 6.7856e-07,
      "loss": 0.2005,
      "step": 58260
    },
    {
      "epoch": 1.86464,
      "grad_norm": 0.021298769861459732,
      "learning_rate": 6.769600000000001e-07,
      "loss": 0.1997,
      "step": 58270
    },
    {
      "epoch": 1.86496,
      "grad_norm": 0.01742374710738659,
      "learning_rate": 6.753600000000001e-07,
      "loss": 0.2134,
      "step": 58280
    },
    {
      "epoch": 1.86528,
      "grad_norm": 0.0329592227935791,
      "learning_rate": 6.737600000000001e-07,
      "loss": 0.1996,
      "step": 58290
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.013603238388895988,
      "learning_rate": 6.721600000000001e-07,
      "loss": 0.1989,
      "step": 58300
    },
    {
      "epoch": 1.86592,
      "grad_norm": 0.013385981321334839,
      "learning_rate": 6.705600000000001e-07,
      "loss": 0.1992,
      "step": 58310
    },
    {
      "epoch": 1.86624,
      "grad_norm": 0.019548894837498665,
      "learning_rate": 6.6896e-07,
      "loss": 0.2015,
      "step": 58320
    },
    {
      "epoch": 1.86656,
      "grad_norm": 0.03385259211063385,
      "learning_rate": 6.673600000000001e-07,
      "loss": 0.2056,
      "step": 58330
    },
    {
      "epoch": 1.86688,
      "grad_norm": 0.017359299585223198,
      "learning_rate": 6.6576e-07,
      "loss": 0.2114,
      "step": 58340
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.040070172399282455,
      "learning_rate": 6.641600000000001e-07,
      "loss": 0.2124,
      "step": 58350
    },
    {
      "epoch": 1.8675199999999998,
      "grad_norm": 0.04391350969672203,
      "learning_rate": 6.6256e-07,
      "loss": 0.2031,
      "step": 58360
    },
    {
      "epoch": 1.86784,
      "grad_norm": 0.02891668677330017,
      "learning_rate": 6.609600000000001e-07,
      "loss": 0.1991,
      "step": 58370
    },
    {
      "epoch": 1.86816,
      "grad_norm": 0.01793079450726509,
      "learning_rate": 6.5936e-07,
      "loss": 0.2111,
      "step": 58380
    },
    {
      "epoch": 1.86848,
      "grad_norm": 0.016967885196208954,
      "learning_rate": 6.5776e-07,
      "loss": 0.1994,
      "step": 58390
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.015265705063939095,
      "learning_rate": 6.5616e-07,
      "loss": 0.2071,
      "step": 58400
    },
    {
      "epoch": 1.8691200000000001,
      "grad_norm": 0.020883498713374138,
      "learning_rate": 6.5456e-07,
      "loss": 0.1995,
      "step": 58410
    },
    {
      "epoch": 1.86944,
      "grad_norm": 0.02553717978298664,
      "learning_rate": 6.529600000000001e-07,
      "loss": 0.2385,
      "step": 58420
    },
    {
      "epoch": 1.8697599999999999,
      "grad_norm": 0.0641915574669838,
      "learning_rate": 6.513600000000001e-07,
      "loss": 0.1993,
      "step": 58430
    },
    {
      "epoch": 1.87008,
      "grad_norm": 0.02289925143122673,
      "learning_rate": 6.497600000000001e-07,
      "loss": 0.2091,
      "step": 58440
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.034970447421073914,
      "learning_rate": 6.481600000000001e-07,
      "loss": 0.1994,
      "step": 58450
    },
    {
      "epoch": 1.87072,
      "grad_norm": 0.02594669908285141,
      "learning_rate": 6.465600000000001e-07,
      "loss": 0.1995,
      "step": 58460
    },
    {
      "epoch": 1.87104,
      "grad_norm": 0.013268472626805305,
      "learning_rate": 6.4496e-07,
      "loss": 0.2034,
      "step": 58470
    },
    {
      "epoch": 1.8713600000000001,
      "grad_norm": 0.024524511769413948,
      "learning_rate": 6.433600000000001e-07,
      "loss": 0.1992,
      "step": 58480
    },
    {
      "epoch": 1.87168,
      "grad_norm": 0.022477736696600914,
      "learning_rate": 6.4176e-07,
      "loss": 0.2001,
      "step": 58490
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.02478613518178463,
      "learning_rate": 6.401600000000001e-07,
      "loss": 0.1992,
      "step": 58500
    },
    {
      "epoch": 1.87232,
      "grad_norm": 0.04764401540160179,
      "learning_rate": 6.3856e-07,
      "loss": 0.1991,
      "step": 58510
    },
    {
      "epoch": 1.87264,
      "grad_norm": 0.02081076242029667,
      "learning_rate": 6.369600000000001e-07,
      "loss": 0.1999,
      "step": 58520
    },
    {
      "epoch": 1.87296,
      "grad_norm": 0.0615970715880394,
      "learning_rate": 6.3536e-07,
      "loss": 0.1993,
      "step": 58530
    },
    {
      "epoch": 1.8732799999999998,
      "grad_norm": 0.026052195578813553,
      "learning_rate": 6.3376e-07,
      "loss": 0.1991,
      "step": 58540
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.02333994396030903,
      "learning_rate": 6.3216e-07,
      "loss": 0.1994,
      "step": 58550
    },
    {
      "epoch": 1.87392,
      "grad_norm": 0.030963804572820663,
      "learning_rate": 6.305600000000001e-07,
      "loss": 0.1992,
      "step": 58560
    },
    {
      "epoch": 1.87424,
      "grad_norm": 0.1427084356546402,
      "learning_rate": 6.2896e-07,
      "loss": 0.2,
      "step": 58570
    },
    {
      "epoch": 1.87456,
      "grad_norm": 0.009369011968374252,
      "learning_rate": 6.273600000000001e-07,
      "loss": 0.1993,
      "step": 58580
    },
    {
      "epoch": 1.87488,
      "grad_norm": 0.0427350178360939,
      "learning_rate": 6.257600000000001e-07,
      "loss": 0.1992,
      "step": 58590
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.09773130714893341,
      "learning_rate": 6.2416e-07,
      "loss": 0.1993,
      "step": 58600
    },
    {
      "epoch": 1.8755199999999999,
      "grad_norm": 0.07884369045495987,
      "learning_rate": 6.2256e-07,
      "loss": 0.1995,
      "step": 58610
    },
    {
      "epoch": 1.87584,
      "grad_norm": 0.03156007453799248,
      "learning_rate": 6.2096e-07,
      "loss": 0.1991,
      "step": 58620
    },
    {
      "epoch": 1.87616,
      "grad_norm": 0.01814030297100544,
      "learning_rate": 6.1936e-07,
      "loss": 0.1992,
      "step": 58630
    },
    {
      "epoch": 1.87648,
      "grad_norm": 0.01116140652447939,
      "learning_rate": 6.1776e-07,
      "loss": 0.1993,
      "step": 58640
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.02876756340265274,
      "learning_rate": 6.1616e-07,
      "loss": 0.1992,
      "step": 58650
    },
    {
      "epoch": 1.8771200000000001,
      "grad_norm": 0.020722104236483574,
      "learning_rate": 6.1456e-07,
      "loss": 0.1991,
      "step": 58660
    },
    {
      "epoch": 1.87744,
      "grad_norm": 0.04434175044298172,
      "learning_rate": 6.129600000000001e-07,
      "loss": 0.2102,
      "step": 58670
    },
    {
      "epoch": 1.8777599999999999,
      "grad_norm": 0.018942940980196,
      "learning_rate": 6.1136e-07,
      "loss": 0.2631,
      "step": 58680
    },
    {
      "epoch": 1.87808,
      "grad_norm": 0.05694327503442764,
      "learning_rate": 6.0976e-07,
      "loss": 0.1993,
      "step": 58690
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.04408351331949234,
      "learning_rate": 6.081600000000001e-07,
      "loss": 0.1991,
      "step": 58700
    },
    {
      "epoch": 1.87872,
      "grad_norm": 0.0202164463698864,
      "learning_rate": 6.065600000000001e-07,
      "loss": 0.199,
      "step": 58710
    },
    {
      "epoch": 1.87904,
      "grad_norm": 0.021893566474318504,
      "learning_rate": 6.049600000000001e-07,
      "loss": 0.1993,
      "step": 58720
    },
    {
      "epoch": 1.8793600000000001,
      "grad_norm": 0.024212080985307693,
      "learning_rate": 6.033600000000001e-07,
      "loss": 0.1991,
      "step": 58730
    },
    {
      "epoch": 1.87968,
      "grad_norm": 0.04137757420539856,
      "learning_rate": 6.017600000000001e-07,
      "loss": 0.1998,
      "step": 58740
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.03158514201641083,
      "learning_rate": 6.0016e-07,
      "loss": 0.1991,
      "step": 58750
    },
    {
      "epoch": 1.88032,
      "grad_norm": 0.022491298615932465,
      "learning_rate": 5.9856e-07,
      "loss": 0.2002,
      "step": 58760
    },
    {
      "epoch": 1.88064,
      "grad_norm": 0.014690956100821495,
      "learning_rate": 5.9696e-07,
      "loss": 0.2113,
      "step": 58770
    },
    {
      "epoch": 1.88096,
      "grad_norm": 0.03398488089442253,
      "learning_rate": 5.9536e-07,
      "loss": 0.2004,
      "step": 58780
    },
    {
      "epoch": 1.8812799999999998,
      "grad_norm": 0.019343219697475433,
      "learning_rate": 5.9376e-07,
      "loss": 0.2044,
      "step": 58790
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.010112096555531025,
      "learning_rate": 5.9216e-07,
      "loss": 0.2195,
      "step": 58800
    },
    {
      "epoch": 1.88192,
      "grad_norm": 0.02345028705894947,
      "learning_rate": 5.905600000000001e-07,
      "loss": 0.1991,
      "step": 58810
    },
    {
      "epoch": 1.88224,
      "grad_norm": 0.1335035115480423,
      "learning_rate": 5.889600000000001e-07,
      "loss": 0.1994,
      "step": 58820
    },
    {
      "epoch": 1.88256,
      "grad_norm": 0.017957445234060287,
      "learning_rate": 5.8736e-07,
      "loss": 0.2022,
      "step": 58830
    },
    {
      "epoch": 1.88288,
      "grad_norm": 0.03638996556401253,
      "learning_rate": 5.857600000000001e-07,
      "loss": 0.1994,
      "step": 58840
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.043504007160663605,
      "learning_rate": 5.841600000000001e-07,
      "loss": 0.1992,
      "step": 58850
    },
    {
      "epoch": 1.8835199999999999,
      "grad_norm": 0.01648944616317749,
      "learning_rate": 5.825600000000001e-07,
      "loss": 0.1992,
      "step": 58860
    },
    {
      "epoch": 1.88384,
      "grad_norm": 0.028163550421595573,
      "learning_rate": 5.809600000000001e-07,
      "loss": 0.2132,
      "step": 58870
    },
    {
      "epoch": 1.88416,
      "grad_norm": 0.009183095768094063,
      "learning_rate": 5.793600000000001e-07,
      "loss": 0.1994,
      "step": 58880
    },
    {
      "epoch": 1.88448,
      "grad_norm": 0.018253961578011513,
      "learning_rate": 5.777600000000001e-07,
      "loss": 0.1994,
      "step": 58890
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.014561070129275322,
      "learning_rate": 5.7616e-07,
      "loss": 0.1996,
      "step": 58900
    },
    {
      "epoch": 1.8851200000000001,
      "grad_norm": 0.012745004147291183,
      "learning_rate": 5.7456e-07,
      "loss": 0.1996,
      "step": 58910
    },
    {
      "epoch": 1.88544,
      "grad_norm": 0.013133980333805084,
      "learning_rate": 5.7296e-07,
      "loss": 0.1994,
      "step": 58920
    },
    {
      "epoch": 1.8857599999999999,
      "grad_norm": 0.015413574874401093,
      "learning_rate": 5.7136e-07,
      "loss": 0.2139,
      "step": 58930
    },
    {
      "epoch": 1.88608,
      "grad_norm": 0.043204523622989655,
      "learning_rate": 5.6976e-07,
      "loss": 0.199,
      "step": 58940
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.020465075969696045,
      "learning_rate": 5.681600000000001e-07,
      "loss": 0.1994,
      "step": 58950
    },
    {
      "epoch": 1.88672,
      "grad_norm": 0.04593552276492119,
      "learning_rate": 5.665600000000001e-07,
      "loss": 0.1992,
      "step": 58960
    },
    {
      "epoch": 1.88704,
      "grad_norm": 0.022405903786420822,
      "learning_rate": 5.6496e-07,
      "loss": 0.1995,
      "step": 58970
    },
    {
      "epoch": 1.8873600000000001,
      "grad_norm": 0.017486248165369034,
      "learning_rate": 5.6336e-07,
      "loss": 0.199,
      "step": 58980
    },
    {
      "epoch": 1.88768,
      "grad_norm": 0.02180139720439911,
      "learning_rate": 5.6176e-07,
      "loss": 0.1992,
      "step": 58990
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.011838167905807495,
      "learning_rate": 5.601600000000001e-07,
      "loss": 0.1992,
      "step": 59000
    },
    {
      "epoch": 1.888,
      "eval_runtime": 75.652,
      "eval_samples_per_second": 132.184,
      "eval_steps_per_second": 8.262,
      "step": 59000
    },
    {
      "epoch": 1.88832,
      "grad_norm": 0.2217116355895996,
      "learning_rate": 5.585600000000001e-07,
      "loss": 0.2089,
      "step": 59010
    },
    {
      "epoch": 1.88864,
      "grad_norm": 0.020122041925787926,
      "learning_rate": 5.569600000000001e-07,
      "loss": 0.2072,
      "step": 59020
    },
    {
      "epoch": 1.88896,
      "grad_norm": 0.025441069155931473,
      "learning_rate": 5.553600000000001e-07,
      "loss": 0.2105,
      "step": 59030
    },
    {
      "epoch": 1.8892799999999998,
      "grad_norm": 0.026196056976914406,
      "learning_rate": 5.537600000000001e-07,
      "loss": 0.1991,
      "step": 59040
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.014660436660051346,
      "learning_rate": 5.5216e-07,
      "loss": 0.2133,
      "step": 59050
    },
    {
      "epoch": 1.88992,
      "grad_norm": 0.015758203342556953,
      "learning_rate": 5.5056e-07,
      "loss": 0.216,
      "step": 59060
    },
    {
      "epoch": 1.89024,
      "grad_norm": 0.018272653222084045,
      "learning_rate": 5.4896e-07,
      "loss": 0.199,
      "step": 59070
    },
    {
      "epoch": 1.89056,
      "grad_norm": 0.024888651445508003,
      "learning_rate": 5.4736e-07,
      "loss": 0.2065,
      "step": 59080
    },
    {
      "epoch": 1.8908800000000001,
      "grad_norm": 0.0488622784614563,
      "learning_rate": 5.457600000000001e-07,
      "loss": 0.1992,
      "step": 59090
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.780725359916687,
      "learning_rate": 5.441600000000001e-07,
      "loss": 0.2282,
      "step": 59100
    },
    {
      "epoch": 1.8915199999999999,
      "grad_norm": 0.02490456961095333,
      "learning_rate": 5.425600000000001e-07,
      "loss": 0.1993,
      "step": 59110
    },
    {
      "epoch": 1.89184,
      "grad_norm": 0.023256322368979454,
      "learning_rate": 5.4096e-07,
      "loss": 0.1993,
      "step": 59120
    },
    {
      "epoch": 1.89216,
      "grad_norm": 0.03355042263865471,
      "learning_rate": 5.3936e-07,
      "loss": 0.2081,
      "step": 59130
    },
    {
      "epoch": 1.89248,
      "grad_norm": 0.010701856575906277,
      "learning_rate": 5.3776e-07,
      "loss": 0.199,
      "step": 59140
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.01460430771112442,
      "learning_rate": 5.3616e-07,
      "loss": 0.1991,
      "step": 59150
    },
    {
      "epoch": 1.8931200000000001,
      "grad_norm": 0.022405503317713737,
      "learning_rate": 5.3456e-07,
      "loss": 0.1991,
      "step": 59160
    },
    {
      "epoch": 1.89344,
      "grad_norm": 0.018468931317329407,
      "learning_rate": 5.329600000000001e-07,
      "loss": 0.1998,
      "step": 59170
    },
    {
      "epoch": 1.8937599999999999,
      "grad_norm": 0.035711899399757385,
      "learning_rate": 5.313600000000001e-07,
      "loss": 0.1993,
      "step": 59180
    },
    {
      "epoch": 1.89408,
      "grad_norm": 0.019579235464334488,
      "learning_rate": 5.297600000000001e-07,
      "loss": 0.1993,
      "step": 59190
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.012743302620947361,
      "learning_rate": 5.2816e-07,
      "loss": 0.1989,
      "step": 59200
    },
    {
      "epoch": 1.89472,
      "grad_norm": 0.021907249465584755,
      "learning_rate": 5.2656e-07,
      "loss": 0.1993,
      "step": 59210
    },
    {
      "epoch": 1.8950399999999998,
      "grad_norm": 1.6156049966812134,
      "learning_rate": 5.2496e-07,
      "loss": 0.2021,
      "step": 59220
    },
    {
      "epoch": 1.8953600000000002,
      "grad_norm": 0.02935941331088543,
      "learning_rate": 5.2336e-07,
      "loss": 0.214,
      "step": 59230
    },
    {
      "epoch": 1.89568,
      "grad_norm": 0.02749924175441265,
      "learning_rate": 5.217600000000001e-07,
      "loss": 0.1993,
      "step": 59240
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.02990117482841015,
      "learning_rate": 5.201600000000001e-07,
      "loss": 0.1991,
      "step": 59250
    },
    {
      "epoch": 1.89632,
      "grad_norm": 0.023257434368133545,
      "learning_rate": 5.185600000000001e-07,
      "loss": 0.2089,
      "step": 59260
    },
    {
      "epoch": 1.89664,
      "grad_norm": 0.026963932439684868,
      "learning_rate": 5.1696e-07,
      "loss": 0.2002,
      "step": 59270
    },
    {
      "epoch": 1.89696,
      "grad_norm": 0.014070544391870499,
      "learning_rate": 5.1536e-07,
      "loss": 0.2159,
      "step": 59280
    },
    {
      "epoch": 1.8972799999999999,
      "grad_norm": 0.03332750126719475,
      "learning_rate": 5.1376e-07,
      "loss": 0.1994,
      "step": 59290
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.06957507878541946,
      "learning_rate": 5.1216e-07,
      "loss": 0.202,
      "step": 59300
    },
    {
      "epoch": 1.89792,
      "grad_norm": 0.04156516119837761,
      "learning_rate": 5.1056e-07,
      "loss": 0.1992,
      "step": 59310
    },
    {
      "epoch": 1.89824,
      "grad_norm": 0.02971847541630268,
      "learning_rate": 5.0896e-07,
      "loss": 0.1991,
      "step": 59320
    },
    {
      "epoch": 1.89856,
      "grad_norm": 0.0694027990102768,
      "learning_rate": 5.073600000000001e-07,
      "loss": 0.1999,
      "step": 59330
    },
    {
      "epoch": 1.8988800000000001,
      "grad_norm": 0.013315610587596893,
      "learning_rate": 5.0576e-07,
      "loss": 0.1991,
      "step": 59340
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.01173201110213995,
      "learning_rate": 5.0416e-07,
      "loss": 0.2111,
      "step": 59350
    },
    {
      "epoch": 1.8995199999999999,
      "grad_norm": 0.03562236577272415,
      "learning_rate": 5.0256e-07,
      "loss": 0.1993,
      "step": 59360
    },
    {
      "epoch": 1.89984,
      "grad_norm": 0.033623188734054565,
      "learning_rate": 5.0096e-07,
      "loss": 0.1991,
      "step": 59370
    },
    {
      "epoch": 1.90016,
      "grad_norm": 0.01298154704272747,
      "learning_rate": 4.993600000000001e-07,
      "loss": 0.1992,
      "step": 59380
    },
    {
      "epoch": 1.90048,
      "grad_norm": 0.018013881519436836,
      "learning_rate": 4.977600000000001e-07,
      "loss": 0.1994,
      "step": 59390
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.026440221816301346,
      "learning_rate": 4.961600000000001e-07,
      "loss": 0.2132,
      "step": 59400
    },
    {
      "epoch": 1.9011200000000001,
      "grad_norm": 0.03621494770050049,
      "learning_rate": 4.945600000000001e-07,
      "loss": 0.1992,
      "step": 59410
    },
    {
      "epoch": 1.90144,
      "grad_norm": 0.04339585080742836,
      "learning_rate": 4.9296e-07,
      "loss": 0.1991,
      "step": 59420
    },
    {
      "epoch": 1.90176,
      "grad_norm": 0.042831823229789734,
      "learning_rate": 4.9136e-07,
      "loss": 0.1991,
      "step": 59430
    },
    {
      "epoch": 1.90208,
      "grad_norm": 0.04697178304195404,
      "learning_rate": 4.8976e-07,
      "loss": 0.1991,
      "step": 59440
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.14076796174049377,
      "learning_rate": 4.8816e-07,
      "loss": 0.1994,
      "step": 59450
    },
    {
      "epoch": 1.90272,
      "grad_norm": 0.030958842486143112,
      "learning_rate": 4.8656e-07,
      "loss": 0.1992,
      "step": 59460
    },
    {
      "epoch": 1.9030399999999998,
      "grad_norm": 0.029871517792344093,
      "learning_rate": 4.8496e-07,
      "loss": 0.1992,
      "step": 59470
    },
    {
      "epoch": 1.9033600000000002,
      "grad_norm": 0.030729474499821663,
      "learning_rate": 4.833600000000001e-07,
      "loss": 0.1997,
      "step": 59480
    },
    {
      "epoch": 1.90368,
      "grad_norm": 0.04380302131175995,
      "learning_rate": 4.8176e-07,
      "loss": 0.1992,
      "step": 59490
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.02790387161076069,
      "learning_rate": 4.8016e-07,
      "loss": 0.1992,
      "step": 59500
    },
    {
      "epoch": 1.90432,
      "grad_norm": 0.011071862652897835,
      "learning_rate": 4.7856e-07,
      "loss": 0.1994,
      "step": 59510
    },
    {
      "epoch": 1.90464,
      "grad_norm": 0.025235000997781754,
      "learning_rate": 4.769600000000001e-07,
      "loss": 0.2003,
      "step": 59520
    },
    {
      "epoch": 1.90496,
      "grad_norm": 0.013772963546216488,
      "learning_rate": 4.7536000000000004e-07,
      "loss": 0.1993,
      "step": 59530
    },
    {
      "epoch": 1.9052799999999999,
      "grad_norm": 0.06076911464333534,
      "learning_rate": 4.7376000000000003e-07,
      "loss": 0.1992,
      "step": 59540
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.018549740314483643,
      "learning_rate": 4.7216000000000007e-07,
      "loss": 0.1996,
      "step": 59550
    },
    {
      "epoch": 1.90592,
      "grad_norm": 0.025785699486732483,
      "learning_rate": 4.7056000000000005e-07,
      "loss": 0.2297,
      "step": 59560
    },
    {
      "epoch": 1.90624,
      "grad_norm": 0.016368025913834572,
      "learning_rate": 4.6896000000000004e-07,
      "loss": 0.1991,
      "step": 59570
    },
    {
      "epoch": 1.90656,
      "grad_norm": 0.12952816486358643,
      "learning_rate": 4.6736e-07,
      "loss": 0.2001,
      "step": 59580
    },
    {
      "epoch": 1.9068800000000001,
      "grad_norm": 0.037093572318553925,
      "learning_rate": 4.6576e-07,
      "loss": 0.2113,
      "step": 59590
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.027962546795606613,
      "learning_rate": 4.6416000000000005e-07,
      "loss": 0.199,
      "step": 59600
    },
    {
      "epoch": 1.9075199999999999,
      "grad_norm": 0.03661825507879257,
      "learning_rate": 4.6256000000000003e-07,
      "loss": 0.1993,
      "step": 59610
    },
    {
      "epoch": 1.90784,
      "grad_norm": 0.019862305372953415,
      "learning_rate": 4.6096e-07,
      "loss": 0.1992,
      "step": 59620
    },
    {
      "epoch": 1.90816,
      "grad_norm": 0.01402343064546585,
      "learning_rate": 4.5936e-07,
      "loss": 0.1995,
      "step": 59630
    },
    {
      "epoch": 1.90848,
      "grad_norm": 0.01709802821278572,
      "learning_rate": 4.5776e-07,
      "loss": 0.199,
      "step": 59640
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.019606783986091614,
      "learning_rate": 4.5616000000000003e-07,
      "loss": 0.1992,
      "step": 59650
    },
    {
      "epoch": 1.9091200000000002,
      "grad_norm": 0.023207349702715874,
      "learning_rate": 4.5456000000000007e-07,
      "loss": 0.1995,
      "step": 59660
    },
    {
      "epoch": 1.90944,
      "grad_norm": 0.020932437852025032,
      "learning_rate": 4.5296000000000005e-07,
      "loss": 0.2095,
      "step": 59670
    },
    {
      "epoch": 1.90976,
      "grad_norm": 0.0271192267537117,
      "learning_rate": 4.5136000000000004e-07,
      "loss": 0.1999,
      "step": 59680
    },
    {
      "epoch": 1.91008,
      "grad_norm": 0.020954584702849388,
      "learning_rate": 4.497600000000001e-07,
      "loss": 0.2011,
      "step": 59690
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.016233911737799644,
      "learning_rate": 4.4816000000000006e-07,
      "loss": 0.2157,
      "step": 59700
    },
    {
      "epoch": 1.91072,
      "grad_norm": 0.029026882722973824,
      "learning_rate": 4.4656000000000005e-07,
      "loss": 0.1992,
      "step": 59710
    },
    {
      "epoch": 1.9110399999999998,
      "grad_norm": 0.022208139300346375,
      "learning_rate": 4.4496000000000004e-07,
      "loss": 0.1993,
      "step": 59720
    },
    {
      "epoch": 1.91136,
      "grad_norm": 0.034989167004823685,
      "learning_rate": 4.4336e-07,
      "loss": 0.1991,
      "step": 59730
    },
    {
      "epoch": 1.91168,
      "grad_norm": 0.014313415624201298,
      "learning_rate": 4.4176000000000006e-07,
      "loss": 0.1991,
      "step": 59740
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.012529932893812656,
      "learning_rate": 4.4016000000000005e-07,
      "loss": 0.1991,
      "step": 59750
    },
    {
      "epoch": 1.91232,
      "grad_norm": 0.028309021145105362,
      "learning_rate": 4.3856000000000003e-07,
      "loss": 0.1992,
      "step": 59760
    },
    {
      "epoch": 1.9126400000000001,
      "grad_norm": 0.014820941723883152,
      "learning_rate": 4.3696e-07,
      "loss": 0.2122,
      "step": 59770
    },
    {
      "epoch": 1.91296,
      "grad_norm": 0.034456852823495865,
      "learning_rate": 4.3536e-07,
      "loss": 0.1991,
      "step": 59780
    },
    {
      "epoch": 1.9132799999999999,
      "grad_norm": 0.0432293526828289,
      "learning_rate": 4.3376000000000004e-07,
      "loss": 0.1991,
      "step": 59790
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.027090273797512054,
      "learning_rate": 4.3216e-07,
      "loss": 0.2006,
      "step": 59800
    },
    {
      "epoch": 1.91392,
      "grad_norm": 0.017162621021270752,
      "learning_rate": 4.3056e-07,
      "loss": 0.1992,
      "step": 59810
    },
    {
      "epoch": 1.91424,
      "grad_norm": 0.016172321513295174,
      "learning_rate": 4.2896e-07,
      "loss": 0.2137,
      "step": 59820
    },
    {
      "epoch": 1.91456,
      "grad_norm": 0.009753107093274593,
      "learning_rate": 4.273600000000001e-07,
      "loss": 0.199,
      "step": 59830
    },
    {
      "epoch": 1.9148800000000001,
      "grad_norm": 0.02099497988820076,
      "learning_rate": 4.257600000000001e-07,
      "loss": 0.1993,
      "step": 59840
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.034025099128484726,
      "learning_rate": 4.2416000000000006e-07,
      "loss": 0.1994,
      "step": 59850
    },
    {
      "epoch": 1.91552,
      "grad_norm": 0.03326684236526489,
      "learning_rate": 4.2256000000000005e-07,
      "loss": 0.2116,
      "step": 59860
    },
    {
      "epoch": 1.91584,
      "grad_norm": 0.034239284694194794,
      "learning_rate": 4.2096000000000003e-07,
      "loss": 0.1994,
      "step": 59870
    },
    {
      "epoch": 1.91616,
      "grad_norm": 0.02417207509279251,
      "learning_rate": 4.1936000000000007e-07,
      "loss": 0.2032,
      "step": 59880
    },
    {
      "epoch": 1.91648,
      "grad_norm": 0.020734572783112526,
      "learning_rate": 4.1776000000000006e-07,
      "loss": 0.1997,
      "step": 59890
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.013899313285946846,
      "learning_rate": 4.1616000000000004e-07,
      "loss": 0.2143,
      "step": 59900
    },
    {
      "epoch": 1.9171200000000002,
      "grad_norm": 0.031174128875136375,
      "learning_rate": 4.1456000000000003e-07,
      "loss": 0.1992,
      "step": 59910
    },
    {
      "epoch": 1.91744,
      "grad_norm": 1.645553708076477,
      "learning_rate": 4.1296e-07,
      "loss": 0.2381,
      "step": 59920
    },
    {
      "epoch": 1.91776,
      "grad_norm": 0.0406520776450634,
      "learning_rate": 4.1136e-07,
      "loss": 0.1992,
      "step": 59930
    },
    {
      "epoch": 1.91808,
      "grad_norm": 0.012239938601851463,
      "learning_rate": 4.0976000000000004e-07,
      "loss": 0.2116,
      "step": 59940
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.03886822611093521,
      "learning_rate": 4.0816e-07,
      "loss": 0.1993,
      "step": 59950
    },
    {
      "epoch": 1.91872,
      "grad_norm": 0.0350310392677784,
      "learning_rate": 4.0656e-07,
      "loss": 0.1993,
      "step": 59960
    },
    {
      "epoch": 1.9190399999999999,
      "grad_norm": 0.014564508572220802,
      "learning_rate": 4.0496e-07,
      "loss": 0.1991,
      "step": 59970
    },
    {
      "epoch": 1.91936,
      "grad_norm": 0.01877877116203308,
      "learning_rate": 4.0336e-07,
      "loss": 0.1992,
      "step": 59980
    },
    {
      "epoch": 1.91968,
      "grad_norm": 0.04479644075036049,
      "learning_rate": 4.0176000000000007e-07,
      "loss": 0.1994,
      "step": 59990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.020678285509347916,
      "learning_rate": 4.0016000000000006e-07,
      "loss": 0.23,
      "step": 60000
    },
    {
      "epoch": 1.92,
      "eval_runtime": 78.2199,
      "eval_samples_per_second": 127.845,
      "eval_steps_per_second": 7.99,
      "step": 60000
    },
    {
      "epoch": 1.92032,
      "grad_norm": 0.7537415623664856,
      "learning_rate": 3.9856000000000004e-07,
      "loss": 0.2154,
      "step": 60010
    },
    {
      "epoch": 1.9206400000000001,
      "grad_norm": 0.01718566194176674,
      "learning_rate": 3.9696000000000003e-07,
      "loss": 0.1996,
      "step": 60020
    },
    {
      "epoch": 1.92096,
      "grad_norm": 0.015587561763823032,
      "learning_rate": 3.9536000000000007e-07,
      "loss": 0.1992,
      "step": 60030
    },
    {
      "epoch": 1.9212799999999999,
      "grad_norm": 0.028824234381318092,
      "learning_rate": 3.9376000000000005e-07,
      "loss": 0.1991,
      "step": 60040
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.0163892712444067,
      "learning_rate": 3.9216000000000004e-07,
      "loss": 0.1993,
      "step": 60050
    },
    {
      "epoch": 1.92192,
      "grad_norm": 0.011624916456639767,
      "learning_rate": 3.9056e-07,
      "loss": 0.2103,
      "step": 60060
    },
    {
      "epoch": 1.92224,
      "grad_norm": 0.0419917069375515,
      "learning_rate": 3.8896e-07,
      "loss": 0.2229,
      "step": 60070
    },
    {
      "epoch": 1.92256,
      "grad_norm": 0.020811783149838448,
      "learning_rate": 3.8736000000000005e-07,
      "loss": 0.1993,
      "step": 60080
    },
    {
      "epoch": 1.9228800000000001,
      "grad_norm": 0.020423192530870438,
      "learning_rate": 3.8576000000000003e-07,
      "loss": 0.1995,
      "step": 60090
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.03573930263519287,
      "learning_rate": 3.8416e-07,
      "loss": 0.1998,
      "step": 60100
    },
    {
      "epoch": 1.92352,
      "grad_norm": 0.037254661321640015,
      "learning_rate": 3.8256e-07,
      "loss": 0.1991,
      "step": 60110
    },
    {
      "epoch": 1.92384,
      "grad_norm": 0.05494852364063263,
      "learning_rate": 3.8096e-07,
      "loss": 0.2098,
      "step": 60120
    },
    {
      "epoch": 1.92416,
      "grad_norm": 0.03410492092370987,
      "learning_rate": 3.7936000000000003e-07,
      "loss": 0.1991,
      "step": 60130
    },
    {
      "epoch": 1.92448,
      "grad_norm": 0.04185876622796059,
      "learning_rate": 3.7776e-07,
      "loss": 0.1996,
      "step": 60140
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.043038394302129745,
      "learning_rate": 3.7616e-07,
      "loss": 0.2087,
      "step": 60150
    },
    {
      "epoch": 1.9251200000000002,
      "grad_norm": 0.027575718238949776,
      "learning_rate": 3.7456000000000004e-07,
      "loss": 0.1991,
      "step": 60160
    },
    {
      "epoch": 1.92544,
      "grad_norm": 0.016173720359802246,
      "learning_rate": 3.729600000000001e-07,
      "loss": 0.1994,
      "step": 60170
    },
    {
      "epoch": 1.92576,
      "grad_norm": 0.05088232085108757,
      "learning_rate": 3.7136000000000006e-07,
      "loss": 0.2073,
      "step": 60180
    },
    {
      "epoch": 1.92608,
      "grad_norm": 0.028105154633522034,
      "learning_rate": 3.6976000000000005e-07,
      "loss": 0.1993,
      "step": 60190
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.009918014518916607,
      "learning_rate": 3.6816000000000004e-07,
      "loss": 0.2123,
      "step": 60200
    },
    {
      "epoch": 1.92672,
      "grad_norm": 1.0611629486083984,
      "learning_rate": 3.6656e-07,
      "loss": 0.2162,
      "step": 60210
    },
    {
      "epoch": 1.9270399999999999,
      "grad_norm": 0.016386333853006363,
      "learning_rate": 3.6496000000000006e-07,
      "loss": 0.1993,
      "step": 60220
    },
    {
      "epoch": 1.92736,
      "grad_norm": 0.019740914925932884,
      "learning_rate": 3.6336000000000005e-07,
      "loss": 0.1995,
      "step": 60230
    },
    {
      "epoch": 1.92768,
      "grad_norm": 0.009710931219160557,
      "learning_rate": 3.6176000000000003e-07,
      "loss": 0.199,
      "step": 60240
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.012960171326994896,
      "learning_rate": 3.6016e-07,
      "loss": 0.2006,
      "step": 60250
    },
    {
      "epoch": 1.92832,
      "grad_norm": 0.009492925368249416,
      "learning_rate": 3.5856e-07,
      "loss": 0.1995,
      "step": 60260
    },
    {
      "epoch": 1.9286400000000001,
      "grad_norm": 0.01710566319525242,
      "learning_rate": 3.5696000000000004e-07,
      "loss": 0.1993,
      "step": 60270
    },
    {
      "epoch": 1.92896,
      "grad_norm": 0.011187996715307236,
      "learning_rate": 3.5536000000000003e-07,
      "loss": 0.1991,
      "step": 60280
    },
    {
      "epoch": 1.9292799999999999,
      "grad_norm": 0.04328587278723717,
      "learning_rate": 3.5376e-07,
      "loss": 0.1994,
      "step": 60290
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.019209979102015495,
      "learning_rate": 3.5216e-07,
      "loss": 0.1991,
      "step": 60300
    },
    {
      "epoch": 1.92992,
      "grad_norm": 0.02264741249382496,
      "learning_rate": 3.5056e-07,
      "loss": 0.1992,
      "step": 60310
    },
    {
      "epoch": 1.93024,
      "grad_norm": 0.03592739254236221,
      "learning_rate": 3.489600000000001e-07,
      "loss": 0.2009,
      "step": 60320
    },
    {
      "epoch": 1.93056,
      "grad_norm": 0.03488888591527939,
      "learning_rate": 3.4736000000000006e-07,
      "loss": 0.1991,
      "step": 60330
    },
    {
      "epoch": 1.9308800000000002,
      "grad_norm": 0.028111208230257034,
      "learning_rate": 3.4576000000000005e-07,
      "loss": 0.2041,
      "step": 60340
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.08062496036291122,
      "learning_rate": 3.4416000000000003e-07,
      "loss": 0.2003,
      "step": 60350
    },
    {
      "epoch": 1.93152,
      "grad_norm": 1.5344200134277344,
      "learning_rate": 3.4256000000000007e-07,
      "loss": 0.232,
      "step": 60360
    },
    {
      "epoch": 1.93184,
      "grad_norm": 0.010984900407493114,
      "learning_rate": 3.4096000000000006e-07,
      "loss": 0.1991,
      "step": 60370
    },
    {
      "epoch": 1.93216,
      "grad_norm": 0.04276056960225105,
      "learning_rate": 3.3936000000000004e-07,
      "loss": 0.2049,
      "step": 60380
    },
    {
      "epoch": 1.93248,
      "grad_norm": 0.016669845208525658,
      "learning_rate": 3.3776000000000003e-07,
      "loss": 0.1993,
      "step": 60390
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.8345122337341309,
      "learning_rate": 3.3616e-07,
      "loss": 0.2141,
      "step": 60400
    },
    {
      "epoch": 1.93312,
      "grad_norm": 0.008678772486746311,
      "learning_rate": 3.3456e-07,
      "loss": 0.2018,
      "step": 60410
    },
    {
      "epoch": 1.93344,
      "grad_norm": 0.023885153234004974,
      "learning_rate": 3.3296000000000004e-07,
      "loss": 0.1992,
      "step": 60420
    },
    {
      "epoch": 1.93376,
      "grad_norm": 0.017349811270833015,
      "learning_rate": 3.3136e-07,
      "loss": 0.199,
      "step": 60430
    },
    {
      "epoch": 1.93408,
      "grad_norm": 0.025713417679071426,
      "learning_rate": 3.2976e-07,
      "loss": 0.1998,
      "step": 60440
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.02707546576857567,
      "learning_rate": 3.2816e-07,
      "loss": 0.1991,
      "step": 60450
    },
    {
      "epoch": 1.93472,
      "grad_norm": 0.04966301843523979,
      "learning_rate": 3.2656e-07,
      "loss": 0.233,
      "step": 60460
    },
    {
      "epoch": 1.9350399999999999,
      "grad_norm": 0.04262188822031021,
      "learning_rate": 3.2496e-07,
      "loss": 0.1994,
      "step": 60470
    },
    {
      "epoch": 1.93536,
      "grad_norm": 0.0885007232427597,
      "learning_rate": 3.2336e-07,
      "loss": 0.1993,
      "step": 60480
    },
    {
      "epoch": 1.93568,
      "grad_norm": 0.012736388482153416,
      "learning_rate": 3.2176000000000004e-07,
      "loss": 0.1991,
      "step": 60490
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.01680377684533596,
      "learning_rate": 3.2016000000000003e-07,
      "loss": 0.1994,
      "step": 60500
    },
    {
      "epoch": 1.93632,
      "grad_norm": 0.014273397624492645,
      "learning_rate": 3.1856000000000007e-07,
      "loss": 0.199,
      "step": 60510
    },
    {
      "epoch": 1.9366400000000001,
      "grad_norm": 0.01345821563154459,
      "learning_rate": 3.1696000000000005e-07,
      "loss": 0.2112,
      "step": 60520
    },
    {
      "epoch": 1.93696,
      "grad_norm": 0.02591857500374317,
      "learning_rate": 3.1536000000000004e-07,
      "loss": 0.2136,
      "step": 60530
    },
    {
      "epoch": 1.93728,
      "grad_norm": 0.01908695511519909,
      "learning_rate": 3.1376e-07,
      "loss": 0.1992,
      "step": 60540
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.13747566938400269,
      "learning_rate": 3.1216e-07,
      "loss": 0.1994,
      "step": 60550
    },
    {
      "epoch": 1.93792,
      "grad_norm": 0.007400827948004007,
      "learning_rate": 3.1056000000000005e-07,
      "loss": 0.199,
      "step": 60560
    },
    {
      "epoch": 1.93824,
      "grad_norm": 0.04424208775162697,
      "learning_rate": 3.0896000000000003e-07,
      "loss": 0.1991,
      "step": 60570
    },
    {
      "epoch": 1.9385599999999998,
      "grad_norm": 0.030510511249303818,
      "learning_rate": 3.0736e-07,
      "loss": 0.2006,
      "step": 60580
    },
    {
      "epoch": 1.9388800000000002,
      "grad_norm": 0.011386283673346043,
      "learning_rate": 3.0576e-07,
      "loss": 0.2299,
      "step": 60590
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.017611540853977203,
      "learning_rate": 3.0416e-07,
      "loss": 0.1992,
      "step": 60600
    },
    {
      "epoch": 1.93952,
      "grad_norm": 0.020093843340873718,
      "learning_rate": 3.0256000000000003e-07,
      "loss": 0.1993,
      "step": 60610
    },
    {
      "epoch": 1.93984,
      "grad_norm": 0.02162579447031021,
      "learning_rate": 3.0096e-07,
      "loss": 0.2107,
      "step": 60620
    },
    {
      "epoch": 1.94016,
      "grad_norm": 0.03273112326860428,
      "learning_rate": 2.9936000000000005e-07,
      "loss": 0.1997,
      "step": 60630
    },
    {
      "epoch": 1.94048,
      "grad_norm": 0.027500910684466362,
      "learning_rate": 2.9776000000000004e-07,
      "loss": 0.1996,
      "step": 60640
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.03266957029700279,
      "learning_rate": 2.9616e-07,
      "loss": 0.199,
      "step": 60650
    },
    {
      "epoch": 1.94112,
      "grad_norm": 0.03796857222914696,
      "learning_rate": 2.9456e-07,
      "loss": 0.2004,
      "step": 60660
    },
    {
      "epoch": 1.94144,
      "grad_norm": 0.03931787237524986,
      "learning_rate": 2.9296e-07,
      "loss": 0.1992,
      "step": 60670
    },
    {
      "epoch": 1.94176,
      "grad_norm": 0.01778034307062626,
      "learning_rate": 2.9136000000000004e-07,
      "loss": 0.1999,
      "step": 60680
    },
    {
      "epoch": 1.94208,
      "grad_norm": 0.013833585195243359,
      "learning_rate": 2.8976e-07,
      "loss": 0.2166,
      "step": 60690
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.014276583679020405,
      "learning_rate": 2.8816000000000006e-07,
      "loss": 0.2178,
      "step": 60700
    },
    {
      "epoch": 1.94272,
      "grad_norm": 0.020388850942254066,
      "learning_rate": 2.8656000000000005e-07,
      "loss": 0.2279,
      "step": 60710
    },
    {
      "epoch": 1.9430399999999999,
      "grad_norm": 0.026379764080047607,
      "learning_rate": 2.8496000000000003e-07,
      "loss": 0.1992,
      "step": 60720
    },
    {
      "epoch": 1.94336,
      "grad_norm": 0.016577724367380142,
      "learning_rate": 2.8336e-07,
      "loss": 0.1992,
      "step": 60730
    },
    {
      "epoch": 1.94368,
      "grad_norm": 0.02875739149749279,
      "learning_rate": 2.8176e-07,
      "loss": 0.2133,
      "step": 60740
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.024754563346505165,
      "learning_rate": 2.8016000000000004e-07,
      "loss": 0.211,
      "step": 60750
    },
    {
      "epoch": 1.94432,
      "grad_norm": 0.041058506816625595,
      "learning_rate": 2.7856000000000003e-07,
      "loss": 0.1992,
      "step": 60760
    },
    {
      "epoch": 1.9446400000000001,
      "grad_norm": 0.01999804563820362,
      "learning_rate": 2.7696e-07,
      "loss": 0.1993,
      "step": 60770
    },
    {
      "epoch": 1.94496,
      "grad_norm": 0.06197182089090347,
      "learning_rate": 2.7536000000000005e-07,
      "loss": 0.1997,
      "step": 60780
    },
    {
      "epoch": 1.94528,
      "grad_norm": 0.02586233802139759,
      "learning_rate": 2.7376000000000004e-07,
      "loss": 0.1991,
      "step": 60790
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.03158648684620857,
      "learning_rate": 2.7216e-07,
      "loss": 0.1992,
      "step": 60800
    },
    {
      "epoch": 1.94592,
      "grad_norm": 0.29914021492004395,
      "learning_rate": 2.7056e-07,
      "loss": 0.1996,
      "step": 60810
    },
    {
      "epoch": 1.94624,
      "grad_norm": 0.053513288497924805,
      "learning_rate": 2.6896e-07,
      "loss": 0.1994,
      "step": 60820
    },
    {
      "epoch": 1.9465599999999998,
      "grad_norm": 0.06047964468598366,
      "learning_rate": 2.6736000000000003e-07,
      "loss": 0.1992,
      "step": 60830
    },
    {
      "epoch": 1.9468800000000002,
      "grad_norm": 0.017321333289146423,
      "learning_rate": 2.6576e-07,
      "loss": 0.1992,
      "step": 60840
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.035585034638643265,
      "learning_rate": 2.6416e-07,
      "loss": 0.199,
      "step": 60850
    },
    {
      "epoch": 1.94752,
      "grad_norm": 0.023863371461629868,
      "learning_rate": 2.6256000000000004e-07,
      "loss": 0.1992,
      "step": 60860
    },
    {
      "epoch": 1.94784,
      "grad_norm": 0.03811072185635567,
      "learning_rate": 2.6096000000000003e-07,
      "loss": 0.1991,
      "step": 60870
    },
    {
      "epoch": 1.9481600000000001,
      "grad_norm": 1.8253843784332275,
      "learning_rate": 2.5936e-07,
      "loss": 0.2099,
      "step": 60880
    },
    {
      "epoch": 1.94848,
      "grad_norm": 0.07906780391931534,
      "learning_rate": 2.5776e-07,
      "loss": 0.199,
      "step": 60890
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.008722437545657158,
      "learning_rate": 2.5616000000000004e-07,
      "loss": 0.1992,
      "step": 60900
    },
    {
      "epoch": 1.94912,
      "grad_norm": 0.02119450457394123,
      "learning_rate": 2.5456e-07,
      "loss": 0.1991,
      "step": 60910
    },
    {
      "epoch": 1.94944,
      "grad_norm": 2.039478063583374,
      "learning_rate": 2.5296e-07,
      "loss": 0.203,
      "step": 60920
    },
    {
      "epoch": 1.94976,
      "grad_norm": 0.017718929797410965,
      "learning_rate": 2.5136e-07,
      "loss": 0.2039,
      "step": 60930
    },
    {
      "epoch": 1.95008,
      "grad_norm": 0.009215430356562138,
      "learning_rate": 2.4976000000000003e-07,
      "loss": 0.1991,
      "step": 60940
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.027730729430913925,
      "learning_rate": 2.4816e-07,
      "loss": 0.232,
      "step": 60950
    },
    {
      "epoch": 1.95072,
      "grad_norm": 0.011693545617163181,
      "learning_rate": 2.4656e-07,
      "loss": 0.1993,
      "step": 60960
    },
    {
      "epoch": 1.9510399999999999,
      "grad_norm": 0.031540971249341965,
      "learning_rate": 2.4496000000000004e-07,
      "loss": 0.1992,
      "step": 60970
    },
    {
      "epoch": 1.95136,
      "grad_norm": 0.03069397807121277,
      "learning_rate": 2.4336000000000003e-07,
      "loss": 0.1992,
      "step": 60980
    },
    {
      "epoch": 1.95168,
      "grad_norm": 0.6287543773651123,
      "learning_rate": 2.4176e-07,
      "loss": 0.2004,
      "step": 60990
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.05218954384326935,
      "learning_rate": 2.4016e-07,
      "loss": 0.1996,
      "step": 61000
    },
    {
      "epoch": 1.952,
      "eval_runtime": 77.0617,
      "eval_samples_per_second": 129.766,
      "eval_steps_per_second": 8.11,
      "step": 61000
    },
    {
      "epoch": 1.95232,
      "grad_norm": 0.03776964172720909,
      "learning_rate": 2.3856e-07,
      "loss": 0.1997,
      "step": 61010
    },
    {
      "epoch": 1.9526400000000002,
      "grad_norm": 0.01185154914855957,
      "learning_rate": 2.3696e-07,
      "loss": 0.1997,
      "step": 61020
    },
    {
      "epoch": 1.95296,
      "grad_norm": 0.027288280427455902,
      "learning_rate": 2.3536000000000004e-07,
      "loss": 0.2044,
      "step": 61030
    },
    {
      "epoch": 1.95328,
      "grad_norm": 0.028706736862659454,
      "learning_rate": 2.3376000000000002e-07,
      "loss": 0.2169,
      "step": 61040
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.05230302736163139,
      "learning_rate": 2.3216000000000004e-07,
      "loss": 0.2108,
      "step": 61050
    },
    {
      "epoch": 1.95392,
      "grad_norm": 0.014542591758072376,
      "learning_rate": 2.3056000000000002e-07,
      "loss": 0.2334,
      "step": 61060
    },
    {
      "epoch": 1.95424,
      "grad_norm": 0.06629697233438492,
      "learning_rate": 2.2896e-07,
      "loss": 0.1992,
      "step": 61070
    },
    {
      "epoch": 1.9545599999999999,
      "grad_norm": 0.020298145711421967,
      "learning_rate": 2.2736000000000002e-07,
      "loss": 0.1995,
      "step": 61080
    },
    {
      "epoch": 1.95488,
      "grad_norm": 0.010454883798956871,
      "learning_rate": 2.2576e-07,
      "loss": 0.1991,
      "step": 61090
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.07520700991153717,
      "learning_rate": 2.2416e-07,
      "loss": 0.2014,
      "step": 61100
    },
    {
      "epoch": 1.95552,
      "grad_norm": 0.08528315275907516,
      "learning_rate": 2.2256000000000003e-07,
      "loss": 0.1994,
      "step": 61110
    },
    {
      "epoch": 1.95584,
      "grad_norm": 0.026588642969727516,
      "learning_rate": 2.2096000000000004e-07,
      "loss": 0.1993,
      "step": 61120
    },
    {
      "epoch": 1.9561600000000001,
      "grad_norm": 0.025188496336340904,
      "learning_rate": 2.1936000000000003e-07,
      "loss": 0.1996,
      "step": 61130
    },
    {
      "epoch": 1.95648,
      "grad_norm": 1.4626259803771973,
      "learning_rate": 2.1776e-07,
      "loss": 0.2021,
      "step": 61140
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.03146911412477493,
      "learning_rate": 2.1616000000000002e-07,
      "loss": 0.1991,
      "step": 61150
    },
    {
      "epoch": 1.95712,
      "grad_norm": 0.024701111018657684,
      "learning_rate": 2.1456e-07,
      "loss": 0.1991,
      "step": 61160
    },
    {
      "epoch": 1.95744,
      "grad_norm": 0.021536851301789284,
      "learning_rate": 2.1296e-07,
      "loss": 0.1991,
      "step": 61170
    },
    {
      "epoch": 1.95776,
      "grad_norm": 0.02139931358397007,
      "learning_rate": 2.1136e-07,
      "loss": 0.1992,
      "step": 61180
    },
    {
      "epoch": 1.95808,
      "grad_norm": 0.026897715404629707,
      "learning_rate": 2.0976000000000002e-07,
      "loss": 0.2085,
      "step": 61190
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.02159023843705654,
      "learning_rate": 2.0816000000000003e-07,
      "loss": 0.1991,
      "step": 61200
    },
    {
      "epoch": 1.95872,
      "grad_norm": 1.089363932609558,
      "learning_rate": 2.0656000000000002e-07,
      "loss": 0.2081,
      "step": 61210
    },
    {
      "epoch": 1.95904,
      "grad_norm": 0.026397958397865295,
      "learning_rate": 2.0496000000000003e-07,
      "loss": 0.1991,
      "step": 61220
    },
    {
      "epoch": 1.95936,
      "grad_norm": 0.009994679130613804,
      "learning_rate": 2.0336000000000002e-07,
      "loss": 0.211,
      "step": 61230
    },
    {
      "epoch": 1.95968,
      "grad_norm": 0.14784035086631775,
      "learning_rate": 2.0176e-07,
      "loss": 0.1996,
      "step": 61240
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.013539808802306652,
      "learning_rate": 2.0016000000000001e-07,
      "loss": 0.199,
      "step": 61250
    },
    {
      "epoch": 1.9603199999999998,
      "grad_norm": 0.032783858478069305,
      "learning_rate": 1.9856e-07,
      "loss": 0.2036,
      "step": 61260
    },
    {
      "epoch": 1.9606400000000002,
      "grad_norm": 0.027075044810771942,
      "learning_rate": 1.9696000000000004e-07,
      "loss": 0.1997,
      "step": 61270
    },
    {
      "epoch": 1.96096,
      "grad_norm": 0.9800527691841125,
      "learning_rate": 1.9536000000000002e-07,
      "loss": 0.2002,
      "step": 61280
    },
    {
      "epoch": 1.96128,
      "grad_norm": 0.20067183673381805,
      "learning_rate": 1.9376000000000004e-07,
      "loss": 0.1997,
      "step": 61290
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.016712039709091187,
      "learning_rate": 1.9216000000000002e-07,
      "loss": 0.1991,
      "step": 61300
    },
    {
      "epoch": 1.96192,
      "grad_norm": 0.02047116868197918,
      "learning_rate": 1.9056e-07,
      "loss": 0.1991,
      "step": 61310
    },
    {
      "epoch": 1.96224,
      "grad_norm": 0.09994881600141525,
      "learning_rate": 1.8896000000000002e-07,
      "loss": 0.1996,
      "step": 61320
    },
    {
      "epoch": 1.9625599999999999,
      "grad_norm": 0.02699926495552063,
      "learning_rate": 1.8736e-07,
      "loss": 0.2015,
      "step": 61330
    },
    {
      "epoch": 1.96288,
      "grad_norm": 0.054196204990148544,
      "learning_rate": 1.8576e-07,
      "loss": 0.1991,
      "step": 61340
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.024561027064919472,
      "learning_rate": 1.8416e-07,
      "loss": 0.1995,
      "step": 61350
    },
    {
      "epoch": 1.96352,
      "grad_norm": 0.0434415377676487,
      "learning_rate": 1.8256000000000004e-07,
      "loss": 0.1991,
      "step": 61360
    },
    {
      "epoch": 1.96384,
      "grad_norm": 0.045220114290714264,
      "learning_rate": 1.8096000000000003e-07,
      "loss": 0.2159,
      "step": 61370
    },
    {
      "epoch": 1.9641600000000001,
      "grad_norm": 0.012525583617389202,
      "learning_rate": 1.7936e-07,
      "loss": 0.1993,
      "step": 61380
    },
    {
      "epoch": 1.96448,
      "grad_norm": 0.01716330088675022,
      "learning_rate": 1.7776000000000002e-07,
      "loss": 0.2018,
      "step": 61390
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.014569242484867573,
      "learning_rate": 1.7616e-07,
      "loss": 0.1992,
      "step": 61400
    },
    {
      "epoch": 1.96512,
      "grad_norm": 0.021332645788788795,
      "learning_rate": 1.7456e-07,
      "loss": 0.2282,
      "step": 61410
    },
    {
      "epoch": 1.96544,
      "grad_norm": 0.010402020066976547,
      "learning_rate": 1.7296e-07,
      "loss": 0.1992,
      "step": 61420
    },
    {
      "epoch": 1.96576,
      "grad_norm": 0.0516967847943306,
      "learning_rate": 1.7136e-07,
      "loss": 0.1998,
      "step": 61430
    },
    {
      "epoch": 1.96608,
      "grad_norm": 0.03412879258394241,
      "learning_rate": 1.6976000000000003e-07,
      "loss": 0.1992,
      "step": 61440
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.032165054231882095,
      "learning_rate": 1.6816000000000002e-07,
      "loss": 0.2142,
      "step": 61450
    },
    {
      "epoch": 1.96672,
      "grad_norm": 0.026341313496232033,
      "learning_rate": 1.6656000000000003e-07,
      "loss": 0.1991,
      "step": 61460
    },
    {
      "epoch": 1.96704,
      "grad_norm": 0.15570834279060364,
      "learning_rate": 1.6496000000000002e-07,
      "loss": 0.1994,
      "step": 61470
    },
    {
      "epoch": 1.96736,
      "grad_norm": 0.018016044050455093,
      "learning_rate": 1.6336e-07,
      "loss": 0.2008,
      "step": 61480
    },
    {
      "epoch": 1.96768,
      "grad_norm": 0.07553699612617493,
      "learning_rate": 1.6176000000000001e-07,
      "loss": 0.1992,
      "step": 61490
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.02924729324877262,
      "learning_rate": 1.6016e-07,
      "loss": 0.1994,
      "step": 61500
    },
    {
      "epoch": 1.9683199999999998,
      "grad_norm": 0.0407274104654789,
      "learning_rate": 1.5856e-07,
      "loss": 0.2088,
      "step": 61510
    },
    {
      "epoch": 1.96864,
      "grad_norm": 0.0438602976500988,
      "learning_rate": 1.5696000000000002e-07,
      "loss": 0.2077,
      "step": 61520
    },
    {
      "epoch": 1.96896,
      "grad_norm": 0.016226140782237053,
      "learning_rate": 1.5536e-07,
      "loss": 0.199,
      "step": 61530
    },
    {
      "epoch": 1.96928,
      "grad_norm": 0.06025372073054314,
      "learning_rate": 1.5376000000000002e-07,
      "loss": 0.1994,
      "step": 61540
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.012588883750140667,
      "learning_rate": 1.5216e-07,
      "loss": 0.1993,
      "step": 61550
    },
    {
      "epoch": 1.9699200000000001,
      "grad_norm": 0.025757886469364166,
      "learning_rate": 1.5056000000000002e-07,
      "loss": 0.1992,
      "step": 61560
    },
    {
      "epoch": 1.97024,
      "grad_norm": 0.042462583631277084,
      "learning_rate": 1.4896e-07,
      "loss": 0.2163,
      "step": 61570
    },
    {
      "epoch": 1.9705599999999999,
      "grad_norm": 0.026722002774477005,
      "learning_rate": 1.4736000000000002e-07,
      "loss": 0.1992,
      "step": 61580
    },
    {
      "epoch": 1.97088,
      "grad_norm": 0.05467865988612175,
      "learning_rate": 1.4576e-07,
      "loss": 0.1992,
      "step": 61590
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.022790908813476562,
      "learning_rate": 1.4416000000000002e-07,
      "loss": 0.2048,
      "step": 61600
    },
    {
      "epoch": 1.97152,
      "grad_norm": 0.04174298793077469,
      "learning_rate": 1.4256e-07,
      "loss": 0.1991,
      "step": 61610
    },
    {
      "epoch": 1.97184,
      "grad_norm": 0.019649861380457878,
      "learning_rate": 1.4096e-07,
      "loss": 0.2003,
      "step": 61620
    },
    {
      "epoch": 1.9721600000000001,
      "grad_norm": 0.01820174604654312,
      "learning_rate": 1.3936000000000002e-07,
      "loss": 0.1996,
      "step": 61630
    },
    {
      "epoch": 1.97248,
      "grad_norm": 0.02900637313723564,
      "learning_rate": 1.3776e-07,
      "loss": 0.2042,
      "step": 61640
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.010813530534505844,
      "learning_rate": 1.3616e-07,
      "loss": 0.2019,
      "step": 61650
    },
    {
      "epoch": 1.97312,
      "grad_norm": 0.07775343209505081,
      "learning_rate": 1.3456e-07,
      "loss": 0.2031,
      "step": 61660
    },
    {
      "epoch": 1.97344,
      "grad_norm": 0.037681449204683304,
      "learning_rate": 1.3296000000000002e-07,
      "loss": 0.1993,
      "step": 61670
    },
    {
      "epoch": 1.97376,
      "grad_norm": 0.02911512926220894,
      "learning_rate": 1.3136e-07,
      "loss": 0.1992,
      "step": 61680
    },
    {
      "epoch": 1.9740799999999998,
      "grad_norm": 0.02270141802728176,
      "learning_rate": 1.2976000000000002e-07,
      "loss": 0.2068,
      "step": 61690
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 0.012808866798877716,
      "learning_rate": 1.2816e-07,
      "loss": 0.1992,
      "step": 61700
    },
    {
      "epoch": 1.97472,
      "grad_norm": 0.02253318764269352,
      "learning_rate": 1.2656000000000002e-07,
      "loss": 0.2163,
      "step": 61710
    },
    {
      "epoch": 1.97504,
      "grad_norm": 0.7372953295707703,
      "learning_rate": 1.2496e-07,
      "loss": 0.2173,
      "step": 61720
    },
    {
      "epoch": 1.97536,
      "grad_norm": 0.013694166205823421,
      "learning_rate": 1.2336000000000001e-07,
      "loss": 0.1995,
      "step": 61730
    },
    {
      "epoch": 1.97568,
      "grad_norm": 0.06872967630624771,
      "learning_rate": 1.2176e-07,
      "loss": 0.1992,
      "step": 61740
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.03308652713894844,
      "learning_rate": 1.2016e-07,
      "loss": 0.2014,
      "step": 61750
    },
    {
      "epoch": 1.9763199999999999,
      "grad_norm": 0.014018632471561432,
      "learning_rate": 1.1856000000000001e-07,
      "loss": 0.2002,
      "step": 61760
    },
    {
      "epoch": 1.97664,
      "grad_norm": 0.027278684079647064,
      "learning_rate": 1.1696000000000001e-07,
      "loss": 0.1991,
      "step": 61770
    },
    {
      "epoch": 1.97696,
      "grad_norm": 0.014906037598848343,
      "learning_rate": 1.1536000000000001e-07,
      "loss": 0.1991,
      "step": 61780
    },
    {
      "epoch": 1.97728,
      "grad_norm": 0.013743932358920574,
      "learning_rate": 1.1376000000000002e-07,
      "loss": 0.1993,
      "step": 61790
    },
    {
      "epoch": 1.9776,
      "grad_norm": 0.01998095028102398,
      "learning_rate": 1.1216e-07,
      "loss": 0.1994,
      "step": 61800
    },
    {
      "epoch": 1.9779200000000001,
      "grad_norm": 0.011520828120410442,
      "learning_rate": 1.1056e-07,
      "loss": 0.1992,
      "step": 61810
    },
    {
      "epoch": 1.97824,
      "grad_norm": 0.026753367856144905,
      "learning_rate": 1.0896e-07,
      "loss": 0.2001,
      "step": 61820
    },
    {
      "epoch": 1.9785599999999999,
      "grad_norm": 0.03351591154932976,
      "learning_rate": 1.0736000000000002e-07,
      "loss": 0.1992,
      "step": 61830
    },
    {
      "epoch": 1.97888,
      "grad_norm": 2.451429605484009,
      "learning_rate": 1.0576000000000002e-07,
      "loss": 0.2058,
      "step": 61840
    },
    {
      "epoch": 1.9792,
      "grad_norm": 0.021085331216454506,
      "learning_rate": 1.0416000000000001e-07,
      "loss": 0.2023,
      "step": 61850
    },
    {
      "epoch": 1.97952,
      "grad_norm": 0.03715444728732109,
      "learning_rate": 1.0256e-07,
      "loss": 0.2167,
      "step": 61860
    },
    {
      "epoch": 1.97984,
      "grad_norm": 0.013235588558018208,
      "learning_rate": 1.0096000000000001e-07,
      "loss": 0.1994,
      "step": 61870
    },
    {
      "epoch": 1.9801600000000001,
      "grad_norm": 0.04071826860308647,
      "learning_rate": 9.936000000000001e-08,
      "loss": 0.2021,
      "step": 61880
    },
    {
      "epoch": 1.98048,
      "grad_norm": 0.5703039169311523,
      "learning_rate": 9.776000000000001e-08,
      "loss": 0.2014,
      "step": 61890
    },
    {
      "epoch": 1.9808,
      "grad_norm": 0.041436560451984406,
      "learning_rate": 9.616000000000001e-08,
      "loss": 0.2161,
      "step": 61900
    },
    {
      "epoch": 1.98112,
      "grad_norm": 0.015127970837056637,
      "learning_rate": 9.456000000000002e-08,
      "loss": 0.1992,
      "step": 61910
    },
    {
      "epoch": 1.98144,
      "grad_norm": 0.040487706661224365,
      "learning_rate": 9.296e-08,
      "loss": 0.2004,
      "step": 61920
    },
    {
      "epoch": 1.98176,
      "grad_norm": 0.034010615199804306,
      "learning_rate": 9.136e-08,
      "loss": 0.1991,
      "step": 61930
    },
    {
      "epoch": 1.9820799999999998,
      "grad_norm": 0.014376416802406311,
      "learning_rate": 8.976e-08,
      "loss": 0.1992,
      "step": 61940
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 0.032276563346385956,
      "learning_rate": 8.816e-08,
      "loss": 0.1991,
      "step": 61950
    },
    {
      "epoch": 1.98272,
      "grad_norm": 0.01577107235789299,
      "learning_rate": 8.656000000000002e-08,
      "loss": 0.1993,
      "step": 61960
    },
    {
      "epoch": 1.98304,
      "grad_norm": 0.01727660372853279,
      "learning_rate": 8.496000000000001e-08,
      "loss": 0.1991,
      "step": 61970
    },
    {
      "epoch": 1.98336,
      "grad_norm": 0.10284843295812607,
      "learning_rate": 8.336e-08,
      "loss": 0.2009,
      "step": 61980
    },
    {
      "epoch": 1.98368,
      "grad_norm": 0.03651108592748642,
      "learning_rate": 8.176e-08,
      "loss": 0.1992,
      "step": 61990
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.04517965763807297,
      "learning_rate": 8.016000000000001e-08,
      "loss": 0.1991,
      "step": 62000
    },
    {
      "epoch": 1.984,
      "eval_runtime": 77.4837,
      "eval_samples_per_second": 129.059,
      "eval_steps_per_second": 8.066,
      "step": 62000
    },
    {
      "epoch": 1.9843199999999999,
      "grad_norm": 0.04185401648283005,
      "learning_rate": 7.856000000000001e-08,
      "loss": 0.1993,
      "step": 62010
    },
    {
      "epoch": 1.98464,
      "grad_norm": 0.01151080522686243,
      "learning_rate": 7.696000000000001e-08,
      "loss": 0.2007,
      "step": 62020
    },
    {
      "epoch": 1.98496,
      "grad_norm": 0.02268451265990734,
      "learning_rate": 7.536000000000001e-08,
      "loss": 0.1994,
      "step": 62030
    },
    {
      "epoch": 1.98528,
      "grad_norm": 0.00899922288954258,
      "learning_rate": 7.376000000000001e-08,
      "loss": 0.1991,
      "step": 62040
    },
    {
      "epoch": 1.9856,
      "grad_norm": 0.017268376424908638,
      "learning_rate": 7.216e-08,
      "loss": 0.1992,
      "step": 62050
    },
    {
      "epoch": 1.9859200000000001,
      "grad_norm": 0.03282767906785011,
      "learning_rate": 7.056e-08,
      "loss": 0.2165,
      "step": 62060
    },
    {
      "epoch": 1.98624,
      "grad_norm": 0.0257318876683712,
      "learning_rate": 6.896e-08,
      "loss": 0.1991,
      "step": 62070
    },
    {
      "epoch": 1.9865599999999999,
      "grad_norm": 0.013914305716753006,
      "learning_rate": 6.736e-08,
      "loss": 0.1993,
      "step": 62080
    },
    {
      "epoch": 1.98688,
      "grad_norm": 0.023528756573796272,
      "learning_rate": 6.576000000000001e-08,
      "loss": 0.2057,
      "step": 62090
    },
    {
      "epoch": 1.9872,
      "grad_norm": 0.02080390974879265,
      "learning_rate": 6.416e-08,
      "loss": 0.1999,
      "step": 62100
    },
    {
      "epoch": 1.98752,
      "grad_norm": 0.026307329535484314,
      "learning_rate": 6.256000000000001e-08,
      "loss": 0.2235,
      "step": 62110
    },
    {
      "epoch": 1.98784,
      "grad_norm": 0.022144760936498642,
      "learning_rate": 6.096e-08,
      "loss": 0.1992,
      "step": 62120
    },
    {
      "epoch": 1.9881600000000001,
      "grad_norm": 0.04056591913104057,
      "learning_rate": 5.936000000000001e-08,
      "loss": 0.2138,
      "step": 62130
    },
    {
      "epoch": 1.98848,
      "grad_norm": 0.029083626344799995,
      "learning_rate": 5.776e-08,
      "loss": 0.1991,
      "step": 62140
    },
    {
      "epoch": 1.9888,
      "grad_norm": 0.04794279858469963,
      "learning_rate": 5.616000000000001e-08,
      "loss": 0.2094,
      "step": 62150
    },
    {
      "epoch": 1.98912,
      "grad_norm": 0.02565579302608967,
      "learning_rate": 5.456000000000001e-08,
      "loss": 0.2005,
      "step": 62160
    },
    {
      "epoch": 1.98944,
      "grad_norm": 0.08869785070419312,
      "learning_rate": 5.2960000000000006e-08,
      "loss": 0.2121,
      "step": 62170
    },
    {
      "epoch": 1.98976,
      "grad_norm": 0.015109138563275337,
      "learning_rate": 5.1360000000000005e-08,
      "loss": 0.1997,
      "step": 62180
    },
    {
      "epoch": 1.9900799999999998,
      "grad_norm": 0.033032167702913284,
      "learning_rate": 4.976000000000001e-08,
      "loss": 0.1992,
      "step": 62190
    },
    {
      "epoch": 1.9904,
      "grad_norm": 0.06968029588460922,
      "learning_rate": 4.816e-08,
      "loss": 0.1992,
      "step": 62200
    },
    {
      "epoch": 1.99072,
      "grad_norm": 0.013556323945522308,
      "learning_rate": 4.656000000000001e-08,
      "loss": 0.1994,
      "step": 62210
    },
    {
      "epoch": 1.99104,
      "grad_norm": 0.03852119296789169,
      "learning_rate": 4.496000000000001e-08,
      "loss": 0.1992,
      "step": 62220
    },
    {
      "epoch": 1.99136,
      "grad_norm": 0.03501082956790924,
      "learning_rate": 4.336e-08,
      "loss": 0.1993,
      "step": 62230
    },
    {
      "epoch": 1.9916800000000001,
      "grad_norm": 0.01265884842723608,
      "learning_rate": 4.1760000000000005e-08,
      "loss": 0.1996,
      "step": 62240
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.03756728395819664,
      "learning_rate": 4.016e-08,
      "loss": 0.203,
      "step": 62250
    },
    {
      "epoch": 1.9923199999999999,
      "grad_norm": 0.03940746933221817,
      "learning_rate": 3.856e-08,
      "loss": 0.2079,
      "step": 62260
    },
    {
      "epoch": 1.99264,
      "grad_norm": 0.03767385706305504,
      "learning_rate": 3.696e-08,
      "loss": 0.1992,
      "step": 62270
    },
    {
      "epoch": 1.99296,
      "grad_norm": 0.037971414625644684,
      "learning_rate": 3.536000000000001e-08,
      "loss": 0.2154,
      "step": 62280
    },
    {
      "epoch": 1.99328,
      "grad_norm": 0.01553971879184246,
      "learning_rate": 3.3760000000000006e-08,
      "loss": 0.1993,
      "step": 62290
    },
    {
      "epoch": 1.9936,
      "grad_norm": 1.1090710163116455,
      "learning_rate": 3.2160000000000005e-08,
      "loss": 0.2044,
      "step": 62300
    },
    {
      "epoch": 1.9939200000000001,
      "grad_norm": 0.019325995817780495,
      "learning_rate": 3.0560000000000004e-08,
      "loss": 0.1992,
      "step": 62310
    },
    {
      "epoch": 1.99424,
      "grad_norm": 0.045707229524850845,
      "learning_rate": 2.8960000000000003e-08,
      "loss": 0.1991,
      "step": 62320
    },
    {
      "epoch": 1.9945599999999999,
      "grad_norm": 0.04773611202836037,
      "learning_rate": 2.7360000000000005e-08,
      "loss": 0.1992,
      "step": 62330
    },
    {
      "epoch": 1.99488,
      "grad_norm": 0.02711646445095539,
      "learning_rate": 2.5760000000000004e-08,
      "loss": 0.1991,
      "step": 62340
    },
    {
      "epoch": 1.9952,
      "grad_norm": 0.05254089832305908,
      "learning_rate": 2.4160000000000003e-08,
      "loss": 0.1991,
      "step": 62350
    },
    {
      "epoch": 1.99552,
      "grad_norm": 0.01835743710398674,
      "learning_rate": 2.2560000000000005e-08,
      "loss": 0.1994,
      "step": 62360
    },
    {
      "epoch": 1.9958399999999998,
      "grad_norm": 0.0216351505368948,
      "learning_rate": 2.0960000000000004e-08,
      "loss": 0.1994,
      "step": 62370
    },
    {
      "epoch": 1.9961600000000002,
      "grad_norm": 0.026477426290512085,
      "learning_rate": 1.9360000000000003e-08,
      "loss": 0.1992,
      "step": 62380
    },
    {
      "epoch": 1.99648,
      "grad_norm": 0.02605520188808441,
      "learning_rate": 1.776e-08,
      "loss": 0.2046,
      "step": 62390
    },
    {
      "epoch": 1.9968,
      "grad_norm": 0.01019720733165741,
      "learning_rate": 1.616e-08,
      "loss": 0.1991,
      "step": 62400
    },
    {
      "epoch": 1.99712,
      "grad_norm": 0.026330405846238136,
      "learning_rate": 1.4560000000000001e-08,
      "loss": 0.2121,
      "step": 62410
    },
    {
      "epoch": 1.99744,
      "grad_norm": 0.024441413581371307,
      "learning_rate": 1.2960000000000002e-08,
      "loss": 0.1997,
      "step": 62420
    },
    {
      "epoch": 1.99776,
      "grad_norm": 0.01332067046314478,
      "learning_rate": 1.1360000000000002e-08,
      "loss": 0.1993,
      "step": 62430
    },
    {
      "epoch": 1.9980799999999999,
      "grad_norm": 0.03429991006851196,
      "learning_rate": 9.760000000000001e-09,
      "loss": 0.2016,
      "step": 62440
    },
    {
      "epoch": 1.9984,
      "grad_norm": 0.048845455050468445,
      "learning_rate": 8.16e-09,
      "loss": 0.1991,
      "step": 62450
    },
    {
      "epoch": 1.99872,
      "grad_norm": 0.031791802495718,
      "learning_rate": 6.5600000000000005e-09,
      "loss": 0.1997,
      "step": 62460
    },
    {
      "epoch": 1.99904,
      "grad_norm": 0.028752589598298073,
      "learning_rate": 4.96e-09,
      "loss": 0.2109,
      "step": 62470
    },
    {
      "epoch": 1.99936,
      "grad_norm": 0.024537477642297745,
      "learning_rate": 3.36e-09,
      "loss": 0.1992,
      "step": 62480
    },
    {
      "epoch": 1.9996800000000001,
      "grad_norm": 0.013387402519583702,
      "learning_rate": 1.7600000000000001e-09,
      "loss": 0.2159,
      "step": 62490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.047274574637413025,
      "learning_rate": 1.6000000000000002e-10,
      "loss": 0.1993,
      "step": 62500
    }
  ],
  "logging_steps": 10,
  "max_steps": 62500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.9925322752e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
